<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: http://leoeditor.com/leo_toc.html -->
<leo_file xmlns:leo="http://leoeditor.com/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2" tnodes="0" max_tnode_index="0" clone_windows="0"/>
<globals body_outline_ratio="0.5" body_secondary_ratio="0.5">
	<global_window_position top="50" left="50" height="500" width="700"/>
	<global_log_window_position top="0" left="0" height="0" width="0"/>
</globals>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="karstenw.20230225123024.1" a="E"><vh>WN-Library</vh>
<v t="karstenw.20230225125511.1"><vh>wn</vh>
<v t="karstenw.20230225123039.1" a="E"><vh>@clean wn/__init__.py</vh>
<v t="karstenw.20230225123108.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230225123120.1"><vh>@clean wn/__main__.py</vh>
<v t="karstenw.20230225123203.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225123203.2"><vh>_download</vh></v>
<v t="karstenw.20230225123203.3"><vh>_lexicons</vh></v>
<v t="karstenw.20230225123203.4"><vh>_projects</vh></v>
<v t="karstenw.20230225123203.5"><vh>_validate</vh></v>
<v t="karstenw.20230225123203.6"><vh>_path_type</vh></v>
<v t="karstenw.20230225123203.7"><vh>_file_path_type</vh></v>
</v>
<v t="karstenw.20230225123122.1"><vh>@clean wn/_add.py</vh>
<v t="karstenw.20230225123218.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225123218.2"><vh>add</vh></v>
<v t="karstenw.20230225123218.3"><vh>_add_lmf</vh></v>
<v t="karstenw.20230225123218.4"><vh>_precheck</vh></v>
<v t="karstenw.20230225123218.5"><vh>_sum_counts</vh></v>
<v t="karstenw.20230225123218.6"><vh>_update_lookup_tables</vh></v>
<v t="karstenw.20230225123218.7"><vh>_insert_lexicon</vh></v>
<v t="karstenw.20230225123218.8"><vh>_build_lexid_map</vh></v>
<v t="karstenw.20230225123218.9"><vh>_batch</vh></v>
<v t="karstenw.20230225123218.10"><vh>_insert_synsets</vh></v>
<v t="karstenw.20230225123218.11"><vh>_insert_synset_definitions</vh></v>
<v t="karstenw.20230225123218.12"><vh>_insert_synset_relations</vh></v>
<v t="karstenw.20230225123218.13"><vh>_insert_entries</vh></v>
<v t="karstenw.20230225123218.14"><vh>_insert_forms</vh></v>
<v t="karstenw.20230225123218.15"><vh>_insert_pronunciations</vh></v>
<v t="karstenw.20230225123218.16"><vh>_insert_tags</vh></v>
<v t="karstenw.20230225123218.17"><vh>_insert_senses</vh></v>
<v t="karstenw.20230225123218.18"><vh>_insert_adjpositions</vh></v>
<v t="karstenw.20230225123218.19"><vh>_insert_counts</vh></v>
<v t="karstenw.20230225123218.20"><vh>_collect_frames</vh></v>
<v t="karstenw.20230225123218.21"><vh>_insert_syntactic_behaviours</vh></v>
<v t="karstenw.20230225123218.22"><vh>_insert_sense_relations</vh></v>
<v t="karstenw.20230225123218.23"><vh>_insert_examples</vh></v>
<v t="karstenw.20230225123218.24"><vh>_add_ili</vh></v>
<v t="karstenw.20230225123218.25"><vh>remove</vh></v>
<v t="karstenw.20230225123218.26"><vh>_find_all_extensions</vh></v>
<v t="karstenw.20230225123218.27"><vh>_entries</vh></v>
<v t="karstenw.20230225123218.28"><vh>_forms</vh></v>
<v t="karstenw.20230225123218.29"><vh>_senses</vh></v>
<v t="karstenw.20230225123218.30"><vh>_synsets</vh></v>
<v t="karstenw.20230225123218.31"><vh>_is_external</vh></v>
<v t="karstenw.20230225123218.32"><vh>_local_synsets</vh></v>
<v t="karstenw.20230225123218.33"><vh>_local_entries</vh></v>
<v t="karstenw.20230225123218.34"><vh>_local_senses</vh></v>
</v>
<v t="karstenw.20230225123123.1"><vh>@clean wn/_config.py</vh>
<v t="karstenw.20230225123241.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225123241.2"><vh>class WNConfig</vh>
<v t="karstenw.20230225123241.3"><vh>__init__</vh></v>
<v t="karstenw.20230225123241.4"><vh>data_directory</vh></v>
<v t="karstenw.20230225123241.5"><vh>data_directory</vh></v>
<v t="karstenw.20230225123241.6"><vh>database_path</vh></v>
<v t="karstenw.20230225123241.7"><vh>downloads_directory</vh></v>
<v t="karstenw.20230225123241.8"><vh>index</vh></v>
<v t="karstenw.20230225123241.9"><vh>add_project</vh></v>
<v t="karstenw.20230225123241.10"><vh>add_project_version</vh></v>
<v t="karstenw.20230225123241.11"><vh>get_project_info</vh></v>
<v t="karstenw.20230225123242.1"><vh>get_cache_path</vh></v>
<v t="karstenw.20230225123242.2"><vh>update</vh></v>
<v t="karstenw.20230225123242.3"><vh>load_index</vh></v>
</v>
<v t="karstenw.20230225123242.4"><vh>_get_cache_path_for_urls</vh></v>
</v>
<v t="karstenw.20230225123225.1"><vh>@clean wn/_core.py</vh>
<v t="karstenw.20230225123250.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225123250.2"><vh>class _DatabaseEntity</vh>
<v t="karstenw.20230225123250.3"><vh>__init__</vh></v>
<v t="karstenw.20230225123250.4"><vh>__eq__</vh></v>
<v t="karstenw.20230225123250.5"><vh>__lt__</vh></v>
<v t="karstenw.20230225123250.6"><vh>__hash__</vh></v>
</v>
<v t="karstenw.20230225123250.7"><vh>class ILI</vh>
<v t="karstenw.20230225123250.8"><vh>__init__</vh></v>
<v t="karstenw.20230225123250.9"><vh>__repr__</vh></v>
<v t="karstenw.20230225123250.10"><vh>definition</vh></v>
<v t="karstenw.20230225123250.11"><vh>metadata</vh></v>
</v>
<v t="karstenw.20230225123250.12"><vh>class Lexicon</vh>
<v t="karstenw.20230225123250.13"><vh>__init__</vh></v>
<v t="karstenw.20230225123250.14"><vh>__repr__</vh></v>
<v t="karstenw.20230225123250.15"><vh>metadata</vh></v>
<v t="karstenw.20230225123250.16"><vh>specifier</vh></v>
<v t="karstenw.20230225123250.17"><vh>modified</vh></v>
<v t="karstenw.20230225123250.18"><vh>requires</vh></v>
<v t="karstenw.20230225123250.19"><vh>extends</vh></v>
<v t="karstenw.20230225123250.20"><vh>extensions</vh></v>
<v t="karstenw.20230225123250.21"><vh>describe</vh></v>
</v>
<v t="karstenw.20230225123250.22"><vh>_desc_counts</vh></v>
<v t="karstenw.20230225123250.23"><vh>class _LexiconElement</vh>
<v t="karstenw.20230225123250.24"><vh>__init__</vh></v>
<v t="karstenw.20230225123250.25"><vh>lexicon</vh></v>
<v t="karstenw.20230225123250.26"><vh>_get_lexicon_ids</vh></v>
</v>
<v t="karstenw.20230225123250.27"><vh>class Pronunciation</vh>
<v t="karstenw.20230225123250.28"><vh>__init__</vh></v>
</v>
<v t="karstenw.20230225123250.29"><vh>class Tag</vh>
<v t="karstenw.20230225123250.30"><vh>__init__</vh></v>
<v t="karstenw.20230225123250.31"><vh>__eq__</vh></v>
</v>
<v t="karstenw.20230225123250.32"><vh>class Form</vh>
<v t="karstenw.20230225123250.33"><vh>__new__</vh></v>
<v t="karstenw.20230225123250.34"><vh>__eq__</vh></v>
<v t="karstenw.20230225123250.35"><vh>__hash__</vh></v>
<v t="karstenw.20230225123250.36"><vh>pronunciations</vh></v>
<v t="karstenw.20230225123250.37"><vh>tags</vh></v>
</v>
<v t="karstenw.20230225123250.38"><vh>class Word</vh>
<v t="karstenw.20230225123250.39"><vh>__init__</vh></v>
<v t="karstenw.20230225123250.40"><vh>__repr__</vh></v>
<v t="karstenw.20230225123250.41"><vh>lemma</vh></v>
<v t="karstenw.20230225123250.42"><vh>forms</vh></v>
<v t="karstenw.20230225123250.43"><vh>senses</vh></v>
<v t="karstenw.20230225123250.44"><vh>metadata</vh></v>
<v t="karstenw.20230225123250.45"><vh>synsets</vh></v>
<v t="karstenw.20230225123250.46"><vh>derived_words</vh></v>
<v t="karstenw.20230225123250.47"><vh>translate</vh></v>
</v>
<v t="karstenw.20230225123250.48"><vh>class _Relatable</vh>
<v t="karstenw.20230225123250.49"><vh>__init__</vh></v>
<v t="karstenw.20230225123250.50"><vh>relations</vh></v>
<v t="karstenw.20230225123250.51"><vh>get_related</vh></v>
<v t="karstenw.20230225123250.52"><vh>closure</vh></v>
<v t="karstenw.20230225123250.53"><vh>relation_paths</vh></v>
</v>
<v t="karstenw.20230225123250.54"><vh>class Synset</vh>
<v t="karstenw.20230225123250.55"><vh>__init__</vh></v>
<v t="karstenw.20230225123250.56"><vh>ili</vh></v>
<v t="karstenw.20230225123250.57"><vh>__hash__</vh></v>
<v t="karstenw.20230225123250.58"><vh>__repr__</vh></v>
<v t="karstenw.20230225123250.59"><vh>definition</vh></v>
<v t="karstenw.20230225123250.60"><vh>examples</vh></v>
<v t="karstenw.20230225123250.61"><vh>senses</vh></v>
<v t="karstenw.20230225123250.62"><vh>lexicalized</vh></v>
<v t="karstenw.20230225123250.63"><vh>lexfile</vh></v>
<v t="karstenw.20230225123250.64"><vh>metadata</vh></v>
<v t="karstenw.20230225123250.65"><vh>words</vh></v>
<v t="karstenw.20230225123250.66"><vh>lemmas</vh></v>
<v t="karstenw.20230225123250.67"><vh>relations</vh></v>
<v t="karstenw.20230225123250.68"><vh>get_related</vh></v>
<v t="karstenw.20230225123250.69"><vh>_get_relations</vh></v>
<v t="karstenw.20230225123250.70"><vh>hypernym_paths</vh></v>
<v t="karstenw.20230225123250.71"><vh>min_depth</vh></v>
<v t="karstenw.20230225123250.72"><vh>max_depth</vh></v>
<v t="karstenw.20230225123250.73"><vh>shortest_path</vh></v>
<v t="karstenw.20230225123250.74"><vh>common_hypernyms</vh></v>
<v t="karstenw.20230225123250.75"><vh>lowest_common_hypernyms</vh></v>
<v t="karstenw.20230225123250.76"><vh>holonyms</vh></v>
<v t="karstenw.20230225123250.77"><vh>meronyms</vh></v>
<v t="karstenw.20230225123250.78"><vh>hypernyms</vh></v>
<v t="karstenw.20230225123250.79"><vh>hyponyms</vh></v>
<v t="karstenw.20230225123250.80"><vh>translate</vh></v>
</v>
<v t="karstenw.20230225123250.81"><vh>class Count</vh>
<v t="karstenw.20230225123250.82"><vh>__new__</vh></v>
<v t="karstenw.20230225123250.83"><vh>metadata</vh></v>
</v>
<v t="karstenw.20230225123250.84"><vh>class Sense</vh>
<v t="karstenw.20230225123250.85"><vh>__init__</vh></v>
<v t="karstenw.20230225123250.86"><vh>__repr__</vh></v>
<v t="karstenw.20230225123250.87"><vh>word</vh></v>
<v t="karstenw.20230225123251.1"><vh>synset</vh></v>
<v t="karstenw.20230225123251.2"><vh>examples</vh></v>
<v t="karstenw.20230225123251.3"><vh>lexicalized</vh></v>
<v t="karstenw.20230225123251.4"><vh>adjposition</vh></v>
<v t="karstenw.20230225123251.5"><vh>frames</vh></v>
<v t="karstenw.20230225123251.6"><vh>counts</vh></v>
<v t="karstenw.20230225123251.7"><vh>metadata</vh></v>
<v t="karstenw.20230225123251.8"><vh>relations</vh></v>
<v t="karstenw.20230225123251.9"><vh>get_related</vh></v>
<v t="karstenw.20230225123251.10"><vh>_get_relations</vh></v>
<v t="karstenw.20230225123251.11"><vh>get_related_synsets</vh></v>
<v t="karstenw.20230225123251.12"><vh>translate</vh></v>
</v>
<v t="karstenw.20230225123251.13"><vh>class Wordnet</vh>
<v t="karstenw.20230225123251.14"><vh>__init__</vh></v>
<v t="karstenw.20230225123251.15"><vh>lexicons</vh></v>
<v t="karstenw.20230225123251.16"><vh>expanded_lexicons</vh></v>
<v t="karstenw.20230225123251.17"><vh>word</vh></v>
<v t="karstenw.20230225123251.18"><vh>words</vh></v>
<v t="karstenw.20230225123251.19"><vh>synset</vh></v>
<v t="karstenw.20230225123251.20"><vh>synsets</vh></v>
<v t="karstenw.20230225123251.21"><vh>sense</vh></v>
<v t="karstenw.20230225123251.22"><vh>senses</vh></v>
<v t="karstenw.20230225123251.23"><vh>ili</vh></v>
<v t="karstenw.20230225123251.24"><vh>ilis</vh></v>
<v t="karstenw.20230225123251.25"><vh>describe</vh></v>
</v>
<v t="karstenw.20230225123251.26"><vh>_to_lexicon</vh></v>
<v t="karstenw.20230225123251.27"><vh>_find_helper</vh></v>
<v t="karstenw.20230225123251.28"><vh>projects</vh></v>
<v t="karstenw.20230225123251.29"><vh>lexicons</vh></v>
<v t="karstenw.20230225123251.30"><vh>word</vh></v>
<v t="karstenw.20230225123251.31"><vh>words</vh></v>
<v t="karstenw.20230225123251.32"><vh>synset</vh></v>
<v t="karstenw.20230225123251.33"><vh>synsets</vh></v>
<v t="karstenw.20230225123251.34"><vh>senses</vh></v>
<v t="karstenw.20230225123251.35"><vh>sense</vh></v>
<v t="karstenw.20230225123251.36"><vh>ili</vh></v>
<v t="karstenw.20230225123251.37"><vh>ilis</vh></v>
</v>
<v t="karstenw.20230225123225.2"><vh>@clean wn/_db.py</vh>
<v t="karstenw.20230225123313.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225123313.2"><vh>_adapt_dict</vh></v>
<v t="karstenw.20230225123313.3"><vh>_convert_dict</vh></v>
<v t="karstenw.20230225123313.4"><vh>_convert_boolean</vh></v>
<v t="karstenw.20230225123313.5"><vh>connect</vh></v>
<v t="karstenw.20230225123313.6"><vh>_init_db</vh></v>
<v t="karstenw.20230225123313.7"><vh>_check_schema_compatibility</vh></v>
<v t="karstenw.20230225123313.8"><vh>schema_hash</vh></v>
</v>
<v t="karstenw.20230225123225.3"><vh>@clean wn/_download.py</vh>
<v t="karstenw.20230225123943.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225123943.2"><vh>download</vh></v>
<v t="karstenw.20230225123943.3"><vh>_get_cache_path_and_urls</vh></v>
<v t="karstenw.20230225123943.4"><vh>_download</vh></v>
<v t="karstenw.20230225123943.5"><vh>_unlink_if_exists</vh></v>
</v>
<v t="karstenw.20230225123226.1"><vh>@clean wn/_exceptions.py</vh>
<v t="karstenw.20230225123946.1"><vh>class Error</vh></v>
<v t="karstenw.20230225123946.2"><vh>class DatabaseError</vh></v>
<v t="karstenw.20230225123946.3"><vh>class ConfigurationError</vh></v>
<v t="karstenw.20230225123946.4"><vh>class ProjectError</vh></v>
<v t="karstenw.20230225123946.5"><vh>class WnWarning</vh></v>
</v>
<v t="karstenw.20230225123226.2"><vh>@clean wn/_export.py</vh>
<v t="karstenw.20230225123948.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225123948.2"><vh>export</vh></v>
<v t="karstenw.20230225123948.3"><vh>_precheck</vh></v>
<v t="karstenw.20230225123948.4"><vh>_export_lexicon</vh></v>
<v t="karstenw.20230225123948.5"><vh>_export_requires</vh></v>
<v t="karstenw.20230225123948.6"><vh>_export_extends</vh></v>
<v t="karstenw.20230225123948.7"><vh>_export_lexical_entries</vh></v>
<v t="karstenw.20230225123948.8"><vh>_export_pronunciations</vh></v>
<v t="karstenw.20230225123948.9"><vh>_export_tags</vh></v>
<v t="karstenw.20230225123948.10"><vh>_export_senses</vh></v>
<v t="karstenw.20230225123948.11"><vh>_export_sense_relations</vh></v>
<v t="karstenw.20230225123948.12"><vh>_export_examples</vh></v>
<v t="karstenw.20230225123948.13"><vh>_export_counts</vh></v>
<v t="karstenw.20230225123948.14"><vh>_export_synsets</vh></v>
<v t="karstenw.20230225123948.15"><vh>_export_definitions</vh></v>
<v t="karstenw.20230225123948.16"><vh>_export_ili_definition</vh></v>
<v t="karstenw.20230225123948.17"><vh>_export_synset_relations</vh></v>
<v t="karstenw.20230225123948.18"><vh>_export_syntactic_behaviours_1_0</vh></v>
<v t="karstenw.20230225123948.19"><vh>_export_syntactic_behaviours_1_1</vh></v>
<v t="karstenw.20230225123948.20"><vh>_export_metadata</vh></v>
</v>
<v t="karstenw.20230225123226.3"><vh>@clean wn/_ili.py</vh>
<v t="karstenw.20230225123950.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225123950.2"><vh>is_ili</vh></v>
<v t="karstenw.20230225123950.3"><vh>load</vh></v>
</v>
<v t="karstenw.20230225123226.4"><vh>@clean wn/_queries.py</vh>
<v t="karstenw.20230225123952.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225123952.2"><vh>find_lexicons</vh></v>
<v t="karstenw.20230225123952.3"><vh>get_lexicon</vh></v>
<v t="karstenw.20230225123952.4"><vh>_get_lexicon</vh></v>
<v t="karstenw.20230225123952.5"><vh>get_modified</vh></v>
<v t="karstenw.20230225123952.6"><vh>get_lexicon_dependencies</vh></v>
<v t="karstenw.20230225123952.7"><vh>get_lexicon_extension_bases</vh></v>
<v t="karstenw.20230225123952.8"><vh>get_lexicon_extensions</vh></v>
<v t="karstenw.20230225123952.9"><vh>find_ilis</vh></v>
<v t="karstenw.20230225123952.10"><vh>_find_existing_ilis</vh></v>
<v t="karstenw.20230225123952.11"><vh>find_proposed_ilis</vh></v>
<v t="karstenw.20230225123952.12"><vh>find_entries</vh></v>
<v t="karstenw.20230225123952.13"><vh>find_senses</vh></v>
<v t="karstenw.20230225123952.14"><vh>find_synsets</vh></v>
<v t="karstenw.20230225123952.15"><vh>get_synsets_for_ilis</vh></v>
<v t="karstenw.20230225123952.16"><vh>get_synset_relations</vh></v>
<v t="karstenw.20230225123952.17"><vh>get_definitions</vh></v>
<v t="karstenw.20230225123952.18"><vh>get_examples</vh></v>
<v t="karstenw.20230225123952.19"><vh>find_syntactic_behaviours</vh></v>
<v t="karstenw.20230225123952.20"><vh>get_syntactic_behaviours</vh></v>
<v t="karstenw.20230225123952.21"><vh>_get_senses</vh></v>
<v t="karstenw.20230225123952.22"><vh>get_entry_senses</vh></v>
<v t="karstenw.20230225123952.23"><vh>get_synset_members</vh></v>
<v t="karstenw.20230225123952.24"><vh>get_sense_relations</vh></v>
<v t="karstenw.20230225123952.25"><vh>get_sense_synset_relations</vh></v>
<v t="karstenw.20230225123952.26"><vh>get_metadata</vh></v>
<v t="karstenw.20230225123952.27"><vh>get_lexicalized</vh></v>
<v t="karstenw.20230225123952.28"><vh>get_adjposition</vh></v>
<v t="karstenw.20230225123952.29"><vh>get_form_pronunciations</vh></v>
<v t="karstenw.20230225123952.30"><vh>get_form_tags</vh></v>
<v t="karstenw.20230225123952.31"><vh>get_sense_counts</vh></v>
<v t="karstenw.20230225123952.32"><vh>get_lexfile</vh></v>
<v t="karstenw.20230225123952.33"><vh>_qs</vh></v>
<v t="karstenw.20230225123952.34"><vh>_vs</vh></v>
<v t="karstenw.20230225123952.35"><vh>_kws</vh></v>
</v>
<v t="karstenw.20230225123226.5"><vh>@clean wn/_types.py</vh>
<v t="karstenw.20230225123954.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230225123226.6"><vh>@clean wn/_util.py</vh>
<v t="karstenw.20230225123957.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225123957.2"><vh>version_info</vh></v>
<v t="karstenw.20230225123957.3"><vh>is_url</vh></v>
<v t="karstenw.20230225123957.4"><vh>is_gzip</vh></v>
<v t="karstenw.20230225123957.5"><vh>is_lzma</vh></v>
<v t="karstenw.20230225123957.6"><vh>is_xml</vh></v>
<v t="karstenw.20230225123957.7"><vh>_inspect_file_signature</vh></v>
<v t="karstenw.20230225123957.8"><vh>short_hash</vh></v>
<v t="karstenw.20230225123957.9"><vh>flatten</vh></v>
<v t="karstenw.20230225123957.10"><vh>normalize_form</vh></v>
</v>
<v t="karstenw.20230225123227.1"><vh>@clean wn/constants.py</vh>
<v t="karstenw.20230225124001.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230225123227.2"><vh>@clean wn/ic.py</vh>
<v t="karstenw.20230225124003.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225124003.2"><vh>information_content</vh></v>
<v t="karstenw.20230225124003.3"><vh>synset_probability</vh></v>
<v t="karstenw.20230225124003.4"><vh>_initialize</vh></v>
<v t="karstenw.20230225124003.5"><vh>compute</vh></v>
<v t="karstenw.20230225124003.6"><vh>load</vh></v>
<v t="karstenw.20230225124003.7"><vh>_parse_ic_file</vh></v>
</v>
<v t="karstenw.20230225123227.3"><vh>@clean wn/lmf.py</vh>
<v t="karstenw.20230225124007.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225124007.2"><vh>class LMFError</vh></v>
<v t="karstenw.20230225124007.3"><vh>class LMFWarning</vh></v>
<v t="karstenw.20230225124007.4"><vh>class Metadata</vh></v>
<v t="karstenw.20230225124007.5"><vh>class ILIDefinition</vh></v>
<v t="karstenw.20230225124007.6"><vh>class Definition</vh></v>
<v t="karstenw.20230225124007.7"><vh>class Relation</vh></v>
<v t="karstenw.20230225124007.8"><vh>class Example</vh></v>
<v t="karstenw.20230225124007.9"><vh>class Synset</vh></v>
<v t="karstenw.20230225124007.10"><vh>class ExternalSynset</vh></v>
<v t="karstenw.20230225124007.11"><vh>class Count</vh></v>
<v t="karstenw.20230225124007.12"><vh>class Sense</vh></v>
<v t="karstenw.20230225124007.13"><vh>class ExternalSense</vh></v>
<v t="karstenw.20230225124007.14"><vh>class Pronunciation</vh></v>
<v t="karstenw.20230225124007.15"><vh>class Tag</vh></v>
<v t="karstenw.20230225124007.16"><vh>class _FormChildren</vh></v>
<v t="karstenw.20230225124007.17"><vh>class Lemma</vh></v>
<v t="karstenw.20230225124007.18"><vh>class ExternalLemma</vh></v>
<v t="karstenw.20230225124007.19"><vh>class Form</vh></v>
<v t="karstenw.20230225124007.20"><vh>class ExternalForm</vh></v>
<v t="karstenw.20230225124007.21"><vh>class _SyntacticBehaviourBase</vh></v>
<v t="karstenw.20230225124007.22"><vh>class SyntacticBehaviour</vh></v>
<v t="karstenw.20230225124007.23"><vh>class _LexicalEntryBase</vh></v>
<v t="karstenw.20230225124007.24"><vh>class LexicalEntry</vh></v>
<v t="karstenw.20230225124007.25"><vh>class ExternalLexicalEntry</vh></v>
<v t="karstenw.20230225124007.26"><vh>class Dependency</vh></v>
<v t="karstenw.20230225124007.27"><vh>class _LexiconBase</vh></v>
<v t="karstenw.20230225124007.28"><vh>class Lexicon</vh></v>
<v t="karstenw.20230225124007.29"><vh>class LexiconExtension</vh></v>
<v t="karstenw.20230225124007.30"><vh>class LexicalResource</vh></v>
<v t="karstenw.20230225124007.31"><vh>is_lmf</vh></v>
<v t="karstenw.20230225124007.32"><vh>_read_header</vh></v>
<v t="karstenw.20230225124007.33"><vh>scan_lexicons</vh></v>
<v t="karstenw.20230225124007.34"><vh>load</vh></v>
<v t="karstenw.20230225124007.35"><vh>_quick_scan</vh></v>
<v t="karstenw.20230225124007.36"><vh>_make_parser</vh></v>
<v t="karstenw.20230225124007.37"><vh>_unexpected</vh></v>
<v t="karstenw.20230225124007.38"><vh>_validate</vh></v>
<v t="karstenw.20230225124007.39"><vh>_validate_lexicon</vh></v>
<v t="karstenw.20230225124007.40"><vh>_validate_entries</vh></v>
<v t="karstenw.20230225124007.41"><vh>_validate_forms</vh></v>
<v t="karstenw.20230225124007.42"><vh>_validate_senses</vh></v>
<v t="karstenw.20230225124007.43"><vh>_validate_frames</vh></v>
<v t="karstenw.20230225124007.44"><vh>_validate_synsets</vh></v>
<v t="karstenw.20230225124007.45"><vh>_validate_metadata</vh></v>
<v t="karstenw.20230225124007.46"><vh>dump</vh></v>
<v t="karstenw.20230225124007.47"><vh>_dump_lexicon</vh></v>
<v t="karstenw.20230225124007.48"><vh>_build_lexicon_attrib</vh></v>
<v t="karstenw.20230225124007.49"><vh>_dump_dependency</vh></v>
<v t="karstenw.20230225124007.50"><vh>_dump_lexical_entry</vh></v>
<v t="karstenw.20230225124007.51"><vh>_build_lemma</vh></v>
<v t="karstenw.20230225124007.52"><vh>_build_form</vh></v>
<v t="karstenw.20230225124007.53"><vh>_build_pronunciation</vh></v>
<v t="karstenw.20230225124007.54"><vh>_build_tag</vh></v>
<v t="karstenw.20230225124007.55"><vh>_build_sense</vh></v>
<v t="karstenw.20230225124007.56"><vh>_build_example</vh></v>
<v t="karstenw.20230225124007.57"><vh>_build_count</vh></v>
<v t="karstenw.20230225124007.58"><vh>_dump_synset</vh></v>
<v t="karstenw.20230225124007.59"><vh>_build_definition</vh></v>
<v t="karstenw.20230225124007.60"><vh>_build_ili_definition</vh></v>
<v t="karstenw.20230225124007.61"><vh>_build_relation</vh></v>
<v t="karstenw.20230225124007.62"><vh>_dump_syntactic_behaviour</vh></v>
<v t="karstenw.20230225124007.63"><vh>_build_syntactic_behaviour</vh></v>
<v t="karstenw.20230225124007.64"><vh>_tostring</vh></v>
<v t="karstenw.20230225124007.65"><vh>_indent</vh></v>
<v t="karstenw.20230225124007.66"><vh>_meta_dict</vh></v>
</v>
<v t="karstenw.20230225123227.4"><vh>@clean wn/metrics.py</vh>
<v t="karstenw.20230225124213.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225124213.2"><vh>ambiguity</vh></v>
<v t="karstenw.20230225124213.3"><vh>average_ambiguity</vh></v>
</v>
<v t="karstenw.20230225123227.5"><vh>@clean wn/morphy.py</vh>
<v t="karstenw.20230225124215.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225124215.2"><vh>class _System</vh></v>
<v t="karstenw.20230225124215.3"><vh>class Morphy</vh>
<v t="karstenw.20230225124215.4"><vh>__init__</vh></v>
<v t="karstenw.20230225124215.5"><vh>__call__</vh></v>
<v t="karstenw.20230225124215.6"><vh>_morphstr</vh></v>
</v>
</v>
<v t="karstenw.20230225123227.6"><vh>@clean wn/project.py</vh>
<v t="karstenw.20230225124217.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225124217.2"><vh>is_package_directory</vh></v>
<v t="karstenw.20230225124217.3"><vh>_package_directory_types</vh></v>
<v t="karstenw.20230225124217.4"><vh>_resource_file_type</vh></v>
<v t="karstenw.20230225124217.5"><vh>is_collection_directory</vh></v>
<v t="karstenw.20230225124217.6"><vh>class _Project</vh>
<v t="karstenw.20230225124217.7"><vh>__init__</vh></v>
<v t="karstenw.20230225124217.8"><vh>readme</vh></v>
<v t="karstenw.20230225124217.9"><vh>license</vh></v>
<v t="karstenw.20230225124217.10"><vh>citation</vh></v>
<v t="karstenw.20230225124217.11"><vh>_find_file</vh></v>
</v>
<v t="karstenw.20230225124217.12"><vh>class Package</vh>
<v t="karstenw.20230225124217.13"><vh>type</vh></v>
<v t="karstenw.20230225124217.14"><vh>resource_file</vh></v>
</v>
<v t="karstenw.20230225124217.15"><vh>class _ResourceOnlyPackage</vh>
<v t="karstenw.20230225124217.16"><vh>resource_file</vh></v>
<v t="karstenw.20230225124217.17"><vh>readme</vh></v>
<v t="karstenw.20230225124217.18"><vh>license</vh></v>
<v t="karstenw.20230225124217.19"><vh>citation</vh></v>
</v>
<v t="karstenw.20230225124217.20"><vh>class Collection</vh>
<v t="karstenw.20230225124217.21"><vh>packages</vh></v>
</v>
<v t="karstenw.20230225124217.22"><vh>iterpackages</vh></v>
<v t="karstenw.20230225124217.23"><vh>_get_decompressed</vh></v>
<v t="karstenw.20230225124217.24"><vh>_check_tar</vh></v>
</v>
<v t="karstenw.20230225123227.7"><vh>@clean wn/schema.sql</vh></v>
<v t="karstenw.20230225123228.1"><vh>@clean wn/similarity.py</vh>
<v t="karstenw.20230225124221.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225124221.2"><vh>path</vh></v>
<v t="karstenw.20230225124221.3"><vh>wup</vh></v>
<v t="karstenw.20230225124221.4"><vh>lch</vh></v>
<v t="karstenw.20230225124221.5"><vh>res</vh></v>
<v t="karstenw.20230225124221.6"><vh>jcn</vh></v>
<v t="karstenw.20230225124221.7"><vh>lin</vh></v>
<v t="karstenw.20230225124221.8"><vh>_least_common_subsumers</vh></v>
<v t="karstenw.20230225124221.9"><vh>_most_informative_lcs</vh></v>
<v t="karstenw.20230225124221.10"><vh>_check_if_pos_compatible</vh></v>
</v>
<v t="karstenw.20230225123228.2"><vh>@clean wn/taxonomy.py</vh>
<v t="karstenw.20230225124223.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225124223.2"><vh>roots</vh></v>
<v t="karstenw.20230225124223.3"><vh>leaves</vh></v>
<v t="karstenw.20230225124223.4"><vh>taxonomy_depth</vh></v>
<v t="karstenw.20230225124223.5"><vh>_synsets_for_pos</vh></v>
<v t="karstenw.20230225124223.6"><vh>_hypernym_paths</vh></v>
<v t="karstenw.20230225124223.7"><vh>hypernym_paths</vh></v>
<v t="karstenw.20230225124223.8"><vh>min_depth</vh></v>
<v t="karstenw.20230225124223.9"><vh>max_depth</vh></v>
<v t="karstenw.20230225124223.10"><vh>_shortest_hyp_paths</vh></v>
<v t="karstenw.20230225124223.11"><vh>shortest_path</vh></v>
<v t="karstenw.20230225124223.12"><vh>common_hypernyms</vh></v>
<v t="karstenw.20230225124223.13"><vh>lowest_common_hypernyms</vh></v>
</v>
<v t="karstenw.20230225123228.3"><vh>@clean wn/util.py</vh>
<v t="karstenw.20230225124225.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225124225.2"><vh>synset_id_formatter</vh></v>
<v t="karstenw.20230225124225.3"><vh>class ProgressHandler</vh>
<v t="karstenw.20230225124225.4"><vh>__init__</vh></v>
<v t="karstenw.20230225124225.5"><vh>update</vh></v>
<v t="karstenw.20230225124225.6"><vh>set</vh></v>
<v t="karstenw.20230225124225.7"><vh>flash</vh></v>
<v t="karstenw.20230225124225.8"><vh>close</vh></v>
</v>
<v t="karstenw.20230225124225.9"><vh>class ProgressBar</vh>
<v t="karstenw.20230225124225.10"><vh>update</vh></v>
<v t="karstenw.20230225124225.11"><vh>format</vh></v>
<v t="karstenw.20230225124225.12"><vh>flash</vh></v>
<v t="karstenw.20230225124225.13"><vh>close</vh></v>
</v>
</v>
<v t="karstenw.20230225123228.4"><vh>@clean wn/validate.py</vh>
<v t="karstenw.20230225124228.1"><vh>Declarations</vh></v>
<v t="karstenw.20230225124228.2"><vh>_non_unique_id</vh></v>
<v t="karstenw.20230225124228.3"><vh>_has_no_senses</vh></v>
<v t="karstenw.20230225124228.4"><vh>_redundant_sense</vh></v>
<v t="karstenw.20230225124228.5"><vh>_redundant_entry</vh></v>
<v t="karstenw.20230225124228.6"><vh>_missing_synset</vh></v>
<v t="karstenw.20230225124228.7"><vh>_empty_synset</vh></v>
<v t="karstenw.20230225124228.8"><vh>_repeated_ili</vh></v>
<v t="karstenw.20230225124228.9"><vh>_missing_ili_definition</vh></v>
<v t="karstenw.20230225124228.10"><vh>_spurious_ili_definition</vh></v>
<v t="karstenw.20230225124228.11"><vh>_missing_relation_target</vh></v>
<v t="karstenw.20230225124228.12"><vh>_invalid_relation_type</vh></v>
<v t="karstenw.20230225124228.13"><vh>_redundant_relation</vh></v>
<v t="karstenw.20230225124228.14"><vh>_missing_reverse_relation</vh></v>
<v t="karstenw.20230225124228.15"><vh>_hypernym_wrong_pos</vh></v>
<v t="karstenw.20230225124228.16"><vh>_self_loop</vh></v>
<v t="karstenw.20230225124228.17"><vh>_multiples</vh></v>
<v t="karstenw.20230225124228.18"><vh>_entries</vh></v>
<v t="karstenw.20230225124228.19"><vh>_forms</vh></v>
<v t="karstenw.20230225124228.20"><vh>_senses</vh></v>
<v t="karstenw.20230225124228.21"><vh>_synsets</vh></v>
<v t="karstenw.20230225124228.22"><vh>_sense_relations</vh></v>
<v t="karstenw.20230225124228.23"><vh>_synset_relations</vh></v>
<v t="karstenw.20230225124228.24"><vh>_select_checks</vh></v>
<v t="karstenw.20230225124228.25"><vh>validate</vh></v>
</v>
<v t="karstenw.20230225125423.1"><vh>wn/web.py  -  ignore because of leo parse error</vh></v>
</v>
</v>
</vnodes>
<tnodes>
<t tx="karstenw.20230225123024.1"></t>
<t tx="karstenw.20230225123039.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230225123108.1">
"""
Wordnet Interface.
"""

__all__ = (
    '__version__',
    'Wordnet',
    'download',
    'add',
    'remove',
    'export',
    'projects',
    'lexicons',
    'Lexicon',
    'word',
    'words',
    'Word',
    'Form',
    'Pronunciation',
    'Tag',
    'sense',
    'senses',
    'Sense',
    'Count',
    'synset',
    'synsets',
    'Synset',
    'ili',
    'ilis',
    'ILI',
    'Error',
    'DatabaseError',
    'ConfigurationError',
    'ProjectError',
    'WnWarning',
)

from wn._exceptions import (
    Error,
    DatabaseError,
    ConfigurationError,
    ProjectError,
    WnWarning,
)
from wn._config import config  # noqa: F401
from wn._add import add, remove
from wn._export import export
from wn._download import download
from wn._core import (
    projects,
    lexicons, Lexicon,
    word, words, Word, Form, Pronunciation, Tag,
    sense, senses, Sense, Count,
    synset, synsets, Synset,
    ili, ilis, ILI,
    Wordnet
)

__version__ = '0.9.3'
</t>
<t tx="karstenw.20230225123120.1">@language python
@tabwidth -4
@others
if args.dir:
    wn.config.data_directory = args.dir

args.func(args)
</t>
<t tx="karstenw.20230225123122.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230225123123.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230225123203.1">
import sys
import argparse
from pathlib import Path
import json
import logging

import wn
from wn.project import iterpackages
from wn import lmf
from wn.validate import validate


</t>
<t tx="karstenw.20230225123203.2">def _download(args):
    if args.index:
        wn.config.load_index(args.index)
    for target in args.target:
        wn.download(target, add=args.add)


</t>
<t tx="karstenw.20230225123203.3">def _lexicons(args):
    for lex in wn.lexicons(lang=args.lang, lexicon=args.lexicon):
        print('\t'.join((lex.id, lex.version, f'[{lex.language}]', lex.label)))


</t>
<t tx="karstenw.20230225123203.4">def _projects(args):
    for info in wn.projects():
        key = 'i'
        key += 'c' if info['cache'] else '-'
        # key += 'a' if False else '-'  # TODO: check if project is added to db
        print(
            '\t'.join((
                key,
                info['id'],
                info['version'],
                f"[{info['language'] or '---'}]",
                info['label'] or '---',
            ))
        )


</t>
<t tx="karstenw.20230225123203.5">def _validate(args):
    all_valid = True
    selectseq = [check.strip() for check in args.select.split(',')]
    for package in iterpackages(args.FILE):
        resource = lmf.load(package.resource_file())
        for lexicon in resource['lexicons']:
            spec = f'{lexicon["id"]}:{lexicon["version"]}'
            print(f'{spec:&lt;20}', end='')
            report = validate(lexicon, select=selectseq)
            if not any(check.get('items', []) for check in report.values()):
                print('passed')
            else:
                print('failed')
                all_valid = False
                # clean up report
                for code in list(report):
                    if not report[code].get('items'):
                        del report[code]
                if args.output_file:
                    with open(args.output_file, 'w') as outfile:
                        json.dump(report, outfile, indent=2)
                else:
                    for code, check in report.items():
                        if not check['items']:
                            continue
                        print(f'  {check["message"]}')
                        for id, context in check['items'].items():
                            print(f'    {id}: {context}' if context else f'    {id}')

    sys.exit(0 if all_valid else 1)


</t>
<t tx="karstenw.20230225123203.6">def _path_type(arg):
    return Path(arg)


</t>
<t tx="karstenw.20230225123203.7">def _file_path_type(arg):
    path = Path(arg)
    if not path.is_file():
        raise argparse.ArgumentTypeError(f'cannot file file: {arg}')
    return path


parser = argparse.ArgumentParser(
    prog='python3 -m wn',
    description="Manage Wn's wordnet data from the command line.",
)
parser.add_argument(
    '-V', '--version', action='version', version=f'Wn {wn.__version__}'
)
parser.add_argument(
    '-v', '--verbose', action='count', dest='verbosity', default=0,
    help='increase verbosity (can repeat: -vv, -vvv)'
)
parser.add_argument(
    '-d', '--dir',
    type=_path_type,
    help="data directory for Wn's database and cache",
)
parser.set_defaults(func=lambda _: parser.print_help())
sub_parsers = parser.add_subparsers(title='subcommands')


parser_download = sub_parsers.add_parser(
    'download',
    description="Download wordnets and add them to Wn's database.",
    help='download wordnets',
)
parser_download.add_argument(
    'target', nargs='+', help='project specifiers or URLs'
)
parser_download.add_argument(
    '--index', type=_file_path_type, help='project index to use for downloading'
)
parser_download.add_argument(
    '--no-add', action='store_false', dest='add',
    help='download and cache without adding to the database'
)
parser_download.set_defaults(func=_download)


parser_lexicons = sub_parsers.add_parser(
    'lexicons',
    description="Display a list of installed lexicons.",
    help='list installed lexicons',
)
parser_lexicons.add_argument(
    '-l', '--lang', help='BCP 47 language code'
)
parser_lexicons.add_argument(
    '--lexicon', help='lexicon specifiers'
)
parser_lexicons.set_defaults(func=_lexicons)


parser_projects = sub_parsers.add_parser(
    'projects',
    description=(
        "Display a list of known projects. The first column shows the "
        "status for a project (i=indexed, c=cached)."
    ),
    help='list known projects',
)
parser_projects.set_defaults(func=_projects)


parser_validate = sub_parsers.add_parser(
    'validate',
    description=(
        "Validate a WN-LMF lexicon"
    ),
    help='validate a lexicon',
)
parser_validate.add_argument(
    'FILE', type=_file_path_type, help='WN-LMF (XML) lexicon file to validate'
)
parser_validate.add_argument(
    '--select', metavar='CHECKS', default='E,W',
    help='comma-separated list of checks to run (default: E,W)'
)
parser_validate.add_argument(
    '--output-file', metavar='FILE',
    help='write report to a JSON file'
)
parser_validate.set_defaults(func=_validate)


args = parser.parse_args()

logging.basicConfig(level=logging.ERROR - (min(args.verbosity, 3) * 10))

</t>
<t tx="karstenw.20230225123218.1">"""
Adding and removing lexicons to/from the database.
"""

from typing import (
    Union, Optional, TypeVar, Type, Tuple, List, Dict, Set,
    Iterator, Iterable, Sequence, cast
)
from pathlib import Path
from itertools import islice
import sqlite3
import logging

import wn
from wn._types import AnyPath
from wn.constants import _WORDNET, _ILI
from wn._db import connect
from wn._queries import find_lexicons, get_lexicon_extensions, get_lexicon
from wn._util import normalize_form
from wn.util import ProgressHandler, ProgressBar
from wn.project import iterpackages
from wn import lmf
from wn import _ili


logger = logging.getLogger('wn')


BATCH_SIZE = 1000
DEFAULT_MEMBER_RANK = 127  # synset member rank when not specified by 'members'

ENTRY_QUERY = '''
    SELECT e.rowid
      FROM entries AS e
     WHERE e.id = ?
       AND e.lexicon_rowid = ?
'''
# forms don't have reliable ids, so also consider rank; this depends
# on each form having a unique rank, and this doesn't work for lexicon
# extensions
FORM_QUERY = '''
    SELECT f.rowid
      FROM forms AS f
      JOIN entries AS e ON f.entry_rowid = e.rowid
     WHERE e.id = ?
       AND e.lexicon_rowid = ?
       AND (f.id = ? OR f.rank = ?)
'''
SENSE_QUERY = '''
    SELECT s.rowid
      FROM senses AS s
     WHERE s.id = ?
       AND s.lexicon_rowid = ?
'''
SYNSET_QUERY = '''
    SELECT ss.rowid
      FROM synsets AS ss
     WHERE ss.id = ?
       AND ss.lexicon_rowid = ?
'''
RELTYPE_QUERY = '''
    SELECT rt.rowid
      FROM relation_types AS rt
     WHERE rt.type = ?
'''
ILISTAT_QUERY = '''
    SELECT ist.rowid
      FROM ili_statuses AS ist
     WHERE ist.status = ?
'''
LEXFILE_QUERY = '''
    SELECT lf.rowid
      FROM lexfiles AS lf
     WHERE lf.name = ?
'''

_AnyLexicon = Union[lmf.Lexicon, lmf.LexiconExtension]
_AnyEntry = Union[lmf.LexicalEntry, lmf.ExternalLexicalEntry]
_AnyLemma = Union[lmf.Lemma, lmf.ExternalLemma]
_AnyForm = Union[lmf.Form, lmf.ExternalForm]
_AnySense = Union[lmf.Sense, lmf.ExternalSense]
_AnySynset = Union[lmf.Synset, lmf.ExternalSynset]


</t>
<t tx="karstenw.20230225123218.10">def _insert_synsets(
    synsets: Sequence[_AnySynset],
    lexid: int,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Synsets')
    # synsets
    ss_query = f'''
        INSERT INTO synsets
        VALUES (null,?,?,(SELECT rowid FROM ilis WHERE id=?),?,?,({LEXFILE_QUERY}),?)
    '''
    # presupposed ILIs
    pre_ili_query = f'''
        INSERT OR IGNORE INTO ilis
        VALUES (null,?,({ILISTAT_QUERY}),?,?)
    '''
    # proposed ILIs
    pro_ili_query = '''
        INSERT INTO proposed_ilis
        VALUES (null,
               (SELECT ss.rowid
                  FROM synsets AS ss
                 WHERE ss.id=? AND lexicon_rowid=?),
               ?,
               ?)
    '''

    for batch in _batch(_local_synsets(synsets)):

        # first add presupposed ILIs
        pre_ili_data = []
        for ss in batch:
            ili = ss['ili']
            if ili and ili != 'in':
                defn = ss.get('ili_definition')  # normally null
                text = defn['text'] if defn else None
                meta = defn['meta'] if defn else None
                pre_ili_data.append((ili, 'presupposed', text, meta))
        cur.executemany(pre_ili_query, pre_ili_data)

        # then add synsets
        ss_data = (
            (ss['id'],
             lexid,
             ss['ili'] if ss['ili'] and ss['ili'] != 'in' else None,
             ss['partOfSpeech'],
             ss.get('lexicalized', True),
             ss.get('lexfile'),
             ss['meta'])
            for ss in batch
        )
        cur.executemany(ss_query, ss_data)

        # finally add proposed ILIs
        pro_ili_data = []
        for ss in batch:
            ili = ss['ili']
            if ili == 'in':
                defn = ss.get('ili_definition')
                text = defn['text'] if defn else None
                meta = defn['meta'] if defn else None
                pro_ili_data.append((ss['id'], lexid, text, meta))
        cur.executemany(pro_ili_query, pro_ili_data)

        progress.update(len(batch))


</t>
<t tx="karstenw.20230225123218.11">def _insert_synset_definitions(
    synsets: Sequence[_AnySynset],
    lexid: int,
    lexidmap: _LexIdMap,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Definitions')
    query = f'''
        INSERT INTO definitions
        VALUES (null,?,({SYNSET_QUERY}),?,?,({SENSE_QUERY}),?)
    '''
    for batch in _batch(synsets):
        data = [
            (lexid,
             synset['id'],
             lexidmap.get(synset['id'], lexid),
             definition['text'],
             definition.get('language'),
             definition.get('sourceSense'),
             lexidmap.get(definition.get('sourceSense', ''), lexid),
             definition['meta'])
            for synset in batch
            for definition in synset.get('definitions', [])
        ]
        cur.executemany(query, data)
        progress.update(len(data))


</t>
<t tx="karstenw.20230225123218.12">def _insert_synset_relations(
    synsets: Sequence[_AnySynset],
    lexid: int,
    lexidmap: _LexIdMap,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Synset Relations')
    query = f'''
        INSERT INTO synset_relations
        VALUES (null,?,({SYNSET_QUERY}),({SYNSET_QUERY}),({RELTYPE_QUERY}),?)
    '''
    for batch in _batch(synsets):
        data = [
            (lexid,
             synset['id'], lexidmap.get(synset['id'], lexid),
             relation['target'], lexidmap.get(relation['target'], lexid),
             relation['relType'],
             relation['meta'])
            for synset in batch
            for relation in synset.get('relations', [])
        ]
        cur.executemany(query, data)
        progress.update(len(data))


</t>
<t tx="karstenw.20230225123218.13">def _insert_entries(
    entries: Sequence[_AnyEntry],
    lexid: int,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Words')
    query = 'INSERT INTO entries VALUES (null,?,?,?,?)'
    for batch in _batch(_local_entries(entries)):
        data = (
            (entry['id'],
             lexid,
             entry['lemma']['partOfSpeech'],
             entry['meta'])
            for entry in batch
        )
        cur.executemany(query, data)
        progress.update(len(batch))


</t>
<t tx="karstenw.20230225123218.14">def _insert_forms(
    entries: Sequence[_AnyEntry],
    lexid: int,
    lexidmap: _LexIdMap,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Word Forms')
    query = f'INSERT INTO forms VALUES (null,?,?,({ENTRY_QUERY}),?,?,?,?)'
    for batch in _batch(entries):
        forms: List[Tuple[Optional[str], int, str, int,
                          str, Optional[str], Optional[str], int]] = []
        for entry in batch:
            eid = entry['id']
            lid = lexidmap.get(eid, lexid)
            if not _is_external(entry):
                entry = cast(lmf.LexicalEntry, entry)
                written_form = entry['lemma']['writtenForm']
                norm = normalize_form(written_form)
                forms.append(
                    (None, lexid, eid, lid,
                     written_form, norm if norm != written_form else None,
                     entry['lemma'].get('script'), 0)
                )
            for i, form in enumerate(_forms(entry), 1):
                if _is_external(form):
                    continue
                form = cast(lmf.Form, form)
                written_form = form['writtenForm']
                norm = normalize_form(written_form)
                forms.append(
                    (form.get('id'), lexid, eid, lid,
                     written_form, norm if norm != written_form else None,
                     form.get('script'), i)
                )
        cur.executemany(query, forms)
        progress.update(len(forms))


</t>
<t tx="karstenw.20230225123218.15">def _insert_pronunciations(
    entries: Sequence[_AnyEntry],
    lexid: int,
    lexidmap: _LexIdMap,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Pronunciations')
    query = f'INSERT INTO pronunciations VALUES (({FORM_QUERY}),?,?,?,?,?)'
    for batch in _batch(entries):
        prons: List[Tuple[str, int, Optional[str], int,
                          str, Optional[str], Optional[str],
                          bool, Optional[str]]] = []
        for entry in batch:
            eid = entry['id']
            lid = lexidmap.get(eid, lexid)
            if entry.get('lemma'):
                for p in entry['lemma'].get('pronunciations', []):
                    prons.append(
                        (eid, lid, None, 0,
                         p['text'], p.get('variety'), p.get('notation'),
                         p.get('phonemic', True), p.get('audio'))
                    )
            for i, form in enumerate(_forms(entry), 1):
                # rank is not valid in FORM_QUERY for external forms
                rank = -1 if _is_external(form) else i
                for p in form.get('pronunciations', []):
                    prons.append(
                        (eid, lid, form.get('id'), rank,
                         p['text'], p.get('variety'), p.get('notation'),
                         p.get('phonemic', True), p.get('audio'))
                    )
        cur.executemany(query, prons)
        progress.update(len(prons))


</t>
<t tx="karstenw.20230225123218.16">def _insert_tags(
    entries: Sequence[_AnyEntry],
    lexid: int,
    lexidmap: _LexIdMap,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Word Form Tags')
    query = f'INSERT INTO tags VALUES (({FORM_QUERY}),?,?)'
    for batch in _batch(entries):
        tags: List[Tuple[str, int, Optional[str], int, str, str]] = []
        for entry in batch:
            eid = entry['id']
            lid = lexidmap.get(eid, lexid)
            if entry.get('lemma'):
                for tag in entry['lemma'].get('tags', []):
                    tags.append((eid, lid, None, 0, tag['text'], tag['category']))
            for i, form in enumerate(_forms(entry), 1):
                # rank is not valid in FORM_QUERY for external forms
                rank = -1 if _is_external(form) else i
                for tag in form.get('tags', []):
                    tags.append(
                        (eid, lid, form.get('id'), rank, tag['text'], tag['category'])
                    )
        cur.executemany(query, tags)
        progress.update(len(tags))


</t>
<t tx="karstenw.20230225123218.17">def _insert_senses(
    entries: Sequence[_AnyEntry],
    synsets: Sequence[_AnySynset],
    lexid: int,
    lexidmap: _LexIdMap,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Senses')
    ssrank = {s: i
              for ss in _local_synsets(synsets)
              for i, s in enumerate(ss.get('members', []))}
    query = f'''
        INSERT INTO senses
        VALUES (null,
                ?,
                ?,
                ({ENTRY_QUERY}),
                ?,
                ({SYNSET_QUERY}),
                ?,
                ?,
                ?)
    '''
    for batch in _batch(entries):
        data = [
            (sense['id'],
             lexid,
             entry['id'], lexidmap.get(entry['id'], lexid),
             i,
             sense['synset'], lexidmap.get(sense['synset'], lexid),
             ssrank.get(sense['id'], DEFAULT_MEMBER_RANK),
             sense.get('lexicalized', True),
             sense['meta'])
            for entry in batch
            for i, sense in enumerate(_local_senses(_senses(entry)))
        ]
        cur.executemany(query, data)
        progress.update(len(data))


</t>
<t tx="karstenw.20230225123218.18">def _insert_adjpositions(
    entries: Sequence[_AnyEntry],
    lexid: int,
    lexidmap: _LexIdMap,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
):
    progress.set(status='Sense Adjpositions')
    data = [(s['id'], lexidmap.get(s['id'], lexid), s['adjposition'])
            for e in entries
            for s in _local_senses(_senses(e))
            if s.get('adjposition')]
    query = f'INSERT INTO adjpositions VALUES (({SENSE_QUERY}),?)'
    cur.executemany(query, data)


</t>
<t tx="karstenw.20230225123218.19">def _insert_counts(
    entries: Sequence[_AnyEntry],
    lexid: int,
    lexidmap: _LexIdMap,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Counts')
    data = [(lexid,
             sense['id'], lexidmap.get(sense['id'], lexid),
             count['value'],
             count['meta'])
            for entry in entries
            for sense in _senses(entry)
            for count in sense.get('counts', [])]
    query = f'INSERT INTO counts VALUES (null,?,({SENSE_QUERY}),?,?)'
    cur.executemany(query, data)
    progress.update(len(data))


</t>
<t tx="karstenw.20230225123218.2">def add(
    source: AnyPath,
    progress_handler: Optional[Type[ProgressHandler]] = ProgressBar,
) -&gt; None:
    """Add the LMF file at *source* to the database.

    The file at *source* may be gzip-compressed or plain text XML.

    &gt;&gt;&gt; wn.add('english-wordnet-2020.xml')
    Added ewn:2020 (English WordNet)

    The *progress_handler* parameter takes a subclass of
    :class:`wn.util.ProgressHandler`. An instance of the class will be
    created, used, and closed by this function.

    """
    if progress_handler is None:
        progress_handler = ProgressHandler
    progress = progress_handler(message='Database')

    logger.info('adding project to database')
    logger.info('  database: %s', wn.config.database_path)
    logger.info('  project file: %s', source)

    try:
        for package in iterpackages(source):
            if package.type == _WORDNET:
                _add_lmf(package.resource_file(), progress, progress_handler)
            elif package.type == _ILI:
                _add_ili(package.resource_file(), progress)
            else:
                raise wn.Error(f'unknown package type: {package.type}')
    finally:
        progress.close()


</t>
<t tx="karstenw.20230225123218.20">def _collect_frames(lexicon: _AnyLexicon) -&gt; List[lmf.SyntacticBehaviour]:
    # WN-LMF 1.0 syntactic behaviours are on lexical entries, and in
    # WN-LMF 1.1 they are at the lexticon level with IDs. This
    # function normalizes the two variants.

    # IDs are not required and frame strings must be unique in a
    # lexicon, so lookup syntactic behaviours by the frame string
    synbhrs: Dict[str, lmf.SyntacticBehaviour] = {
        frame['subcategorizationFrame']: {
            'id': frame['id'],
            'subcategorizationFrame': frame['subcategorizationFrame'],
            'senses': frame.get('senses', []),
        }
        for frame in lexicon.get('frames', [])
    }
    # all relevant senses are collected into the 'senses' key
    id_senses_map = {sb['id']: sb['senses']
                     for sb in synbhrs.values() if sb.get('id')}
    for entry in _entries(lexicon):
        # for WN-LMF 1.1
        for sense in _local_senses(_senses(entry)):
            for sbid in sense.get('subcat', []):
                id_senses_map[sbid].append(sense['id'])
        # for WN-LMF 1.0
        if _is_external(entry) or not entry.get('frames'):
            continue
        entry = cast(lmf.LexicalEntry, entry)
        all_senses = [s['id'] for s in _senses(entry)]
        for frame in entry.get('frames', []):
            subcat_frame = frame['subcategorizationFrame']
            if subcat_frame not in synbhrs:
                synbhrs[subcat_frame] = {'subcategorizationFrame': subcat_frame,
                                         'senses': []}
            senses = frame.get('senses', []) or all_senses
            synbhrs[subcat_frame]['senses'].extend(senses)
    return list(synbhrs.values())


</t>
<t tx="karstenw.20230225123218.21">def _insert_syntactic_behaviours(
    synbhrs: Sequence[lmf.SyntacticBehaviour],
    lexid: int,
    lexidmap: _LexIdMap,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Syntactic Behaviours')

    query = 'INSERT INTO syntactic_behaviours VALUES (null,?,?,?)'
    sbdata = [(sb.get('id') or None, lexid, sb['subcategorizationFrame'])
              for sb in synbhrs]
    cur.executemany(query, sbdata)

    # syntactic behaviours don't have a required ID; index on frame
    framemap: Dict[str, List[str]] = {
        sb['subcategorizationFrame']: sb.get('senses', []) for sb in synbhrs
    }
    query = f'''
        INSERT INTO syntactic_behaviour_senses
        VALUES ((SELECT rowid
                   FROM syntactic_behaviours
                  WHERE lexicon_rowid=? AND frame=?),
                ({SENSE_QUERY}))
    '''
    sbsdata = [(lexid, frame, sid, lexidmap.get(sid, lexid))
               for frame in framemap
               for sid in framemap[frame]]
    cur.executemany(query, sbsdata)

    progress.update(len(synbhrs))


</t>
<t tx="karstenw.20230225123218.22">def _insert_sense_relations(
    lexicon: _AnyLexicon,
    lexid: int,
    lexidmap: _LexIdMap,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Sense Relations')
    # need to separate relations into those targeting senses vs synsets
    synset_ids = {ss['id'] for ss in _synsets(lexicon)}
    sense_ids = {s['id']
                 for e in _entries(lexicon)
                 for s in _senses(e)}
    s_s_rels = []
    s_ss_rels = []
    for entry in _entries(lexicon):
        for sense in _senses(entry):
            slid = lexidmap.get(sense['id'], lexid)
            for relation in sense.get('relations', []):
                target_id = relation['target']
                tlid = lexidmap.get(target_id, lexid)
                if target_id in sense_ids:
                    s_s_rels.append((sense['id'], slid, tlid, relation))
                elif target_id in synset_ids:
                    s_ss_rels.append((sense['id'], slid, tlid, relation))
                else:
                    raise wn.Error(
                        f'relation target is not a known sense or synset: {target_id}'
                    )
    hyperparams = [
        ('sense_relations', SENSE_QUERY, s_s_rels),
        ('sense_synset_relations', SYNSET_QUERY, s_ss_rels),
    ]
    for table, target_query, rels in hyperparams:
        query = f'''
            INSERT INTO {table}
            VALUES (null,?,({SENSE_QUERY}),({target_query}),({RELTYPE_QUERY}),?)
        '''
        for batch in _batch(rels):
            data = [
                (lexid,
                 sense_id, slid,
                 relation['target'], tlid,
                 relation['relType'],
                 relation['meta'])
                for sense_id, slid, tlid, relation in batch
            ]
            cur.executemany(query, data)
            progress.update(len(data))


</t>
<t tx="karstenw.20230225123218.23">def _insert_examples(
    objs: Sequence[Union[lmf.Sense, lmf.ExternalSense, lmf.Synset, lmf.ExternalSynset]],
    lexid: int,
    lexidmap: _LexIdMap,
    table: str,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; None:
    progress.set(status='Examples')
    if table == 'sense_examples':
        query = f'INSERT INTO {table} VALUES (null,?,({SENSE_QUERY}),?,?,?)'
    else:
        query = f'INSERT INTO {table} VALUES (null,?,({SYNSET_QUERY}),?,?,?)'
    for batch in _batch(objs):
        data = [
            (lexid,
             obj['id'], lexidmap.get(obj['id'], lexid),
             example['text'],
             example.get('language'),
             example['meta'])
            for obj in batch
            for example in obj.get('examples', [])
        ]
        # be careful of SQL injection here
        cur.executemany(query, data)
        progress.update(len(data))


</t>
<t tx="karstenw.20230225123218.24">def _add_ili(
    source: Path,
    progress: ProgressHandler,
) -&gt; None:
    query = f'''
        INSERT INTO ilis
        VALUES (null,?,({ILISTAT_QUERY}),?,null)
            ON CONFLICT(id) DO
               UPDATE SET status_rowid=excluded.status_rowid,
                          definition=excluded.definition
    '''
    with connect() as conn:
        cur = conn.cursor()

        progress.flash(f'Reading ILI file: {source!s}')
        ili = list(_ili.load(source))

        progress.flash('Updating ILI Status Names')
        statuses = set(info.get('status', 'active') for info in ili)
        cur.executemany('INSERT OR IGNORE INTO ili_statuses VALUES (null,?)',
                        [(stat,) for stat in sorted(statuses)])

        progress.set(count=0, total=len(ili), status='ILI')
        for batch in _batch(ili):
            data = [
                (info['ili'],
                 info.get('status', 'active'),
                 info.get('definition'))
                for info in batch
            ]
            cur.executemany(query, data)
            progress.update(len(data))


</t>
<t tx="karstenw.20230225123218.25">def remove(
    lexicon: str,
    progress_handler: Optional[Type[ProgressHandler]] = ProgressBar
) -&gt; None:
    """Remove lexicon(s) from the database.

    The *lexicon* argument is a :ref:`lexicon specifier
    &lt;lexicon-specifiers&gt;`. Note that this removes a lexicon and not a
    project, so the lexicons of projects containing multiple lexicons
    will need to be removed individually or, if applicable, a star
    specifier.

    The *progress_handler* parameter takes a subclass of
    :class:`wn.util.ProgressHandler`. An instance of the class will be
    created, used, and closed by this function.

    &gt;&gt;&gt; wn.remove('ewn:2019')  # removes a single lexicon
    &gt;&gt;&gt; wn.remove('*:1.3+omw')  # removes all lexicons with version 1.3+omw

    """
    if progress_handler is None:
        progress_handler = ProgressHandler
    progress = progress_handler(message='Removing', unit='\be5 operations')

    conn = connect()
    conn.set_progress_handler(progress.update, 100000)
    try:
        for rowid, id, _, _, _, _, version, *_ in find_lexicons(lexicon=lexicon):
            extensions = _find_all_extensions(rowid)

            with conn:

                for ext_id, ext_spec in reversed(extensions):
                    progress.set(status=f'{ext_spec} (extension)')
                    conn.execute('DELETE from lexicons WHERE rowid = ?', (ext_id,))
                    progress.flash(f'Removed {ext_spec}\n')

                spec = f'{id}:{version}'
                extra = f' (and {len(extensions)} extension(s))' if extensions else ''
                progress.set(status=f'{spec}', count=0)
                conn.execute('DELETE from lexicons WHERE rowid = ?', (rowid,))
                progress.flash(f'Removed {spec}{extra}\n')

    finally:
        progress.close()
        conn.set_progress_handler(None, 0)


</t>
<t tx="karstenw.20230225123218.26">def _find_all_extensions(rowid: int) -&gt; List[Tuple[int, str]]:
    exts: List[Tuple[int, str]] = []
    for ext_id in get_lexicon_extensions(rowid):
        lexinfo = get_lexicon(ext_id)
        exts.append((ext_id, f'{lexinfo[1]}:{lexinfo[6]}'))
    return exts


</t>
<t tx="karstenw.20230225123218.27">def _entries(lex: _AnyLexicon) -&gt; Sequence[_AnyEntry]: return lex.get('entries', [])
</t>
<t tx="karstenw.20230225123218.28">def _forms(e: _AnyEntry) -&gt; Sequence[_AnyForm]: return e.get('forms', [])
</t>
<t tx="karstenw.20230225123218.29">def _senses(e: _AnyEntry) -&gt; Sequence[_AnySense]: return e.get('senses', [])
</t>
<t tx="karstenw.20230225123218.3">def _add_lmf(
    source: Path,
    progress: ProgressHandler,
    progress_handler: Type[ProgressHandler],
) -&gt; None:
    with connect() as conn:
        cur = conn.cursor()
        # these two settings increase the risk of database corruption
        # if the system crashes during a write, but they should also
        # make inserts much faster
        cur.execute('PRAGMA synchronous = OFF')
        cur.execute('PRAGMA journal_mode = MEMORY')

        # abort if any lexicon in *source* is already added
        progress.flash(f'Checking {source!s}')
        all_infos = _precheck(source, cur)

        if not all_infos:
            progress.flash(f'{source}: No lexicons found')
            return
        for info in all_infos:
            skip = info.get('skip', '')
            if skip:
                id, ver, lbl = info['id'], info['version'], info['label']
                progress.flash(f'Skipping {id}:{ver} ({lbl}); {skip}\n')
        if all('skip' in info for info in all_infos):
            return

        # all clear, try to add them
        progress.flash(f'Reading {source!s}')
        resource = lmf.load(source, progress_handler)

        for lexicon, info in zip(resource['lexicons'], all_infos):
            if 'skip' in info:
                continue

            progress.flash('Updating lookup tables')
            _update_lookup_tables(lexicon, cur)

            progress.set(count=0, total=_sum_counts(lexicon))
            synsets: Sequence[_AnySynset] = _synsets(lexicon)
            entries: Sequence[_AnyEntry] = _entries(lexicon)
            synbhrs: Sequence[lmf.SyntacticBehaviour] = _collect_frames(lexicon)

            lexid, extid = _insert_lexicon(lexicon, info, cur, progress)

            lexidmap = _build_lexid_map(lexicon, lexid, extid)

            _insert_synsets(synsets, lexid, cur, progress)
            _insert_entries(entries, lexid, cur, progress)
            _insert_forms(entries, lexid, lexidmap, cur, progress)
            _insert_pronunciations(entries, lexid, lexidmap, cur, progress)
            _insert_tags(entries, lexid, lexidmap, cur, progress)
            _insert_senses(entries, synsets, lexid, lexidmap, cur, progress)
            _insert_adjpositions(entries, lexid, lexidmap, cur, progress)
            _insert_counts(entries, lexid, lexidmap, cur, progress)
            _insert_syntactic_behaviours(synbhrs, lexid, lexidmap, cur, progress)

            _insert_synset_relations(synsets, lexid, lexidmap, cur, progress)
            _insert_sense_relations(lexicon, lexid, lexidmap, cur, progress)

            _insert_synset_definitions(synsets, lexid, lexidmap, cur, progress)
            _insert_examples([sense for e in entries for sense in _senses(e)],
                             lexid, lexidmap, 'sense_examples', cur, progress)
            _insert_examples(synsets, lexid, lexidmap, 'synset_examples', cur, progress)

            progress.set(status='')  # clear type string
            progress.flash(
                f"Added {lexicon['id']}:{lexicon['version']} ({lexicon['label']})\n"
            )


</t>
<t tx="karstenw.20230225123218.30">def _synsets(lex: _AnyLexicon) -&gt; Sequence[_AnySynset]: return lex.get('synsets', [])


</t>
<t tx="karstenw.20230225123218.31">def _is_external(
    x: Union[_AnyForm, _AnyLemma, _AnyEntry, _AnySense, _AnySynset]
) -&gt; bool:
    return x.get('external', False) is True


</t>
<t tx="karstenw.20230225123218.32">def _local_synsets(synsets: Sequence[_AnySynset]) -&gt; Iterator[lmf.Synset]:
    for ss in synsets:
        if _is_external(ss):
            continue
        yield cast(lmf.Synset, ss)


</t>
<t tx="karstenw.20230225123218.33">def _local_entries(entries: Sequence[_AnyEntry]) -&gt; Iterator[lmf.LexicalEntry]:
    for e in entries:
        if _is_external(e):
            continue
        yield cast(lmf.LexicalEntry, e)


</t>
<t tx="karstenw.20230225123218.34">def _local_senses(senses: Sequence[_AnySense]) -&gt; Iterator[lmf.Sense]:
    for s in senses:
        if _is_external(s):
            continue
        yield cast(lmf.Sense, s)
</t>
<t tx="karstenw.20230225123218.4">def _precheck(source: Path, cur: sqlite3.Cursor) -&gt; List[Dict]:
    lexqry = 'SELECT * FROM lexicons WHERE id = :id AND version = :version'
    infos = lmf.scan_lexicons(source)
    for info in infos:
        base = info.get('extends')
        if cur.execute(lexqry, info).fetchone():
            info['skip'] = 'already added'
        if base and cur.execute(lexqry, base).fetchone() is None:
            id_, ver = base['id'], base['version']
            info['skip'] = f'base lexicon ({id_}:{ver}) not available'
    return infos


</t>
<t tx="karstenw.20230225123218.5">def _sum_counts(lex: _AnyLexicon) -&gt; int:
    ents = _entries(lex)
    locs = _local_entries(ents)
    lems = [e['lemma'] for e in locs if e.get('lemma')]
    frms = [f for e in ents for f in _forms(e)]
    sens = [s for e in ents for s in _senses(e)]
    syns = _synsets(lex)
    return sum([
        # lexical entries
        len(ents),
        len(lems),
        sum(len(lem.get('pronunciations', [])) for lem in lems),
        sum(len(lem.get('tags', [])) for lem in lems),
        len(frms),
        sum(len(frm.get('pronunciations', [])) for frm in frms),
        sum(len(frm.get('tags', [])) for frm in frms),
        # senses
        len(sens),
        sum(len(sen.get('relations', [])) for sen in sens),
        sum(len(sen.get('examples', [])) for sen in sens),
        sum(len(sen.get('counts', [])) for sen in sens),
        # synsets
        len(syns),
        sum(len(syn.get('definitions', [])) for syn in syns),
        sum(len(syn.get('relations', [])) for syn in syns),
        sum(len(syn.get('examples', [])) for syn in syns),
        # syntactic behaviours
        sum(len(ent.get('frames', [])) for ent in locs),
        len(lex.get('frames', [])),
    ])


</t>
<t tx="karstenw.20230225123218.6">def _update_lookup_tables(
    lexicon: _AnyLexicon,
    cur: sqlite3.Cursor
) -&gt; None:
    reltypes = set(rel['relType']
                   for ss in _synsets(lexicon)
                   for rel in ss.get('relations', []))
    reltypes.update(rel['relType']
                    for e in _entries(lexicon)
                    for s in _senses(e)
                    for rel in s.get('relations', []))
    cur.executemany('INSERT OR IGNORE INTO relation_types VALUES (null,?)',
                    [(rt,) for rt in sorted(reltypes)])
    lexfiles: Set[str] = {ss.get('lexfile', '')
                          for ss in _local_synsets(_synsets(lexicon))
                          if ss.get('lexfile')}
    cur.executemany('INSERT OR IGNORE INTO lexfiles VALUES (null,?)',
                    [(lf,) for lf in sorted(lexfiles)])


</t>
<t tx="karstenw.20230225123218.7">def _insert_lexicon(
    lexicon: _AnyLexicon,
    info: dict,
    cur: sqlite3.Cursor,
    progress: ProgressHandler
) -&gt; Tuple[int, int]:
    progress.set(status='Lexicon Info')
    cur.execute(
        'INSERT INTO lexicons VALUES (null,?,?,?,?,?,?,?,?,?,?,?)',
        (lexicon['id'],
         lexicon['label'],
         lexicon['language'],
         lexicon['email'],
         lexicon['license'],
         lexicon['version'],
         lexicon.get('url'),
         lexicon.get('citation'),
         lexicon.get('logo'),
         lexicon.get('meta'),
         False))
    lexid = cur.lastrowid

    if not isinstance(lexid, int):
        raise wn.Error('failed to insert lexicon')

    query = '''
        UPDATE lexicon_dependencies
           SET provider_rowid = ?
         WHERE provider_id = ? AND provider_version = ?
    '''
    cur.execute(query, (lexid, lexicon['id'], lexicon['version']))

    query = '''
        INSERT INTO {table}
        VALUES (:lid,
                :id,
                :version,
                :url,
                (SELECT rowid FROM lexicons WHERE id=:id AND version=:version))
    '''
    params = []
    for dep in lexicon.get('requires', []):
        param_dict = dict(dep)
        param_dict.setdefault('url', None)
        param_dict['lid'] = lexid
        params.append(param_dict)
    if params:
        cur.executemany(query.format(table='lexicon_dependencies'), params)

    if lexicon.get('extends'):
        lexicon = cast(lmf.LexiconExtension, lexicon)
        param_dict = dict(lexicon['extends'])
        param_dict.setdefault('url', None)
        param_dict['lid'] = lexid
        cur.execute(query.format(table='lexicon_extensions'), param_dict)
        extid = cur.execute(
            'SELECT rowid FROM lexicons WHERE id=? AND version=?',
            (param_dict['id'], param_dict['version'])
        ).fetchone()[0]
    else:
        extid = lexid

    return lexid, extid


_LexIdMap = Dict[str, int]


</t>
<t tx="karstenw.20230225123218.8">def _build_lexid_map(lexicon: _AnyLexicon, lexid: int, extid: int) -&gt; _LexIdMap:
    """Build a mapping of entity IDs to extended lexicon rowid."""
    lexidmap: _LexIdMap = {}
    if lexid != extid:
        lexidmap.update((e['id'], extid)
                        for e in _entries(lexicon)
                        if _is_external(e))
        lexidmap.update((s['id'], extid)
                        for e in _entries(lexicon)
                        for s in _senses(e)
                        if _is_external(s))
        lexidmap.update((ss['id'], extid)
                        for ss in _synsets(lexicon)
                        if _is_external(ss))
    return lexidmap


T = TypeVar('T')


</t>
<t tx="karstenw.20230225123218.9">def _batch(sequence: Iterable[T]) -&gt; Iterator[List[T]]:
    it = iter(sequence)
    batch = list(islice(it, 0, BATCH_SIZE))
    while len(batch):
        yield batch
        batch = list(islice(it, 0, BATCH_SIZE))


</t>
<t tx="karstenw.20230225123225.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230225123225.2">@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123225.3">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123226.1">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123226.2">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123226.3">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123226.4">@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123226.5">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123226.6">@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123227.1">@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123227.2">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123227.3">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123227.4">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123227.5">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123227.6">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123227.7">@language plsql

-- ILI : Interlingual Index

CREATE TABLE ilis (
    rowid INTEGER PRIMARY KEY,
    id TEXT NOT NULL,
    status_rowid INTEGER NOT NULL REFERENCES ili_statuses (rowid),
    definition TEXT,
    metadata META,
    UNIQUE (id)
);
CREATE INDEX ili_id_index ON ilis (id);

CREATE TABLE proposed_ilis (
    rowid INTEGER PRIMARY KEY,
    synset_rowid INTEGER REFERENCES synsets (rowid) ON DELETE CASCADE,
    definition TEXT,
    metadata META,
    UNIQUE (synset_rowid)
);
CREATE INDEX proposed_ili_synset_rowid_index ON proposed_ilis (synset_rowid);


-- Wordnet lexicons

CREATE TABLE lexicons (
    rowid INTEGER PRIMARY KEY,  -- unique database-internal id
    id TEXT NOT NULL,           -- user-facing id
    label TEXT NOT NULL,
    language TEXT NOT NULL,     -- bcp-47 language tag
    email TEXT NOT NULL,
    license TEXT NOT NULL,
    version TEXT NOT NULL,
    url TEXT,
    citation TEXT,
    logo TEXT,
    metadata META,
    modified BOOLEAN CHECK( modified IN (0, 1) ) DEFAULT 0 NOT NULL,
    UNIQUE (id, version)
);

CREATE TABLE lexicon_dependencies (
    dependent_rowid INTEGER NOT NULL REFERENCES lexicons (rowid) ON DELETE CASCADE,
    provider_id TEXT NOT NULL,
    provider_version TEXT NOT NULL,
    provider_url TEXT,
    provider_rowid INTEGER REFERENCES lexicons (rowid) ON DELETE SET NULL
);
CREATE INDEX lexicon_dependent_index ON lexicon_dependencies(dependent_rowid);

CREATE TABLE lexicon_extensions (
    extension_rowid INTEGER NOT NULL REFERENCES lexicons (rowid) ON DELETE CASCADE,
    base_id TEXT NOT NULL,
    base_version TEXT NOT NULL,
    base_url TEXT,
    base_rowid INTEGER REFERENCES lexicons (rowid),
    UNIQUE (extension_rowid, base_rowid)
);
CREATE INDEX lexicon_extension_index ON lexicon_extensions(extension_rowid);


-- Lexical Entries

/* The 'lemma' entity of a lexical entry is just a form, but it should
   be the only form with rank = 0. After that, rank can be used to
   indicate preference for a form. */


CREATE TABLE entries (
    rowid INTEGER PRIMARY KEY,
    id TEXT NOT NULL,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons (rowid) ON DELETE CASCADE,
    pos TEXT NOT NULL,
    metadata META,
    UNIQUE (id, lexicon_rowid)
);
CREATE INDEX entry_id_index ON entries (id);

CREATE TABLE forms (
    rowid INTEGER PRIMARY KEY,
    id TEXT,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons(rowid) ON DELETE CASCADE,
    entry_rowid INTEGER NOT NULL REFERENCES entries(rowid) ON DELETE CASCADE,
    form TEXT NOT NULL,
    normalized_form TEXT,
    script TEXT,
    rank INTEGER DEFAULT 1,  -- rank 0 is the preferred lemma
    UNIQUE (entry_rowid, form, script)
);
CREATE INDEX form_entry_index ON forms (entry_rowid);
CREATE INDEX form_index ON forms (form);
CREATE INDEX form_norm_index ON forms (normalized_form);

CREATE TABLE pronunciations (
    form_rowid INTEGER NOT NULL REFERENCES forms (rowid) ON DELETE CASCADE,
    value TEXT,
    variety TEXT,
    notation TEXT,
    phonemic BOOLEAN CHECK( phonemic IN (0, 1) ) DEFAULT 1 NOT NULL,
    audio TEXT
);
CREATE INDEX pronunciation_form_index ON pronunciations (form_rowid);

CREATE TABLE tags (
    form_rowid INTEGER NOT NULL REFERENCES forms (rowid) ON DELETE CASCADE,
    tag TEXT,
    category TEXT
);
CREATE INDEX tag_form_index ON tags (form_rowid);


-- Synsets

CREATE TABLE synsets (
    rowid INTEGER PRIMARY KEY,
    id TEXT NOT NULL,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons (rowid) ON DELETE CASCADE,
    ili_rowid INTEGER REFERENCES ilis (rowid),
    pos TEXT,
    lexicalized BOOLEAN CHECK( lexicalized IN (0, 1) ) DEFAULT 1 NOT NULL,
    lexfile_rowid INTEGER REFERENCES lexfiles (rowid),
    metadata META
);
CREATE INDEX synset_id_index ON synsets (id);
CREATE INDEX synset_ili_rowid_index ON synsets (ili_rowid);

CREATE TABLE synset_relations (
    rowid INTEGER PRIMARY KEY,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons (rowid) ON DELETE CASCADE,
    source_rowid INTEGER NOT NULL REFERENCES synsets(rowid) ON DELETE CASCADE,
    target_rowid INTEGER NOT NULL REFERENCES synsets(rowid) ON DELETE CASCADE,
    type_rowid INTEGER NOT NULL REFERENCES relation_types(rowid),
    metadata META
);
CREATE INDEX synset_relation_source_index ON synset_relations (source_rowid);
CREATE INDEX synset_relation_target_index ON synset_relations (target_rowid);

CREATE TABLE definitions (
    rowid INTEGER PRIMARY KEY,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons(rowid) ON DELETE CASCADE,
    synset_rowid INTEGER NOT NULL REFERENCES synsets(rowid) ON DELETE CASCADE,
    definition TEXT,
    language TEXT,  -- bcp-47 language tag
    sense_rowid INTEGER REFERENCES senses(rowid) ON DELETE SET NULL,
    metadata META
);
CREATE INDEX definition_rowid_index ON definitions (synset_rowid);
CREATE INDEX definition_sense_index ON definitions (sense_rowid);

CREATE TABLE synset_examples (
    rowid INTEGER PRIMARY KEY,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons(rowid) ON DELETE CASCADE,
    synset_rowid INTEGER NOT NULL REFERENCES synsets(rowid) ON DELETE CASCADE,
    example TEXT,
    language TEXT,  -- bcp-47 language tag
    metadata META
);
CREATE INDEX synset_example_rowid_index ON synset_examples(synset_rowid);


-- Senses

CREATE TABLE senses (
    rowid INTEGER PRIMARY KEY,
    id TEXT NOT NULL,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons(rowid) ON DELETE CASCADE,
    entry_rowid INTEGER NOT NULL REFERENCES entries(rowid) ON DELETE CASCADE,
    entry_rank INTEGER DEFAULT 1,
    synset_rowid INTEGER NOT NULL REFERENCES synsets(rowid) ON DELETE CASCADE,
    synset_rank INTEGER DEFAULT 1,
    lexicalized BOOLEAN CHECK( lexicalized IN (0, 1) ) DEFAULT 1 NOT NULL,
    metadata META
);
CREATE INDEX sense_id_index ON senses(id);
CREATE INDEX sense_entry_rowid_index ON senses (entry_rowid);
CREATE INDEX sense_synset_rowid_index ON senses (synset_rowid);

CREATE TABLE sense_relations (
    rowid INTEGER PRIMARY KEY,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons (rowid) ON DELETE CASCADE,
    source_rowid INTEGER NOT NULL REFERENCES senses(rowid) ON DELETE CASCADE,
    target_rowid INTEGER NOT NULL REFERENCES senses(rowid) ON DELETE CASCADE,
    type_rowid INTEGER NOT NULL REFERENCES relation_types(rowid),
    metadata META
);
CREATE INDEX sense_relation_source_index ON sense_relations (source_rowid);
CREATE INDEX sense_relation_target_index ON sense_relations (target_rowid);

CREATE TABLE sense_synset_relations (
    rowid INTEGER PRIMARY KEY,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons (rowid) ON DELETE CASCADE,
    source_rowid INTEGER NOT NULL REFERENCES senses(rowid) ON DELETE CASCADE,
    target_rowid INTEGER NOT NULL REFERENCES synsets(rowid) ON DELETE CASCADE,
    type_rowid INTEGER NOT NULL REFERENCES relation_types(rowid),
    metadata META
);
CREATE INDEX sense_synset_relation_source_index ON sense_synset_relations (source_rowid);
CREATE INDEX sense_synset_relation_target_index ON sense_synset_relations (target_rowid);

CREATE TABLE adjpositions (
    sense_rowid INTEGER NOT NULL REFERENCES senses(rowid) ON DELETE CASCADE,
    adjposition TEXT NOT NULL
);
CREATE INDEX adjposition_sense_index ON adjpositions (sense_rowid);

CREATE TABLE sense_examples (
    rowid INTEGER PRIMARY KEY,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons(rowid) ON DELETE CASCADE,
    sense_rowid INTEGER NOT NULL REFERENCES senses(rowid) ON DELETE CASCADE,
    example TEXT,
    language TEXT,  -- bcp-47 language tag
    metadata META
);
CREATE INDEX sense_example_index ON sense_examples (sense_rowid);

CREATE TABLE counts (
    rowid INTEGER PRIMARY KEY,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons(rowid) ON DELETE CASCADE,
    sense_rowid INTEGER NOT NULL REFERENCES senses(rowid) ON DELETE CASCADE,
    count INTEGER NOT NULL,
    metadata META
);
CREATE INDEX count_index ON counts(sense_rowid);


-- Syntactic Behaviours

CREATE TABLE syntactic_behaviours (
    rowid INTEGER PRIMARY KEY,
    id TEXT,
    lexicon_rowid INTEGER NOT NULL REFERENCES lexicons (rowid) ON DELETE CASCADE,
    frame TEXT NOT NULL,
    UNIQUE (lexicon_rowid, id),
    UNIQUE (lexicon_rowid, frame)
);
CREATE INDEX syntactic_behaviour_id_index ON syntactic_behaviours (id);

CREATE TABLE syntactic_behaviour_senses (
    syntactic_behaviour_rowid INTEGER NOT NULL REFERENCES syntactic_behaviours (rowid) ON DELETE CASCADE,
    sense_rowid INTEGER NOT NULL REFERENCES senses (rowid) ON DELETE CASCADE
);
CREATE INDEX syntactic_behaviour_sense_sb_index
    ON syntactic_behaviour_senses (syntactic_behaviour_rowid);
CREATE INDEX syntactic_behaviour_sense_sense_index
    ON syntactic_behaviour_senses (sense_rowid);


-- Lookup Tables

CREATE TABLE relation_types (
    rowid INTEGER PRIMARY KEY,
    type TEXT NOT NULL,
    UNIQUE (type)
);
CREATE INDEX relation_type_index ON relation_types (type);

CREATE TABLE ili_statuses (
    rowid INTEGER PRIMARY KEY,
    status TEXT NOT NULL,
    UNIQUE (status)
);
CREATE INDEX ili_status_index ON ili_statuses (status);

CREATE TABLE lexfiles (
    rowid INTEGER PRIMARY KEY,
    name TEXT NOT NULL,
    UNIQUE (name)
);
CREATE INDEX lexfile_index ON lexfiles (name);
</t>
<t tx="karstenw.20230225123228.1">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123228.2">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123228.3">@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123228.4">
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230225123241.1">
"""
Local configuration settings.
"""

from typing import Optional, Dict, Sequence, Any
from pathlib import Path

import tomli

from wn import ConfigurationError, ProjectError
from wn._types import AnyPath
from wn.constants import _WORDNET
from wn._util import resources, short_hash

# The directory where downloaded and added data will be stored.
DEFAULT_DATA_DIRECTORY = Path.home() / '.wn_data'
DATABASE_FILENAME = 'wn.db'


</t>
<t tx="karstenw.20230225123241.10">def add_project_version(
    self,
    id: str,
    version: str,
    url: Optional[str] = None,
    error: Optional[str] = None,
    license: Optional[str] = None,
) -&gt; None:
    """Add a new resource version for a project.

    Exactly one of *url* or *error* must be specified.

    Arguments:
        id: short identifier of the project
        version: version string of the resource
        url: space-separated list of web addresses for the resource
        license: link or name of the resource's license; if not
          given, the project's default license will be used.
        error: if set, the error message to use when the project
          is accessed

    """
    version_data: Dict[str, Any]
    if url and not error:
        version_data = {'resource_urls': url.split()}
    elif error and not url:
        version_data = {'error': error}
    elif url and error:
        raise ConfigurationError(f'{id}:{version} specifies both url and redirect')
    else:
        version_data = {}
    if license:
        version_data['license'] = license
    project = self._projects[id]
    project['versions'][version] = version_data

</t>
<t tx="karstenw.20230225123241.11">def get_project_info(self, arg: str) -&gt; Dict:
    """Return information about an indexed project version.

    If the project has been downloaded and cached, the ``"cache"``
    key will point to the path of the cached file, otherwise its
    value is ``None``.

    Arguments:
        arg: a project specifier

    Example:

        &gt;&gt;&gt; info = wn.config.get_project_info('oewn:2021')
        &gt;&gt;&gt; info['label']
        'Open English WordNet'

    """
    id, _, version = arg.partition(':')
    if id not in self._projects:
        raise ProjectError(f'no such project id: {id}')
    project: Dict = self._projects[id]
    if 'error' in project:
        raise ProjectError(project['error'])

    versions: Dict = project['versions']
    if not version or version == '*':
        version = next(iter(versions), '')
    if not version:
        raise ProjectError(f'no versions available for {id}')
    elif version not in versions:
        raise ProjectError(f'no such version: {version!r} ({id})')
    info = versions[version]
    if 'error' in info:
        raise ProjectError(info['error'])

    urls = info.get('resource_urls', [])

    return dict(
        id=id,
        version=version,
        type=project['type'],
        label=project['label'],
        language=project['language'],
        license=info.get('license', project.get('license')),
        resource_urls=urls,
        cache=_get_cache_path_for_urls(self, urls),
    )

</t>
<t tx="karstenw.20230225123241.2">class WNConfig:

    @others
</t>
<t tx="karstenw.20230225123241.3">def __init__(self):
    self._data_directory = DEFAULT_DATA_DIRECTORY
    self._projects = {}
    self._dbpath = self._data_directory / DATABASE_FILENAME
    self.allow_multithreading = False

</t>
<t tx="karstenw.20230225123241.4">@property
def data_directory(self) -&gt; Path:
    """The file system directory where Wn's data is stored."""
    dir = self._data_directory
    dir.mkdir(exist_ok=True)
    return dir

</t>
<t tx="karstenw.20230225123241.5">@data_directory.setter
def data_directory(self, path):
    dir = Path(path).expanduser()
    if dir.exists() and not dir.is_dir():
        raise ConfigurationError(f'path exists and is not a directory: {dir}')
    self._data_directory = dir
    self._dbpath = dir / DATABASE_FILENAME

</t>
<t tx="karstenw.20230225123241.6">@property
def database_path(self):
    """The path to the database file."""
    return self._dbpath

</t>
<t tx="karstenw.20230225123241.7">@property
def downloads_directory(self):
    """The file system directory where downloads are cached."""
    dir = self.data_directory / 'downloads'
    dir.mkdir(exist_ok=True)
    return dir

</t>
<t tx="karstenw.20230225123241.8">@property
def index(self) -&gt; Dict[str, Dict]:
    """The project index."""
    return self._projects

</t>
<t tx="karstenw.20230225123241.9">def add_project(
    self,
    id: str,
    type: str = _WORDNET,
    label: Optional[str] = None,
    language: Optional[str] = None,
    license: Optional[str] = None,
    error: Optional[str] = None,
) -&gt; None:
    """Add a new wordnet project to the index.

    Arguments:
        id: short identifier of the project
        type: project type (default 'wordnet')
        label: full name of the project
        language: `BCP 47`_ language code of the resource
        license: link or name of the project's default license
        error: if set, the error message to use when the project
          is accessed

    .. _BCP 47: https://en.wikipedia.org/wiki/IETF_language_tag
    """
    if id in self._projects:
        raise ValueError(f'project already added: {id}')
    self._projects[id] = {
        'type': type,
        'label': label,
        'language': language,
        'versions': {},
        'license': license,
    }
    if error:
        self._projects[id]['error'] = error

</t>
<t tx="karstenw.20230225123242.1">def get_cache_path(self, url: str) -&gt; Path:
    """Return the path for caching *url*.

    Note that in general this is just a path operation and does
    not signify that the file exists in the file system.

    """
    filename = short_hash(url)
    return self.downloads_directory / filename

</t>
<t tx="karstenw.20230225123242.2">def update(self, data: dict) -&gt; None:
    """Update the configuration with items in *data*.

    Items are only inserted or replaced, not deleted. If a project
    index is provided in the ``"index"`` key, then either the
    project must not already be indexed or any project fields
    (label, language, or license) that are specified must be equal
    to the indexed project.

    """
    if 'data_directory' in data:
        self.data_directory = data['data_directory']
    for id, project in data.get('index', {}).items():
        if id in self._projects:
            # validate that they are the same
            _project = self._projects[id]
            for attr in ('label', 'language', 'license'):
                if attr in project and project[attr] != _project[attr]:
                    raise ConfigurationError(f'{attr} mismatch for {id}')
        else:
            self.add_project(
                id,
                type=project.get('type', _WORDNET),
                label=project.get('label'),
                language=project.get('language'),
                license=project.get('license'),
                error=project.get('error'),
            )
        for version, info in project.get('versions', {}).items():
            if 'url' in info and 'error' in project:
                raise ConfigurationError(
                    f'{id}:{version} url specified with default error'
                )
            self.add_project_version(
                id,
                version,
                url=info.get('url'),
                license=info.get('license'),
                error=info.get('error'),
            )

</t>
<t tx="karstenw.20230225123242.3">def load_index(self, path: AnyPath) -&gt; None:
    """Load and update with the project index at *path*.

    The project index is a TOML_ file containing project and
    version information. For example:

    .. code-block:: toml

       [ewn]
         label = "Open English WordNet"
         language = "en"
         license = "https://creativecommons.org/licenses/by/4.0/"
         [ewn.versions.2019]
           url = "https://en-word.net/static/english-wordnet-2019.xml.gz"
         [ewn.versions.2020]
           url = "https://en-word.net/static/english-wordnet-2020.xml.gz"

    .. _TOML: https://toml.io

    """
    path = Path(path).expanduser()
    with path.open('rb') as indexfile:
        try:
            index = tomli.load(indexfile)
        except tomli.TOMLDecodeError as exc:
            raise ConfigurationError('malformed index file') from exc
    self.update({'index': index})


</t>
<t tx="karstenw.20230225123242.4">def _get_cache_path_for_urls(
    config: WNConfig,
    urls: Sequence[str],
) -&gt; Optional[Path]:
    for url in urls:
        path = config.get_cache_path(url)
        if path.is_file():
            return path
    return None


config = WNConfig()
with resources.path('wn', 'index.toml') as index_path:
    config.load_index(index_path)
</t>
<t tx="karstenw.20230225123250.1">
from typing import (
    Type,
    TypeVar,
    Callable,
    Optional,
    List,
    Tuple,
    Dict,
    Set,
    Sequence,
    Iterator,
)
import warnings
import textwrap

import wn
from wn._types import (
    Metadata,
    NormalizeFunction,
    LemmatizeFunction,
)
from wn._util import normalize_form
from wn._db import NON_ROWID
from wn._queries import (
    find_lexicons,
    find_ilis,
    find_proposed_ilis,
    find_entries,
    find_senses,
    find_synsets,
    get_lexicon,
    get_modified,
    get_lexicon_dependencies,
    get_lexicon_extension_bases,
    get_lexicon_extensions,
    get_form_pronunciations,
    get_form_tags,
    get_entry_senses,
    get_sense_relations,
    get_sense_synset_relations,
    get_synset_relations,
    get_synset_members,
    get_synsets_for_ilis,
    get_examples,
    get_definitions,
    get_syntactic_behaviours,
    get_metadata,
    get_lexicalized,
    get_adjposition,
    get_sense_counts,
    get_lexfile,
)
from wn import taxonomy

_INFERRED_SYNSET = '*INFERRED*'


</t>
<t tx="karstenw.20230225123250.10">def definition(self) -&gt; Optional[str]:
    return self._definition

</t>
<t tx="karstenw.20230225123250.11">def metadata(self) -&gt; Metadata:
    """Return the ILI's metadata."""
    table = 'proposed_ilis' if self.status == 'proposed' else 'ilis'
    return get_metadata(self._id, table)


</t>
<t tx="karstenw.20230225123250.12">class Lexicon(_DatabaseEntity):
    """A class representing a wordnet lexicon.

    Attributes:
        id: The lexicon's identifier.
        label: The full name of lexicon.
        language: The BCP 47 language code of lexicon.
        email: The email address of the wordnet maintainer.
        license: The URL or name of the wordnet's license.
        version: The version string of the resource.
        url: The project URL of the wordnet.
        citation: The canonical citation for the project.
        logo: A URL or path to a project logo.
    """
    __slots__ = ('id', 'label', 'language', 'email', 'license',
                 'version', 'url', 'citation', 'logo')
    __module__ = 'wn'

    _ENTITY_TYPE = 'lexicons'

    @others
</t>
<t tx="karstenw.20230225123250.13">def __init__(
    self,
    id: str,
    label: str,
    language: str,
    email: str,
    license: str,
    version: str,
    url: Optional[str] = None,
    citation: Optional[str] = None,
    logo: Optional[str] = None,
    _id: int = NON_ROWID,
):
    super().__init__(_id=_id)
    self.id = id
    self.label = label
    self.language = language
    self.email = email
    self.license = license
    self.version = version
    self.url = url
    self.citation = citation
    self.logo = logo

</t>
<t tx="karstenw.20230225123250.14">def __repr__(self):
    id, ver, lg = self.id, self.version, self.language
    return f'&lt;Lexicon {id}:{ver} [{lg}]&gt;'

</t>
<t tx="karstenw.20230225123250.15">def metadata(self) -&gt; Metadata:
    """Return the lexicon's metadata."""
    return get_metadata(self._id, 'lexicons')

</t>
<t tx="karstenw.20230225123250.16">def specifier(self) -&gt; str:
    """Return the *id:version* lexicon specifier."""
    return f'{self.id}:{self.version}'

</t>
<t tx="karstenw.20230225123250.17">def modified(self) -&gt; bool:
    """Return True if the lexicon has local modifications."""
    return get_modified(self._id)

</t>
<t tx="karstenw.20230225123250.18">def requires(self) -&gt; Dict[str, Optional['Lexicon']]:
    """Return the lexicon dependencies."""
    return dict(
        (f'{id}:{version}',
         None if _id is None else _to_lexicon(get_lexicon(_id)))
        for id, version, _, _id in get_lexicon_dependencies(self._id)
    )

</t>
<t tx="karstenw.20230225123250.19">def extends(self) -&gt; Optional['Lexicon']:
    """Return the lexicon this lexicon extends, if any.

    If this lexicon is not an extension, return None.
    """
    bases = get_lexicon_extension_bases(self._id, depth=1)
    if bases:
        return _to_lexicon(get_lexicon(bases[0]))
    return None

</t>
<t tx="karstenw.20230225123250.2">class _DatabaseEntity:
    __slots__ = '_id',

    _ENTITY_TYPE = ''

    @others
</t>
<t tx="karstenw.20230225123250.20">def extensions(self, depth: int = 1) -&gt; List['Lexicon']:
    """Return the list of lexicons extending this one.

    By default, only direct extensions are included. This is
    controlled by the *depth* parameter, which if you view
    extensions as children in a tree where the current lexicon is
    the root, *depth=1* are the immediate extensions. Increasing
    this number gets extensions of extensions, or setting it to a
    negative number gets all "descendant" extensions.

    """
    return [_to_lexicon(get_lexicon(rowid))
            for rowid in get_lexicon_extensions(self._id, depth=depth)]

</t>
<t tx="karstenw.20230225123250.21">def describe(self, full: bool = True) -&gt; str:
    """Return a formatted string describing the lexicon.

    The *full* argument (default: :python:`True`) may be set to
    :python:`False` to omit word and sense counts.

    Also see: :meth:`Wordnet.describe`

    """
    _id = self._id
    substrings: List[str] = [
        f'{self.specifier()}',
        f'  Label  : {self.label}',
        f'  URL    : {self.url}',
        f'  License: {self.license}',
    ]
    if full:
        substrings.extend([
            f'  Words  : {_desc_counts(find_entries, _id)}',
            f'  Senses : {sum(1 for _ in find_senses(lexicon_rowids=(_id,)))}',
        ])
    substrings.extend([
        f'  Synsets: {_desc_counts(find_synsets, _id)}',
        f'  ILIs   : {sum(1 for ili, *_ in find_ilis(lexicon_rowids=(_id,))):&gt;6}',
    ])
    return '\n'.join(substrings)


</t>
<t tx="karstenw.20230225123250.22">def _desc_counts(query: Callable, lexid: int) -&gt; str:
    count: Dict[str, int] = {}
    for _, pos, *_ in query(lexicon_rowids=(lexid,)):
        if pos not in count:
            count[pos] = 1
        else:
            count[pos] += 1
    subcounts = ', '.join(f'{pos}: {count[pos]}' for pos in sorted(count))
    return f'{sum(count.values()):&gt;6} ({subcounts})'


</t>
<t tx="karstenw.20230225123250.23">class _LexiconElement(_DatabaseEntity):
    __slots__ = '_lexid', '_wordnet'

    @others
</t>
<t tx="karstenw.20230225123250.24">def __init__(
    self,
    _lexid: int = NON_ROWID,
    _id: int = NON_ROWID,
    _wordnet: Optional['Wordnet'] = None
):
    super().__init__(_id=_id)
    self._lexid = _lexid  # Database-internal lexicon id
    if _wordnet is None:
        _wordnet = Wordnet()
    self._wordnet: 'Wordnet' = _wordnet

</t>
<t tx="karstenw.20230225123250.25">def lexicon(self):
    return _to_lexicon(get_lexicon(self._lexid))

</t>
<t tx="karstenw.20230225123250.26">def _get_lexicon_ids(self) -&gt; Tuple[int, ...]:
    if self._wordnet._default_mode:
        return tuple(
            {self._lexid}
            | set(get_lexicon_extension_bases(self._lexid))
            | set(get_lexicon_extensions(self._lexid))
        )
    else:
        return self._wordnet._lexicon_ids


</t>
<t tx="karstenw.20230225123250.27">class Pronunciation:
    """A class for word form pronunciations."""

    __slots__ = 'value', 'variety', 'notation', 'phonemic', 'audio'

    @others
</t>
<t tx="karstenw.20230225123250.28">def __init__(
    self,
    value: str,
    variety: Optional[str] = None,
    notation: Optional[str] = None,
    phonemic: bool = True,
    audio: Optional[str] = None,
):
    self.value = value
    self.variety = variety
    self.notation = notation
    self.phonemic = phonemic
    self.audio = audio


</t>
<t tx="karstenw.20230225123250.29">class Tag:
    """A general-purpose tag class for word forms."""
    __slots__ = 'tag', 'category',
    __module__ = 'wn'

    @others
</t>
<t tx="karstenw.20230225123250.3">def __init__(self, _id: int = NON_ROWID):
    self._id = _id        # Database-internal id (e.g., rowid)

</t>
<t tx="karstenw.20230225123250.30">def __init__(self, tag: str, category: str):
    self.tag = tag
    self.category = category

</t>
<t tx="karstenw.20230225123250.31">def __eq__(self, other):
    if not isinstance(other, Tag):
        return NotImplemented
    return self.tag == other.tag and self.category == other.category


</t>
<t tx="karstenw.20230225123250.32">class Form(str):
    """A word-form string with additional attributes."""
    __slots__ = '_id', 'id', 'script',
    __module__ = 'wn'

    _id: int
    id: Optional[str]
    script: Optional[str]

    @others
</t>
<t tx="karstenw.20230225123250.33">def __new__(
    cls,
    form: str,
    id: Optional[str] = None,
    script: Optional[str] = None,
    _id: int = NON_ROWID
):
    obj = str.__new__(cls, form)  # type: ignore
    obj.id = id
    obj.script = script
    obj._id = _id
    return obj

</t>
<t tx="karstenw.20230225123250.34">def __eq__(self, other):
    if isinstance(other, Form) and self.script != other.script:
        return False
    return str.__eq__(self, other)

</t>
<t tx="karstenw.20230225123250.35">def __hash__(self):
    return str.__hash__(self)

</t>
<t tx="karstenw.20230225123250.36">def pronunciations(self) -&gt; List[Pronunciation]:
    return [Pronunciation(*data) for data in get_form_pronunciations(self._id)]

</t>
<t tx="karstenw.20230225123250.37">def tags(self) -&gt; List[Tag]:
    return [Tag(tag, category) for tag, category in get_form_tags(self._id)]


</t>
<t tx="karstenw.20230225123250.38">class Word(_LexiconElement):
    """A class for words (also called lexical entries) in a wordnet."""
    __slots__ = 'id', 'pos', '_forms'
    __module__ = 'wn'

    _ENTITY_TYPE = 'entries'

    @others
T = TypeVar('T', bound='_Relatable')


</t>
<t tx="karstenw.20230225123250.39">def __init__(
    self,
    id: str,
    pos: str,
    forms: List[Tuple[str, Optional[str], Optional[str], int]],
    _lexid: int = NON_ROWID,
    _id: int = NON_ROWID,
    _wordnet: Optional['Wordnet'] = None
):
    super().__init__(_lexid=_lexid, _id=_id, _wordnet=_wordnet)
    self.id = id
    self.pos = pos
    self._forms = forms

</t>
<t tx="karstenw.20230225123250.4">def __eq__(self, other):
    if not isinstance(other, _DatabaseEntity):
        return NotImplemented
    # the _id of different kinds of entities, such as Synset and
    # Sense, can be the same, so make sure they are the same type
    # of object first
    return (self._ENTITY_TYPE == other._ENTITY_TYPE
            and self._id == other._id)

</t>
<t tx="karstenw.20230225123250.40">def __repr__(self) -&gt; str:
    return f'Word({self.id!r})'

</t>
<t tx="karstenw.20230225123250.41">def lemma(self) -&gt; Form:
    """Return the canonical form of the word.

    Example:

        &gt;&gt;&gt; wn.words('wolves')[0].lemma()
        'wolf'

    """
    return Form(*self._forms[0])

</t>
<t tx="karstenw.20230225123250.42">def forms(self) -&gt; List[Form]:
    """Return the list of all encoded forms of the word.

    Example:

        &gt;&gt;&gt; wn.words('wolf')[0].forms()
        ['wolf', 'wolves']

    """
    return [Form(*form_data) for form_data in self._forms]

</t>
<t tx="karstenw.20230225123250.43">def senses(self) -&gt; List['Sense']:
    """Return the list of senses of the word.

    Example:

        &gt;&gt;&gt; wn.words('zygoma')[0].senses()
        [Sense('ewn-zygoma-n-05292350-01')]

    """
    lexids = self._get_lexicon_ids()
    iterable = get_entry_senses(self._id, lexids)
    return [Sense(*sense_data, self._wordnet) for sense_data in iterable]

</t>
<t tx="karstenw.20230225123250.44">def metadata(self) -&gt; Metadata:
    """Return the word's metadata."""
    return get_metadata(self._id, 'entries')

</t>
<t tx="karstenw.20230225123250.45">def synsets(self) -&gt; List['Synset']:
    """Return the list of synsets of the word.

    Example:

        &gt;&gt;&gt; wn.words('addendum')[0].synsets()
        [Synset('ewn-06411274-n')]

    """
    return [sense.synset() for sense in self.senses()]

</t>
<t tx="karstenw.20230225123250.46">def derived_words(self) -&gt; List['Word']:
    """Return the list of words linked through derivations on the senses.

    Example:

        &gt;&gt;&gt; wn.words('magical')[0].derived_words()
        [Word('ewn-magic-n'), Word('ewn-magic-n')]

    """
    return [derived_sense.word()
            for sense in self.senses()
            for derived_sense in sense.get_related('derivation')]

</t>
<t tx="karstenw.20230225123250.47">def translate(
    self, lexicon: Optional[str] = None, *, lang: Optional[str] = None,
) -&gt; Dict['Sense', List['Word']]:
    """Return a mapping of word senses to lists of translated words.

    Arguments:
        lexicon: if specified, translate to words in the target lexicon(s)
        lang: if specified, translate to words with the language code

    Example:

        &gt;&gt;&gt; w = wn.words('water bottle', pos='n')[0]
        &gt;&gt;&gt; for sense, words in w.translate(lang='ja').items():
        ...     print(sense, [jw.lemma() for jw in words])
        ...
        Sense('ewn-water_bottle-n-04564934-01') ['']

    """
    result = {}
    for sense in self.senses():
        result[sense] = [
            t_sense.word()
            for t_sense in sense.translate(lang=lang, lexicon=lexicon)
        ]
    return result


</t>
<t tx="karstenw.20230225123250.48">class _Relatable(_LexiconElement):
    __slots__ = 'id',

    @others
</t>
<t tx="karstenw.20230225123250.49">def __init__(
    self,
    id: str,
    _lexid: int = NON_ROWID,
    _id: int = NON_ROWID,
    _wordnet: Optional['Wordnet'] = None
):
    super().__init__(_lexid=_lexid, _id=_id, _wordnet=_wordnet)
    self.id = id

</t>
<t tx="karstenw.20230225123250.5">def __lt__(self, other):
    if not isinstance(other, _DatabaseEntity):
        return NotImplemented
    elif self._ENTITY_TYPE != other._ENTITY_TYPE:
        return NotImplemented
    else:
        return self._id &lt; other._id

</t>
<t tx="karstenw.20230225123250.50">def relations(self: T, *args: str) -&gt; Dict[str, List[T]]:
    raise NotImplementedError

</t>
<t tx="karstenw.20230225123250.51">def get_related(self: T, *args: str) -&gt; List[T]:
    raise NotImplementedError

</t>
<t tx="karstenw.20230225123250.52">def closure(self: T, *args: str) -&gt; Iterator[T]:
    visited = set()
    queue = self.get_related(*args)
    while queue:
        relatable = queue.pop(0)
        if relatable.id not in visited:
            visited.add(relatable.id)
            yield relatable
            queue.extend(relatable.get_related(*args))

</t>
<t tx="karstenw.20230225123250.53">def relation_paths(
    self: T,
    *args: str,
    end: Optional[T] = None
) -&gt; Iterator[List[T]]:
    agenda: List[Tuple[List[T], Set[T]]] = [
        ([target], {self, target})
        for target in self.get_related(*args)
        if target._id != self._id  # avoid self loops?
    ]
    while agenda:
        path, visited = agenda.pop()
        if end is not None and path[-1] == end:
            yield path
        else:
            related = [target for target in path[-1].get_related(*args)
                       if target not in visited]
            if related:
                for synset in reversed(related):
                    new_path = list(path) + [synset]
                    new_visited = visited | {synset}
                    agenda.append((new_path, new_visited))
            elif end is None:
                yield path


</t>
<t tx="karstenw.20230225123250.54">class Synset(_Relatable):
    """Class for modeling wordnet synsets."""
    __slots__ = 'pos', '_ili'
    __module__ = 'wn'

    _ENTITY_TYPE = 'synsets'

    @others
</t>
<t tx="karstenw.20230225123250.55">def __init__(
    self,
    id: str,
    pos: str,
    ili: Optional[str] = None,
    _lexid: int = NON_ROWID,
    _id: int = NON_ROWID,
    _wordnet: Optional['Wordnet'] = None
):
    super().__init__(id=id, _lexid=_lexid, _id=_id, _wordnet=_wordnet)
    self.pos = pos
    self._ili = ili

</t>
<t tx="karstenw.20230225123250.56">@classmethod
def empty(
    cls,
    id: str,
    ili: Optional[str] = None,
    _lexid: int = NON_ROWID,
    _wordnet: Optional['Wordnet'] = None
):
    return cls(id, pos='', ili=ili, _lexid=_lexid, _wordnet=_wordnet)

@property
def ili(self):
    if self._ili:
        row = next(find_ilis(id=self._ili), None)
    else:
        row = next(find_proposed_ilis(synset_rowid=self._id), None)
    if row is not None:
        return ILI(*row)
    return None

</t>
<t tx="karstenw.20230225123250.57">def __hash__(self):
    # include ili and lexid in the hash so inferred synsets don't
    # hash the same
    return hash((self._ENTITY_TYPE, self._ili, self._lexid, self._id))

</t>
<t tx="karstenw.20230225123250.58">def __repr__(self) -&gt; str:
    return f'Synset({self.id!r})'

</t>
<t tx="karstenw.20230225123250.59">def definition(self) -&gt; Optional[str]:
    """Return the first definition found for the synset.

    Example:

        &gt;&gt;&gt; wn.synsets('cartwheel', pos='n')[0].definition()
        'a wheel that has wooden spokes and a metal rim'

    """
    lexids = self._get_lexicon_ids()
    return next(
        (text for text, _, _, _ in get_definitions(self._id, lexids)),
        None
    )

</t>
<t tx="karstenw.20230225123250.6">def __hash__(self):
    return hash((self._ENTITY_TYPE, self._id))


</t>
<t tx="karstenw.20230225123250.60">def examples(self) -&gt; List[str]:
    """Return the list of examples for the synset.

    Example:

        &gt;&gt;&gt; wn.synsets('orbital', pos='a')[0].examples()
        ['"orbital revolution"', '"orbital velocity"']

    """
    lexids = self._get_lexicon_ids()
    exs = get_examples(self._id, 'synsets', lexids)
    return [ex for ex, _, _ in exs]

</t>
<t tx="karstenw.20230225123250.61">def senses(self) -&gt; List['Sense']:
    """Return the list of sense members of the synset.

    Example:

        &gt;&gt;&gt; wn.synsets('umbrella', pos='n')[0].senses()
        [Sense('ewn-umbrella-n-04514450-01')]

    """
    lexids = self._get_lexicon_ids()
    iterable = get_synset_members(self._id, lexids)
    return [Sense(*sense_data, self._wordnet) for sense_data in iterable]

</t>
<t tx="karstenw.20230225123250.62">def lexicalized(self) -&gt; bool:
    """Return True if the synset is lexicalized."""
    return get_lexicalized(self._id, 'synsets')

</t>
<t tx="karstenw.20230225123250.63">def lexfile(self) -&gt; Optional[str]:
    """Return the lexicographer file name for this synset, if any."""
    return get_lexfile(self._id)

</t>
<t tx="karstenw.20230225123250.64">def metadata(self) -&gt; Metadata:
    """Return the synset's metadata."""
    return get_metadata(self._id, 'synsets')

</t>
<t tx="karstenw.20230225123250.65">def words(self) -&gt; List[Word]:
    """Return the list of words linked by the synset's senses.

    Example:

        &gt;&gt;&gt; wn.synsets('exclusive', pos='n')[0].words()
        [Word('ewn-scoop-n'), Word('ewn-exclusive-n')]

    """
    return [sense.word() for sense in self.senses()]

</t>
<t tx="karstenw.20230225123250.66">def lemmas(self) -&gt; List[Form]:
    """Return the list of lemmas of words for the synset.

    Example:

        &gt;&gt;&gt; wn.synsets('exclusive', pos='n')[0].words()
        ['scoop', 'exclusive']

    """
    return [w.lemma() for w in self.words()]

</t>
<t tx="karstenw.20230225123250.67">def relations(self, *args: str) -&gt; Dict[str, List['Synset']]:
    """Return a mapping of relation names to lists of synsets.

    One or more relation names may be given as positional
    arguments to restrict the relations returned. If no such
    arguments are given, all relations starting from the synset
    are returned.

    See :meth:`get_related` for getting a flat list of related
    synsets.

    Example:

        &gt;&gt;&gt; button_rels = wn.synsets('button')[0].relations()
        &gt;&gt;&gt; for relname, sslist in button_rels.items():
        ...     print(relname, [ss.lemmas() for ss in sslist])
        ...
        hypernym [['fixing', 'holdfast', 'fastener', 'fastening']]
        hyponym [['coat button'], ['shirt button']]

    """
    d: Dict[str, List['Synset']] = {}
    for relname, ss in self._get_relations(args):
        if relname in d:
            d[relname].append(ss)
        else:
            d[relname] = [ss]
    return d

</t>
<t tx="karstenw.20230225123250.68">def get_related(self, *args: str) -&gt; List['Synset']:
    """Return the list of related synsets.

    One or more relation names may be given as positional
    arguments to restrict the relations returned. If no such
    arguments are given, all relations starting from the synset
    are returned.

    This method does not preserve the relation names that lead to
    the related synsets. For a mapping of relation names to
    related synsets, see :meth:`relations`.

    Example:

        &gt;&gt;&gt; fulcrum = wn.synsets('fulcrum')[0]
        &gt;&gt;&gt; [ss.lemmas() for ss in fulcrum.get_related()]
        [['pin', 'pivot'], ['lever']]
    """
    return [ss for _, ss in self._get_relations(args)]

</t>
<t tx="karstenw.20230225123250.69">def _get_relations(self, args: Sequence[str]) -&gt; List[Tuple[str, 'Synset']]:
    targets: List[Tuple[str, 'Synset']] = []

    lexids = self._get_lexicon_ids()

    # first get relations from the current lexicon(s)
    if self._id != NON_ROWID:
        targets.extend(
            (row[0], Synset(*row[2:], self._wordnet))
            for row in get_synset_relations({self._id}, args, lexids)
            if row[5] in lexids
        )

    # then attempt to expand via ILI
    if self._ili is not None and self._wordnet and self._wordnet._expanded_ids:
        expids = self._wordnet._expanded_ids

        # get expanded relation
        expss = find_synsets(ili=self._ili, lexicon_rowids=expids)
        rowids = {rowid for _, _, _, _, rowid in expss} - {self._id, NON_ROWID}
        relations: Dict[str, Set[str]] = {}
        for rel_row in get_synset_relations(rowids, args, expids):
            rel_type, ili = rel_row[0], rel_row[4]
            if ili is not None:
                relations.setdefault(rel_type, set()).add(ili)

        # map back to target lexicons
        seen = {ss._id for _, ss in targets}
        for rel_type, ilis in relations.items():
            for row in get_synsets_for_ilis(ilis, lexicon_rowids=lexids):
                if row[-1] not in seen:
                    targets.append((rel_type, Synset(*row, self._wordnet)))

        # add empty synsets for ILIs without a target in lexids
        seen_ilis = {tgt._ili for _, tgt in targets}
        for rel_type, ilis in relations.items():
            unseen_ilis = ilis - seen_ilis
            for ili in unseen_ilis:
                ss = Synset.empty(
                    id=_INFERRED_SYNSET,
                    ili=ili,
                    _lexid=self._lexid,
                    _wordnet=self._wordnet
                )
                targets.append((rel_type, ss))

    return targets

</t>
<t tx="karstenw.20230225123250.7">class ILI(_DatabaseEntity):
    """A class for interlingual indices."""
    __slots__ = 'id', 'status', '_definition'
    __module__ = 'wn'

    @others
</t>
<t tx="karstenw.20230225123250.70">def hypernym_paths(self, simulate_root: bool = False) -&gt; List[List['Synset']]:
    """Return the list of hypernym paths to a root synset."""
    return taxonomy.hypernym_paths(self, simulate_root=simulate_root)

</t>
<t tx="karstenw.20230225123250.71">def min_depth(self, simulate_root: bool = False) -&gt; int:
    """Return the minimum taxonomy depth of the synset."""
    return taxonomy.min_depth(self, simulate_root=simulate_root)

</t>
<t tx="karstenw.20230225123250.72">def max_depth(self, simulate_root: bool = False) -&gt; int:
    """Return the maximum taxonomy depth of the synset."""
    return taxonomy.max_depth(self, simulate_root=simulate_root)

</t>
<t tx="karstenw.20230225123250.73">def shortest_path(
        self, other: 'Synset', simulate_root: bool = False
) -&gt; List['Synset']:
    """Return the shortest path from the synset to the *other* synset."""
    return taxonomy.shortest_path(
        self, other, simulate_root=simulate_root
    )

</t>
<t tx="karstenw.20230225123250.74">def common_hypernyms(
        self, other: 'Synset', simulate_root: bool = False
) -&gt; List['Synset']:
    """Return the common hypernyms for the current and *other* synsets."""
    return taxonomy.common_hypernyms(
        self, other, simulate_root=simulate_root
    )

</t>
<t tx="karstenw.20230225123250.75">def lowest_common_hypernyms(
        self, other: 'Synset', simulate_root: bool = False
) -&gt; List['Synset']:
    """Return the common hypernyms furthest from the root."""
    return taxonomy.lowest_common_hypernyms(
        self, other, simulate_root=simulate_root
    )

</t>
<t tx="karstenw.20230225123250.76">def holonyms(self) -&gt; List['Synset']:
    """Return the list of synsets related by any holonym relation.

    Any of the following relations are traversed: ``holonym``,
    ``holo_location``, ``holo_member``, ``holo_part``,
    ``holo_portion``, ``holo_substance``.

    """
    return self.get_related(
        'holonym',
        'holo_location',
        'holo_member',
        'holo_part',
        'holo_portion',
        'holo_substance',
    )

</t>
<t tx="karstenw.20230225123250.77">def meronyms(self) -&gt; List['Synset']:
    """Return the list of synsets related by any meronym relation.

    Any of the following relations are traversed: ``meronym``,
    ``mero_location``, ``mero_member``, ``mero_part``,
    ``mero_portion``, ``mero_substance``.

    """
    return self.get_related(
        'meronym',
        'mero_location',
        'mero_member',
        'mero_part',
        'mero_portion',
        'mero_substance',
    )

</t>
<t tx="karstenw.20230225123250.78">def hypernyms(self) -&gt; List['Synset']:
    """Return the list of synsets related by any hypernym relation.

    Both the ``hypernym`` and ``instance_hypernym`` relations are
    traversed.

    """
    return self.get_related(
        'hypernym',
        'instance_hypernym'
    )

</t>
<t tx="karstenw.20230225123250.79">def hyponyms(self) -&gt; List['Synset']:
    """Return the list of synsets related by any hyponym relation.

    Both the ``hyponym`` and ``instance_hyponym`` relations are
    traversed.

    """
    return self.get_related(
        'hyponym',
        'instance_hyponym'
    )

</t>
<t tx="karstenw.20230225123250.8">def __init__(
    self,
    id: Optional[str],
    status: str,
    definition: Optional[str] = None,
    _id: int = NON_ROWID,
):
    super().__init__(_id=_id)
    self.id = id
    self.status = status
    self._definition = definition

</t>
<t tx="karstenw.20230225123250.80">def translate(
    self,
    lexicon: Optional[str] = None,
    *,
    lang: Optional[str] = None
) -&gt; List['Synset']:
    """Return a list of translated synsets.

    Arguments:
        lexicon: if specified, translate to synsets in the target lexicon(s)
        lang: if specified, translate to synsets with the language code

    Example:

        &gt;&gt;&gt; es = wn.synsets('araa', lang='es')[0]
        &gt;&gt;&gt; en = es.translate(lexicon='ewn')[0]
        &gt;&gt;&gt; en.lemmas()
        ['spider']

    """
    ili = self._ili
    if not ili:
        return []
    return synsets(ili=ili, lang=lang, lexicon=lexicon)


</t>
<t tx="karstenw.20230225123250.81">class Count(int):
    """A count of sense occurrences in some corpus."""
    __module__ = 'wn'

    _id: int

    @others
</t>
<t tx="karstenw.20230225123250.82">def __new__(cls, value, _id: int = NON_ROWID):
    obj = int.__new__(cls, value)  # type: ignore
    obj._id = _id
    return obj

</t>
<t tx="karstenw.20230225123250.83">def metadata(self) -&gt; Metadata:
    """Return the count's metadata."""
    return get_metadata(self._id, 'counts')


</t>
<t tx="karstenw.20230225123250.84">class Sense(_Relatable):
    """Class for modeling wordnet senses."""
    __slots__ = '_entry_id', '_synset_id'
    __module__ = 'wn'

    _ENTITY_TYPE = 'senses'

    @others
# Useful for factory functions of Word, Sense, or Synset
C = TypeVar('C', Word, Sense, Synset)


</t>
<t tx="karstenw.20230225123250.85">def __init__(
    self,
    id: str,
    entry_id: str,
    synset_id: str,
    _lexid: int = NON_ROWID,
    _id: int = NON_ROWID,
    _wordnet: Optional['Wordnet'] = None
):
    super().__init__(id=id, _lexid=_lexid, _id=_id, _wordnet=_wordnet)
    self._entry_id = entry_id
    self._synset_id = synset_id

</t>
<t tx="karstenw.20230225123250.86">def __repr__(self) -&gt; str:
    return f'Sense({self.id!r})'

</t>
<t tx="karstenw.20230225123250.87">def word(self) -&gt; Word:
    """Return the word of the sense.

    Example:

        &gt;&gt;&gt; wn.senses('spigot')[0].word()
        Word('pwn-spigot-n')

    """
    return self._wordnet.word(id=self._entry_id)

</t>
<t tx="karstenw.20230225123250.9">def __repr__(self) -&gt; str:
    return f'ILI({repr(self.id) if self.id else "*PROPOSED*"})'

</t>
<t tx="karstenw.20230225123251.1">def synset(self) -&gt; Synset:
    """Return the synset of the sense.

    Example:

        &gt;&gt;&gt; wn.senses('spigot')[0].synset()
        Synset('pwn-03325088-n')

    """
    return self._wordnet.synset(id=self._synset_id)

</t>
<t tx="karstenw.20230225123251.10">def _get_relations(self, args: Sequence[str]) -&gt; List[Tuple[str, 'Sense']]:
    lexids = self._get_lexicon_ids()
    iterable = get_sense_relations(self._id, args, lexids)
    return [(relname, Sense(sid, eid, ssid, lexid, rowid, self._wordnet))
            for relname, _, sid, eid, ssid, lexid, rowid in iterable
            if lexids is None or lexid in lexids]

</t>
<t tx="karstenw.20230225123251.11">def get_related_synsets(self, *args: str) -&gt; List[Synset]:
    """Return a list of related synsets."""
    lexids = self._get_lexicon_ids()
    iterable = get_sense_synset_relations(self._id, args, lexids)
    return [Synset(ssid, pos, ili, lexid, rowid, self._wordnet)
            for _, _, ssid, pos, ili, lexid, rowid in iterable
            if lexids is None or lexid in lexids]

</t>
<t tx="karstenw.20230225123251.12">def translate(
    self,
    lexicon: Optional[str] = None,
    *,
    lang: Optional[str] = None
) -&gt; List['Sense']:
    """Return a list of translated senses.

    Arguments:
        lexicon: if specified, translate to senses in the target lexicon(s)
        lang: if specified, translate to senses with the language code

    Example:

        &gt;&gt;&gt; en = wn.senses('petiole', lang='en')[0]
        &gt;&gt;&gt; pt = en.translate(lang='pt')[0]
        &gt;&gt;&gt; pt.word().lemma()
        'pecolo'

    """
    synset = self.synset()
    return [t_sense
            for t_synset in synset.translate(lang=lang, lexicon=lexicon)
            for t_sense in t_synset.senses()]


</t>
<t tx="karstenw.20230225123251.13">class Wordnet:

    """Class for interacting with wordnet data.

    A wordnet object acts essentially as a filter by first selecting
    matching lexicons and then searching only within those lexicons
    for later queries. On instantiation, a *lang* argument is a `BCP
    47`_ language code that restricts the selected lexicons to those
    whose language matches the given code. A *lexicon* argument is a
    space-separated list of lexicon specifiers that more directly
    selects lexicons by their ID and version; this is preferable when
    there are multiple lexicons in the same language or multiple
    version with the same ID.

    Some wordnets were created by translating the words from a larger
    wordnet, namely the Princeton WordNet, and then relying on the
    larger wordnet for structural relations. An *expand* argument is a
    second space-separated list of lexicon specifiers which are used
    for traversing relations, but not as the results of
    queries. Setting *expand* to an empty string (:python:`expand=''`)
    disables expand lexicons.

    The *normalizer* argument takes a callable that normalizes word
    forms in order to expand the search. The default function
    downcases the word and removes diacritics via NFKD_ normalization
    so that, for example, searching for *san jos* in the English
    WordNet will find the entry for *San Jose*. Setting *normalizer*
    to :python:`None` disables normalization and forces exact-match
    searching.

    The *lemmatizer* argument may be :python:`None`, which is the
    default and disables lemmatizer-based query expansion, or a
    callable that takes a word form and optional part of speech and
    returns base forms of the original word. To support lemmatizers
    that use the wordnet for instantiation, such as :mod:`wn.morphy`,
    the lemmatizer may be assigned to the :attr:`lemmatizer` attribute
    after creation.

    If the *search_all_forms* argument is :python:`True` (the
    default), searches of word forms consider all forms in the
    lexicon; if :python:`False`, only lemmas are searched. Non-lemma
    forms may include, depending on the lexicon, morphological
    exceptions, alternate scripts or spellings, etc.

    .. _BCP 47: https://en.wikipedia.org/wiki/IETF_language_tag
    .. _NFKD: https://en.wikipedia.org/wiki/Unicode_equivalence#Normal_forms

    Attributes:

        lemmatizer: A lemmatization function or :python:`None`.

    """

    __slots__ = ('_lexicons', '_lexicon_ids', '_expanded', '_expanded_ids',
                 '_default_mode', '_normalizer', 'lemmatizer',
                 '_search_all_forms',)
    __module__ = 'wn'

    @others
</t>
<t tx="karstenw.20230225123251.14">def __init__(
    self,
    lexicon: Optional[str] = None,
    *,
    lang: Optional[str] = None,
    expand: Optional[str] = None,
    normalizer: Optional[NormalizeFunction] = normalize_form,
    lemmatizer: Optional[LemmatizeFunction] = None,
    search_all_forms: bool = True,
):
    # default mode means any lexicon is searched or expanded upon,
    # but relation traversals only target the source's lexicon
    self._default_mode = (not lexicon and not lang)

    lexs = list(find_lexicons(lexicon or '*', lang=lang))
    self._lexicons: Tuple[Lexicon, ...] = tuple(map(_to_lexicon, lexs))
    self._lexicon_ids: Tuple[int, ...] = tuple(lx._id for lx in self._lexicons)

    self._expanded: Tuple[Lexicon, ...] = ()
    if expand is None:
        if self._default_mode:
            expand = '*'
        else:
            deps = [(id, ver, _id)
                    for lex in self._lexicons
                    for id, ver, _, _id in get_lexicon_dependencies(lex._id)]
            # warn only if a dep is missing and a lexicon was specified
            if not self._default_mode:
                missing = ' '.join(
                    f'{id}:{ver}' for id, ver, _id in deps if _id is None
                )
                if missing:
                    warnings.warn(
                        f'lexicon dependencies not available: {missing}',
                        wn.WnWarning
                    )
            expand = ' '.join(
                f'{id}:{ver}' for id, ver, _id in deps if _id is not None
            )
    if expand:
        self._expanded = tuple(map(_to_lexicon, find_lexicons(lexicon=expand)))
    self._expanded_ids: Tuple[int, ...] = tuple(lx._id for lx in self._expanded)

    self._normalizer = normalizer
    self.lemmatizer = lemmatizer
    self._search_all_forms = search_all_forms

</t>
<t tx="karstenw.20230225123251.15">def lexicons(self) -&gt; List[Lexicon]:
    """Return the list of lexicons covered by this wordnet."""
    return list(self._lexicons)

</t>
<t tx="karstenw.20230225123251.16">def expanded_lexicons(self) -&gt; List[Lexicon]:
    """Return the list of expand lexicons for this wordnet."""
    return list(self._expanded)

</t>
<t tx="karstenw.20230225123251.17">def word(self, id: str) -&gt; Word:
    """Return the first word in this wordnet with identifier *id*."""
    iterable = find_entries(id=id, lexicon_rowids=self._lexicon_ids)
    try:
        return Word(*next(iterable), self)
    except StopIteration:
        raise wn.Error(f'no such lexical entry: {id}')

</t>
<t tx="karstenw.20230225123251.18">def words(
    self,
    form: Optional[str] = None,
    pos: Optional[str] = None
) -&gt; List[Word]:
    """Return the list of matching words in this wordnet.

    Without any arguments, this function returns all words in the
    wordnet's selected lexicons. A *form* argument restricts the
    words to those matching the given word form, and *pos*
    restricts words by their part of speech.

    """
    return _find_helper(self, Word, find_entries, form, pos)

</t>
<t tx="karstenw.20230225123251.19">def synset(self, id: str) -&gt; Synset:
    """Return the first synset in this wordnet with identifier *id*."""
    iterable = find_synsets(id=id, lexicon_rowids=self._lexicon_ids)
    try:
        return Synset(*next(iterable), self)
    except StopIteration:
        raise wn.Error(f'no such synset: {id}')

</t>
<t tx="karstenw.20230225123251.2">def examples(self) -&gt; List[str]:
    """Return the list of examples for the sense."""
    lexids = self._get_lexicon_ids()
    exs = get_examples(self._id, 'senses', lexids)
    return [ex for ex, _, _ in exs]

</t>
<t tx="karstenw.20230225123251.20">def synsets(
    self,
    form: Optional[str] = None,
    pos: Optional[str] = None,
    ili: Optional[str] = None
) -&gt; List[Synset]:
    """Return the list of matching synsets in this wordnet.

    Without any arguments, this function returns all synsets in
    the wordnet's selected lexicons. A *form* argument restricts
    synsets to those whose member words match the given word
    form. A *pos* argument restricts synsets to those with the
    given part of speech. An *ili* argument restricts synsets to
    those with the given interlingual index; generally this should
    select a unique synset within a single lexicon.

    """
    return _find_helper(self, Synset, find_synsets, form, pos, ili=ili)

</t>
<t tx="karstenw.20230225123251.21">def sense(self, id: str) -&gt; Sense:
    """Return the first sense in this wordnet with identifier *id*."""
    iterable = find_senses(id=id, lexicon_rowids=self._lexicon_ids)
    try:
        return Sense(*next(iterable), self)
    except StopIteration:
        raise wn.Error(f'no such sense: {id}')

</t>
<t tx="karstenw.20230225123251.22">def senses(
    self,
    form: Optional[str] = None,
    pos: Optional[str] = None
) -&gt; List[Sense]:
    """Return the list of matching senses in this wordnet.

    Without any arguments, this function returns all senses in the
    wordnet's selected lexicons. A *form* argument restricts the
    senses to those whose word matches the given word form, and
    *pos* restricts senses by their word's part of speech.

    """
    return _find_helper(self, Sense, find_senses, form, pos)

</t>
<t tx="karstenw.20230225123251.23">def ili(self, id: str) -&gt; ILI:
    """Return the first ILI in this wordnet with identifer *id*."""
    iterable = find_ilis(id=id, lexicon_rowids=self._lexicon_ids)
    try:
        return ILI(*next(iterable))
    except StopIteration:
        raise wn.Error(f'no such ILI: {id}')

</t>
<t tx="karstenw.20230225123251.24">def ilis(self, status: Optional[str] = None) -&gt; List[ILI]:
    """Return the list of ILIs in this wordnet.

    If *status* is given, only return ILIs with a matching status.

    """
    iterable = find_ilis(status=status, lexicon_rowids=self._lexicon_ids)
    return [ILI(*ili_data) for ili_data in iterable]

</t>
<t tx="karstenw.20230225123251.25">def describe(self) -&gt; str:
    """Return a formatted string describing the lexicons in this wordnet.

    Example:

        &gt;&gt;&gt; oewn = wn.Wordnet('oewn:2021')
        &gt;&gt;&gt; print(oewn.describe())
        Primary lexicons:
          oewn:2021
            Label  : Open English WordNet
            URL    : https://github.com/globalwordnet/english-wordnet
            License: https://creativecommons.org/licenses/by/4.0/
            Words  : 163161 (a: 8386, n: 123456, r: 4481, s: 15231, v: 11607)
            Senses : 211865
            Synsets: 120039 (a: 7494, n: 84349, r: 3623, s: 10727, v: 13846)
            ILIs   : 120039

    """
    substrings = ['Primary lexicons:']
    for lex in self.lexicons():
        substrings.append(textwrap.indent(lex.describe(), '  '))
    expanded = self.expanded_lexicons()
    if expanded:
        substrings.append('Expand lexicons:')
        for lex in self.expanded_lexicons():
            substrings.append(textwrap.indent(lex.describe(full=False), '  '))
    return '\n'.join(substrings)


</t>
<t tx="karstenw.20230225123251.26">def _to_lexicon(data) -&gt; Lexicon:
    rowid, id, label, language, email, license, version, url, citation, logo = data
    return Lexicon(
        id,
        label,
        language,
        email,
        license,
        version,
        url=url,
        citation=citation,
        logo=logo,
        _id=rowid
    )


</t>
<t tx="karstenw.20230225123251.27">def _find_helper(
    w: Wordnet,
    cls: Type[C],
    query_func: Callable,
    form: Optional[str],
    pos: Optional[str],
    ili: Optional[str] = None
) -&gt; List[C]:
    """Return the list of matching wordnet entities.

    If the wordnet has a normalizer and the search includes a word
    form, the original word form is searched against both the
    original and normalized columns in the database. Then, if no
    results are found, the search is repeated with the normalized
    form. If the wordnet does not have a normalizer, only exact
    string matches are used.

    """
    kwargs: Dict = {
        'lexicon_rowids': w._lexicon_ids,
        'search_all_forms': w._search_all_forms,
    }
    if ili is not None:
        kwargs['ili'] = ili

    # easy case is when there is no form
    if form is None:
        return [cls(*data, w)  # type: ignore
                for data in query_func(pos=pos, **kwargs)]

    # if there's a form, we may need to lemmatize and normalize
    lemmatize = w.lemmatizer
    normalize = w._normalizer
    kwargs['normalized'] = bool(normalize)

    forms = lemmatize(form, pos) if lemmatize else {}
    # if no lemmatizer or word not covered by lemmatizer, back off to
    # the original form and pos
    if not forms:
        forms = {pos: {form}}

    # we want unique results here, but a set can make the order
    # erratic, so filter manually later
    results = [
        cls(*data, w)  # type: ignore
        for _pos, _forms in forms.items()
        for data in query_func(forms=_forms, pos=_pos, **kwargs)
    ]
    if not results and normalize:
        results = [
            cls(*data, w)  # type: ignore
            for _pos, _forms in forms.items()
            for data in query_func(
                forms=[normalize(f) for f in _forms], pos=_pos, **kwargs
            )
        ]
    unique_results: List[C] = []
    seen: Set[C] = set()
    for result in results:
        if result not in seen:
            unique_results.append(result)
            seen.add(result)
    return unique_results


</t>
<t tx="karstenw.20230225123251.28">def projects() -&gt; List[Dict]:
    """Return the list of indexed projects.

    This returns the same dictionaries of information as
    :meth:`wn.config.get_project_info
    &lt;wn._config.WNConfig.get_project_info&gt;`, but for all indexed
    projects.

    Example:

        &gt;&gt;&gt; infos = wn.projects()
        &gt;&gt;&gt; len(infos)
        36
        &gt;&gt;&gt; infos[0]['label']
        'Open English WordNet'

    """
    index = wn.config.index
    return [
        wn.config.get_project_info(f'{project_id}:{version}')
        for project_id, project_info in index.items()
        for version in project_info.get('versions', [])
        if 'resource_urls' in project_info['versions'][version]
    ]


</t>
<t tx="karstenw.20230225123251.29">def lexicons(
    *,
    lexicon: Optional[str] = None,
    lang: Optional[str] = None
) -&gt; List[Lexicon]:
    """Return the lexicons matching a language or lexicon specifier.

    Example:

        &gt;&gt;&gt; wn.lexicons(lang='en')
        [&lt;Lexicon ewn:2020 [en]&gt;, &lt;Lexicon pwn:3.0 [en]&gt;]

    """
    try:
        w = Wordnet(lang=lang, lexicon=lexicon)
    except wn.Error:
        return []
    else:
        return w.lexicons()


</t>
<t tx="karstenw.20230225123251.3">def lexicalized(self) -&gt; bool:
    """Return True if the sense is lexicalized."""
    return get_lexicalized(self._id, 'senses')

</t>
<t tx="karstenw.20230225123251.30">def word(
    id: str,
    *,
    lexicon: Optional[str] = None,
    lang: Optional[str] = None
) -&gt; Word:
    """Return the word with *id* in *lexicon*.

    This will create a :class:`Wordnet` object using the *lang* and
    *lexicon* arguments. The *id* argument is then passed to the
    :meth:`Wordnet.word` method.

    &gt;&gt;&gt; wn.word('ewn-cell-n')
    Word('ewn-cell-n')

    """
    return Wordnet(lang=lang, lexicon=lexicon).word(id=id)


</t>
<t tx="karstenw.20230225123251.31">def words(
    form: Optional[str] = None,
    pos: Optional[str] = None,
    *,
    lexicon: Optional[str] = None,
    lang: Optional[str] = None,
) -&gt; List[Word]:
    """Return the list of matching words.

    This will create a :class:`Wordnet` object using the *lang* and
    *lexicon* arguments. The remaining arguments are passed to the
    :meth:`Wordnet.words` method.

    &gt;&gt;&gt; len(wn.words())
    282902
    &gt;&gt;&gt; len(wn.words(pos='v'))
    34592
    &gt;&gt;&gt; wn.words(form="scurry")
    [Word('ewn-scurry-n'), Word('ewn-scurry-v')]

    """
    return Wordnet(lang=lang, lexicon=lexicon).words(form=form, pos=pos)


</t>
<t tx="karstenw.20230225123251.32">def synset(
    id: str,
    *,
    lexicon: Optional[str] = None,
    lang: Optional[str] = None
) -&gt; Synset:
    """Return the synset with *id* in *lexicon*.

    This will create a :class:`Wordnet` object using the *lang* and
    *lexicon* arguments. The *id* argument is then passed to the
    :meth:`Wordnet.synset` method.

    &gt;&gt;&gt; wn.synset('ewn-03311152-n')
    Synset('ewn-03311152-n')

    """
    return Wordnet(lang=lang, lexicon=lexicon).synset(id=id)


</t>
<t tx="karstenw.20230225123251.33">def synsets(
    form: Optional[str] = None,
    pos: Optional[str] = None,
    ili: Optional[str] = None,
    *,
    lexicon: Optional[str] = None,
    lang: Optional[str] = None,
) -&gt; List[Synset]:
    """Return the list of matching synsets.

    This will create a :class:`Wordnet` object using the *lang* and
    *lexicon* arguments. The remaining arguments are passed to the
    :meth:`Wordnet.synsets` method.

    &gt;&gt;&gt; len(wn.synsets('couch'))
    4
    &gt;&gt;&gt; wn.synsets('couch', pos='v')
    [Synset('ewn-00983308-v')]

    """
    return Wordnet(lang=lang, lexicon=lexicon).synsets(form=form, pos=pos, ili=ili)


</t>
<t tx="karstenw.20230225123251.34">def senses(
    form: Optional[str] = None,
    pos: Optional[str] = None,
    *,
    lexicon: Optional[str] = None,
    lang: Optional[str] = None,
) -&gt; List[Sense]:
    """Return the list of matching senses.

    This will create a :class:`Wordnet` object using the *lang* and
    *lexicon* arguments. The remaining arguments are passed to the
    :meth:`Wordnet.senses` method.

    &gt;&gt;&gt; len(wn.senses('twig'))
    3
    &gt;&gt;&gt; wn.senses('twig', pos='n')
    [Sense('ewn-twig-n-13184889-02')]

    """
    return Wordnet(lang=lang, lexicon=lexicon).senses(form=form, pos=pos)


</t>
<t tx="karstenw.20230225123251.35">def sense(
    id: str,
    *,
    lexicon: Optional[str] = None,
    lang: Optional[str] = None
) -&gt; Sense:
    """Return the sense with *id* in *lexicon*.

    This will create a :class:`Wordnet` object using the *lang* and
    *lexicon* arguments. The *id* argument is then passed to the
    :meth:`Wordnet.sense` method.

    &gt;&gt;&gt; wn.sense('ewn-flutter-v-01903884-02')
    Sense('ewn-flutter-v-01903884-02')

    """
    return Wordnet(lang=lang, lexicon=lexicon).sense(id=id)


</t>
<t tx="karstenw.20230225123251.36">def ili(
    id: str,
    *,
    lexicon: Optional[str] = None,
    lang: Optional[str] = None
) -&gt; ILI:
    """Return the interlingual index with *id*.

    This will create a :class:`Wordnet` object using the *lang* and
    *lexicon* arguments. The *id* argument is then passed to the
    :meth:`Wordnet.ili` method.

    &gt;&gt;&gt; wn.ili(id='i1234')
    ILI('i1234')
    &gt;&gt;&gt; wn.ili(id='i1234').status
    'presupposed'

    """
    return Wordnet(lang=lang, lexicon=lexicon).ili(id=id)


</t>
<t tx="karstenw.20230225123251.37">def ilis(
    status: Optional[str] = None,
    *,
    lexicon: Optional[str] = None,
    lang: Optional[str] = None,
) -&gt; List[ILI]:
    """Return the list of matching interlingual indices.

    This will create a :class:`Wordnet` object using the *lang* and
    *lexicon* arguments. The remaining arguments are passed to the
    :meth:`Wordnet.ilis` method.

    &gt;&gt;&gt; len(wn.ilis())
    120071
    &gt;&gt;&gt; len(wn.ilis(status='proposed'))
    2573
    &gt;&gt;&gt; wn.ilis(status='proposed')[-1].definition()
    'the neutrino associated with the tau lepton.'
    &gt;&gt;&gt; len(wn.ilis(lang='de'))
    13818

    """
    return Wordnet(lang=lang, lexicon=lexicon).ilis(status=status)
</t>
<t tx="karstenw.20230225123251.4">def adjposition(self) -&gt; Optional[str]:
    """Return the adjective position of the sense.

    Values include :python:`"a"` (attributive), :python:`"p"`
    (predicative), and :python:`"ip"` (immediate
    postnominal). Note that this is only relevant for adjectival
    senses. Senses for other parts of speech, or for adjectives
    that are not annotated with this feature, will return
    ``None``.

    """
    return get_adjposition(self._id)

</t>
<t tx="karstenw.20230225123251.5">def frames(self) -&gt; List[str]:
    """Return the list of subcategorization frames for the sense."""
    lexids = self._get_lexicon_ids()
    return get_syntactic_behaviours(self._id, lexids)

</t>
<t tx="karstenw.20230225123251.6">def counts(self) -&gt; List[Count]:
    """Return the corpus counts stored for this sense."""
    lexids = self._get_lexicon_ids()
    return [Count(value, _id=_id)
            for value, _id in get_sense_counts(self._id, lexids)]

</t>
<t tx="karstenw.20230225123251.7">def metadata(self) -&gt; Metadata:
    """Return the sense's metadata."""
    return get_metadata(self._id, 'senses')

</t>
<t tx="karstenw.20230225123251.8">def relations(self, *args: str) -&gt; Dict[str, List['Sense']]:
    """Return a mapping of relation names to lists of senses.

    One or more relation names may be given as positional
    arguments to restrict the relations returned. If no such
    arguments are given, all relations starting from the sense
    are returned.

    See :meth:`get_related` for getting a flat list of related
    senses.

    """
    d: Dict[str, List['Sense']] = {}
    for relname, s in self._get_relations(args):
        if relname in d:
            d[relname].append(s)
        else:
            d[relname] = [s]
    return d

</t>
<t tx="karstenw.20230225123251.9">def get_related(self, *args: str) -&gt; List['Sense']:
    """Return a list of related senses.

    One or more relation types should be passed as arguments which
    determine the kind of relations returned.

    Example:

        &gt;&gt;&gt; physics = wn.senses('physics', lexicon='ewn')[0]
        &gt;&gt;&gt; for sense in physics.get_related('has_domain_topic'):
        ...     print(sense.word().lemma())
        ...
        coherent
        chaotic
        incoherent

    """
    return [s for _, s in self._get_relations(args)]

</t>
<t tx="karstenw.20230225123313.1">"""
Storage back-end interface.
"""

from typing import Dict
from pathlib import Path
import json
import sqlite3
import logging

import wn
from wn._types import AnyPath
from wn._util import resources, short_hash


logger = logging.getLogger('wn')


# Module Constants

DEBUG = False
NON_ROWID = 0  # imaginary rowid of non-existent row

# This stores hashes of the schema to check for version differences.
# When the schema changes, the hash will change. If the new hash is
# not added here, the 'test_schema_compatibility' test will fail. It
# is the developer's responsibility to only add compatible schema
# hashes here. If the schema change is not backwards-compatible, then
# clear all old hashes and only put the latest hash here. A hash can
# be generated like this:
#
# &gt;&gt;&gt; import sqlite3
# &gt;&gt;&gt; import wn
# &gt;&gt;&gt; conn = sqlite3.connect(wn.config.database_path)
# &gt;&gt;&gt; wn._db.schema_hash(conn)
#
COMPATIBLE_SCHEMA_HASHES = {
    '4c8ad03af5422d6979039ee2b80838d07c12d2c8',
}


# Optional metadata is stored as a JSON string

</t>
<t tx="karstenw.20230225123313.2">def _adapt_dict(d: dict) -&gt; bytes:
    return json.dumps(d).encode('utf-8')


</t>
<t tx="karstenw.20230225123313.3">def _convert_dict(s: bytes) -&gt; dict:
    return json.loads(s)


</t>
<t tx="karstenw.20230225123313.4">def _convert_boolean(s: bytes) -&gt; bool:
    return bool(int(s))


sqlite3.register_adapter(dict, _adapt_dict)
sqlite3.register_converter('meta', _convert_dict)
sqlite3.register_converter('boolean', _convert_boolean)


# The pool is a cache of open connections. Unless the database path is
# changed, there should only be zero or one.
pool: Dict[AnyPath, sqlite3.Connection] = {}


# The connect() function should be used for all connections

</t>
<t tx="karstenw.20230225123313.5">def connect() -&gt; sqlite3.Connection:
    dbpath = wn.config.database_path
    if dbpath not in pool:
        initialized = dbpath.is_file()
        conn = sqlite3.connect(
            dbpath.as_posix(),
            detect_types=sqlite3.PARSE_DECLTYPES,
            check_same_thread=not wn.config.allow_multithreading,
        )
        # foreign key support needs to be enabled for each connection
        conn.execute('PRAGMA foreign_keys = ON')
        if DEBUG:
            conn.set_trace_callback(print)
        if not initialized:
            logger.info('initializing database: %s', dbpath.as_posix())
            _init_db(conn)
        _check_schema_compatibility(conn, dbpath)

        pool[dbpath] = conn
    return pool[dbpath]


</t>
<t tx="karstenw.20230225123313.6">def _init_db(conn: sqlite3.Connection) -&gt; None:
    schema = resources.read_text('wn', 'schema.sql')
    conn.executescript(schema)
    with conn:
        conn.executemany('INSERT INTO ili_statuses VALUES (null,?)',
                         [('presupposed',), ('proposed',)])


</t>
<t tx="karstenw.20230225123313.7">def _check_schema_compatibility(conn: sqlite3.Connection, dbpath: Path) -&gt; None:
    hash = schema_hash(conn)

    # if the hash is known, then we're all good here
    if hash in COMPATIBLE_SCHEMA_HASHES:
        return

    logger.debug('current schema hash:\n  %s', hash)
    logger.debug('compatible schema hashes:\n  %s',
                 '\n  '.join(COMPATIBLE_SCHEMA_HASHES))
    # otherwise, try to raise a helpful error message
    msg = ("Wn's schema has changed and is no longer compatible with the "
           f"database. Please move or delete {dbpath} and rebuild it.")
    try:
        specs = conn.execute('SELECT id, version FROM lexicons').fetchall()
    except sqlite3.OperationalError as exc:
        raise wn.DatabaseError(msg) from exc
    else:
        if specs:
            installed = '\n  '.join(f'{id}:{ver}' for id, ver in specs)
            msg += f" Lexicons currently installed:\n  {installed}"
        else:
            msg += ' No lexicons are currently installed.'
        raise wn.DatabaseError(msg)


</t>
<t tx="karstenw.20230225123313.8">def schema_hash(conn: sqlite3.Connection) -&gt; str:
    query = 'SELECT sql FROM sqlite_master WHERE NOT sql ISNULL'
    schema = '\n\n'.join(row[0] for row in conn.execute(query))
    return short_hash(schema)
</t>
<t tx="karstenw.20230225123943.1">from typing import Optional, Type, List, Sequence, Tuple
from pathlib import Path
import logging

import requests

import wn
from wn._util import is_url
from wn.util import ProgressHandler, ProgressBar
from wn._add import add as add_to_db
from wn import config


CHUNK_SIZE = 8 * 1024  # how many KB to read at a time
TIMEOUT = 10  # number of seconds to wait for a server response


logger = logging.getLogger('wn')


</t>
<t tx="karstenw.20230225123943.2">def download(
        project_or_url: str,
        add: bool = True,
        progress_handler: Optional[Type[ProgressHandler]] = ProgressBar,
) -&gt; Path:
    """Download the resource specified by *project_or_url*.

    First the URL of the resource is determined and then, depending on
    the parameters, the resource is downloaded and added to the
    database.  The function then returns the path of the cached file.

    If *project_or_url* starts with `'http://'` or `'https://'`, then
    it is taken to be the URL for the resource. Otherwise,
    *project_or_url* is taken as a :ref:`project specifier
    &lt;lexicon-specifiers&gt;` and the URL is taken from a matching entry
    in Wn's project index. If no project matches the specifier,
    :exc:`wn.Error` is raised.

    If the URL has been downloaded and cached before, the cached file
    is used. Otherwise the URL is retrieved and stored in the cache.

    If the *add* paramter is ``True`` (default), the downloaded
    resource is added to the database.

    &gt;&gt;&gt; wn.download('ewn:2020')
    Added ewn:2020 (English WordNet)

    The *progress_handler* parameter takes a subclass of
    :class:`wn.util.ProgressHandler`. An instance of the class will be
    created, used, and closed by this function.

    """
    if progress_handler is None:
        progress_handler = ProgressHandler
    progress = progress_handler(message='Download', unit=' bytes')

    cache_path, urls = _get_cache_path_and_urls(project_or_url)

    try:
        if cache_path and cache_path.exists():
            progress.flash(f'Cached file found: {cache_path!s}')
            path = cache_path
        elif urls:
            path = _download(urls, progress)
        else:
            raise wn.Error('no urls to download')
    finally:
        progress.close()

    if add:
        try:
            add_to_db(path, progress_handler=progress_handler)
        except wn.Error as exc:
            raise wn.Error(
                f'could not add downloaded file: {path}\n  You might try '
                'deleting the cached file and trying the download again.'
            ) from exc

    return path


</t>
<t tx="karstenw.20230225123943.3">def _get_cache_path_and_urls(project_or_url: str) -&gt; Tuple[Optional[Path], List[str]]:
    if is_url(project_or_url):
        return config.get_cache_path(project_or_url), [project_or_url]
    else:
        info = config.get_project_info(project_or_url)
        return info.get('cache'), info['resource_urls']


</t>
<t tx="karstenw.20230225123943.4">def _download(urls: Sequence[str], progress: ProgressHandler) -&gt; Path:
    try:
        for i, url in enumerate(urls, 1):
            path = config.get_cache_path(url)
            logger.info('download url: %s', url)
            logger.info('download cache path: %s', path)
            try:
                with open(path, 'wb') as f:
                    progress.set(status='Requesting', count=0)
                    with requests.get(url, stream=True, timeout=TIMEOUT) as response:
                        response.raise_for_status()
                        size = int(response.headers.get('Content-Length', 0))
                        progress.set(total=size, status='Receiving')
                        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):
                            if chunk:
                                f.write(chunk)
                            progress.update(len(chunk))
                        progress.set(status='Complete')
            except requests.exceptions.RequestException as exc:
                _unlink_if_exists(path)
                count = progress.kwargs['count']
                if i == len(urls):
                    raise wn.Error(f'download failed at {count} bytes') from exc
                else:
                    logger.info('download failed at %d bytes; trying next url', count)
            else:
                break  # success

    except KeyboardInterrupt as exc:
        _unlink_if_exists(path)
        count = progress.kwargs['count']
        raise wn.Error(f'download cancelled at {count} bytes') from exc
    except Exception:
        _unlink_if_exists(path)
        raise

    return path


</t>
<t tx="karstenw.20230225123943.5">def _unlink_if_exists(path: Path) -&gt; None:
    """Unlink *path* if it exists.

    From Python 3.8, this function is unnecessary. Just use:
        path.unlink(exist_ok=True)
    """
    if path.exists():
        path.unlink()
</t>
<t tx="karstenw.20230225123946.1">class Error(Exception):
    """Generic error class for invalid wordnet operations."""

    # reset the module so the user sees the public name
    __module__ = 'wn'


</t>
<t tx="karstenw.20230225123946.2">class DatabaseError(Error):
    """Error class for issues with the database."""

    __module__ = 'wn'


</t>
<t tx="karstenw.20230225123946.3">class ConfigurationError(Error):
    """Raised on invalid configurations."""
    __module__ = 'wn'


</t>
<t tx="karstenw.20230225123946.4">class ProjectError(Error):
    """Raised when a project is not found or on errors defined in the index."""
    __module__ = 'wn'


</t>
<t tx="karstenw.20230225123946.5">class WnWarning(Warning):
    """Generic warning class for dubious worndet operations."""

    # reset the module so the user sees the public name
    __module__ = 'wn'
</t>
<t tx="karstenw.20230225123948.1">from typing import List, Dict, Set, Tuple, Sequence, Optional, cast

import wn
from wn._types import AnyPath, VersionInfo
from wn._util import version_info
from wn import lmf
from wn._queries import (
    find_entries,
    find_senses,
    find_synsets,
    find_syntactic_behaviours,
    find_proposed_ilis,
    get_entry_senses,
    get_sense_relations,
    get_sense_synset_relations,
    get_synset_relations,
    get_synset_members,
    get_examples,
    get_definitions,
    get_metadata,
    get_lexicalized,
    get_adjposition,
    get_form_pronunciations,
    get_form_tags,
    get_sense_counts,
    get_lexfile,
    get_lexicon,
    get_lexicon_dependencies,
    get_lexicon_extension_bases,
)
from wn._core import Lexicon


</t>
<t tx="karstenw.20230225123948.10">def _export_senses(
    entry_rowid: int,
    lexids: Sequence[int],
    sbmap: _SBMap,
    version: VersionInfo,
) -&gt; List[lmf.Sense]:
    senses: List[lmf.Sense] = []
    for id, _, synset, _, rowid in get_entry_senses(entry_rowid, lexids):
        sense: lmf.Sense = {
            'id': id,
            'synset': synset,
            'relations': _export_sense_relations(rowid, lexids),
            'examples': _export_examples(rowid, 'senses', lexids),
            'counts': _export_counts(rowid, lexids),
            'lexicalized': get_lexicalized(rowid, 'senses'),
            'adjposition': get_adjposition(rowid) or '',
            'meta': _export_metadata(rowid, 'senses'),
        }
        if version &gt;= (1, 1) and id in sbmap:
            sense['subcat'] = sorted(sbid for sbid, _ in sbmap[id])
        senses.append(sense)
    return senses


</t>
<t tx="karstenw.20230225123948.11">def _export_sense_relations(
    sense_rowid: int,
    lexids: Sequence[int]
) -&gt; List[lmf.Relation]:
    relations: List[lmf.Relation] = [
        {'target': id,
         'relType': type,
         'meta': _export_metadata(rowid, 'sense_relations')}
        for type, rowid, id, *_
        in get_sense_relations(sense_rowid, '*', lexids)
    ]
    relations.extend(
        {'target': id,
         'relType': type,
         'meta': _export_metadata(rowid, 'sense_synset_relations')}
        for type, rowid, id, *_
        in get_sense_synset_relations(sense_rowid, '*', lexids)
    )
    return relations


</t>
<t tx="karstenw.20230225123948.12">def _export_examples(
    rowid: int,
    table: str,
    lexids: Sequence[int]
) -&gt; List[lmf.Example]:
    return [
        {'text': text,
         'language': language,
         'meta': _export_metadata(rowid, f'{table[:-1]}_examples')}
        for text, language, rowid
        in get_examples(rowid, table, lexids)
    ]


</t>
<t tx="karstenw.20230225123948.13">def _export_counts(rowid: int, lexids: Sequence[int]) -&gt; List[lmf.Count]:
    return [
        {'value': val,
         'meta': _export_metadata(id, 'counts')}
        for val, id in get_sense_counts(rowid, lexids)
    ]


</t>
<t tx="karstenw.20230225123948.14">def _export_synsets(lexids: Sequence[int], version: VersionInfo) -&gt; List[lmf.Synset]:
    synsets: List[lmf.Synset] = []
    for id, pos, ili, _, rowid in find_synsets(lexicon_rowids=lexids):
        ilidef = _export_ili_definition(rowid)
        if ilidef and not ili:
            ili = 'in'  # special case for proposed ILIs
        ss: lmf.Synset = {
            'id': id,
            'ili': ili or '',
            'partOfSpeech': pos,
            'definitions': _export_definitions(rowid, lexids),
            'relations': _export_synset_relations(rowid, lexids),
            'examples': _export_examples(rowid, 'synsets', lexids),
            'lexicalized': get_lexicalized(rowid, 'synsets'),
            'lexfile': get_lexfile(rowid) or '',
            'meta': _export_metadata(rowid, 'synsets'),
        }
        if ilidef:
            ss['ili_definition'] = ilidef
        if version &gt;= (1, 1):
            ss['members'] = [row[0] for row in get_synset_members(rowid, lexids)]
        synsets.append(ss)
    return synsets


</t>
<t tx="karstenw.20230225123948.15">def _export_definitions(rowid: int, lexids: Sequence[int]) -&gt; List[lmf.Definition]:
    return [
        {'text': text,
         'language': language,
         'sourceSense': sense_id,
         'meta': _export_metadata(rowid, 'definitions')}
        for text, language, sense_id, rowid
        in get_definitions(rowid, lexids)
    ]


</t>
<t tx="karstenw.20230225123948.16">def _export_ili_definition(synset_rowid: int) -&gt; Optional[lmf.ILIDefinition]:
    _, _, defn, rowid = next(find_proposed_ilis(synset_rowid=synset_rowid),
                             (None, None, None, None))
    ilidef: Optional[lmf.ILIDefinition] = None
    if defn:
        meta = None
        if rowid is not None:
            meta = _export_metadata(rowid, 'proposed_ilis')
        ilidef = {'text': defn, 'meta': meta}
    return ilidef


</t>
<t tx="karstenw.20230225123948.17">def _export_synset_relations(
    synset_rowid: int,
    lexids: Sequence[int]
) -&gt; List[lmf.Relation]:
    return [
        {'target': id,
         'relType': type,
         'meta': _export_metadata(rowid, 'synset_relations')}
        for type, rowid, id, *_
        in get_synset_relations((synset_rowid,), '*', lexids)
    ]


</t>
<t tx="karstenw.20230225123948.18">def _export_syntactic_behaviours_1_0(
    entry: lmf.LexicalEntry,
    sbmap: _SBMap,
) -&gt; List[lmf.SyntacticBehaviour]:
    frames: List[lmf.SyntacticBehaviour] = []
    sense_ids = {s['id'] for s in entry.get('senses', [])}
    sbs: Dict[str, Set[str]] = {}
    for sid in sense_ids:
        for _, subcat_frame in sbmap.get(sid, []):
            sbs.setdefault(subcat_frame, set()).add(sid)
    for subcat_frame, sids in sbs.items():
        frame: lmf.SyntacticBehaviour = {
            'subcategorizationFrame': subcat_frame,
            'senses': sorted(sids),
        }
        frames.append(frame)
    return frames


</t>
<t tx="karstenw.20230225123948.19">def _export_syntactic_behaviours_1_1(
    lexids: Sequence[int]
) -&gt; List[lmf.SyntacticBehaviour]:
    return [
        {'id': id or '',
         'subcategorizationFrame': frame}
        for id, frame, _ in find_syntactic_behaviours(lexicon_rowids=lexids)
    ]


</t>
<t tx="karstenw.20230225123948.2">def export(
        lexicons: Sequence[Lexicon], destination: AnyPath, version: str = '1.0'
) -&gt; None:
    """Export lexicons from the database to a WN-LMF file.

    More than one lexicon may be exported in the same file, subject to
    these conditions:

    - identifiers on wordnet entities must be unique in all lexicons
    - lexicons extensions may not be exported with their dependents

    &gt;&gt;&gt; w = wn.Wordnet(lexicon='cmnwn zsmwn')
    &gt;&gt;&gt; wn.export(w.lexicons(), 'cmn-zsm.xml')

    Args:
        lexicons: sequence of :class:`wn.Lexicon` objects
        destination: path to the destination file
        version: LMF version string

    """
    _precheck(lexicons)
    assert version in lmf.SUPPORTED_VERSIONS
    _version = version_info(version)
    resource: lmf.LexicalResource = {
        'lmf_version': version,
        'lexicons': [_export_lexicon(lex, _version) for lex in lexicons]
    }
    lmf.dump(resource, destination)


</t>
<t tx="karstenw.20230225123948.20">def _export_metadata(rowid: int, table: str) -&gt; lmf.Metadata:
    return cast(lmf.Metadata, get_metadata(rowid, table))
</t>
<t tx="karstenw.20230225123948.3">def _precheck(lexicons: Sequence[Lexicon]) -&gt; None:
    all_ids: Set[str] = set()
    for lex in lexicons:
        lexids = (lex._id,)
        idset = {lex.id}
        idset.update(row[0] for row in find_entries(lexicon_rowids=lexids))
        idset.update(row[0] for row in find_senses(lexicon_rowids=lexids))
        idset.update(row[0] for row in find_synsets(lexicon_rowids=lexids))
        # TODO: syntactic behaviours
        if all_ids.intersection(idset):
            raise wn.Error('cannot export: non-unique identifiers in lexicons')
        all_ids |= idset


_SBMap = Dict[str, List[Tuple[str, str]]]


</t>
<t tx="karstenw.20230225123948.4">def _export_lexicon(lexicon: Lexicon, version: VersionInfo) -&gt; lmf.Lexicon:
    lexids = (lexicon._id,)

    # WN-LMF 1.0 lexicons put syntactic behaviours on lexical entries
    # WN-LMF 1.1 lexicons use a 'subcat' IDREFS attribute
    sbmap: _SBMap = {}
    if version &lt; (1, 1):
        for sbid, frame, sids in find_syntactic_behaviours(lexicon_rowids=lexids):
            for sid in sids:
                sbmap.setdefault(sid, []).append((sbid, frame))

    lex: lmf.Lexicon = {
        'id': lexicon.id,
        'label': lexicon.label,
        'language': lexicon.language,
        'email': lexicon.email,
        'license': lexicon.license,
        'version': lexicon.version,
        'url': lexicon.url or '',
        'citation': lexicon.citation or '',
        'entries': _export_lexical_entries(lexids, sbmap, version),
        'synsets': _export_synsets(lexids, version),
        'meta': _export_metadata(lexicon._id, 'lexicons'),
    }
    if version &gt;= (1, 1):
        lex['logo'] = lexicon.logo or ''
        lex['requires'] = _export_requires(lexicon._id)
        lex['frames'] = _export_syntactic_behaviours_1_1(lexids)

    return lex


</t>
<t tx="karstenw.20230225123948.5">def _export_requires(lexid: int) -&gt; List[lmf.Dependency]:
    return [
        {'id': id, 'version': version, 'url': url}
        for id, version, url, _ in get_lexicon_dependencies(lexid)
    ]


</t>
<t tx="karstenw.20230225123948.6">def _export_extends(lexid: int) -&gt; lmf.Dependency:
    ext_lexid = get_lexicon_extension_bases(lexid, depth=1)[0]
    _, id, _, _, _, _, version, url, *_ = get_lexicon(ext_lexid)
    return {'id': id, 'version': version, 'url': url}


</t>
<t tx="karstenw.20230225123948.7">def _export_lexical_entries(
    lexids: Sequence[int],
    sbmap: _SBMap,
    version: VersionInfo
) -&gt; List[lmf.LexicalEntry]:
    entries: List[lmf.LexicalEntry] = []
    for id, pos, forms, _, rowid in find_entries(lexicon_rowids=lexids):
        entry: lmf.LexicalEntry = {
            'id': id,
            'lemma': {
                'writtenForm': forms[0][0],
                'partOfSpeech': pos,
                'script': forms[0][2] or '',
                'tags': _export_tags(forms[0][3]),
            },
            'forms': [],
            'senses': _export_senses(rowid, lexids, sbmap, version),
            'meta': _export_metadata(rowid, 'entries'),
        }
        if version &gt;= (1, 1):
            entry['lemma']['pronunciations'] = _export_pronunciations(forms[0][3])
        for form, fid, script, frowid in forms[1:]:
            _form: lmf.Form = {
                'id': fid or '',
                'writtenForm': form,
                'script': script or '',
                'tags': _export_tags(frowid),
            }
            if version &gt;= (1, 1):
                _form['pronunciations'] = _export_pronunciations(frowid)
            entry['forms'].append(_form)
        if version &lt; (1, 1):
            entry['frames'] = _export_syntactic_behaviours_1_0(entry, sbmap)
        entries.append(entry)
    return entries


</t>
<t tx="karstenw.20230225123948.8">def _export_pronunciations(rowid: int) -&gt; List[lmf.Pronunciation]:
    return [
        {'text': text,
         'variety': variety,
         'notation': notation,
         'phonemic': phonemic,
         'audio': audio}
        for text, variety, notation, phonemic, audio
        in get_form_pronunciations(rowid)
    ]


</t>
<t tx="karstenw.20230225123948.9">def _export_tags(rowid: int) -&gt; List[lmf.Tag]:
    return [
        {'text': text, 'category': category}
        for text, category in get_form_tags(rowid)
    ]


</t>
<t tx="karstenw.20230225123950.1">from typing import Iterator, Dict
from pathlib import Path

from wn._types import AnyPath


</t>
<t tx="karstenw.20230225123950.2">def is_ili(source: AnyPath) -&gt; bool:
    """Return True if *source* is an ILI tab-separated-value file.

    This only checks that the first column, split by tabs, of the
    first line is 'ili' or 'ILI'. It does not check if each line has
    the correct number of columns.

    """
    source = Path(source).expanduser()
    if source.is_file():
        try:
            with source.open('rb') as fh:
                return next(fh).split(b'\t')[0] in (b'ili', b'ILI')
        except (StopIteration, IndexError):
            pass
    return False


</t>
<t tx="karstenw.20230225123950.3">def load(source: AnyPath) -&gt; Iterator[Dict[str, str]]:
    """Load an interlingual index file.

    Args:
        source: path to an ILI file
    """
    source = Path(source).expanduser()
    with source.open(encoding='utf-8') as fh:
        header = next(fh).rstrip('\r\n')
        fields = tuple(map(str.lower, header.split('\t')))
        for line in fh:
            yield dict(zip(fields, line.rstrip('\r\n').split('\t')))
</t>
<t tx="karstenw.20230225123952.1">"""
Database retrieval queries.
"""

from typing import (
    Optional, List, Tuple, Collection, Iterator, Sequence
)
import itertools
import sqlite3

import wn
from wn._types import Metadata
from wn._db import connect, NON_ROWID


# Local Types

_Pronunciation = Tuple[
    str,   # value
    str,   # variety
    str,   # notation
    bool,  # phonemic
    str,   # audio
]
_Tag = Tuple[str, str]  # tag, category
_Form = Tuple[
    str,            # form
    Optional[str],  # id
    Optional[str],  # script
    int             # rowid
]
_Word = Tuple[
    str,          # id
    str,          # pos
    List[_Form],  # forms
    int,          # lexid
    int,          # rowid
]
_Synset = Tuple[
    str,  # id
    str,  # pos
    str,  # ili
    int,  # lexid
    int,  # rowid
]
_Synset_Relation = Tuple[str, int, str, str, str, int, int]  # relname, relid, *_Synset
_Sense = Tuple[
    str,  # id
    str,  # entry_id
    str,  # synset_id
    int,  # lexid
    int,  # rowid
]
_Sense_Relation = Tuple[str, int, str, str, str, int, int]  # relname, relid,  *_Sense
_Count = Tuple[int, int]  # count, count_id
_SyntacticBehaviour = Tuple[
    str,       # id
    str,       # frame
    List[str]  # sense ids
]
_ILI = Tuple[
    Optional[str],  # id
    str,            # status
    Optional[str],  # definition
    int,            # rowid
]
_Lexicon = Tuple[
    int,       # rowid
    str,       # id
    str,       # label
    str,       # language
    str,       # email
    str,       # license
    str,       # version
    str,       # url
    str,       # citation
    str,       # logo
    Metadata,  # metadata
]


</t>
<t tx="karstenw.20230225123952.10">def _find_existing_ilis(
    id: Optional[str] = None,
    status: Optional[str] = None,
    lexicon_rowids: Sequence[int] = (),
) -&gt; Iterator[_ILI]:
    query = '''
        SELECT DISTINCT i.id, ist.status, i.definition, i.rowid
          FROM ilis AS i
          JOIN ili_statuses AS ist ON i.status_rowid = ist.rowid
    '''
    conditions: List[str] = []
    params: List = []
    if id:
        conditions.append('i.id = ?')
        params.append(id)
    if status:
        conditions.append('ist.status = ?')
        params.append(status)
    if lexicon_rowids:
        conditions.append(f'''
            i.rowid IN
            (SELECT ss.ili_rowid
               FROM synsets AS ss
              WHERE ss.lexicon_rowid IN ({_qs(lexicon_rowids)}))
        ''')
        params.extend(lexicon_rowids)
    if conditions:
        query += '\n WHERE ' + '\n   AND '.join(conditions)
    yield from connect().execute(query, params)


</t>
<t tx="karstenw.20230225123952.11">def find_proposed_ilis(
    synset_rowid: Optional[int] = None,
    lexicon_rowids: Sequence[int] = (),
) -&gt; Iterator[_ILI]:
    query = '''
        SELECT null, "proposed", definition, rowid
          FROM proposed_ilis
    '''
    conditions = []
    params = []
    if synset_rowid is not None:
        conditions.append('synset_rowid = ?')
        params.append(synset_rowid)
    if lexicon_rowids:
        conditions.append(f'''
            synset_rowid IN
            (SELECT ss.rowid FROM synsets AS ss
              WHERE ss.lexicon_rowid IN ({_qs(lexicon_rowids)}))
        ''')
        params.extend(lexicon_rowids)
    if conditions:
        query += '\n WHERE ' + '\n   AND '.join(conditions)
    yield from connect().execute(query, params)


</t>
<t tx="karstenw.20230225123952.12">def find_entries(
    id: Optional[str] = None,
    forms: Sequence[str] = (),
    pos: Optional[str] = None,
    lexicon_rowids: Sequence[int] = (),
    normalized: bool = False,
    search_all_forms: bool = False,
) -&gt; Iterator[_Word]:
    conn = connect()
    cte = ''
    params: List = []
    conditions = []
    if id:
        conditions.append('e.id = ?')
        params.append(id)
    if forms:
        cte = f'WITH wordforms(s) AS (VALUES {_vs(forms)})'
        or_norm = 'OR normalized_form IN wordforms' if normalized else ''
        and_rank = '' if search_all_forms else 'AND rank = 0'
        conditions.append(f'''
            e.rowid IN
               (SELECT entry_rowid
                  FROM forms
                 WHERE (form IN wordforms {or_norm}) {and_rank})
        '''.strip())
        params.extend(forms)
    if pos:
        conditions.append('e.pos = ?')
        params.append(pos)
    if lexicon_rowids:
        conditions.append(f'e.lexicon_rowid IN ({_qs(lexicon_rowids)})')
        params.extend(lexicon_rowids)

    condition = ''
    if conditions:
        condition = 'WHERE ' + '\n           AND '.join(conditions)

    query = f'''
          {cte}
        SELECT DISTINCT e.lexicon_rowid, e.rowid, e.id, e.pos,
                        f.form, f.id, f.script, f.rowid
          FROM entries AS e
          JOIN forms AS f ON f.entry_rowid = e.rowid
         {condition}
         ORDER BY e.rowid, e.id, f.rank
    '''

    rows: Iterator[
        Tuple[int, int, str, str, str, Optional[str], Optional[str], int]
    ] = conn.execute(query, params)
    groupby = itertools.groupby
    for key, group in groupby(rows, lambda row: row[0:4]):
        lexid, rowid, id, pos = key
        wordforms = [(row[4], row[5], row[6], row[7]) for row in group]
        yield (id, pos, wordforms, lexid, rowid)


</t>
<t tx="karstenw.20230225123952.13">def find_senses(
    id: Optional[str] = None,
    forms: Sequence[str] = (),
    pos: Optional[str] = None,
    lexicon_rowids: Sequence[int] = (),
    normalized: bool = False,
    search_all_forms: bool = False,
) -&gt; Iterator[_Sense]:
    conn = connect()
    cte = ''
    params: List = []
    conditions = []
    if id:
        conditions.append('s.id = ?')
        params.append(id)
    if forms:
        cte = f'WITH wordforms(s) AS (VALUES {_vs(forms)})'
        or_norm = 'OR normalized_form IN wordforms' if normalized else ''
        and_rank = '' if search_all_forms else 'AND rank = 0'
        conditions.append(f'''
            s.entry_rowid IN
               (SELECT entry_rowid
                  FROM forms
                 WHERE (form IN wordforms {or_norm}) {and_rank})
        '''.strip())
        params.extend(forms)
    if pos:
        conditions.append('e.pos = ?')
        params.append(pos)
    if lexicon_rowids:
        conditions.append(f's.lexicon_rowid IN ({_qs(lexicon_rowids)})')
        params.extend(lexicon_rowids)

    condition = ''
    if conditions:
        condition = 'WHERE ' + '\n           AND '.join(conditions)

    query = f'''
          {cte}
        SELECT DISTINCT s.id, e.id, ss.id, s.lexicon_rowid, s.rowid
          FROM senses AS s
          JOIN entries AS e ON e.rowid = s.entry_rowid
          JOIN synsets AS ss ON ss.rowid = s.synset_rowid
         {condition}
    '''

    rows: Iterator[_Sense] = conn.execute(query, params)
    yield from rows


</t>
<t tx="karstenw.20230225123952.14">def find_synsets(
    id: Optional[str] = None,
    forms: Sequence[str] = (),
    pos: Optional[str] = None,
    ili: Optional[str] = None,
    lexicon_rowids: Sequence[int] = (),
    normalized: bool = False,
    search_all_forms: bool = False,
) -&gt; Iterator[_Synset]:
    conn = connect()
    cte = ''
    join = ''
    conditions = []
    order = ''
    params: List = []
    if id:
        conditions.append('ss.id = ?')
        params.append(id)
    if forms:
        cte = f'WITH wordforms(s) AS (VALUES {_vs(forms)})'
        or_norm = 'OR normalized_form IN wordforms' if normalized else ''
        and_rank = '' if search_all_forms else 'AND rank = 0'
        join = f'''\
          JOIN (SELECT _s.entry_rowid, _s.synset_rowid, _s.entry_rank
                  FROM forms AS f
                  JOIN senses AS _s ON _s.entry_rowid = f.entry_rowid
                 WHERE (f.form IN wordforms {or_norm}) {and_rank}) AS s
            ON s.synset_rowid = ss.rowid
        '''.strip()
        params.extend(forms)
        order = 'ORDER BY s.entry_rowid, s.entry_rank'
    if pos:
        conditions.append('ss.pos = ?')
        params.append(pos)
    if ili:
        conditions.append(
            'ss.ili_rowid IN (SELECT ilis.rowid FROM ilis WHERE ilis.id = ?)'
        )
        params.append(ili)
    if lexicon_rowids:
        conditions.append(f'ss.lexicon_rowid IN ({_qs(lexicon_rowids)})')
        params.extend(lexicon_rowids)

    condition = ''
    if conditions:
        condition = 'WHERE ' + '\n           AND '.join(conditions)

    query = f'''
          {cte}
        SELECT DISTINCT ss.id, ss.pos,
                        (SELECT ilis.id FROM ilis WHERE ilis.rowid=ss.ili_rowid),
                        ss.lexicon_rowid, ss.rowid
          FROM synsets AS ss
          {join}
         {condition}
         {order}
    '''

    rows: Iterator[_Synset] = conn.execute(query, params)
    yield from rows


</t>
<t tx="karstenw.20230225123952.15">def get_synsets_for_ilis(
        ilis: Collection[str],
        lexicon_rowids: Sequence[int],
) -&gt; Iterator[_Synset]:
    conn = connect()
    query = f'''
        SELECT DISTINCT ss.id, ss.pos, ili.id, ss.lexicon_rowid, ss.rowid
          FROM synsets as ss
          JOIN ilis as ili ON ss.ili_rowid = ili.rowid
         WHERE ili.id IN ({_qs(ilis)})
           AND ss.lexicon_rowid IN ({_qs(lexicon_rowids)})
    '''
    params = *ilis, *lexicon_rowids
    result_rows: Iterator[_Synset] = conn.execute(query, params)
    yield from result_rows


</t>
<t tx="karstenw.20230225123952.16">def get_synset_relations(
    source_rowids: Collection[int],
    relation_types: Collection[str],
    lexicon_rowids: Sequence[int],
) -&gt; Iterator[_Synset_Relation]:
    conn = connect()
    params: List = []
    constraint = ''
    if relation_types and '*' not in relation_types:
        constraint = f'WHERE type IN ({_qs(relation_types)})'
        params.extend(relation_types)
    params.extend(source_rowids)
    params.extend(lexicon_rowids)
    query = f'''
          WITH rt(rowid, type) AS
               (SELECT rowid, type FROM relation_types {constraint})
        SELECT DISTINCT rel.type, rel.rowid, tgt.id, tgt.pos,
                        (SELECT ilis.id FROM ilis WHERE ilis.rowid = tgt.ili_rowid),
                        tgt.lexicon_rowid, tgt.rowid
          FROM (SELECT rt.type, target_rowid, srel.rowid
                  FROM synset_relations AS srel
                  JOIN rt ON srel.type_rowid = rt.rowid
                 WHERE source_rowid IN ({_qs(source_rowids)})
                   AND lexicon_rowid IN ({_qs(lexicon_rowids)})
               ) AS rel
          JOIN synsets AS tgt
            ON tgt.rowid = rel.target_rowid
    '''
    result_rows: Iterator[_Synset_Relation] = conn.execute(query, params)
    yield from result_rows


</t>
<t tx="karstenw.20230225123952.17">def get_definitions(
    synset_rowid: int,
    lexicon_rowids: Sequence[int],
) -&gt; List[Tuple[str, str, str, int]]:
    conn = connect()
    query = f'''
        SELECT d.definition,
               d.language,
               (SELECT s.id FROM senses AS s WHERE s.rowid=d.sense_rowid),
               d.rowid
          FROM definitions AS d
         WHERE d.synset_rowid = ?
           AND d.lexicon_rowid IN ({_qs(lexicon_rowids)})
    '''
    return conn.execute(query, (synset_rowid, *lexicon_rowids)).fetchall()


_SANITIZED_EXAMPLE_PREFIXES = {
    'senses': 'sense',
    'synsets': 'synset',
}


</t>
<t tx="karstenw.20230225123952.18">def get_examples(
    rowid: int,
    table: str,
    lexicon_rowids: Sequence[int],
) -&gt; List[Tuple[str, str, int]]:
    conn = connect()
    prefix = _SANITIZED_EXAMPLE_PREFIXES.get(table)
    if prefix is None:
        raise wn.Error(f"'{table}' does not have examples")
    query = f'''
        SELECT example, language, rowid
          FROM {prefix}_examples
         WHERE {prefix}_rowid = ?
           AND lexicon_rowid IN ({_qs(lexicon_rowids)})
    '''
    return conn.execute(query, (rowid, *lexicon_rowids)).fetchall()


</t>
<t tx="karstenw.20230225123952.19">def find_syntactic_behaviours(
    id: Optional[str] = None,
    lexicon_rowids: Sequence[int] = (),
) -&gt; Iterator[_SyntacticBehaviour]:
    conn = connect()
    query = '''
        SELECT sb.id, sb.frame, s.id
          FROM syntactic_behaviours AS sb
          JOIN syntactic_behaviour_senses AS sbs
            ON sbs.syntactic_behaviour_rowid = sb.rowid
          JOIN senses AS s
            ON s.rowid = sbs.sense_rowid
    '''
    conditions: List[str] = []
    params: List = []
    if id:
        conditions.append('sb.id = ?')
        params.append(id)
    if lexicon_rowids:
        conditions.append(f'sb.lexicon_rowid IN ({_qs(lexicon_rowids)})')
        params.extend(lexicon_rowids)
    if conditions:
        query += '\n WHERE ' + '\n   AND '.join(conditions)
    rows: Iterator[Tuple[str, str, str]] = conn.execute(query, params)
    for key, group in itertools.groupby(rows, lambda row: row[0:2]):
        id, frame = key
        sense_ids = [row[2] for row in group]
        yield id, frame, sense_ids


</t>
<t tx="karstenw.20230225123952.2">def find_lexicons(
    lexicon: str,
    lang: Optional[str] = None,
) -&gt; Iterator[_Lexicon]:
    cur = connect().cursor()
    found = False
    for specifier in lexicon.split():
        limit = '-1' if '*' in lexicon else '1'
        if ':' not in specifier:
            specifier += ':*'
        query = f'''
            SELECT DISTINCT rowid, id, label, language, email, license,
                            version, url, citation, logo
              FROM lexicons
             WHERE id || ":" || version GLOB :specifier
               AND (:language ISNULL OR language = :language)
             LIMIT {limit}
        '''
        params = {'specifier': specifier, 'language': lang}
        for row in cur.execute(query, params):
            yield row
            found = True
    # only raise an error when the query specifies something
    if not found and (lexicon != '*' or lang is not None):
        raise wn.Error(
            f'no lexicon found with lang={lang!r} and lexicon={lexicon!r}'
        )


</t>
<t tx="karstenw.20230225123952.20">def get_syntactic_behaviours(
    rowid: int,
    lexicon_rowids: Sequence[int],
) -&gt; List[str]:
    conn = connect()
    query = f'''
        SELECT sb.frame
          FROM syntactic_behaviours AS sb
          JOIN syntactic_behaviour_senses AS sbs
            ON sbs.syntactic_behaviour_rowid = sb.rowid
         WHERE sbs.sense_rowid = ?
           AND sb.lexicon_rowid IN ({_qs(lexicon_rowids)})
    '''
    return [row[0] for row in conn.execute(query, (rowid, *lexicon_rowids))]


</t>
<t tx="karstenw.20230225123952.21">def _get_senses(
    rowid: int,
    sourcetype: str,
    lexicon_rowids: Sequence[int],
) -&gt; Iterator[_Sense]:
    conn = connect()
    query = f'''
        SELECT s.id, e.id, ss.id, s.lexicon_rowid, s.rowid
          FROM senses AS s
          JOIN entries AS e
            ON e.rowid = s.entry_rowid
          JOIN synsets AS ss
            ON ss.rowid = s.synset_rowid
         WHERE s.{sourcetype}_rowid = ?
           AND s.lexicon_rowid IN ({_qs(lexicon_rowids)})
         ORDER BY s.{sourcetype}_rank
    '''
    return conn.execute(query, (rowid, *lexicon_rowids))


</t>
<t tx="karstenw.20230225123952.22">def get_entry_senses(rowid: int, lexicon_rowids: Sequence[int]) -&gt; Iterator[_Sense]:
    yield from _get_senses(rowid, 'entry', lexicon_rowids)


</t>
<t tx="karstenw.20230225123952.23">def get_synset_members(rowid: int, lexicon_rowids: Sequence[int]) -&gt; Iterator[_Sense]:
    yield from _get_senses(rowid, 'synset', lexicon_rowids)


</t>
<t tx="karstenw.20230225123952.24">def get_sense_relations(
    source_rowid: int,
    relation_types: Collection[str],
    lexicon_rowids: Sequence[int],
) -&gt; Iterator[_Sense_Relation]:
    params: List = []
    constraint = ''
    if relation_types and '*' not in relation_types:
        constraint = f'WHERE type IN ({_qs(relation_types)})'
        params.extend(relation_types)
    params.append(source_rowid)
    params.extend(lexicon_rowids)
    query = f'''
          WITH rt(rowid, type) AS
               (SELECT rowid, type FROM relation_types {constraint})
        SELECT DISTINCT rel.type, rel.rowid,
                        s.id, e.id, ss.id,
                        s.lexicon_rowid, s.rowid
          FROM (SELECT rt.type, target_rowid, srel.rowid
                  FROM sense_relations AS srel
                  JOIN rt ON srel.type_rowid = rt.rowid
                 WHERE source_rowid = ?
                   AND lexicon_rowid IN ({_qs(lexicon_rowids)})
               ) AS rel
          JOIN senses AS s
            ON s.rowid = rel.target_rowid
          JOIN entries AS e
            ON e.rowid = s.entry_rowid
          JOIN synsets AS ss
            ON ss.rowid = s.synset_rowid
    '''
    rows: Iterator[_Sense_Relation] = connect().execute(query, params)
    yield from rows


</t>
<t tx="karstenw.20230225123952.25">def get_sense_synset_relations(
        source_rowid: int,
        relation_types: Collection[str],
        lexicon_rowids: Sequence[int],
) -&gt; Iterator[_Synset_Relation]:
    params: List = []
    constraint = ''
    if '*' not in relation_types:
        constraint = f'WHERE type IN ({_qs(relation_types)})'
        params.extend(relation_types)
    params.append(source_rowid)
    params.extend(lexicon_rowids)
    query = f'''
          WITH rt(rowid, type) AS
               (SELECT rowid, type FROM relation_types {constraint})
        SELECT DISTINCT rel.type, rel.rowid, ss.id, ss.pos,
                        (SELECT ilis.id FROM ilis WHERE ilis.rowid = ss.ili_rowid),
                        ss.lexicon_rowid, ss.rowid
          FROM (SELECT rt.type, target_rowid, srel.rowid
                  FROM sense_synset_relations AS srel
                  JOIN rt ON srel.type_rowid = rt.rowid
                 WHERE source_rowid = ?
                   AND lexicon_rowid IN ({_qs(lexicon_rowids)})
               ) AS rel
          JOIN synsets AS ss
            ON ss.rowid = rel.target_rowid
    '''
    rows: Iterator[_Synset_Relation] = connect().execute(query, params)
    yield from rows


_SANITIZED_METADATA_TABLES = {
    'ilis': 'ilis',
    'proposed_ilis': 'proposed_ilis',
    'lexicons': 'lexicons',
    'entries': 'entries',
    'senses': 'senses',
    'synsets': 'synsets',
    'sense_relations': 'sense_relations',
    'sense_synset_relations': 'sense_synset_relations',
    'synset_relations': 'synset_relations',
    'sense_examples': 'sense_examples',
    'counts': 'counts',
    'synset_examples': 'synset_examples',
    'definitions': 'definitions',
}


</t>
<t tx="karstenw.20230225123952.26">def get_metadata(
    rowid: int, table: str
) -&gt; Metadata:
    conn = connect()
    tablename = _SANITIZED_METADATA_TABLES.get(table)
    if tablename is None:
        raise wn.Error(f"'{table}' does not contain metadata")
    query = f'SELECT metadata FROM {tablename} WHERE rowid=?'
    return conn.execute(query, (rowid,)).fetchone()[0] or {}


_SANITIZED_LEXICALIZED_TABLES = {
    'senses': 'senses',
    'synsets': 'synsets',
}


</t>
<t tx="karstenw.20230225123952.27">def get_lexicalized(rowid: int, table: str) -&gt; bool:
    conn = connect()
    tablename = _SANITIZED_LEXICALIZED_TABLES.get(table)
    if tablename is None:
        raise wn.Error(f"'{table}' does not mark lexicalization")
    if rowid == NON_ROWID:
        return False
    query = f'SELECT lexicalized FROM {tablename} WHERE rowid=?'
    return conn.execute(query, (rowid,)).fetchone()[0]


</t>
<t tx="karstenw.20230225123952.28">def get_adjposition(rowid: int) -&gt; Optional[str]:
    conn = connect()
    query = 'SELECT adjposition FROM adjpositions WHERE sense_rowid = ?'
    row = conn.execute(query, (rowid,)).fetchone()
    if row:
        return row[0]
    return None


</t>
<t tx="karstenw.20230225123952.29">def get_form_pronunciations(form_rowid: int) -&gt; List[_Pronunciation]:
    # TODO: restrict by lexicon ids
    conn = connect()
    query = '''
        SELECT value, variety, notation, phonemic, audio
          FROM pronunciations
         WHERE form_rowid = ?
    '''
    rows: List[_Pronunciation] = conn.execute(query, (form_rowid,)).fetchall()
    return rows


</t>
<t tx="karstenw.20230225123952.3">def get_lexicon(rowid: int) -&gt; _Lexicon:
    conn = connect()
    return _get_lexicon(conn, rowid)


</t>
<t tx="karstenw.20230225123952.30">def get_form_tags(form_rowid: int) -&gt; List[_Tag]:
    # TODO: restrict by lexicon ids
    conn = connect()
    query = 'SELECT tag, category FROM tags WHERE form_rowid = ?'
    rows: List[_Tag] = conn.execute(query, (form_rowid,)).fetchall()
    return rows


</t>
<t tx="karstenw.20230225123952.31">def get_sense_counts(sense_rowid: int, lexicon_rowids: Sequence[int]) -&gt; List[_Count]:
    conn = connect()
    query = f'''
        SELECT count, rowid
          FROM counts
         WHERE sense_rowid = ?
           AND lexicon_rowid IN ({_qs(lexicon_rowids)})
    '''
    rows: List[_Count] = conn.execute(
        query, (sense_rowid, *lexicon_rowids)
    ).fetchall()
    return rows


</t>
<t tx="karstenw.20230225123952.32">def get_lexfile(synset_rowid: int) -&gt; Optional[str]:
    conn = connect()
    query = '''
        SELECT lf.name
          FROM lexfiles AS lf
          JOIN synsets AS ss ON ss.lexfile_rowid = lf.rowid
         WHERE ss.rowid = ?
    '''
    row = conn.execute(query, (synset_rowid,)).fetchone()
    if row is not None and row[0] is not None:
        return row[0]
    return None


</t>
<t tx="karstenw.20230225123952.33">def _qs(xs: Collection) -&gt; str: return ','.join('?' * len(xs))
</t>
<t tx="karstenw.20230225123952.34">def _vs(xs: Collection) -&gt; str: return ','.join(['(?)'] * len(xs))
</t>
<t tx="karstenw.20230225123952.35">def _kws(xs: Collection) -&gt; str: return ','.join(f':{x}' for x in xs)
</t>
<t tx="karstenw.20230225123952.4">def _get_lexicon(conn: sqlite3.Connection, rowid: int) -&gt; _Lexicon:
    query = '''
        SELECT DISTINCT rowid, id, label, language, email, license,
                        version, url, citation, logo
        FROM lexicons
        WHERE rowid = ?
    '''
    row: Optional[_Lexicon] = conn.execute(query, (rowid,)).fetchone()
    if row is None:
        raise LookupError(rowid)  # should we have a WnLookupError?
    return row


</t>
<t tx="karstenw.20230225123952.5">def get_modified(rowid: int) -&gt; bool:
    query = 'SELECT modified FROM lexicons WHERE rowid = ?'
    return connect().execute(query, (rowid,)).fetchone()[0]


</t>
<t tx="karstenw.20230225123952.6">def get_lexicon_dependencies(rowid: int) -&gt; List[Tuple[str, str, str, Optional[int]]]:
    query = '''
        SELECT provider_id, provider_version, provider_url, provider_rowid
          FROM lexicon_dependencies
         WHERE dependent_rowid = ?
    '''
    return connect().execute(query, (rowid,)).fetchall()


</t>
<t tx="karstenw.20230225123952.7">def get_lexicon_extension_bases(rowid: int, depth: int = -1) -&gt; List[int]:
    query = '''
          WITH RECURSIVE ext(x, d) AS
               (SELECT base_rowid, 1
                  FROM lexicon_extensions
                 WHERE extension_rowid = :rowid
                 UNION SELECT base_rowid, d+1
                         FROM lexicon_extensions
                         JOIN ext ON extension_rowid = x)
        SELECT x FROM ext
         WHERE :depth &lt; 0 OR d &lt;= :depth
         ORDER BY d
    '''
    rows = connect().execute(query, {'rowid': rowid, 'depth': depth})
    return [row[0] for row in rows]


</t>
<t tx="karstenw.20230225123952.8">def get_lexicon_extensions(rowid: int, depth: int = -1) -&gt; List[int]:
    query = '''
          WITH RECURSIVE ext(x, d) AS
               (SELECT extension_rowid, 1
                  FROM lexicon_extensions
                 WHERE base_rowid = :rowid
                 UNION SELECT extension_rowid, d+1
                         FROM lexicon_extensions
                         JOIN ext ON base_rowid = x)
        SELECT x FROM ext
         WHERE :depth &lt; 0 OR d &lt;= :depth
         ORDER BY d
    '''
    rows = connect().execute(query, {'rowid': rowid, 'depth': depth})
    return [row[0] for row in rows]


</t>
<t tx="karstenw.20230225123952.9">def find_ilis(
    id: Optional[str] = None,
    status: Optional[str] = None,
    lexicon_rowids: Sequence[int] = (),
) -&gt; Iterator[_ILI]:
    if status != 'proposed':
        yield from _find_existing_ilis(
            id=id, status=status, lexicon_rowids=lexicon_rowids
        )
    if not id and (not status or status == 'proposed'):
        yield from find_proposed_ilis(lexicon_rowids=lexicon_rowids)


</t>
<t tx="karstenw.20230225123954.1">from typing import (
    Optional, Union, Callable, Mapping, Sequence, Tuple, Dict, Set, Any,
)
from pathlib import Path

# For functions taking a filesystem path as a str or a pathlib.Path
AnyPath = Union[str, Path]

# LMF versions for comparison
VersionInfo = Tuple[int, ...]

# Synset and Sense relations map a relation type to one or more ids
RelationMap = Mapping[str, Sequence[str]]

# User-facing metadata representation
Metadata = Dict[str, Any]

# A callable that returns a normalized word form for a given word form
NormalizeFunction = Callable[[str], str]

# Lemmatization returns a mapping of parts of speech (or None) to
# lists of wordforms that are potential lemmas for some query word
LemmatizeResult = Dict[Optional[str], Set[str]]

# A callable that returns a LemmatizationResult for a given word form
# and optional part of speech
LemmatizeFunction = Callable[[str, Optional[str]], LemmatizeResult]
</t>
<t tx="karstenw.20230225123957.1">"""Non-public Wn utilities."""

from typing import TypeVar, Iterable, List
import sys
from pathlib import Path
import hashlib
from unicodedata import normalize, combining
# version check is for mypy; see https://github.com/python/mypy/issues/1153
if sys.version_info &gt;= (3, 7):
    import importlib.resources as resources
else:
    import importlib_resources as resources  # noqa: F401


from wn._types import VersionInfo


</t>
<t tx="karstenw.20230225123957.10">def normalize_form(s: str) -&gt; str:
    return ''.join(c for c in normalize('NFKD', s.lower()) if not combining(c))
</t>
<t tx="karstenw.20230225123957.2">def version_info(version_string: str) -&gt; VersionInfo:
    return tuple(map(int, version_string.split('.')))


</t>
<t tx="karstenw.20230225123957.3">def is_url(string: str) -&gt; bool:
    """Return True if *string* appears to be a URL."""
    # TODO: ETags?
    return any(string.startswith(scheme)
               for scheme in ('http://', 'https://'))


</t>
<t tx="karstenw.20230225123957.4">def is_gzip(path: Path) -&gt; bool:
    """Return True if the file at *path* appears to be gzipped."""
    return _inspect_file_signature(path, b'\x1F\x8B')


</t>
<t tx="karstenw.20230225123957.5">def is_lzma(path: Path) -&gt; bool:
    """Return True if the file at *path* appears to be lzma-compressed."""
    return _inspect_file_signature(path, b'\xFD7zXZ\x00')


</t>
<t tx="karstenw.20230225123957.6">def is_xml(path: Path) -&gt; bool:
    """Return True if the file at *path* appears to be an XML file."""
    return _inspect_file_signature(path, b'&lt;?xml ')


</t>
<t tx="karstenw.20230225123957.7">def _inspect_file_signature(path: Path, signature: bytes) -&gt; bool:
    if path.is_file():
        with path.open('rb') as f:
            return f.read(len(signature)) == signature
    return False


</t>
<t tx="karstenw.20230225123957.8">def short_hash(string: str) -&gt; str:
    """Return a short hash of *string*."""
    b2 = hashlib.blake2b(digest_size=20)
    b2.update(string.encode('utf-8'))
    return b2.hexdigest()


T = TypeVar('T')


</t>
<t tx="karstenw.20230225123957.9">def flatten(iterable: Iterable[Iterable[T]]) -&gt; List[T]:
    return [x for xs in iterable for x in xs]


</t>
<t tx="karstenw.20230225124001.1">"""
Constants and literals used in wordnets.
"""


SENSE_RELATIONS = frozenset([
    'antonym',
    'also',
    'participle',
    'pertainym',
    'derivation',
    'domain_topic',
    'has_domain_topic',
    'domain_region',
    'has_domain_region',
    'exemplifies',
    'is_exemplified_by',
    'similar',
    'other',
    'feminine',
    'has_feminine',
    'masculine',
    'has_masculine',
    'young',
    'has_young',
    'diminutive',
    'has_diminutive',
    'augmentative',
    'has_augmentative',
    'anto_gradable',
    'anto_simple',
    'anto_converse',
    'simple_aspect_ip',
    'secondary_aspect_ip',
    'simple_aspect_pi',
    'secondary_aspect_pi',
])

SENSE_SYNSET_RELATIONS = frozenset([
    'other',
    'domain_topic',
    'domain_region',
    'exemplifies',
])

SYNSET_RELATIONS = frozenset([
    'agent',
    'also',
    'attribute',
    'be_in_state',
    'causes',
    'classified_by',
    'classifies',
    'co_agent_instrument',
    'co_agent_patient',
    'co_agent_result',
    'co_instrument_agent',
    'co_instrument_patient',
    'co_instrument_result',
    'co_patient_agent',
    'co_patient_instrument',
    'co_result_agent',
    'co_result_instrument',
    'co_role',
    'direction',
    'domain_region',
    'domain_topic',
    'exemplifies',
    'entails',
    'eq_synonym',
    'has_domain_region',
    'has_domain_topic',
    'is_exemplified_by',
    'holo_location',
    'holo_member',
    'holo_part',
    'holo_portion',
    'holo_substance',
    'holonym',
    'hypernym',
    'hyponym',
    'in_manner',
    'instance_hypernym',
    'instance_hyponym',
    'instrument',
    'involved',
    'involved_agent',
    'involved_direction',
    'involved_instrument',
    'involved_location',
    'involved_patient',
    'involved_result',
    'involved_source_direction',
    'involved_target_direction',
    'is_caused_by',
    'is_entailed_by',
    'location',
    'manner_of',
    'mero_location',
    'mero_member',
    'mero_part',
    'mero_portion',
    'mero_substance',
    'meronym',
    'similar',
    'other',
    'patient',
    'restricted_by',
    'restricts',
    'result',
    'role',
    'source_direction',
    'state_of',
    'target_direction',
    'subevent',
    'is_subevent_of',
    'antonym',
    'feminine',
    'has_feminine',
    'masculine',
    'has_masculine',
    'young',
    'has_young',
    'diminutive',
    'has_diminutive',
    'augmentative',
    'has_augmentative',
    'anto_gradable',
    'anto_simple',
    'anto_converse',
    'ir_synonym',
])


REVERSE_RELATIONS = {
    'hypernym': 'hyponym',
    'hyponym': 'hypernym',
    'instance_hypernym': 'instance_hyponym',
    'instance_hyponym': 'instance_hypernym',
    'antonym': 'antonym',
    'eq_synonym': 'eq_synonym',
    'similar': 'similar',
    'meronym': 'holonym',
    'holonym': 'meronym',
    'mero_location': 'holo_location',
    'holo_location': 'mero_location',
    'mero_member': 'holo_member',
    'holo_member': 'mero_member',
    'mero_part': 'holo_part',
    'holo_part': 'mero_part',
    'mero_portion': 'holo_portion',
    'holo_portion': 'mero_portion',
    'mero_substance': 'holo_substance',
    'holo_substance': 'mero_substance',
    'also': 'also',
    'state_of': 'be_in_state',
    'be_in_state': 'state_of',
    'causes': 'is_caused_by',
    'is_caused_by': 'causes',
    'subevent': 'is_subevent_of',
    'is_subevent_of': 'subevent',
    'manner_of': 'in_manner',
    'in_manner': 'manner_of',
    'attribute': 'attribute',
    'restricts': 'restricted_by',
    'restricted_by': 'restricts',
    'classifies': 'classified_by',
    'classified_by': 'classifies',
    'entails': 'is_entailed_by',
    'is_entailed_by': 'entails',
    'domain_topic': 'has_domain_topic',
    'has_domain_topic': 'domain_topic',
    'domain_region': 'has_domain_region',
    'has_domain_region': 'domain_region',
    'exemplifies': 'is_exemplified_by',
    'is_exemplified_by': 'exemplifies',
    'role': 'involved',
    'involved': 'role',
    'agent': 'involved_agent',
    'involved_agent': 'agent',
    'patient': 'involved_patient',
    'involved_patient': 'patient',
    'result': 'involved_result',
    'involved_result': 'result',
    'instrument': 'involved_instrument',
    'involved_instrument': 'instrument',
    'location': 'involved_location',
    'involved_location': 'location',
    'direction': 'involved_direction',
    'involved_direction': 'direction',
    'target_direction': 'involved_target_direction',
    'involved_target_direction': 'target_direction',
    'source_direction': 'involved_source_direction',
    'involved_source_direction': 'source_direction',
    'co_role': 'co_role',
    'co_agent_patient': 'co_patient_agent',
    'co_patient_agent': 'co_agent_patient',
    'co_agent_instrument': 'co_instrument_agent',
    'co_instrument_agent': 'co_agent_instrument',
    'co_agent_result': 'co_result_agent',
    'co_result_agent': 'co_agent_result',
    'co_patient_instrument': 'co_instrument_patient',
    'co_instrument_patient': 'co_patient_instrument',
    'co_result_instrument': 'co_instrument_result',
    'co_instrument_result': 'co_result_instrument',
    'pertainym': 'pertainym',
    'derivation': 'derivation',
    'simple_aspect_ip': 'simple_aspect_pi',
    'simple_aspect_pi': 'simple_aspect_ip',
    'secondary_aspect_ip': 'secondary_aspect_pi',
    'secondary_aspect_pi': 'secondary_aspect_ip',
    'feminine': 'has_feminine',
    'has_feminine': 'feminine',
    'masculine': 'has_masculine',
    'has_masculine': 'masculine',
    'young': 'has_young',
    'has_young': 'young',
    'diminutive': 'has_diminutive',
    'has_diminutive': 'diminutive',
    'augmentative': 'has_augmentative',
    'has_augmentative': 'augmentative',
    'anto_gradable': 'anto_gradable',
    'anto_simple': 'anto_simple',
    'anto_converse': 'anto_converse',
    'ir_synonym': 'ir_synonym',
    # 'participle': '',
    # 'other': '',
}

# Adjective Positions

ADJPOSITIONS = frozenset((
    'a',   # attributive
    'ip',  # immediate postnominal
    'p',   # predicative
))


# Parts of Speech

NOUN = 'n'  #:
VERB = 'v'  #:
ADJ = ADJECTIVE = 'a'  #:
ADV = ADVERB = 'r'  #:
ADJ_SAT = ADJECTIVE_SATELLITE = 's'  #:
PHRASE = 't'  #:
CONJ = CONJUNCTION = 'c'  #:
ADP = ADPOSITION = 'p'  #:
OTHER = 'x'  #:
UNKNOWN = 'u'  #:

PARTS_OF_SPEECH = frozenset((
    NOUN,
    VERB,
    ADJECTIVE,
    ADVERB,
    ADJECTIVE_SATELLITE,
    PHRASE,
    CONJUNCTION,
    ADPOSITION,
    OTHER,
    UNKNOWN,
))


# Lexicographer Files
# from https://wordnet.princeton.edu/documentation/lexnames5wn

LEXICOGRAPHER_FILES = {
    'adj.all': 0,
    'adj.pert': 1,
    'adv.all': 2,
    'noun.Tops': 3,
    'noun.act': 4,
    'noun.animal': 5,
    'noun.artifact': 6,
    'noun.attribute': 7,
    'noun.body': 8,
    'noun.cognition': 9,
    'noun.communication': 10,
    'noun.event': 11,
    'noun.feeling': 12,
    'noun.food': 13,
    'noun.group': 14,
    'noun.location': 15,
    'noun.motive': 16,
    'noun.object': 17,
    'noun.person': 18,
    'noun.phenomenon': 19,
    'noun.plant': 20,
    'noun.possession': 21,
    'noun.process': 22,
    'noun.quantity': 23,
    'noun.relation': 24,
    'noun.shape': 25,
    'noun.state': 26,
    'noun.substance': 27,
    'noun.time': 28,
    'verb.body': 29,
    'verb.change': 30,
    'verb.cognition': 31,
    'verb.communication': 32,
    'verb.competition': 33,
    'verb.consumption': 34,
    'verb.contact': 35,
    'verb.creation': 36,
    'verb.emotion': 37,
    'verb.motion': 38,
    'verb.perception': 39,
    'verb.possession': 40,
    'verb.social': 41,
    'verb.stative': 42,
    'verb.weather': 43,
    'adj.ppl': 44,
}

# resource types

_WORDNET = 'wordnet'
_ILI = 'ili'
</t>
<t tx="karstenw.20230225124003.1">"""Information Content is a corpus-based metrics of synset or sense
specificity.

"""

from typing import (
    Callable, Optional, Iterator, Iterable, Dict, List, Tuple, Set, TextIO
)
from pathlib import Path
from collections import Counter
from math import log

from wn._types import AnyPath
from wn._core import Synset, Wordnet
from wn.constants import NOUN, VERB, ADJ, ADV, ADJ_SAT
from wn.util import synset_id_formatter


# Just use a subset of all available parts of speech
IC_PARTS_OF_SPEECH = frozenset((NOUN, VERB, ADJ, ADV))
Freq = Dict[str, Dict[Optional[str], float]]


</t>
<t tx="karstenw.20230225124003.2">def information_content(synset: Synset, freq: Freq) -&gt; float:
    """Calculate the Information Content value for a synset.

    The information content of a synset is the negative log of the
    synset probability (see :func:`synset_probability`).

    """
    return -log(synset_probability(synset, freq))


</t>
<t tx="karstenw.20230225124003.3">def synset_probability(synset: Synset, freq: Freq) -&gt; float:
    """Calculate the synset probability.

    The synset probability is defined as freq(ss)/N where freq(ss) is
    the IC weight for the synset and N is the total IC weight for all
    synsets with the same part of speech.

    Note: this function is not generally used directly, but indirectly
    through :func:`information_content`.

    """
    pos_freq = freq[synset.pos]
    return pos_freq[synset.id] / pos_freq[None]


</t>
<t tx="karstenw.20230225124003.4">def _initialize(
    wordnet: Wordnet,
    smoothing: float,
) -&gt; Freq:
    """Populate an Information Content weight mapping to a smoothing value.

    All synsets in *wordnet* are inserted into the dictionary and
    mapped to *smoothing*.

    """
    freq: Freq = {
        pos: {synset.id: smoothing for synset in wordnet.synsets(pos=pos)}
        for pos in IC_PARTS_OF_SPEECH
    }
    # pretend ADJ_SAT is just ADJ
    for synset in wordnet.synsets(pos=ADJ_SAT):
        freq[ADJ][synset.id] = smoothing
    # also initialize totals (when synset is None) for each part-of-speech
    for pos in IC_PARTS_OF_SPEECH:
        freq[pos][None] = smoothing
    return freq


</t>
<t tx="karstenw.20230225124003.5">def compute(
    corpus: Iterable[str],
    wordnet: Wordnet,
    distribute_weight: bool = True,
    smoothing: float = 1.0
) -&gt; Freq:
    """Compute Information Content weights from a corpus.

    Arguments:
        corpus: An iterable of string tokens. This is a flat list of
            words and the order does not matter. Tokens may be single
            words or multiple words separated by a space.

        wordnet: An instantiated :class:`wn.Wordnet` object, used to
            look up synsets from words.

        distribute_weight: If :python:`True`, the counts for a word
            are divided evenly among all synsets for the word.

        smoothing: The initial value given to each synset.

    Example:
        &gt;&gt;&gt; import wn, wn.ic, wn.morphy
        &gt;&gt;&gt; ewn = wn.Wordnet('ewn:2020', lemmatizer=wn.morphy.morphy)
        &gt;&gt;&gt; freq = wn.ic.compute(["Dogs", "run", ".", "Cats", "sleep", "."], ewn)
        &gt;&gt;&gt; dog = ewn.synsets('dog', pos='n')[0]
        &gt;&gt;&gt; cat = ewn.synsets('cat', pos='n')[0]
        &gt;&gt;&gt; frog = ewn.synsets('frog', pos='n')[0]
        &gt;&gt;&gt; freq['n'][dog.id]
        1.125
        &gt;&gt;&gt; freq['n'][cat.id]
        1.1
        &gt;&gt;&gt; freq['n'][frog.id]  # no occurrence; smoothing value only
        1.0
        &gt;&gt;&gt; carnivore = dog.lowest_common_hypernyms(cat)[0]
        &gt;&gt;&gt; freq['n'][carnivore.id]
        1.3250000000000002
    """
    freq = _initialize(wordnet, smoothing)
    counts = Counter(corpus)

    hypernym_cache: Dict[Synset, List[Synset]] = {}
    for word, count in counts.items():
        synsets = wordnet.synsets(word)
        num = len(synsets)
        if num == 0:
            continue

        weight = float(count / num if distribute_weight else count)

        for synset in synsets:
            pos = synset.pos
            if pos == ADJ_SAT:
                pos = ADJ
            if pos not in IC_PARTS_OF_SPEECH:
                continue

            freq[pos][None] += weight

            # The following while-loop is equivalent to:
            #
            # freq[pos][synset.id] += weight
            # for path in synset.hypernym_paths():
            #     for ss in path:
            #         freq[pos][ss.id] += weight
            #
            # ...but it caches hypernym lookups for speed

            agenda: List[Tuple[Synset, Set[Synset]]] = [(synset, set())]
            while agenda:
                ss, seen = agenda.pop()

                # avoid cycles
                if ss in seen:
                    continue

                freq[pos][ss.id] += weight

                if ss not in hypernym_cache:
                    hypernym_cache[ss] = ss.hypernyms()
                agenda.extend((hyp, seen | {ss}) for hyp in hypernym_cache[ss])

    return freq


</t>
<t tx="karstenw.20230225124003.6">def load(
    source: AnyPath,
    wordnet: Wordnet,
    get_synset_id: Optional[Callable] = None,
) -&gt; Freq:
    """Load an Information Content mapping from a file.

    Arguments:

        source: A path to an information content weights file.

        wordnet: A :class:`wn.Wordnet` instance with synset
            identifiers matching the offsets in the weights file.

        get_synset_id: A callable that takes a synset offset and part
            of speech and returns a synset ID valid in *wordnet*.

    Raises:

        :class:`wn.Error`: If *wordnet* does not have exactly one
            lexicon.

    Example:

        &gt;&gt;&gt; import wn, wn.ic
        &gt;&gt;&gt; pwn = wn.Wordnet('pwn:3.0')
        &gt;&gt;&gt; path = '~/nltk_data/corpora/wordnet_ic/ic-brown-resnik-add1.dat'
        &gt;&gt;&gt; freq = wn.ic.load(path, pwn)

    """
    source = Path(source).expanduser().resolve(strict=True)
    assert len(wordnet.lexicons()) == 1
    lexid = wordnet.lexicons()[0].id
    if get_synset_id is None:
        get_synset_id = synset_id_formatter(prefix=lexid)

    freq = _initialize(wordnet, 0.0)

    with source.open() as icfile:
        for offset, pos, weight, is_root in _parse_ic_file(icfile):
            ssid = get_synset_id(offset=offset, pos=pos)
            # synset = wordnet.synset(ssid)
            freq[pos][ssid] = weight
            if is_root:
                freq[pos][None] += weight
    return freq


</t>
<t tx="karstenw.20230225124003.7">def _parse_ic_file(icfile: TextIO) -&gt; Iterator[Tuple[int, str, float, bool]]:
    """Parse the Information Content file.

    A sample of the format is::

        wnver::eOS9lXC6GvMWznF1wkZofDdtbBU
        1740n 1915712 ROOT
        1930n 859272
        2137n 1055337

    """
    next(icfile)  # skip header
    for line in icfile:
        ssinfo, value, *isroot = line.split()
        yield (int(ssinfo[:-1]),
               ssinfo[-1],
               float(value),
               bool(isroot))
</t>
<t tx="karstenw.20230225124007.1">"""
Reader for the Lexical Markup Framework (LMF) format.
"""

from typing import (
    Type,
    List,
    Tuple,
    Dict,
    Optional,
    TextIO,
    BinaryIO,
    Any,
    Union,
    cast
)
import sys
if sys.version_info &gt;= (3, 8):
    from typing import TypedDict, Literal
else:
    from typing_extensions import TypedDict, Literal
import re
from pathlib import Path
import xml.etree.ElementTree as ET  # for general XML parsing
import xml.parsers.expat  # for fast scanning of Lexicon versions
from xml.sax.saxutils import quoteattr

import wn
from wn._types import AnyPath, VersionInfo
from wn._util import is_xml, version_info
from wn.util import ProgressHandler, ProgressBar


</t>
<t tx="karstenw.20230225124007.10">class ExternalSynset(_HasId, _External, total=False):
    definitions: List[Definition]
    relations: List[Relation]
    examples: List[Example]


</t>
<t tx="karstenw.20230225124007.11">class Count(_HasMeta):
    value: int


</t>
<t tx="karstenw.20230225124007.12">class Sense(_HasId, _HasSynset, _HasMeta, total=False):
    relations: List[Relation]
    examples: List[Example]
    counts: List[Count]
    lexicalized: bool
    adjposition: str
    subcat: List[str]


</t>
<t tx="karstenw.20230225124007.13">class ExternalSense(_HasId, _External, total=False):
    relations: List[Relation]
    examples: List[Example]
    counts: List[Count]


</t>
<t tx="karstenw.20230225124007.14">class Pronunciation(_HasText, total=False):
    variety: str
    notation: str
    phonemic: bool
    audio: str


</t>
<t tx="karstenw.20230225124007.15">class Tag(_HasText):
    category: str


</t>
<t tx="karstenw.20230225124007.16">class _FormChildren(TypedDict, total=False):
    pronunciations: List[Pronunciation]
    tags: List[Tag]


</t>
<t tx="karstenw.20230225124007.17">class Lemma(_MaybeScript, _FormChildren):
    writtenForm: str
    partOfSpeech: str


</t>
<t tx="karstenw.20230225124007.18">class ExternalLemma(_FormChildren, _External):
    ...


</t>
<t tx="karstenw.20230225124007.19">class Form(_MaybeId, _MaybeScript, _FormChildren):
    writtenForm: str


</t>
<t tx="karstenw.20230225124007.2">class LMFError(wn.Error):
    """Raised on invalid LMF-XML documents."""


</t>
<t tx="karstenw.20230225124007.20">class ExternalForm(_HasId, _FormChildren, _External):
    ...


</t>
<t tx="karstenw.20230225124007.21">class _SyntacticBehaviourBase(_MaybeId):
    subcategorizationFrame: str


</t>
<t tx="karstenw.20230225124007.22">class SyntacticBehaviour(_SyntacticBehaviourBase, total=False):
    senses: List[str]


</t>
<t tx="karstenw.20230225124007.23">class _LexicalEntryBase(_HasId, _HasMeta, total=False):
    forms: List[Form]
    senses: List[Sense]
    frames: List[SyntacticBehaviour]


</t>
<t tx="karstenw.20230225124007.24">class LexicalEntry(_LexicalEntryBase):
    lemma: Lemma


</t>
<t tx="karstenw.20230225124007.25">class ExternalLexicalEntry(_HasId, _External, total=False):
    lemma: ExternalLemma
    forms: List[Union[Form, ExternalForm]]
    senses: List[Union[Sense, ExternalSense]]


</t>
<t tx="karstenw.20230225124007.26">class Dependency(_HasId, _HasVersion, total=False):
    url: str


</t>
<t tx="karstenw.20230225124007.27">class _LexiconBase(_HasMeta):
    id: str
    version: str
    label: str
    language: str
    email: str
    license: str


</t>
<t tx="karstenw.20230225124007.28">class Lexicon(_LexiconBase, total=False):
    url: str
    citation: str
    logo: str
    requires: List[Dependency]
    entries: List[LexicalEntry]
    synsets: List[Synset]
    frames: List[SyntacticBehaviour]


</t>
<t tx="karstenw.20230225124007.29">class LexiconExtension(_LexiconBase, total=False):
    url: str
    citation: str
    logo: str
    extends: Dependency
    requires: List[Dependency]
    entries: List[Union[LexicalEntry, ExternalLexicalEntry]]
    synsets: List[Union[Synset, ExternalSynset]]
    frames: List[SyntacticBehaviour]


</t>
<t tx="karstenw.20230225124007.3">class LMFWarning(Warning):
    """Issued on non-conforming LFM values."""


SUPPORTED_VERSIONS = {'1.0', '1.1'}
_XMLDECL = b'&lt;?xml version="1.0" encoding="UTF-8"?&gt;'
_DOCTYPE = '&lt;!DOCTYPE LexicalResource SYSTEM "{schema}"&gt;'
_SCHEMAS = {
    '1.0': 'http://globalwordnet.github.io/schemas/WN-LMF-1.0.dtd',
    '1.1': 'http://globalwordnet.github.io/schemas/WN-LMF-1.1.dtd',
}
_DOCTYPES = {
    _DOCTYPE.format(schema=schema): version for version, schema in _SCHEMAS.items()
}

_DC_URIS = {
    '1.0': 'http://purl.org/dc/elements/1.1/',
    '1.1': 'https://globalwordnet.github.io/schemas/dc/',
}
_DC_ATTRS = [
    'contributor',
    'coverage',
    'creator',
    'date',
    'description',
    'format',
    'identifier',
    'publisher',
    'relation',
    'rights',
    'source',
    'subject',
    'title',
    'type',
]
_NS_ATTRS = {
    version: dict(
        [(f'{uri} {attr}', attr) for attr in _DC_ATTRS]
        + [('status', 'status'),
           ('note', 'note'),
           ('confidenceScore', 'confidenceScore')]
    )
    for version, uri in _DC_URIS.items()
}

_LMF_1_0_ELEMS: Dict[str, str] = {
    'LexicalResource': 'lexical-resource',
    'Lexicon': 'lexicons',
    'LexicalEntry': 'entries',
    'Lemma': 'lemma',
    'Form': 'forms',
    'Tag': 'tags',
    'Sense': 'senses',
    'SenseRelation': 'relations',
    'Example': 'examples',
    'Count': 'counts',
    'SyntacticBehaviour': 'frames',
    'Synset': 'synsets',
    'Definition': 'definitions',
    'ILIDefinition': 'ili_definition',
    'SynsetRelation': 'relations',
}
_LMF_1_1_ELEMS = dict(_LMF_1_0_ELEMS)
_LMF_1_1_ELEMS.update({
    'Requires': 'requires',
    'Extends': 'extends',
    'Pronunciation': 'pronunciations',
    'LexiconExtension': 'lexicons',
    'ExternalLexicalEntry': 'entries',
    'ExternalLemma': 'lemma',
    'ExternalForm': 'forms',
    'ExternalSense': 'senses',
    'ExternalSynset': 'synsets',
})
_VALID_ELEMS = {
    '1.0': _LMF_1_0_ELEMS,
    '1.1': _LMF_1_1_ELEMS,
}
_LIST_ELEMS = {  # elements that collect into lists
    'Lexicon',
    'LexicalEntry',
    'Form',
    'Pronunciation',
    'Tag',
    'Sense',
    'SenseRelation',
    'Example',
    'Count',
    'Synset',
    'Definition',
    'SynsetRelation',
    'SyntacticBehaviour',
    'LexiconExtension',
    'Requires',
    'ExternalLexicalEntry',
    'ExternalForm',
    'ExternalSense',
    'ExternalSynset',
}
_CDATA_ELEMS = {  # elements with inner text
    'Pronunciation',
    'Tag',
    'Definition',
    'ILIDefinition',
    'Example',
    'Count',
}
_META_ELEMS = {  # elements with metadata
    'Lexicon',
    'LexicalEntry',
    'Sense',
    'SenseRelation',
    'Example',
    'Count',
    'Synset',
    'Definition',
    'ILIDefinition',
    'SynsetRelation',
    'LexiconExtension',
}


# WN-LMF Modeling ######################################################

# WN-LMF type-checking is handled via TypedDicts.  Inheritance and
# `total=False` are used to model optionality. For more information
# about this tactic, see https://www.python.org/dev/peps/pep-0589/.

</t>
<t tx="karstenw.20230225124007.30">class LexicalResource(TypedDict):
    lmf_version: str
    lexicons: List[Union[Lexicon, LexiconExtension]]


# Reading ##############################################################

</t>
<t tx="karstenw.20230225124007.31">def is_lmf(source: AnyPath) -&gt; bool:
    """Return True if *source* is a WN-LMF file."""
    source = Path(source).expanduser()
    if not is_xml(source):
        return False
    with source.open(mode='rb') as fh:
        try:
            _read_header(fh)
        except LMFError:
            return False
    return True


</t>
<t tx="karstenw.20230225124007.32">def _read_header(fh: BinaryIO) -&gt; str:
    xmldecl = fh.readline().rstrip().replace(b"'", b'"')
    doctype = fh.readline().rstrip().replace(b"'", b'"')

    if xmldecl != _XMLDECL:
        raise LMFError('invalid or missing XML declaration')

    # the XML declaration states that the file is UTF-8 (other
    # encodings are not allowed)
    doctype_decoded = doctype.decode('utf-8')
    if doctype_decoded not in _DOCTYPES:
        raise LMFError('invalid or missing DOCTYPE declaration')

    return _DOCTYPES[doctype_decoded]


</t>
<t tx="karstenw.20230225124007.33">def scan_lexicons(source: AnyPath) -&gt; List[Dict]:
    """Scan *source* and return only the top-level lexicon info."""

    source = Path(source).expanduser()
    infos: List[Dict] = []

    lex_re = re.compile(b'&lt;(Lexicon|LexiconExtension|Extends)\\b([^&gt;]*)&gt;', flags=re.M)
    attr_re = re.compile(b'''\\b(id|version|label)=["']([^"']+)["']''', flags=re.M)

    with open(source, 'rb') as fh:
        for m in lex_re.finditer(fh.read()):
            lextype, remainder = m.groups()
            info = {_m.group(1).decode('utf-8'): _m.group(2).decode('utf-8')
                    for _m in attr_re.finditer(remainder)}
            if 'id' not in info or 'version' not in info:
                raise LMFError(f'&lt;{lextype.decode("utf-8")}&gt; missing id or version')
            if lextype != b'Extends':
                infos.append(info)
            elif len(infos) &gt; 0:
                infos[-1]['extends'] = info
            else:
                raise LMFError('invalid use of &lt;Extends&gt; in WN-LMF file')

    return infos


_Elem = Dict[str, Any]  # basic type for the loaded XML data


</t>
<t tx="karstenw.20230225124007.34">def load(
    source: AnyPath,
    progress_handler: Optional[Type[ProgressHandler]] = ProgressBar
) -&gt; LexicalResource:
    """Load wordnets encoded in the WN-LMF format.

    Args:
        source: path to a WN-LMF file
    """
    source = Path(source).expanduser()
    if progress_handler is None:
        progress_handler = ProgressHandler

    version, num_elements = _quick_scan(source)
    progress = progress_handler(
        message='Read', total=num_elements, refresh_interval=10000
    )

    root: Dict[str, _Elem] = {}
    parser = _make_parser(root, version, progress)

    with open(source, 'rb') as fh:
        try:
            parser.ParseFile(fh)
        except xml.parsers.expat.ExpatError as exc:
            raise LMFError('invalid or ill-formed XML') from exc

    progress.close()

    resource: LexicalResource = {
        'lmf_version': version,
        'lexicons': [_validate(lex)
                     for lex in root['lexical-resource'].get('lexicons', [])],
    }

    return resource


</t>
<t tx="karstenw.20230225124007.35">def _quick_scan(source: Path) -&gt; Tuple[str, int]:
    with source.open('rb') as fh:
        version = _read_header(fh)
        # _read_header() only reads the first 2 lines
        remainder = fh.read()
        num_elements = remainder.count(b'&lt;/') + remainder.count(b'/&gt;')
    return version, num_elements


</t>
<t tx="karstenw.20230225124007.36">def _make_parser(root, version, progress):  # noqa: C901
    stack = [root]
    ELEMS = _VALID_ELEMS[version]
    NS_ATTRS = _NS_ATTRS[version]
    CDATA_ELEMS = _CDATA_ELEMS &amp; set(ELEMS)
    LIST_ELEMS = _LIST_ELEMS &amp; set(ELEMS)

    p = xml.parsers.expat.ParserCreate(namespace_separator=' ')
    has_text = False

    def start(name, attrs):
        nonlocal has_text
        if name in CDATA_ELEMS:
            has_text = True

        if name in _META_ELEMS:
            meta = {}
            for attr in list(attrs):
                if attr in NS_ATTRS:
                    meta[NS_ATTRS[attr]] = attrs.pop(attr)
            attrs['meta'] = meta or None

        if name.startswith('External'):
            attrs['external'] = True

        parent = stack[-1]
        key = ELEMS.get(name)
        if name in LIST_ELEMS:
            parent.setdefault(key, []).append(attrs)
        elif key is None or key in parent:
            raise _unexpected(name, p)
        else:
            parent[key] = attrs

        stack.append(attrs)

    def char_data(data):
        if has_text:
            parent = stack[-1]
            # sometimes the buffering occurs in the middle of text; if
            # so, append the current data
            if 'text' in parent:
                parent['text'] += data
            else:
                parent['text'] = data

    def end(name):
        nonlocal has_text
        has_text = False
        stack.pop()
        progress.update(force=(name == 'LexicalResource'))

    p.StartElementHandler = start
    p.EndElementHandler = end
    p.CharacterDataHandler = char_data

    return p


</t>
<t tx="karstenw.20230225124007.37">def _unexpected(name: str, p: xml.parsers.expat.XMLParserType) -&gt; LMFError:
    return LMFError(f'unexpected element at line {p.CurrentLineNumber}: {name}')


# Validation ###########################################################

</t>
<t tx="karstenw.20230225124007.38">def _validate(elem: _Elem) -&gt; Union[Lexicon, LexiconExtension]:
    ext = elem.get('extends')
    if ext:
        assert 'id' in ext
        assert 'version' in ext
        _validate_lexicon(elem, True)
        return cast(LexiconExtension, elem)
    else:
        _validate_lexicon(elem, False)
        return cast(Lexicon, elem)


</t>
<t tx="karstenw.20230225124007.39">def _validate_lexicon(elem: _Elem, extension: bool) -&gt; None:
    for attr in 'id', 'version', 'label', 'language', 'email', 'license':
        assert attr in elem, f'&lt;Lexicon&gt; missing required attribute: {attr}'
    for dep in elem.get('requires', []):
        assert 'id' in dep
        assert 'version' in dep
    _validate_entries(elem.get('entries', []), extension)
    _validate_synsets(elem.get('synsets', []), extension)
    _validate_frames(elem.get('frames', []))


</t>
<t tx="karstenw.20230225124007.4">class Metadata(TypedDict, total=False):
    contributor: str
    coverage: str
    creator: str
    date: str
    description: str
    format: str
    identifier: str
    publisher: str
    relation: str
    rights: str
    source: str
    subject: str
    title: str
    type: str
    status: str
    note: str
    confidenceScore: float


_HasId = TypedDict('_HasId', {'id': str})
_HasVersion = TypedDict('_HasVersion', {'version': str})
_HasILI = TypedDict('_HasILI', {'ili': str})
_HasSynset = TypedDict('_HasSynset', {'synset': str})
_MaybeId = TypedDict('_MaybeId', {'id': str}, total=False)
_HasText = TypedDict('_HasText', {'text': str})
_MaybeScript = TypedDict('_MaybeScript', {'script': str}, total=False)
_HasMeta = TypedDict('_HasMeta', {'meta': Optional[Metadata]})
_External = TypedDict('_External', {'external': Literal['true']})


</t>
<t tx="karstenw.20230225124007.40">def _validate_entries(elems: List[_Elem], extension: bool) -&gt; None:
    for elem in elems:
        assert 'id' in elem
        if not extension:
            assert not elem.get('external')
        lemma = elem.get('lemma')
        if not elem.get('external'):
            assert lemma is not None
            elem.setdefault('meta')
        # lemma and forms are the same except for partOfSpeech and id
        if lemma is not None and not lemma.get('external'):
            assert 'partOfSpeech' in lemma
        for form in elem.get('forms', []):
            assert not form.get('external') or form.get('id')
        _validate_forms(([lemma] if lemma else []) + elem.get('forms', []), extension)
        _validate_senses(elem.get('senses', []), extension)
        _validate_frames(elem.get('frames', []))


</t>
<t tx="karstenw.20230225124007.41">def _validate_forms(elems: List[_Elem], extension: bool) -&gt; None:
    for elem in elems:
        if not extension:
            assert not elem.get('external')
        if not elem.get('external'):
            assert 'writtenForm' in elem
        for pron in elem.get('pronunciations', []):
            pron.setdefault('text', '')
            if pron.get('phonemic'):
                pron['phonemic'] = False if pron['phonemic'] == 'false' else True
        for tag in elem.get('tags', []):
            tag.setdefault('text', '')
            assert 'category' in tag


</t>
<t tx="karstenw.20230225124007.42">def _validate_senses(elems: List[_Elem], extension: bool) -&gt; None:
    for elem in elems:
        assert 'id' in elem
        if not extension:
            assert not elem.get('external')
        if not elem.get('external'):
            assert 'synset' in elem
            elem.setdefault('meta')
        for rel in elem.get('relations', []):
            assert 'target' in rel
            assert 'relType' in rel
            rel.setdefault('meta')
        for ex in elem.get('examples', []):
            ex.setdefault('text', '')
            ex.setdefault('meta')
        for cnt in elem.get('counts', []):
            assert 'text' in cnt
            cnt['value'] = int(cnt.pop('text'))
            cnt.setdefault('meta')
        if elem.get('lexicalized'):
            elem['lexicalized'] = False if elem['lexicalized'] == 'false' else True
        if elem.get('subcat'):
            elem['subcat'] = elem['subcat'].split()


</t>
<t tx="karstenw.20230225124007.43">def _validate_frames(elems: List[_Elem]) -&gt; None:
    for elem in elems:
        assert 'subcategorizationFrame' in elem
        if elem.get('senses'):
            elem['senses'] = elem['senses'].split()


</t>
<t tx="karstenw.20230225124007.44">def _validate_synsets(elems: List[_Elem], extension: bool) -&gt; None:
    for elem in elems:
        assert 'id' in elem
        if not extension:
            assert not elem.get('external')
        if not elem.get('external'):
            assert 'ili' in elem
            elem.setdefault('meta')
        for defn in elem.get('definitions', []):
            defn.setdefault('text', '')
            defn.setdefault('meta')
        for rel in elem.get('relations', []):
            assert 'target' in rel
            assert 'relType' in rel
            rel.setdefault('meta')
        for ex in elem.get('examples', []):
            ex.setdefault('text', '')
            ex.setdefault('meta')
        if elem.get('lexicalized'):
            elem['lexicalized'] = False if elem['lexicalized'] == 'false' else True
        if elem.get('members'):
            elem['members'] = elem['members'].split()


</t>
<t tx="karstenw.20230225124007.45">def _validate_metadata(elem: _Elem) -&gt; None:
    if elem.get('confidenceScore'):
        elem['confidenceScore'] = float(elem['confidenceScore'])


# Serialization ########################################################

</t>
<t tx="karstenw.20230225124007.46">def dump(resource: LexicalResource, destination: AnyPath) -&gt; None:
    """Write wordnets in the WN-LMF format.

    Args:
        lexicons: a list of :class:`Lexicon` objects
    """
    version = resource['lmf_version']
    if version not in SUPPORTED_VERSIONS:
        raise LMFError(f'invalid version: {version}')
    destination = Path(destination).expanduser()
    doctype = _DOCTYPE.format(schema=_SCHEMAS[version])
    dc_uri = _DC_URIS[version]
    _version = version_info(version)
    with destination.open('wt', encoding='utf-8') as out:
        print(_XMLDECL.decode('utf-8'), file=out)
        print(doctype, file=out)
        print(f'&lt;LexicalResource xmlns:dc="{dc_uri}"&gt;', file=out)
        for lexicon in resource['lexicons']:
            _dump_lexicon(lexicon, out, _version)
        print('&lt;/LexicalResource&gt;', file=out)


</t>
<t tx="karstenw.20230225124007.47">def _dump_lexicon(
    lexicon: Union[Lexicon, LexiconExtension],
    out: TextIO,
    version: VersionInfo
) -&gt; None:
    lexicontype = 'LexiconExtension' if lexicon.get('extends') else 'Lexicon'
    attrib = _build_lexicon_attrib(lexicon, version)
    attrdelim = '\n' + (' ' * len(f'  &lt;{lexicontype} '))
    attrs = attrdelim.join(
        f'{attr}={quoteattr(str(val))}' for attr, val in attrib.items()
    )
    print(f'  &lt;{lexicontype} {attrs}&gt;', file=out)

    if version &gt;= (1, 1):
        if lexicontype == 'LexiconExtension':
            assert lexicon.get('extends')
            lexicon = cast(LexiconExtension, lexicon)
            _dump_dependency(lexicon['extends'], 'Extends', out)
        for req in lexicon.get('requires', []):
            _dump_dependency(req, 'Requires', out)

    for entry in lexicon.get('entries', []):
        _dump_lexical_entry(entry, out, version)

    for synset in lexicon.get('synsets', []):
        _dump_synset(synset, out, version)

    if version &gt;= (1, 1):
        for sb in lexicon.get('frames', []):
            _dump_syntactic_behaviour(sb, out, version)

    print(f'  &lt;/{lexicontype}&gt;', file=out)


</t>
<t tx="karstenw.20230225124007.48">def _build_lexicon_attrib(
    lexicon: Union[Lexicon, LexiconExtension],
    version: VersionInfo
) -&gt; Dict[str, str]:
    attrib = {
        'id': lexicon['id'],
        'label': lexicon['label'],
        'language': lexicon['language'],
        'email': lexicon['email'],
        'license': lexicon['license'],
        'version': lexicon['version'],
    }
    if lexicon.get('url'):
        attrib['url'] = lexicon['url']
    if lexicon.get('citation'):
        attrib['citation'] = lexicon['citation']
    if version &gt;= (1, 1) and lexicon.get('logo'):
        attrib['logo'] = lexicon['logo']
    attrib.update(_meta_dict(lexicon.get('meta')))
    return attrib


</t>
<t tx="karstenw.20230225124007.49">def _dump_dependency(
    dep: Dependency, deptype: str, out: TextIO
) -&gt; None:
    attrib = {'id': dep['id'], 'version': dep['version']}
    if dep.get('url'):
        attrib['url'] = dep['url']
    elem = ET.Element(deptype, attrib=attrib)
    print(_tostring(elem, 2), file=out)


</t>
<t tx="karstenw.20230225124007.5">class ILIDefinition(_HasText, _HasMeta):
    ...


</t>
<t tx="karstenw.20230225124007.50">def _dump_lexical_entry(
    entry: Union[LexicalEntry, ExternalLexicalEntry],
    out: TextIO,
    version: VersionInfo,
) -&gt; None:
    frames = []
    attrib = {'id': entry['id']}
    if entry.get('external', False):
        elem = ET.Element('ExternalLexicalEntry', attrib=attrib)
        if entry.get('lemma'):
            assert entry['lemma'].get('external', False)
            elem.append(_build_lemma(entry['lemma'], version))
    else:
        entry = cast(LexicalEntry, entry)
        attrib.update(_meta_dict(entry.get('meta')))
        elem = ET.Element('LexicalEntry', attrib=attrib)
        elem.append(_build_lemma(entry['lemma'], version))
        if version &lt; (1, 1):
            frames = [_build_syntactic_behaviour(sb, version)
                      for sb in entry.get('frames', [])]
    elem.extend([_build_form(form, version) for form in entry.get('forms', [])])
    elem.extend([_build_sense(sense, version)
                 for sense in entry.get('senses', [])])
    elem.extend(frames)
    print(_tostring(elem, 2), file=out)


</t>
<t tx="karstenw.20230225124007.51">def _build_lemma(
    lemma: Union[Lemma, ExternalLemma],
    version: VersionInfo
) -&gt; ET.Element:
    if lemma.get('external', False):
        elem = ET.Element('ExternalLemma')
    else:
        lemma = cast(Lemma, lemma)
        attrib = {'writtenForm': lemma['writtenForm']}
        if lemma.get('script'):
            attrib['script'] = lemma['script']
        attrib['partOfSpeech'] = lemma['partOfSpeech']
        elem = ET.Element('Lemma', attrib=attrib)
    if version &gt;= (1, 1):
        for pron in lemma.get('pronunciations', []):
            elem.append(_build_pronunciation(pron))
    for tag in lemma.get('tags', []):
        elem.append(_build_tag(tag))
    return elem


</t>
<t tx="karstenw.20230225124007.52">def _build_form(form: Union[Form, ExternalForm], version: VersionInfo) -&gt; ET.Element:
    attrib = {}
    if version &gt;= (1, 1) and form['id']:
        attrib['id'] = form['id']
    if form.get('external', False):
        elem = ET.Element('ExternalForm', attrib=attrib)
    else:
        form = cast(Form, form)
        attrib['writtenForm'] = form['writtenForm']
        if form.get('script'):
            attrib['script'] = form['script']
        elem = ET.Element('Form', attrib=attrib)
    if version &gt;= (1, 1):
        for pron in form.get('pronunciations', []):
            elem.append(_build_pronunciation(pron))
    for tag in form.get('tags', []):
        elem.append(_build_tag(tag))
    return elem


</t>
<t tx="karstenw.20230225124007.53">def _build_pronunciation(pron: Pronunciation) -&gt; ET.Element:
    attrib = {}
    if pron.get('variety'):
        attrib['variety'] = pron['variety']
    if pron.get('notation'):
        attrib['notation'] = pron['notation']
    if not pron.get('phonemic', True):
        attrib['phonemic'] = 'false'
    if pron.get('audio'):
        attrib['audio'] = pron['audio']
    elem = ET.Element('Pronunciation', attrib=attrib)
    elem.text = pron['text']
    return elem


</t>
<t tx="karstenw.20230225124007.54">def _build_tag(tag: Tag) -&gt; ET.Element:
    elem = ET.Element('Tag', category=tag['category'])
    elem.text = tag['text']
    return elem


</t>
<t tx="karstenw.20230225124007.55">def _build_sense(
    sense: Union[Sense, ExternalSense],
    version: VersionInfo,
) -&gt; ET.Element:
    attrib = {'id': sense['id']}
    if sense.get('external'):
        elem = ET.Element('ExternalSense', attrib=attrib)
    else:
        sense = cast(Sense, sense)
        attrib['synset'] = sense['synset']
        attrib.update(_meta_dict(sense.get('meta')))
        if not sense.get('lexicalized', True):
            attrib['lexicalized'] = 'false'
        if sense.get('adjposition'):
            attrib['adjposition'] = sense['adjposition']
        if version &gt;= (1, 1) and sense.get('subcat'):
            attrib['subcat'] = ' '.join(sense['subcat'])
        elem = ET.Element('Sense', attrib=attrib)
    elem.extend([_build_relation(rel, 'SenseRelation')
                 for rel in sense.get('relations', [])])
    elem.extend([_build_example(ex) for ex in sense.get('examples', [])])
    elem.extend([_build_count(cnt) for cnt in sense.get('counts', [])])
    return elem


</t>
<t tx="karstenw.20230225124007.56">def _build_example(example: Example) -&gt; ET.Element:
    elem = ET.Element('Example')
    elem.text = example['text']
    if example.get('language'):
        elem.set('language', example['language'])
    return elem


</t>
<t tx="karstenw.20230225124007.57">def _build_count(count: Count) -&gt; ET.Element:
    elem = ET.Element('Count', attrib=_meta_dict(count.get('meta')))
    elem.text = str(count['value'])
    return elem


</t>
<t tx="karstenw.20230225124007.58">def _dump_synset(
    synset: Union[Synset, ExternalSynset],
    out: TextIO,
    version: VersionInfo
) -&gt; None:
    attrib: Dict[str, str] = {'id': synset['id']}
    if synset.get('external', False):
        elem = ET.Element('ExternalSynset', attrib=attrib)
        elem.extend([_build_definition(defn) for defn in synset.get('definitions', [])])
    else:
        synset = cast(Synset, synset)
        attrib['ili'] = synset['ili']
        if synset.get('partOfSpeech'):
            attrib['partOfSpeech'] = synset['partOfSpeech']
        if not synset.get('lexicalized', True):
            attrib['lexicalized'] = 'false'
        if version &gt;= (1, 1):
            if synset.get('members'):
                attrib['members'] = ' '.join(synset['members'])
            if synset.get('lexfile'):
                attrib['lexfile'] = synset['lexfile']
        attrib.update(_meta_dict(synset.get('meta')))
        elem = ET.Element('Synset', attrib=attrib)
        elem.extend([_build_definition(defn) for defn in synset.get('definitions', [])])
        if synset.get('ili_definition'):
            elem.append(_build_ili_definition(synset['ili_definition']))
    elem.extend([_build_relation(rel, 'SynsetRelation')
                 for rel in synset.get('relations', [])])
    elem.extend([_build_example(ex) for ex in synset.get('examples', [])])
    print(_tostring(elem, 2), file=out)


</t>
<t tx="karstenw.20230225124007.59">def _build_definition(definition: Definition) -&gt; ET.Element:
    attrib = {}
    if definition.get('language'):
        attrib['language'] = definition['language']
    if definition.get('sourceSense'):
        attrib['sourceSense'] = definition['sourceSense']
    attrib.update(_meta_dict(definition.get('meta')))
    elem = ET.Element('Definition', attrib=attrib)
    elem.text = definition['text']
    return elem


</t>
<t tx="karstenw.20230225124007.6">class Definition(_HasText, _HasMeta, total=False):
    language: str
    sourceSense: str


</t>
<t tx="karstenw.20230225124007.60">def _build_ili_definition(ili_definition: ILIDefinition) -&gt; ET.Element:
    elem = ET.Element('ILIDefinition', attrib=_meta_dict(ili_definition.get('meta')))
    elem.text = ili_definition['text']
    return elem


</t>
<t tx="karstenw.20230225124007.61">def _build_relation(relation: Relation, elemtype: str) -&gt; ET.Element:
    attrib = {'target': relation['target'], 'relType': relation['relType']}
    attrib.update(_meta_dict(relation.get('meta')))
    return ET.Element(elemtype, attrib=attrib)


</t>
<t tx="karstenw.20230225124007.62">def _dump_syntactic_behaviour(
    syntactic_behaviour: SyntacticBehaviour,
    out: TextIO,
    version: VersionInfo
) -&gt; None:
    elem = _build_syntactic_behaviour(syntactic_behaviour, version)
    print('    ' + _tostring(elem, 2), file=out)


</t>
<t tx="karstenw.20230225124007.63">def _build_syntactic_behaviour(
    syntactic_behaviour: SyntacticBehaviour,
    version: VersionInfo
) -&gt; ET.Element:
    attrib = {'subcategorizationFrame': syntactic_behaviour['subcategorizationFrame']}
    if version &gt;= (1, 1) and syntactic_behaviour.get('id'):
        attrib['id'] = syntactic_behaviour['id']
    elif version &lt; (1, 1) and syntactic_behaviour.get('senses'):
        attrib['senses'] = ' '.join(syntactic_behaviour['senses'])
    return ET.Element('SyntacticBehaviour', attrib=attrib)


</t>
<t tx="karstenw.20230225124007.64">def _tostring(
        elem: ET.Element, level: int, short_empty_elements: bool = True
) -&gt; str:
    _indent(elem, level)
    return ('  ' * level) + ET.tostring(
        elem,
        encoding='unicode',
        short_empty_elements=short_empty_elements
    )


</t>
<t tx="karstenw.20230225124007.65">def _indent(elem: ET.Element, level: int) -&gt; None:
    self_indent = '\n' + '  ' * level
    child_indent = self_indent + '  '
    if len(elem):
        if not elem.text or not elem.text.strip():
            elem.text = child_indent
        for child in elem[:-1]:
            _indent(child, level + 1)
            child.tail = child_indent
        _indent(elem[-1], level + 1)
        elem[-1].tail = self_indent


</t>
<t tx="karstenw.20230225124007.66">def _meta_dict(meta: Optional[Metadata]) -&gt; Dict[str, str]:
    if meta is not None:
        # Literal keys are required for typing purposes, so first
        # construct the dict and then remove those that weren't specified.
        d = {
            'dc:contributor': meta.get('contributor', ''),
            'dc:coverage': meta.get('coverage', ''),
            'dc:creator': meta.get('creator', ''),
            'dc:date': meta.get('date', ''),
            'dc:description': meta.get('description', ''),
            'dc:format': meta.get('format', ''),
            'dc:identifier': meta.get('identifier', ''),
            'dc:publisher': meta.get('publisher', ''),
            'dc:relation': meta.get('relation', ''),
            'dc:rights': meta.get('rights', ''),
            'dc:source': meta.get('source', ''),
            'dc:subject': meta.get('subject', ''),
            'dc:title': meta.get('title', ''),
            'dc:type': meta.get('type', ''),
            'status': meta.get('status', ''),
            'note': meta.get('note', ''),
        }
        d = {key: val for key, val in d.items() if val}
        # this one requires a conversion, so do it separately
        if 'confidenceScore' in meta:
            d['confidenceScore'] = str(meta['confidenceScore'])
    else:
        d = {}
    return d
</t>
<t tx="karstenw.20230225124007.7">class Relation(_HasMeta):
    target: str
    relType: str


</t>
<t tx="karstenw.20230225124007.8">class Example(_HasText, _HasMeta, total=False):
    language: str


</t>
<t tx="karstenw.20230225124007.9">class Synset(_HasId, _HasILI, _HasMeta, total=False):
    ili_definition: ILIDefinition
    partOfSpeech: str
    definitions: List[Definition]
    relations: List[Relation]
    examples: List[Example]
    lexicalized: bool
    members: List[str]
    lexfile: str


</t>
<t tx="karstenw.20230225124213.1">from wn._core import Word, Synset


# Word-based Metrics

</t>
<t tx="karstenw.20230225124213.2">def ambiguity(word: Word) -&gt; int:
    return len(word.synsets())


</t>
<t tx="karstenw.20230225124213.3">def average_ambiguity(synset: Synset) -&gt; float:
    words = synset.words()
    return sum(len(word.synsets()) for word in words) / len(words)
</t>
<t tx="karstenw.20230225124215.1">"""A simple English lemmatizer that finds and removes known suffixes.

"""

from typing import Optional, Dict, Set, List, Tuple
from enum import Flag, auto

import wn
from wn._types import LemmatizeResult
from wn.constants import NOUN, VERB, ADJ, ADJ_SAT, ADV, PARTS_OF_SPEECH

POSExceptionMap = Dict[str, Set[str]]
ExceptionMap = Dict[str, POSExceptionMap]


</t>
<t tx="karstenw.20230225124215.2">class _System(Flag):
    """Flags to track suffix rules in various implementations of Morphy."""
    PWN = auto()
    NLTK = auto()
    WN = auto()
    ALL = PWN | NLTK | WN


_PWN = _System.PWN
_NLTK = _System.NLTK
_WN = _System.WN
_ALL = _System.ALL


Rule = Tuple[str, str, _System]

DETACHMENT_RULES: Dict[str, List[Rule]] = {
    NOUN: [
        ("s",    "",    _ALL),
        ("ces",  "x",   _WN),
        ("ses",  "s",   _ALL),
        ("ves",  "f",   _NLTK | _WN),
        ("ives", "ife", _WN),
        ("xes",  "x",   _ALL),
        ("xes",  "xis", _WN),
        ("zes",  "z",   _ALL),
        ("ches", "ch",  _ALL),
        ("shes", "sh",  _ALL),
        ("men",  "man", _ALL),
        ("ies",  "y",   _ALL),
    ],
    VERB: [
        ("s",   "",  _ALL),
        ("ies", "y", _ALL),
        ("es",  "e", _ALL),
        ("es",  "",  _ALL),
        ("ed",  "e", _ALL),
        ("ed",  "",  _ALL),
        ("ing", "e", _ALL),
        ("ing", "",  _ALL),
    ],
    ADJ: [
        ("er",  "",  _ALL),
        ("est", "",  _ALL),
        ("er",  "e", _ALL),
        ("est", "e", _ALL),
    ],
    ADV: [],
}
DETACHMENT_RULES[ADJ_SAT] = DETACHMENT_RULES[ADJ]


</t>
<t tx="karstenw.20230225124215.3">class Morphy:
    """The Morphy lemmatizer class.

    Objects of this class are callables that take a wordform and an
    optional part of speech and return a dictionary mapping parts of
    speech to lemmas. If objects of this class are not created with a
    :class:`wn.Wordnet` object, the returned lemmas may be invalid.

    Arguments:
        wordnet: optional :class:`wn.Wordnet` instance

    Example:

        &gt;&gt;&gt; import wn
        &gt;&gt;&gt; from wn.morphy import Morphy
        &gt;&gt;&gt; ewn = wn.Wordnet('ewn:2020')
        &gt;&gt;&gt; m = Morphy(ewn)
        &gt;&gt;&gt; m('axes', pos='n')
        {'n': {'axe', 'ax', 'axis'}}
        &gt;&gt;&gt; m('geese', pos='n')
        {'n': {'goose'}}
        &gt;&gt;&gt; m('gooses')
        {'n': {'goose'}, 'v': {'goose'}}
        &gt;&gt;&gt; m('goosing')
        {'v': {'goose'}}

    """

    @others
morphy = Morphy()
</t>
<t tx="karstenw.20230225124215.4">def __init__(self, wordnet: Optional[wn.Wordnet] = None):
    self._rules = {
        pos: [rule for rule in rules if rule[2] &amp; _System.WN]
        for pos, rules in DETACHMENT_RULES.items()
    }
    exceptions: ExceptionMap = {pos: {} for pos in PARTS_OF_SPEECH}
    all_lemmas: Dict[str, Set[str]] = {pos: set() for pos in PARTS_OF_SPEECH}
    if wordnet:
        for word in wordnet.words():
            pos = word.pos
            pos_exc = exceptions[pos]
            lemma, *others = word.forms()
            # store every lemma whether it has other forms or not
            all_lemmas[pos].add(lemma)
            # those with other forms map to the original lemmas
            for other in others:
                if other in pos_exc:
                    pos_exc[other].add(lemma)
                else:
                    pos_exc[other] = {lemma}
        self._initialized = True
    else:
        self._initialized = False
    self._exceptions = exceptions
    self._all_lemmas = all_lemmas

</t>
<t tx="karstenw.20230225124215.5">def __call__(self, form: str, pos: Optional[str] = None) -&gt; LemmatizeResult:
    result = {}
    if not self._initialized:
        result[pos] = {form}  # always include original when not initialized

    if pos is None:
        pos_list = list(DETACHMENT_RULES)
    elif pos in DETACHMENT_RULES:
        pos_list = [pos]
    else:
        pos_list = []  # not handled by morphy

    no_pos_forms = result.get(None, set())  # avoid unnecessary duplicates
    for _pos in pos_list:
        candidates = self._morphstr(form, _pos) - no_pos_forms
        if candidates:
            result.setdefault(_pos, set()).update(candidates)

    return result

</t>
<t tx="karstenw.20230225124215.6">def _morphstr(self, form: str, pos: str) -&gt; Set[str]:
    candidates: Set[str] = set()

    initialized = self._initialized
    if initialized:
        all_lemmas = self._all_lemmas[pos]
        if form in all_lemmas:
            candidates.add(form)
        candidates.update(self._exceptions[pos].get(form, set()))
    else:
        all_lemmas = set()

    for suffix, repl, _ in self._rules[pos]:
        # avoid applying rules that perform full suppletion
        if form.endswith(suffix) and len(suffix) &lt; len(form):
            candidate = f'{form[:-len(suffix)]}{repl}'
            if not initialized or candidate in all_lemmas:
                candidates.add(candidate)

    return candidates


</t>
<t tx="karstenw.20230225124217.1">"""
Wordnet and ILI Packages and Collections
"""

from typing import Optional, Iterator, List, Tuple
from pathlib import Path
import tarfile
import tempfile
from contextlib import contextmanager
import gzip
import lzma
import shutil


import wn
from wn._types import AnyPath
from wn.constants import _WORDNET, _ILI
from wn._util import is_gzip, is_lzma
from wn import lmf
from wn import _ili


_ADDITIONAL_FILE_SUFFIXES = ('', '.txt', '.md', '.rst')


</t>
<t tx="karstenw.20230225124217.10">def citation(self) -&gt; Optional[Path]:
    """Return the path of the citation, or ``None`` if none exists."""
    return self._find_file(self._path / 'citation', ('.bib',))

</t>
<t tx="karstenw.20230225124217.11">def _find_file(self, base: Path, suffixes: Tuple[str, ...]) -&gt; Optional[Path]:
    for suffix in suffixes:
        base = base.with_suffix(suffix)
        if base.is_file():
            return base
    return None


</t>
<t tx="karstenw.20230225124217.12">class Package(_Project):
    """This class represents a wordnet or ILI package -- a directory with
       a resource file and optional metadata.

    """

    @others
</t>
<t tx="karstenw.20230225124217.13">@property
def type(self) -&gt; Optional[str]:
    return _resource_file_type(self.resource_file())

</t>
<t tx="karstenw.20230225124217.14">def resource_file(self) -&gt; Path:
    """Return the path of the package's resource file."""
    files = _package_directory_types(self._path)
    if not files:
        raise wn.Error(f'no resource found in package: {self._path!s}')
    elif len(files) &gt; 1:
        raise wn.Error(f'multiple resource found in package: {self._path!s}')
    return files[0][0]


</t>
<t tx="karstenw.20230225124217.15">class _ResourceOnlyPackage(Package):

    @others
</t>
<t tx="karstenw.20230225124217.16">def resource_file(self) -&gt; Path:
    return self._path

</t>
<t tx="karstenw.20230225124217.17">def readme(self): return None
</t>
<t tx="karstenw.20230225124217.18">def license(self): return None
</t>
<t tx="karstenw.20230225124217.19">def citation(self): return None


</t>
<t tx="karstenw.20230225124217.2">def is_package_directory(path: AnyPath) -&gt; bool:
    """Return ``True`` if *path* appears to be a wordnet or ILI package."""
    path = Path(path).expanduser()
    return len(_package_directory_types(path)) == 1


</t>
<t tx="karstenw.20230225124217.20">class Collection(_Project):
    """This class represents a wordnet or ILI collection -- a directory
       with one or more wordnet/ILI packages and optional metadata.

    """

    @others
</t>
<t tx="karstenw.20230225124217.21">def packages(self) -&gt; List[Package]:
    """Return the list of packages in the collection."""
    return [Package(path)
            for path in self._path.iterdir()
            if is_package_directory(path)]


</t>
<t tx="karstenw.20230225124217.22">def iterpackages(path: AnyPath) -&gt; Iterator[Package]:
    """Yield any wordnet or ILI packages found at *path*.

    The *path* argument can point to one of the following:
      - a lexical resource file or ILI file
      - a wordnet package directory
      - a wordnet collection directory
      - a tar archive containing one of the above
      - a compressed (gzip or lzma) resource file or tar archive
    """
    path = Path(path).expanduser()

    if path.is_dir():
        if is_package_directory(path):
            yield Package(path)

        elif is_collection_directory(path):
            yield from Collection(path).packages()

        else:
            raise wn.Error(
                f'does not appear to be a valid package or collection: {path!s}'
            )

    elif tarfile.is_tarfile(path):
        with tarfile.open(path) as tar:
            _check_tar(tar)
            with tempfile.TemporaryDirectory() as tmpdir:
                tar.extractall(path=tmpdir)
                contents = list(Path(tmpdir).iterdir())
                if len(contents) != 1:
                    raise wn.Error(
                        'archive may only have one resource, package, or collection'
                    )
                yield from iterpackages(contents[0])

    else:
        decompressed: Path
        with _get_decompressed(path) as decompressed:
            if lmf.is_lmf(decompressed) or _ili.is_ili(decompressed):
                yield _ResourceOnlyPackage(decompressed)
            else:
                raise wn.Error(
                    f'not a valid lexical resource: {path!s}'
                )


</t>
<t tx="karstenw.20230225124217.23">@contextmanager
def _get_decompressed(source: Path) -&gt; Iterator[Path]:
    gzipped = is_gzip(source)
    xzipped = is_lzma(source)
    if not (gzipped or xzipped):
        yield source
    else:
        tmp = tempfile.NamedTemporaryFile(suffix='.xml', delete=False)
        path = Path(tmp.name)
        try:
            if gzipped:
                with gzip.open(source, 'rb') as gzip_src:
                    shutil.copyfileobj(gzip_src, tmp)
            else:  # xzipped
                with lzma.open(source, 'rb') as lzma_src:
                    shutil.copyfileobj(lzma_src, tmp)

            tmp.close()  # Windows cannot reliably reopen until it's closed

            yield path

        except (OSError, EOFError, lzma.LZMAError) as exc:
            raise wn.Error(f'could not decompress file: {source}') from exc

        finally:
            path.unlink()


</t>
<t tx="karstenw.20230225124217.24">def _check_tar(tar: tarfile.TarFile) -&gt; None:
    """Check the tarfile to avoid potential security issues.

    Currently collections and packages have the following constraints:
    - Only regular files or directories
    - No paths starting with '/' or containing '..'
    """
    for info in tar.getmembers():
        if not (info.isfile() or info.isdir()):
            raise wn.Error(
                f'tarfile member is not a regular file or directory: {info.name}'
            )
        if info.name.startswith('/') or '..' in info.name:
            raise wn.Error(
                f'tarfile member paths may not be absolute or contain ..: {info.name}'
            )
</t>
<t tx="karstenw.20230225124217.3">def _package_directory_types(path: Path) -&gt; List[Tuple[Path, str]]:
    types: List[Tuple[Path, str]] = []
    if path.is_dir():
        for p in path.iterdir():
            typ = _resource_file_type(p)
            if typ is not None:
                types.append((p, typ))
    return types


</t>
<t tx="karstenw.20230225124217.4">def _resource_file_type(path: Path) -&gt; Optional[str]:
    if lmf.is_lmf(path):
        return _WORDNET
    elif _ili.is_ili(path):
        return _ILI
    return None


</t>
<t tx="karstenw.20230225124217.5">def is_collection_directory(path: AnyPath) -&gt; bool:
    """Return ``True`` if *path* appears to be a wordnet collection."""
    path = Path(path).expanduser()
    return (path.is_dir()
            and len(list(filter(is_package_directory, path.iterdir()))) &gt;= 1)


</t>
<t tx="karstenw.20230225124217.6">class _Project:
    __slots__ = '_path',

    @others
</t>
<t tx="karstenw.20230225124217.7">def __init__(self, path: AnyPath):
    self._path: Path = Path(path).expanduser()

</t>
<t tx="karstenw.20230225124217.8">def readme(self) -&gt; Optional[Path]:
    """Return the path of the README file, or ``None`` if none exists."""
    return self._find_file(self._path / 'README', _ADDITIONAL_FILE_SUFFIXES)

</t>
<t tx="karstenw.20230225124217.9">def license(self) -&gt; Optional[Path]:
    """Return the path of the license, or ``None`` if none exists."""
    return self._find_file(self._path / 'LICENSE', _ADDITIONAL_FILE_SUFFIXES)

</t>
<t tx="karstenw.20230225124221.1">"""Synset similarity metrics."""

from typing import List
import math

import wn
from wn.constants import ADJ, ADJ_SAT
from wn._core import Synset
from wn.ic import Freq, information_content


</t>
<t tx="karstenw.20230225124221.10">def _check_if_pos_compatible(pos1: str, pos2: str) -&gt; None:
    _pos1 = ADJ if pos1 == ADJ_SAT else pos1
    _pos2 = ADJ if pos2 == ADJ_SAT else pos2
    if _pos1 != _pos2:
        raise wn.Error('synsets must have the same part of speech')
</t>
<t tx="karstenw.20230225124221.2">def path(synset1: Synset, synset2: Synset, simulate_root: bool = False) -&gt; float:
    """Return the Path similarity of *synset1* and *synset2*.

    Arguments:
        synset1: The first synset to compare.
        synset2: The second synset to compare.
        simulate_root: When :python:`True`, a fake root node connects
            all other roots; default: :python:`False`.

    Example:
        &gt;&gt;&gt; import wn
        &gt;&gt;&gt; from wn.similarity import path
        &gt;&gt;&gt; ewn = wn.Wordnet('ewn:2020')
        &gt;&gt;&gt; spatula = ewn.synsets('spatula')[0]
        &gt;&gt;&gt; path(spatula, ewn.synsets('pancake')[0])
        0.058823529411764705
        &gt;&gt;&gt; path(spatula, ewn.synsets('utensil')[0])
        0.2
        &gt;&gt;&gt; path(spatula, spatula)
        1.0
        &gt;&gt;&gt; flip = ewn.synsets('flip', pos='v')[0]
        &gt;&gt;&gt; turn_over = ewn.synsets('turn over', pos='v')[0]
        &gt;&gt;&gt; path(flip, turn_over)
        0.0
        &gt;&gt;&gt; path(flip, turn_over, simulate_root=True)
        0.16666666666666666

     """
    _check_if_pos_compatible(synset1.pos, synset2.pos)
    try:
        path = synset1.shortest_path(synset2, simulate_root=simulate_root)
    except wn.Error:
        distance = float('inf')
    else:
        distance = len(path)
    return 1 / (distance + 1)


</t>
<t tx="karstenw.20230225124221.3">def wup(synset1: Synset, synset2: Synset, simulate_root=False) -&gt; float:
    """Return the Wu-Palmer similarity of *synset1* and *synset2*.

    Arguments:
        synset1: The first synset to compare.
        synset2: The second synset to compare.
        simulate_root: When :python:`True`, a fake root node connects
            all other roots; default: :python:`False`.

    Raises:
        wn.Error: When no path connects the *synset1* and *synset2*.

    Example:
        &gt;&gt;&gt; import wn
        &gt;&gt;&gt; from wn.similarity import wup
        &gt;&gt;&gt; ewn = wn.Wordnet('ewn:2020')
        &gt;&gt;&gt; spatula = ewn.synsets('spatula')[0]
        &gt;&gt;&gt; wup(spatula, ewn.synsets('pancake')[0])
        0.2
        &gt;&gt;&gt; wup(spatula, ewn.synsets('utensil')[0])
        0.8
        &gt;&gt;&gt; wup(spatula, spatula)
        1.0
        &gt;&gt;&gt; flip = ewn.synsets('flip', pos='v')[0]
        &gt;&gt;&gt; turn_over = ewn.synsets('turn over', pos='v')[0]
        &gt;&gt;&gt; wup(flip, turn_over, simulate_root=True)
        0.2857142857142857

    """
    _check_if_pos_compatible(synset1.pos, synset2.pos)
    lcs_list = _least_common_subsumers(synset1, synset2, simulate_root)
    lcs = lcs_list[0]
    i = len(synset1.shortest_path(lcs, simulate_root=simulate_root))
    j = len(synset2.shortest_path(lcs, simulate_root=simulate_root))
    k = lcs.max_depth() + 1
    return (2*k) / (i + j + 2*k)


</t>
<t tx="karstenw.20230225124221.4">def lch(
    synset1: Synset,
    synset2: Synset,
    max_depth: int,
    simulate_root: bool = False
) -&gt; float:
    """Return the Leacock-Chodorow similarity between *synset1* and *synset2*.

    Arguments:
        synset1: The first synset to compare.
        synset2: The second synset to compare.
        max_depth: The taxonomy depth (see :func:`wn.taxonomy.taxonomy_depth`)
        simulate_root: When :python:`True`, a fake root node connects
            all other roots; default: :python:`False`.

    Example:
        &gt;&gt;&gt; import wn, wn.taxonomy
        &gt;&gt;&gt; from wn.similarity import lch
        &gt;&gt;&gt; ewn = wn.Wordnet('ewn:2020')
        &gt;&gt;&gt; n_depth = wn.taxonomy.taxonomy_depth(ewn, 'n')
        &gt;&gt;&gt; spatula = ewn.synsets('spatula')[0]
        &gt;&gt;&gt; lch(spatula, ewn.synsets('pancake')[0], n_depth)
        0.8043728156701697
        &gt;&gt;&gt; lch(spatula, ewn.synsets('utensil')[0], n_depth)
        2.0281482472922856
        &gt;&gt;&gt; lch(spatula, spatula, n_depth)
        3.6375861597263857
        &gt;&gt;&gt; v_depth = taxonomy.taxonomy_depth(ewn, 'v')
        &gt;&gt;&gt; flip = ewn.synsets('flip', pos='v')[0]
        &gt;&gt;&gt; turn_over = ewn.synsets('turn over', pos='v')[0]
        &gt;&gt;&gt; lch(flip, turn_over, v_depth, simulate_root=True)
        1.3862943611198906

    """
    _check_if_pos_compatible(synset1.pos, synset2.pos)
    distance = len(synset1.shortest_path(synset2, simulate_root=simulate_root))
    if max_depth &lt;= 0:
        raise wn.Error('max_depth must be greater than 0')
    return -math.log((distance + 1) / (2 * max_depth))


</t>
<t tx="karstenw.20230225124221.5">def res(synset1: Synset, synset2: Synset, ic: Freq) -&gt; float:
    """Return the Resnik similarity between *synset1* and *synset2*.

    Arguments:
        synset1: The first synset to compare.
        synset2: The second synset to compare.
        ic: Information Content weights.

    Example:
        &gt;&gt;&gt; import wn, wn.ic, wn.taxonomy
        &gt;&gt;&gt; from wn.similarity import res
        &gt;&gt;&gt; pwn = wn.Wordnet('pwn:3.0')
        &gt;&gt;&gt; ic = wn.ic.load('~/nltk_data/corpora/wordnet_ic/ic-brown.dat', pwn)
        &gt;&gt;&gt; spatula = pwn.synsets('spatula')[0]
        &gt;&gt;&gt; res(spatula, pwn.synsets('pancake')[0], ic)
        0.8017591149538994
        &gt;&gt;&gt; res(spatula, pwn.synsets('utensil')[0], ic)
        5.87738923441087

    """
    _check_if_pos_compatible(synset1.pos, synset2.pos)
    lcs = _most_informative_lcs(synset1, synset2, ic)
    return information_content(lcs, ic)


</t>
<t tx="karstenw.20230225124221.6">def jcn(synset1: Synset, synset2: Synset, ic: Freq) -&gt; float:
    """Return the Jiang-Conrath similarity of two synsets.

    Arguments:
        synset1: The first synset to compare.
        synset2: The second synset to compare.
        ic: Information Content weights.

    Example:
        &gt;&gt;&gt; import wn, wn.ic, wn.taxonomy
        &gt;&gt;&gt; from wn.similarity import jcn
        &gt;&gt;&gt; pwn = wn.Wordnet('pwn:3.0')
        &gt;&gt;&gt; ic = wn.ic.load('~/nltk_data/corpora/wordnet_ic/ic-brown.dat', pwn)
        &gt;&gt;&gt; spatula = pwn.synsets('spatula')[0]
        &gt;&gt;&gt; jcn(spatula, pwn.synsets('pancake')[0], ic)
        0.04061799236354239
        &gt;&gt;&gt; jcn(spatula, pwn.synsets('utensil')[0], ic)
        0.10794048564613007

    """
    _check_if_pos_compatible(synset1.pos, synset2.pos)
    ic1 = information_content(synset1, ic)
    ic2 = information_content(synset2, ic)
    lcs = _most_informative_lcs(synset1, synset2, ic)
    ic_lcs = information_content(lcs, ic)
    if ic1 == ic2 == ic_lcs == 0:
        return 0
    elif ic1 + ic2 == 2 * ic_lcs:
        return float('inf')
    else:
        return 1 / (ic1 + ic2 - 2 * ic_lcs)


</t>
<t tx="karstenw.20230225124221.7">def lin(synset1: Synset, synset2: Synset, ic: Freq) -&gt; float:
    """Return the Lin similarity of two synsets.

    Arguments:
        synset1: The first synset to compare.
        synset2: The second synset to compare.
        ic: Information Content weights.

    Example:
        &gt;&gt;&gt; import wn, wn.ic, wn.taxonomy
        &gt;&gt;&gt; from wn.similarity import lin
        &gt;&gt;&gt; pwn = wn.Wordnet('pwn:3.0')
        &gt;&gt;&gt; ic = wn.ic.load('~/nltk_data/corpora/wordnet_ic/ic-brown.dat', pwn)
        &gt;&gt;&gt; spatula = pwn.synsets('spatula')[0]
        &gt;&gt;&gt; lin(spatula, pwn.synsets('pancake')[0], ic)
        0.061148956278604116
        &gt;&gt;&gt; lin(spatula, pwn.synsets('utensil')[0], ic)
        0.5592415686750427

    """
    _check_if_pos_compatible(synset1.pos, synset2.pos)
    lcs = _most_informative_lcs(synset1, synset2, ic)
    ic1 = information_content(synset1, ic)
    ic2 = information_content(synset2, ic)
    if ic1 == 0 or ic2 == 0:
        return 0.0
    return 2 * information_content(lcs, ic) / (ic1 + ic2)


# Helper functions

</t>
<t tx="karstenw.20230225124221.8">def _least_common_subsumers(
    synset1: Synset,
    synset2: Synset,
    simulate_root: bool
) -&gt; List[Synset]:
    lcs = synset1.lowest_common_hypernyms(synset2, simulate_root=simulate_root)
    if not lcs:
        raise wn.Error(f'no common hypernyms for {synset1!r} and {synset2!r}')
    return lcs


</t>
<t tx="karstenw.20230225124221.9">def _most_informative_lcs(synset1: Synset, synset2: Synset, ic: Freq) -&gt; Synset:
    pos_ic = ic[synset1.pos]
    lcs = _least_common_subsumers(synset1, synset2, False)
    return max(lcs, key=lambda ss: pos_ic[ss.id])


</t>
<t tx="karstenw.20230225124223.1">"""Functions for working with hypernym/hyponym taxonomies."""

from typing import Optional, Tuple, List, Set, Dict, TYPE_CHECKING

import wn
from wn.constants import ADJ, ADJ_SAT
from wn._util import flatten
from wn import _core

if TYPE_CHECKING:
    from wn._core import Wordnet, Synset


_FAKE_ROOT = '*ROOT*'


</t>
<t tx="karstenw.20230225124223.10">def _shortest_hyp_paths(
        synset: 'Synset', other: 'Synset', simulate_root: bool
) -&gt; Dict[Tuple['Synset', int], List['Synset']]:
    if synset == other:
        return {(synset, 0): []}

    from_self = _hypernym_paths(synset, simulate_root, True)
    from_other = _hypernym_paths(other, simulate_root, True)
    common = set(flatten(from_self)).intersection(flatten(from_other))

    if not common:
        return {}

    # Compute depths of common hypernyms from their distances.
    # Doing this now avoid more expensive lookups later.
    depths: Dict['Synset', int] = {}
    # subpaths accumulates paths to common hypernyms from both sides
    subpaths: Dict['Synset', Tuple[List[List['Synset']], List[List['Synset']]]]
    subpaths = {ss: ([], []) for ss in common}
    for which, paths in (0, from_self), (1, from_other):
        for path in paths:
            for dist, ss in enumerate(path):
                if ss in common:
                    # synset or other subpath to ss (not including ss)
                    subpaths[ss][which].append(path[:dist + 1])
                    # keep maximum depth
                    depth = len(path) - dist - 1
                    if ss not in depths or depths[ss] &lt; depth:
                        depths[ss] = depth

    shortest: Dict[Tuple['Synset', int], List['Synset']] = {}
    for ss in common:
        from_self_subpaths, from_other_subpaths = subpaths[ss]
        shortest_from_self = min(from_self_subpaths, key=len)
        # for the other path, we need to reverse it and remove the pivot synset
        shortest_from_other = min(from_other_subpaths, key=len)[-2::-1]
        shortest[(ss, depths[ss])] = shortest_from_self + shortest_from_other

    return shortest


</t>
<t tx="karstenw.20230225124223.11">def shortest_path(
        synset: 'Synset', other: 'Synset', simulate_root: bool = False
) -&gt; List['Synset']:
    """Return the shortest path from *synset* to the *other* synset.

    Arguments:
        other: endpoint synset of the path
        simulate_root: if :python:`True`, ensure any two synsets
          are always connected by positing a fake root node

    Example:

        &gt;&gt;&gt; import wn, wn.taxonomy
        &gt;&gt;&gt; dog = ewn.synsets('dog', pos='n')[0]
        &gt;&gt;&gt; squirrel = ewn.synsets('squirrel', pos='n')[0]
        &gt;&gt;&gt; for ss in wn.taxonomy.shortest_path(dog, squirrel):
        ...     print(ss.lemmas())
        ...
        ['canine', 'canid']
        ['carnivore']
        ['eutherian mammal', 'placental', 'placental mammal', 'eutherian']
        ['rodent', 'gnawer']
        ['squirrel']

    """
    pathmap = _shortest_hyp_paths(synset, other, simulate_root)
    key = min(pathmap, key=lambda key: len(pathmap[key]), default=None)
    if key is None:
        raise wn.Error(f'no path between {synset!r} and {other!r}')
    return pathmap[key][1:]


</t>
<t tx="karstenw.20230225124223.12">def common_hypernyms(
        synset: 'Synset', other: 'Synset', simulate_root: bool = False
) -&gt; List['Synset']:
    """Return the common hypernyms for the current and *other* synsets.

    Arguments:
        other: synset that is a hyponym of any shared hypernyms
        simulate_root: if :python:`True`, ensure any two synsets
          always share a hypernym by positing a fake root node

    Example:

        &gt;&gt;&gt; import wn, wn.taxonomy
        &gt;&gt;&gt; dog = ewn.synsets('dog', pos='n')[0]
        &gt;&gt;&gt; squirrel = ewn.synsets('squirrel', pos='n')[0]
        &gt;&gt;&gt; for ss in wn.taxonomy.common_hypernyms(dog, squirrel):
        ...     print(ss.lemmas())
        ...
        ['entity']
        ['physical entity']
        ['object', 'physical object']
        ['unit', 'whole']
        ['animate thing', 'living thing']
        ['organism', 'being']
        ['fauna', 'beast', 'animate being', 'brute', 'creature', 'animal']
        ['chordate']
        ['craniate', 'vertebrate']
        ['mammalian', 'mammal']
        ['eutherian mammal', 'placental', 'placental mammal', 'eutherian']

    """
    from_self = _hypernym_paths(synset, simulate_root, True)
    from_other = _hypernym_paths(other, simulate_root, True)
    common = set(flatten(from_self)).intersection(flatten(from_other))
    return sorted(common)


</t>
<t tx="karstenw.20230225124223.13">def lowest_common_hypernyms(
        synset: 'Synset', other: 'Synset', simulate_root: bool = False
) -&gt; List['Synset']:
    """Return the common hypernyms furthest from the root.

    Arguments:
        other: synset that is a hyponym of any shared hypernyms
        simulate_root: if :python:`True`, ensure any two synsets
          always share a hypernym by positing a fake root node

    Example:

        &gt;&gt;&gt; import wn, wn.taxonomy
        &gt;&gt;&gt; dog = ewn.synsets('dog', pos='n')[0]
        &gt;&gt;&gt; squirrel = ewn.synsets('squirrel', pos='n')[0]
        &gt;&gt;&gt; len(wn.taxonomy.lowest_common_hypernyms(dog, squirrel))
        1
        &gt;&gt;&gt; wn.taxonomy.lowest_common_hypernyms(dog, squirrel)[0].lemmas()
        ['eutherian mammal', 'placental', 'placental mammal', 'eutherian']

    """
    pathmap = _shortest_hyp_paths(synset, other, simulate_root)
    # keys of pathmap are (synset, depth_of_synset)
    max_depth: int = max([depth for _, depth in pathmap], default=-1)
    if max_depth == -1:
        return []
    else:
        return [ss for ss, d in pathmap if d == max_depth]
</t>
<t tx="karstenw.20230225124223.2">def roots(wordnet: 'Wordnet', pos: Optional[str] = None) -&gt; List['Synset']:
    """Return the list of root synsets in *wordnet*.

    Arguments:

        wordnet: The wordnet from which root synsets are found.

        pos: If given, only return synsets with the specified part of
            speech.

    Example:

        &gt;&gt;&gt; import wn, wn.taxonomy
        &gt;&gt;&gt; ewn = wn.Wordnet('ewn:2020')
        &gt;&gt;&gt; len(wn.taxonomy.roots(ewn, pos='v'))
        573


    """
    return [ss for ss in _synsets_for_pos(wordnet, pos) if not ss.hypernyms()]


</t>
<t tx="karstenw.20230225124223.3">def leaves(wordnet: 'Wordnet', pos: Optional[str] = None) -&gt; List['Synset']:
    """Return the list of leaf synsets in *wordnet*.

    Arguments:

        wordnet: The wordnet from which leaf synsets are found.

        pos: If given, only return synsets with the specified part of
            speech.

    Example:

        &gt;&gt;&gt; import wn, wn.taxonomy
        &gt;&gt;&gt; ewn = wn.Wordnet('ewn:2020')
        &gt;&gt;&gt; len(wn.taxonomy.leaves(ewn, pos='v'))
        10525

    """
    return [ss for ss in _synsets_for_pos(wordnet, pos) if not ss.hyponyms()]


</t>
<t tx="karstenw.20230225124223.4">def taxonomy_depth(wordnet: 'Wordnet', pos: str) -&gt; int:
    """Return the list of leaf synsets in *wordnet*.

    Arguments:

        wordnet: The wordnet for which the taxonomy depth will be
            calculated.

        pos: The part of speech for which the taxonomy depth will be
            calculated.

    Example:

        &gt;&gt;&gt; import wn, wn.taxonomy
        &gt;&gt;&gt; ewn = wn.Wordnet('ewn:2020')
        &gt;&gt;&gt; wn.taxonomy.taxonomy_depth(ewn, 'n')
        19

    """
    seen: Set['Synset'] = set()
    depth = 0
    for ss in _synsets_for_pos(wordnet, pos):
        if all(hyp in seen for hyp in ss.hypernyms()):
            continue
        paths = ss.hypernym_paths()
        if paths:
            depth = max(depth, max(len(path) for path in paths))
            seen.update(hyp for path in paths for hyp in path)
    return depth


</t>
<t tx="karstenw.20230225124223.5">def _synsets_for_pos(wordnet: 'Wordnet', pos: Optional[str]) -&gt; List['Synset']:
    """Get the list of synsets for a part of speech. If *pos* is 'a' or
    's', also include those for the other.

    """
    synsets = wordnet.synsets(pos=pos)
    if pos == ADJ:
        synsets.extend(wordnet.synsets(pos=ADJ_SAT))
    elif pos == ADJ_SAT:
        synsets.extend(wordnet.synsets(pos=ADJ))
    return synsets


</t>
<t tx="karstenw.20230225124223.6">def _hypernym_paths(
        synset: 'Synset', simulate_root: bool, include_self: bool
) -&gt; List[List['Synset']]:
    paths = list(synset.relation_paths('hypernym', 'instance_hypernym'))
    if include_self:
        paths = [[synset] + path for path in paths] or [[synset]]
    if simulate_root and synset.id != _FAKE_ROOT:
        root = _core.Synset.empty(
            id=_FAKE_ROOT, _lexid=synset._lexid, _wordnet=synset._wordnet
        )
        paths = [path + [root] for path in paths] or [[root]]
    return paths


</t>
<t tx="karstenw.20230225124223.7">def hypernym_paths(
    synset: 'Synset',
    simulate_root: bool = False
) -&gt; List[List['Synset']]:
    """Return the list of hypernym paths to a root synset.

    Arguments:

        synset: The starting synset for paths to a root.

        simulate_root: If :python:`True`, find the path to a simulated
            root node.

    Example:

        &gt;&gt;&gt; import wn, wn.taxonomy
        &gt;&gt;&gt; dog = wn.synsets('dog', pos='n')[0]
        &gt;&gt;&gt; for path in wn.taxonomy.hypernym_paths(dog):
        ...     for i, ss in enumerate(path):
        ...         print(' ' * i, ss, ss.lemmas()[0])
        ...
         Synset('pwn-02083346-n') canine
          Synset('pwn-02075296-n') carnivore
           Synset('pwn-01886756-n') eutherian mammal
            Synset('pwn-01861778-n') mammalian
             Synset('pwn-01471682-n') craniate
              Synset('pwn-01466257-n') chordate
               Synset('pwn-00015388-n') animal
                Synset('pwn-00004475-n') organism
                 Synset('pwn-00004258-n') animate thing
                  Synset('pwn-00003553-n') unit
                   Synset('pwn-00002684-n') object
                    Synset('pwn-00001930-n') physical entity
                     Synset('pwn-00001740-n') entity
         Synset('pwn-01317541-n') domesticated animal
          Synset('pwn-00015388-n') animal
           Synset('pwn-00004475-n') organism
            Synset('pwn-00004258-n') animate thing
             Synset('pwn-00003553-n') unit
              Synset('pwn-00002684-n') object
               Synset('pwn-00001930-n') physical entity
                Synset('pwn-00001740-n') entity

    """
    return _hypernym_paths(synset, simulate_root, False)


</t>
<t tx="karstenw.20230225124223.8">def min_depth(synset: 'Synset', simulate_root: bool = False) -&gt; int:
    """Return the minimum taxonomy depth of the synset.

    Arguments:

        synset: The starting synset for paths to a root.

        simulate_root: If :python:`True`, find the depth to a
            simulated root node.

    Example:

        &gt;&gt;&gt; import wn, wn.taxonomy
        &gt;&gt;&gt; dog = wn.synsets('dog', pos='n')[0]
        &gt;&gt;&gt; wn.taxonomy.min_depth(dog)
        8

    """
    return min(
        (len(path) for path in synset.hypernym_paths(simulate_root=simulate_root)),
        default=0
    )


</t>
<t tx="karstenw.20230225124223.9">def max_depth(synset: 'Synset', simulate_root: bool = False) -&gt; int:
    """Return the maximum taxonomy depth of the synset.

    Arguments:

        synset: The starting synset for paths to a root.

        simulate_root: If :python:`True`, find the depth to a
            simulated root node.

    Example:

        &gt;&gt;&gt; import wn, wn.taxonomy
        &gt;&gt;&gt; dog = wn.synsets('dog', pos='n')[0]
        &gt;&gt;&gt; wn.taxonomy.max_depth(dog)
        13

    """
    return max(
        (len(path) for path in synset.hypernym_paths(simulate_root=simulate_root)),
        default=0
    )


</t>
<t tx="karstenw.20230225124225.1">"""Wn utility classes."""
from typing import TextIO, Callable
import sys


</t>
<t tx="karstenw.20230225124225.10">def update(self, n: int = 1, force: bool = False) -&gt; None:
    """Increment the count by *n* and print the reformatted bar."""
    self.kwargs['count'] += n  # type: ignore
    self._refresh_quota -= n
    if force or self._refresh_quota &lt;= 0:
        self._refresh_quota = self.kwargs['refresh_interval']  # type: ignore
        s = self.format()
        if self.file:
            print('\r\033[K', end='', file=self.file)
            print(s, end='', file=self.file)

</t>
<t tx="karstenw.20230225124225.11">def format(self) -&gt; str:
    """Format and return the progress bar.

    The bar is is formatted according to :attr:`FMT`, using
    variables from :attr:`kwargs` and two computed variables:

    - ``bar``: visualization of the progress bar, empty when
      ``total`` is 0

    - ``counter``: display of ``count``, ``total``, and ``units``

    &gt;&gt;&gt; p = ProgressBar(message='Progress', count=2, total=10, unit='K')
    &gt;&gt;&gt; p.format()
    '\\rProgress [######                        ] (2/10K) '
    &gt;&gt;&gt; p = ProgressBar(count=2, status='Counting...')
    &gt;&gt;&gt; p.format()
    '\\r (2) Counting...'

    """
    _kw = self.kwargs
    width = 30
    total: int = _kw['total']  # type: ignore
    count: int = _kw['count']  # type: ignore

    if total &gt; 0:
        num = min(count, total) * width
        fill = (num // total) * '#'
        part = ((num % total) * 3) // total
        if part:
            fill += '-='[part-1]
        bar = f' [{fill:&lt;{width}}]'
        counter = f' ({count}/{total}{_kw["unit"]}) '
    else:
        bar = ''
        counter = f' ({count}{_kw["unit"]}) '

    return self.FMT.format(bar=bar, counter=counter, **_kw)

</t>
<t tx="karstenw.20230225124225.12">def flash(self, message: str) -&gt; None:
    """Overwrite the progress bar with *message*."""
    print(f'\r\033[K{message}', end='', file=self.file)

</t>
<t tx="karstenw.20230225124225.13">def close(self) -&gt; None:
    """Print a newline so the last printed bar remains on screen."""
    print(file=self.file)
</t>
<t tx="karstenw.20230225124225.2">def synset_id_formatter(
    fmt: str = '{prefix}-{offset:08}-{pos}',
    **kwargs
) -&gt; Callable:
    """Return a function for formatting synset ids.

    The *fmt* argument can be customized. It will be formatted using
    any other keyword arguments given to this function and any given
    to the resulting function. By default, the format string expects a
    ``prefix`` string argument for the namespace (such as a lexicon
    id), an ``offset`` integer argument (such as a WNDB offset), and a
    ``pos`` string argument.

    Arguments:
        fmt: A Python format string
        **kwargs: Keyword arguments for the format string.

    Example:

        &gt;&gt;&gt; pwn_synset_id = synset_id_formatter(prefix='pwn')
        &gt;&gt;&gt; pwn_synset_id(offset=1174, pos='n')
        'pwn-00001174-n'

    """

    def format_synset_id(**_kwargs) -&gt; str:
        return fmt.format(**kwargs, **_kwargs)

    return format_synset_id


</t>
<t tx="karstenw.20230225124225.3">class ProgressHandler:
    """An interface for updating progress in long-running processes.

    Long-running processes in Wn, such as :func:`wn.download` and
    :func:`wn.add`, call to a progress handler object as they go.  The
    default progress handler used by Wn is :class:`ProgressBar`, which
    updates progress by formatting and printing a textual bar to
    stderr. The :class:`ProgressHandler` class may be used directly,
    which does nothing, or users may create their own subclasses for,
    e.g., updating a GUI or some other handler.

    The initialization parameters, except for ``file``, are stored in
    a :attr:`kwargs` member and may be updated after the handler is
    created through the :meth:`set` method. The :meth:`update` method
    is the primary way a counter is updated. The :meth:`flash` method
    is sometimes called for simple messages. When the process is
    complete, the :meth:`close` method is called, optionally with a
    message.

    """

    @others
</t>
<t tx="karstenw.20230225124225.4">def __init__(
    self,
    *,
    message: str = '',
    count: int = 0,
    total: int = 0,
    refresh_interval: int = 0,
    unit: str = '',
    status: str = '',
    file: TextIO = sys.stderr,
):
    self.file = file
    self.kwargs = {
        'count': count,
        'total': total,
        'refresh_interval': refresh_interval,
        'message': message,
        'unit': unit,
        'status': status,
    }
    self._refresh_quota: int = refresh_interval

</t>
<t tx="karstenw.20230225124225.5">def update(self, n: int = 1, force: bool = False) -&gt; None:
    """Update the counter with the increment value *n*.

    This method should update the ``count`` key of :attr:`kwargs`
    with the increment value *n*. After this, it is expected to
    update some user-facing progress indicator.

    If *force* is :python:`True`, any indicator will be refreshed
    regardless of the value of the refresh interval.

    """
    self.kwargs['count'] += n  # type: ignore

</t>
<t tx="karstenw.20230225124225.6">def set(self, **kwargs) -&gt; None:
    """Update progress handler parameters.

    Calling this method also runs :meth:`update` with an increment
    of 0, which causes a refresh of any indicator without changing
    the counter.

    """
    self.kwargs.update(**kwargs)
    self.update(0, force=True)

</t>
<t tx="karstenw.20230225124225.7">def flash(self, message: str) -&gt; None:
    """Issue a message unrelated to the current counter.

    This may be useful for multi-stage processes to indicate the
    move to a new stage, or to log unexpected situations.

    """
    pass

</t>
<t tx="karstenw.20230225124225.8">def close(self) -&gt; None:
    """Close the progress handler.

    This might be useful for closing file handles or cleaning up
    resources.

    """
    pass


</t>
<t tx="karstenw.20230225124225.9">class ProgressBar(ProgressHandler):
    """A :class:`ProgressHandler` subclass for printing a progress bar.

    Example:
        &gt;&gt;&gt; p = ProgressBar(message='Progress: ', total=10, unit=' units')
        &gt;&gt;&gt; p.update(3)
        Progress: [#########                     ] (3/10 units)

    See :meth:`format` for a description of how the progress bar is
    formatted.

    """

    #: The default formatting template.
    FMT = '\r{message}{bar}{counter}{status}'

    @others
</t>
<t tx="karstenw.20230225124228.1">"""Wordnet lexicon validation.

This module is for checking whether the the contents of a lexicon are
valid according to a series of checks. Those checks are:

====  ==========================================================
Code  Message
====  ==========================================================
E101  ID is not unique within the lexicon.
W201  Lexical entry has no senses.
W202  Redundant sense between lexical entry and synset.
W203  Redundant lexical entry with the same lemma and synset.
E204  Synset of sense is missing.
W301  Synset is empty (not associated with any lexical entries).
W302  ILI is repeated across synsets.
W303  Proposed ILI is missing a definition.
W304  Existing ILI has a spurious definition.
E401  Relation target is missing or invalid.
W402  Relation type is invalid for the source and target.
W403  Redundant relation between source and target.
W404  Reverse relation is missing.
W501  Synset's part-of-speech is different from its hypernym's.
W502  Relation is a self-loop.
====  ==========================================================

"""

from typing import (Sequence, Iterator, Tuple, List, Dict,
                    Optional, Type, Union, Callable, cast)
from collections import Counter
from itertools import chain

from wn import lmf
from wn.constants import (
    SENSE_RELATIONS,
    SENSE_SYNSET_RELATIONS,
    SYNSET_RELATIONS,
    REVERSE_RELATIONS,
)
from wn.util import ProgressHandler, ProgressBar


_Ids = Dict[str, Counter]
_Result = Dict[str, Dict]
_CheckFunction = Callable[[lmf.Lexicon, _Ids], _Result]
_Report = Dict[str, Dict[str, Union[str, _Result]]]


</t>
<t tx="karstenw.20230225124228.10">def _spurious_ili_definition(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """existing ILI has a spurious definition"""
    return {ss['id']: {'ili_definitin': ss['ili_definition']}
            for ss in _synsets(lex)
            if ss['ili'] and ss['ili'] != 'in' and ss.get('ili_definition')}


</t>
<t tx="karstenw.20230225124228.11">def _missing_relation_target(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """relation target is missing or invalid"""
    result = {s['id']: {'type': r['relType'], 'target': r['target']}
              for s, r in _sense_relations(lex)
              if r['target'] not in ids['sense'] and r['target'] not in ids['synset']}
    result.update((ss['id'], {'type': r['relType'], 'target': r['target']})
                  for ss, r in _synset_relations(lex)
                  if r['target'] not in ids['synset'])
    return result


</t>
<t tx="karstenw.20230225124228.12">def _invalid_relation_type(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """relation type is invalid for the source and target"""
    result = {s['id']: {'type': r['relType'], 'target': r['target']}
              for s, r in _sense_relations(lex)
              if (r['target'] in ids['sense']
                  and r['relType'] not in SENSE_RELATIONS)
              or (r['target'] in ids['synset']
                  and r['relType'] not in SENSE_SYNSET_RELATIONS)}
    result.update((ss['id'], {'type': r['relType'], 'target': r['target']})
                  for ss, r in _synset_relations(lex)
                  if r['relType'] not in SYNSET_RELATIONS)
    return result


</t>
<t tx="karstenw.20230225124228.13">def _redundant_relation(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """redundant relation between source and target"""
    redundant = _multiples(chain(
        ((s['id'], r['relType'], r['target']) for s, r in _sense_relations(lex)),
        ((ss['id'], r['relType'], r['target']) for ss, r in _synset_relations(lex)),
    ))
    return {src: {'type': typ, 'target': tgt} for src, typ, tgt in redundant}


</t>
<t tx="karstenw.20230225124228.14">def _missing_reverse_relation(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """reverse relation is missing"""
    regular = {(s['id'], r['relType'], r['target'])
               for s, r in _sense_relations(lex)
               if r['target'] in ids['sense']}
    regular.update((ss['id'], r['relType'], r['target'])
                   for ss, r in _synset_relations(lex))
    return {tgt: {'type': REVERSE_RELATIONS[typ], 'target': src}
            for src, typ, tgt in regular
            if typ in REVERSE_RELATIONS
            and (tgt, REVERSE_RELATIONS[typ], src) not in regular}


</t>
<t tx="karstenw.20230225124228.15">def _hypernym_wrong_pos(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """synset's part-of-speech is different from its hypernym's"""
    sspos = {ss['id']: ss.get('partOfSpeech') for ss in _synsets(lex)}
    return {ss['id']: {'type': r['relType'], 'target': r['target']}
            for ss, r in _synset_relations(lex)
            if r['relType'] == 'hypernym'
            and ss.get('partOfSpeech') != sspos[r['target']]}


</t>
<t tx="karstenw.20230225124228.16">def _self_loop(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """relation is a self-loop"""
    relations = chain(_sense_relations(lex), _synset_relations(lex))
    return {x['id']: {'type': r['relType'], 'target': r['target']}
            for x, r in relations
            if x['id'] == r['target']}


# Helpers

</t>
<t tx="karstenw.20230225124228.17">def _multiples(iterable):
    counts = Counter(iterable)
    return {x: {'count': cnt} for x, cnt in counts.items() if cnt &gt; 1}


</t>
<t tx="karstenw.20230225124228.18">def _entries(lex: lmf.Lexicon) -&gt; List[lmf.LexicalEntry]: return lex.get('entries', [])
</t>
<t tx="karstenw.20230225124228.19">def _forms(e: lmf.LexicalEntry) -&gt; List[lmf.Form]: return e.get('forms', [])
</t>
<t tx="karstenw.20230225124228.2">def _non_unique_id(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """ID is not unique within the lexicon"""
    return _multiples(chain(
        [lex['id']],
        (f['id'] for e in _entries(lex) for f in _forms(e) if f.get('id')),
        (sb['id'] for sb in lex.get('frames', []) if sb.get('id')),
        ids['entry'],
        ids['sense'],
        ids['synset'],
    ))


</t>
<t tx="karstenw.20230225124228.20">def _senses(e: lmf.LexicalEntry) -&gt; List[lmf.Sense]: return e.get('senses', [])
</t>
<t tx="karstenw.20230225124228.21">def _synsets(lex: lmf.Lexicon) -&gt; List[lmf.Synset]: return lex.get('synsets', [])


</t>
<t tx="karstenw.20230225124228.22">def _sense_relations(lex: lmf.Lexicon) -&gt; Iterator[Tuple[lmf.Sense, lmf.Relation]]:
    for e in _entries(lex):
        for s in _senses(e):
            for r in s.get('relations', []):
                yield (s, r)


</t>
<t tx="karstenw.20230225124228.23">def _synset_relations(lex: lmf.Lexicon) -&gt; Iterator[Tuple[lmf.Synset, lmf.Relation]]:
    for ss in _synsets(lex):
        for r in ss.get('relations', []):
            yield (ss, r)


# Check codes and messages
#
# categories:
#   E - errors
#   W - warnings
# subcategories:
#   100 - general
#   200 - words and senses
#   300 - synsets and ilis
#   400 - relations
#   500 - graph and taxonomy

_codes: Dict[str, _CheckFunction] = {
    # 100 - general
    'E101': _non_unique_id,
    # 200 - words and senses
    'W201': _has_no_senses,
    'W202': _redundant_sense,
    'W203': _redundant_entry,
    'E204': _missing_synset,
    # 300 - synsets and ilis
    'W301': _empty_synset,
    'W302': _repeated_ili,
    'W303': _missing_ili_definition,
    'W304': _spurious_ili_definition,
    # 400 - relations
    'E401': _missing_relation_target,
    'W402': _invalid_relation_type,
    'W403': _redundant_relation,
    'W404': _missing_reverse_relation,
    # 500 - graph
    'W501': _hypernym_wrong_pos,
    'W502': _self_loop,
}


</t>
<t tx="karstenw.20230225124228.24">def _select_checks(select: Sequence[str]) -&gt; List[Tuple[str, _CheckFunction, str]]:
    selectset = set(select)
    return [(code, func, func.__doc__ or '')
            for code, func in _codes.items()
            if code in selectset or code[0] in selectset]


# Main function

</t>
<t tx="karstenw.20230225124228.25">def validate(
    lex: Union[lmf.Lexicon, lmf.LexiconExtension],
    select: Sequence[str] = ('E', 'W'),
    progress_handler: Optional[Type[ProgressHandler]] = ProgressBar
) -&gt; _Report:
    """Check *lex* for validity and return a report of the results.

    The *select* argument is a sequence of check codes (e.g.,
    ``E101``) or categories (``E`` or ``W``).

    The *progress_handler* parameter takes a subclass of
    :class:`wn.util.ProgressHandler`. An instance of the class will be
    created, used, and closed by this function.
    """
    if lex.get('extends'):
        print('validation of lexicon extensions is not supported')
        return {}
    lex = cast(lmf.Lexicon, lex)

    if progress_handler is None:
        progress_handler = ProgressHandler

    ids: _Ids = {
        'entry': Counter(entry['id'] for entry in _entries(lex)),
        'sense': Counter(sense['id']
                         for entry in _entries(lex)
                         for sense in _senses(entry)),
        'synset': Counter(synset['id'] for synset in _synsets(lex)),
    }

    checks = _select_checks(select)

    progress = progress_handler(message='Validate', total=len(checks))

    report: _Report = {}
    for code, func, message in checks:
        progress.set(status=func.__name__.replace('_', ' '))
        report[code] = {'message': message,
                        'items': func(lex, ids)}
        progress.update()
    progress.set(status='')
    progress.close()
    return report
</t>
<t tx="karstenw.20230225124228.3">def _has_no_senses(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """lexical entry has no senses"""
    return {e['id']: {} for e in _entries(lex) if not _senses(e)}


</t>
<t tx="karstenw.20230225124228.4">def _redundant_sense(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """redundant sense between lexical entry and synset"""
    result: _Result = {}
    for e in _entries(lex):
        redundant = _multiples(s['synset'] for s in _senses(e))
        result.update((s['id'], {'entry': e['id'], 'synset': s['synset']})
                      for s in _senses(e)
                      if s['synset'] in redundant)
    return result


</t>
<t tx="karstenw.20230225124228.5">def _redundant_entry(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """redundant lexical entry with the same lemma and synset"""
    redundant = _multiples((e['lemma']['writtenForm'], s['synset'])
                           for e in _entries(lex)
                           for s in _senses(e))
    return {form: {'synset': synset} for form, synset in redundant}


</t>
<t tx="karstenw.20230225124228.6">def _missing_synset(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """synset of sense is missing"""
    synset_ids = ids['synset']
    return {s['id']: {'synset': s['synset']}
            for e in _entries(lex)
            for s in _senses(e)
            if s['synset'] not in synset_ids}


</t>
<t tx="karstenw.20230225124228.7">def _empty_synset(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """synset is empty (not associated with any lexical entries)"""
    synsets = {s['synset'] for e in _entries(lex) for s in _senses(e)}
    return {ss['id']: {} for ss in _synsets(lex) if ss['id'] not in synsets}


</t>
<t tx="karstenw.20230225124228.8">def _repeated_ili(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """ILI is repeated across synsets"""
    repeated = _multiples(
        ss['ili'] for ss in _synsets(lex) if ss['ili'] and ss['ili'] != 'in'
    )
    return {ss['id']: {'ili': ss['ili']}
            for ss in _synsets(lex)
            if ss['ili'] in repeated}


</t>
<t tx="karstenw.20230225124228.9">def _missing_ili_definition(lex: lmf.Lexicon, ids: _Ids) -&gt; _Result:
    """proposed ILI is missing a definition"""
    return {ss['id']: {} for ss in _synsets(lex)
            if ss['ili'] == 'in' and not ss.get('ili_definition')}


</t>
<t tx="karstenw.20230225125423.1"></t>
<t tx="karstenw.20230225125511.1"></t>
</tnodes>
</leo_file>
