<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: http://leoeditor.com/leo_toc.html -->
<leo_file xmlns:leo="http://leoeditor.com/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2" tnodes="0" max_tnode_index="0" clone_windows="0"/>
<globals body_outline_ratio="0.5" body_secondary_ratio="0.5">
	<global_window_position top="50" left="50" height="500" width="700"/>
	<global_log_window_position top="0" left="0" height="0" width="0"/>
</globals>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="karstenw.20230306115851.2" a="E"><vh>textblob</vh>
<v t="karstenw.20230306115915.1"><vh>@clean textblob/__init__.py</vh>
<v t="karstenw.20230306115933.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230306120052.1"><vh>@clean textblob/_text.py</vh>
<v t="karstenw.20230306120100.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120100.2"><vh>decode_string</vh></v>
<v t="karstenw.20230306120100.3"><vh>encode_string</vh></v>
<v t="karstenw.20230306120100.4"><vh>isnumeric</vh></v>
<v t="karstenw.20230306120100.5"><vh>class lazydict</vh>
<v t="karstenw.20230306120100.6"><vh>load</vh></v>
<v t="karstenw.20230306120100.7"><vh>_lazy</vh></v>
<v t="karstenw.20230306120100.8"><vh>__repr__</vh></v>
<v t="karstenw.20230306120100.9"><vh>__len__</vh></v>
<v t="karstenw.20230306120100.10"><vh>__iter__</vh></v>
<v t="karstenw.20230306120100.11"><vh>__contains__</vh></v>
<v t="karstenw.20230306120100.12"><vh>__getitem__</vh></v>
<v t="karstenw.20230306120100.13"><vh>__setitem__</vh></v>
<v t="karstenw.20230306120100.14"><vh>setdefault</vh></v>
<v t="karstenw.20230306120100.15"><vh>get</vh></v>
<v t="karstenw.20230306120100.16"><vh>items</vh></v>
<v t="karstenw.20230306120100.17"><vh>keys</vh></v>
<v t="karstenw.20230306120100.18"><vh>values</vh></v>
<v t="karstenw.20230306120100.19"><vh>update</vh></v>
<v t="karstenw.20230306120100.20"><vh>pop</vh></v>
<v t="karstenw.20230306120100.21"><vh>popitem</vh></v>
</v>
<v t="karstenw.20230306120100.22"><vh>class lazylist</vh>
<v t="karstenw.20230306120100.23"><vh>load</vh></v>
<v t="karstenw.20230306120100.24"><vh>_lazy</vh></v>
<v t="karstenw.20230306120100.25"><vh>__repr__</vh></v>
<v t="karstenw.20230306120100.26"><vh>__len__</vh></v>
<v t="karstenw.20230306120100.27"><vh>__iter__</vh></v>
<v t="karstenw.20230306120100.28"><vh>__contains__</vh></v>
<v t="karstenw.20230306120100.29"><vh>insert</vh></v>
<v t="karstenw.20230306120100.30"><vh>append</vh></v>
<v t="karstenw.20230306120100.31"><vh>extend</vh></v>
<v t="karstenw.20230306120100.32"><vh>remove</vh></v>
<v t="karstenw.20230306120100.33"><vh>pop</vh></v>
</v>
<v t="karstenw.20230306120100.34"><vh>penntreebank2universal</vh></v>
<v t="karstenw.20230306120100.35"><vh>find_tokens</vh></v>
<v t="karstenw.20230306120100.36"><vh>_read</vh></v>
<v t="karstenw.20230306120100.37"><vh>class Lexicon</vh>
<v t="karstenw.20230306120100.38"><vh>__init__</vh></v>
<v t="karstenw.20230306120100.39"><vh>load</vh></v>
<v t="karstenw.20230306120100.40"><vh>path</vh></v>
<v t="karstenw.20230306120100.41"><vh>language</vh></v>
</v>
<v t="karstenw.20230306120100.42"><vh>class Rules</vh>
<v t="karstenw.20230306120100.43"><vh>__init__</vh></v>
<v t="karstenw.20230306120100.44"><vh>apply</vh></v>
</v>
<v t="karstenw.20230306120100.45"><vh>class Morphology</vh>
<v t="karstenw.20230306120100.46"><vh>__init__</vh></v>
<v t="karstenw.20230306120100.47"><vh>path</vh></v>
<v t="karstenw.20230306120100.48"><vh>load</vh></v>
<v t="karstenw.20230306120100.49"><vh>apply</vh></v>
<v t="karstenw.20230306120100.50"><vh>insert</vh></v>
<v t="karstenw.20230306120100.51"><vh>append</vh></v>
<v t="karstenw.20230306120100.52"><vh>extend</vh></v>
</v>
<v t="karstenw.20230306120100.53"><vh>class Context</vh>
<v t="karstenw.20230306120100.54"><vh>__init__</vh></v>
<v t="karstenw.20230306120100.55"><vh>path</vh></v>
<v t="karstenw.20230306120100.56"><vh>load</vh></v>
<v t="karstenw.20230306120100.57"><vh>apply</vh></v>
<v t="karstenw.20230306120100.58"><vh>insert</vh></v>
<v t="karstenw.20230306120100.59"><vh>append</vh></v>
<v t="karstenw.20230306120100.60"><vh>extend</vh></v>
</v>
<v t="karstenw.20230306120100.61"><vh>class Entities</vh>
<v t="karstenw.20230306120100.62"><vh>__init__</vh></v>
<v t="karstenw.20230306120100.63"><vh>path</vh></v>
<v t="karstenw.20230306120100.64"><vh>load</vh></v>
<v t="karstenw.20230306120100.65"><vh>apply</vh></v>
<v t="karstenw.20230306120100.66"><vh>append</vh></v>
<v t="karstenw.20230306120100.67"><vh>extend</vh></v>
</v>
<v t="karstenw.20230306120100.68"><vh>avg</vh></v>
<v t="karstenw.20230306120100.69"><vh>class Score</vh>
<v t="karstenw.20230306120100.70"><vh>__new__</vh></v>
<v t="karstenw.20230306120100.71"><vh>__init__</vh></v>
</v>
<v t="karstenw.20230306120100.72"><vh>class Sentiment</vh>
<v t="karstenw.20230306120100.73"><vh>__init__</vh></v>
<v t="karstenw.20230306120100.74"><vh>path</vh></v>
<v t="karstenw.20230306120100.75"><vh>language</vh></v>
<v t="karstenw.20230306120100.76"><vh>confidence</vh></v>
<v t="karstenw.20230306120100.77"><vh>load</vh></v>
<v t="karstenw.20230306120100.78"><vh>synset</vh></v>
<v t="karstenw.20230306120100.79"><vh>__call__</vh></v>
<v t="karstenw.20230306120100.80"><vh>assessments</vh></v>
<v t="karstenw.20230306120100.81"><vh>annotate</vh></v>
</v>
<v t="karstenw.20230306120100.82"><vh>_suffix_rules</vh></v>
<v t="karstenw.20230306120100.83"><vh>find_tags</vh></v>
<v t="karstenw.20230306120100.84"><vh>find_chunks</vh></v>
<v t="karstenw.20230306120100.85"><vh>find_prepositions</vh></v>
<v t="karstenw.20230306120100.86"><vh>class Parser</vh>
<v t="karstenw.20230306120100.87"><vh>__init__</vh></v>
<v t="karstenw.20230306120100.88"><vh>find_tokens</vh></v>
<v t="karstenw.20230306120100.89"><vh>find_tags</vh></v>
<v t="karstenw.20230306120100.90"><vh>find_chunks</vh></v>
<v t="karstenw.20230306120100.91"><vh>find_prepositions</vh></v>
<v t="karstenw.20230306120100.92"><vh>find_labels</vh></v>
<v t="karstenw.20230306120100.93"><vh>find_lemmata</vh></v>
<v t="karstenw.20230306120100.94"><vh>parse</vh></v>
</v>
<v t="karstenw.20230306120100.95"><vh>class TaggedString</vh>
<v t="karstenw.20230306120100.96"><vh>__new__</vh></v>
<v t="karstenw.20230306120100.97"><vh>split</vh></v>
</v>
<v t="karstenw.20230306120100.98"><vh>class Spelling</vh>
<v t="karstenw.20230306120100.99"><vh>__init__</vh></v>
<v t="karstenw.20230306120100.100"><vh>load</vh></v>
<v t="karstenw.20230306120100.101"><vh>path</vh></v>
<v t="karstenw.20230306120100.102"><vh>language</vh></v>
<v t="karstenw.20230306120100.103"><vh>train</vh></v>
<v t="karstenw.20230306120100.104"><vh>_edit1</vh></v>
<v t="karstenw.20230306120100.105"><vh>_edit2</vh></v>
<v t="karstenw.20230306120100.106"><vh>_known</vh></v>
<v t="karstenw.20230306120100.107"><vh>suggest</vh></v>
</v>
</v>
<v t="karstenw.20230306120124.1"><vh>@clean textblob/base.py</vh>
<v t="karstenw.20230306120511.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120511.2"><vh>class BaseTagger</vh>
<v t="karstenw.20230306120511.3"><vh>tag</vh></v>
</v>
<v t="karstenw.20230306120511.4"><vh>class BaseNPExtractor</vh>
<v t="karstenw.20230306120511.5"><vh>extract</vh></v>
</v>
<v t="karstenw.20230306120511.6"><vh>class BaseTokenizer</vh>
<v t="karstenw.20230306120511.7"><vh>tokenize</vh></v>
<v t="karstenw.20230306120511.8"><vh>itokenize</vh></v>
</v>
<v t="karstenw.20230306120511.9"><vh>class BaseSentimentAnalyzer</vh>
<v t="karstenw.20230306120511.10"><vh>__init__</vh></v>
<v t="karstenw.20230306120511.11"><vh>train</vh></v>
<v t="karstenw.20230306120511.12"><vh>analyze</vh></v>
</v>
<v t="karstenw.20230306120511.13"><vh>class BaseParser</vh>
<v t="karstenw.20230306120511.14"><vh>parse</vh></v>
</v>
</v>
<v t="karstenw.20230306120140.1"><vh>@clean textblob/blob.py</vh>
<v t="karstenw.20230306120514.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120514.2"><vh>_penn_to_wordnet</vh></v>
<v t="karstenw.20230306120514.3"><vh>class Word</vh>
<v t="karstenw.20230306120514.4"><vh>__new__</vh></v>
<v t="karstenw.20230306120514.5"><vh>__init__</vh></v>
<v t="karstenw.20230306120514.6"><vh>__repr__</vh></v>
<v t="karstenw.20230306120514.7"><vh>__str__</vh></v>
<v t="karstenw.20230306120514.8"><vh>singularize</vh></v>
<v t="karstenw.20230306120514.9"><vh>pluralize</vh></v>
<v t="karstenw.20230306120514.10"><vh>translate</vh></v>
<v t="karstenw.20230306120514.11"><vh>detect_language</vh></v>
<v t="karstenw.20230306120514.12"><vh>spellcheck</vh></v>
<v t="karstenw.20230306120514.13"><vh>correct</vh></v>
<v t="karstenw.20230306120514.14"><vh>lemma</vh></v>
<v t="karstenw.20230306120514.15"><vh>lemmatize</vh></v>
<v t="karstenw.20230306120514.16"><vh>stem</vh></v>
<v t="karstenw.20230306120514.17"><vh>synsets</vh></v>
<v t="karstenw.20230306120514.18"><vh>definitions</vh></v>
<v t="karstenw.20230306120514.19"><vh>get_synsets</vh></v>
<v t="karstenw.20230306120514.20"><vh>define</vh></v>
</v>
<v t="karstenw.20230306120514.21"><vh>class WordList</vh>
<v t="karstenw.20230306120514.22"><vh>__init__</vh></v>
<v t="karstenw.20230306120514.23"><vh>__str__</vh></v>
<v t="karstenw.20230306120514.24"><vh>__repr__</vh></v>
<v t="karstenw.20230306120514.25"><vh>__getitem__</vh></v>
<v t="karstenw.20230306120514.26"><vh>__getslice__</vh></v>
<v t="karstenw.20230306120514.27"><vh>__setitem__</vh></v>
<v t="karstenw.20230306120514.28"><vh>count</vh></v>
<v t="karstenw.20230306120514.29"><vh>append</vh></v>
<v t="karstenw.20230306120514.30"><vh>extend</vh></v>
<v t="karstenw.20230306120514.31"><vh>upper</vh></v>
<v t="karstenw.20230306120514.32"><vh>lower</vh></v>
<v t="karstenw.20230306120514.33"><vh>singularize</vh></v>
<v t="karstenw.20230306120514.34"><vh>pluralize</vh></v>
<v t="karstenw.20230306120514.35"><vh>lemmatize</vh></v>
<v t="karstenw.20230306120514.36"><vh>stem</vh></v>
</v>
<v t="karstenw.20230306120514.37"><vh>_validated_param</vh></v>
<v t="karstenw.20230306120514.38"><vh>_initialize_models</vh></v>
<v t="karstenw.20230306120514.39"><vh>class BaseBlob</vh>
<v t="karstenw.20230306120514.40"><vh>__init__</vh></v>
<v t="karstenw.20230306120514.41"><vh>words</vh></v>
<v t="karstenw.20230306120514.42"><vh>tokens</vh></v>
<v t="karstenw.20230306120514.43"><vh>tokenize</vh></v>
<v t="karstenw.20230306120514.44"><vh>parse</vh></v>
<v t="karstenw.20230306120514.45"><vh>classify</vh></v>
<v t="karstenw.20230306120514.46"><vh>sentiment</vh></v>
<v t="karstenw.20230306120514.47"><vh>sentiment_assessments</vh></v>
<v t="karstenw.20230306120514.48"><vh>polarity</vh></v>
<v t="karstenw.20230306120514.49"><vh>subjectivity</vh></v>
<v t="karstenw.20230306120514.50"><vh>noun_phrases</vh></v>
<v t="karstenw.20230306120514.51"><vh>pos_tags</vh></v>
<v t="karstenw.20230306120514.52"><vh>word_counts</vh></v>
<v t="karstenw.20230306120514.53"><vh>np_counts</vh></v>
<v t="karstenw.20230306120514.54"><vh>ngrams</vh></v>
<v t="karstenw.20230306120514.55"><vh>translate</vh></v>
<v t="karstenw.20230306120514.56"><vh>detect_language</vh></v>
<v t="karstenw.20230306120514.57"><vh>correct</vh></v>
<v t="karstenw.20230306120514.58"><vh>_cmpkey</vh></v>
<v t="karstenw.20230306120514.59"><vh>_strkey</vh></v>
<v t="karstenw.20230306120514.60"><vh>__hash__</vh></v>
<v t="karstenw.20230306120514.61"><vh>__add__</vh></v>
<v t="karstenw.20230306120514.62"><vh>split</vh></v>
</v>
<v t="karstenw.20230306120514.63"><vh>class TextBlob</vh>
<v t="karstenw.20230306120514.64"><vh>sentences</vh></v>
<v t="karstenw.20230306120514.65"><vh>words</vh></v>
<v t="karstenw.20230306120514.66"><vh>raw_sentences</vh></v>
<v t="karstenw.20230306120514.67"><vh>serialized</vh></v>
<v t="karstenw.20230306120514.68"><vh>to_json</vh></v>
<v t="karstenw.20230306120514.69"><vh>json</vh></v>
<v t="karstenw.20230306120514.70"><vh>_create_sentence_objects</vh></v>
</v>
<v t="karstenw.20230306120514.71"><vh>class Sentence</vh>
<v t="karstenw.20230306120514.72"><vh>__init__</vh></v>
<v t="karstenw.20230306120514.73"><vh>dict</vh></v>
</v>
<v t="karstenw.20230306120514.74"><vh>class Blobber</vh>
<v t="karstenw.20230306120514.75"><vh>__init__</vh></v>
<v t="karstenw.20230306120514.76"><vh>__call__</vh></v>
<v t="karstenw.20230306120514.77"><vh>__repr__</vh></v>
</v>
</v>
<v t="karstenw.20230306120145.1"><vh>@clean textblob/classifiers.py</vh>
<v t="karstenw.20230306120516.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120516.2"><vh>_get_words_from_dataset</vh></v>
<v t="karstenw.20230306120516.3"><vh>_get_document_tokens</vh></v>
<v t="karstenw.20230306120516.4"><vh>basic_extractor</vh></v>
<v t="karstenw.20230306120516.5"><vh>contains_extractor</vh></v>
<v t="karstenw.20230306120516.6"><vh>class BaseClassifier</vh>
<v t="karstenw.20230306120516.7"><vh>__init__</vh></v>
<v t="karstenw.20230306120516.8"><vh>_read_data</vh></v>
<v t="karstenw.20230306120516.9"><vh>classifier</vh></v>
<v t="karstenw.20230306120516.10"><vh>classify</vh></v>
<v t="karstenw.20230306120516.11"><vh>train</vh></v>
<v t="karstenw.20230306120516.12"><vh>labels</vh></v>
<v t="karstenw.20230306120516.13"><vh>extract_features</vh></v>
</v>
<v t="karstenw.20230306120516.14"><vh>class NLTKClassifier</vh>
<v t="karstenw.20230306120516.15"><vh>__init__</vh></v>
<v t="karstenw.20230306120516.16"><vh>__repr__</vh></v>
<v t="karstenw.20230306120516.17"><vh>classifier</vh></v>
<v t="karstenw.20230306120516.18"><vh>train</vh></v>
<v t="karstenw.20230306120516.19"><vh>labels</vh></v>
<v t="karstenw.20230306120516.20"><vh>classify</vh></v>
<v t="karstenw.20230306120516.21"><vh>accuracy</vh></v>
<v t="karstenw.20230306120516.22"><vh>update</vh></v>
</v>
<v t="karstenw.20230306120516.23"><vh>class NaiveBayesClassifier</vh>
<v t="karstenw.20230306120516.24"><vh>prob_classify</vh></v>
<v t="karstenw.20230306120516.25"><vh>informative_features</vh></v>
<v t="karstenw.20230306120516.26"><vh>show_informative_features</vh></v>
</v>
<v t="karstenw.20230306120516.27"><vh>class DecisionTreeClassifier</vh>
<v t="karstenw.20230306120516.28"><vh>pretty_format</vh></v>
<v t="karstenw.20230306120516.29"><vh>pseudocode</vh></v>
</v>
<v t="karstenw.20230306120516.30"><vh>class PositiveNaiveBayesClassifier</vh>
<v t="karstenw.20230306120516.31"><vh>__init__</vh></v>
<v t="karstenw.20230306120516.32"><vh>__repr__</vh></v>
<v t="karstenw.20230306120516.33"><vh>train</vh></v>
<v t="karstenw.20230306120516.34"><vh>update</vh></v>
</v>
<v t="karstenw.20230306120516.35"><vh>class MaxEntClassifier</vh>
<v t="karstenw.20230306120516.36"><vh>prob_classify</vh></v>
</v>
</v>
<v t="karstenw.20230306120155.1"><vh>@clean textblob/compat.py</vh>
<v t="karstenw.20230306120518.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120518.2"><vh>with_metaclass</vh></v>
</v>
<v t="karstenw.20230306120200.1"><vh>@clean textblob/decorators.py</vh>
<v t="karstenw.20230306120520.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120520.2"><vh>class cached_property</vh>
<v t="karstenw.20230306120520.3"><vh>__init__</vh></v>
<v t="karstenw.20230306120520.4"><vh>__get__</vh></v>
</v>
<v t="karstenw.20230306120520.5"><vh>requires_nltk_corpus</vh></v>
</v>
<v t="karstenw.20230306120207.1"><vh>@clean textblob/download_corpora.py</vh>
<v t="karstenw.20230306120523.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120523.2"><vh>download_lite</vh></v>
<v t="karstenw.20230306120523.3"><vh>download_all</vh></v>
<v t="karstenw.20230306120523.4"><vh>main</vh></v>
</v>
<v t="karstenw.20230306120217.1"><vh>@clean textblob/exceptions.py</vh>
<v t="karstenw.20230306120524.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120524.2"><vh>class TextBlobError</vh></v>
<v t="karstenw.20230306120524.3"><vh>class MissingCorpusError</vh>
<v t="karstenw.20230306120524.4"><vh>__init__</vh></v>
</v>
<v t="karstenw.20230306120524.5"><vh>class DeprecationError</vh></v>
<v t="karstenw.20230306120524.6"><vh>class TranslatorError</vh></v>
<v t="karstenw.20230306120525.1"><vh>class NotTranslated</vh></v>
<v t="karstenw.20230306120525.2"><vh>class FormatError</vh></v>
</v>
<v t="karstenw.20230306120231.1"><vh>@clean textblob/formats.py</vh>
<v t="karstenw.20230306120526.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120526.2"><vh>class BaseFormat</vh>
<v t="karstenw.20230306120526.3"><vh>__init__</vh></v>
<v t="karstenw.20230306120526.4"><vh>to_iterable</vh></v>
<v t="karstenw.20230306120526.5"><vh>detect</vh></v>
</v>
<v t="karstenw.20230306120526.6"><vh>class DelimitedFormat</vh>
<v t="karstenw.20230306120526.7"><vh>__init__</vh></v>
<v t="karstenw.20230306120526.8"><vh>to_iterable</vh></v>
<v t="karstenw.20230306120526.9"><vh>detect</vh></v>
</v>
<v t="karstenw.20230306120526.10"><vh>class CSV</vh></v>
<v t="karstenw.20230306120526.11"><vh>class TSV</vh></v>
<v t="karstenw.20230306120526.12"><vh>class JSON</vh>
<v t="karstenw.20230306120526.13"><vh>__init__</vh></v>
<v t="karstenw.20230306120526.14"><vh>to_iterable</vh></v>
<v t="karstenw.20230306120526.15"><vh>detect</vh></v>
</v>
<v t="karstenw.20230306120526.16"><vh>detect</vh></v>
<v t="karstenw.20230306120526.17"><vh>get_registry</vh></v>
<v t="karstenw.20230306120526.18"><vh>register</vh></v>
</v>
<v t="karstenw.20230306120234.1"><vh>@clean textblob/inflect.py</vh>
<v t="karstenw.20230306120528.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230306120240.1"><vh>@clean textblob/mixins.py</vh>
<v t="karstenw.20230306120530.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120530.2"><vh>class ComparableMixin</vh>
<v t="karstenw.20230306120530.3"><vh>_compare</vh></v>
<v t="karstenw.20230306120530.4"><vh>__lt__</vh></v>
<v t="karstenw.20230306120530.5"><vh>__le__</vh></v>
<v t="karstenw.20230306120530.6"><vh>__eq__</vh></v>
<v t="karstenw.20230306120530.7"><vh>__ge__</vh></v>
<v t="karstenw.20230306120530.8"><vh>__gt__</vh></v>
<v t="karstenw.20230306120530.9"><vh>__ne__</vh></v>
</v>
<v t="karstenw.20230306120530.10"><vh>class BlobComparableMixin</vh>
<v t="karstenw.20230306120530.11"><vh>_compare</vh></v>
</v>
<v t="karstenw.20230306120530.12"><vh>class StringlikeMixin</vh>
<v t="karstenw.20230306120530.13"><vh>__repr__</vh></v>
<v t="karstenw.20230306120530.14"><vh>__str__</vh></v>
<v t="karstenw.20230306120530.15"><vh>__len__</vh></v>
<v t="karstenw.20230306120530.16"><vh>__iter__</vh></v>
<v t="karstenw.20230306120530.17"><vh>__contains__</vh></v>
<v t="karstenw.20230306120530.18"><vh>__getitem__</vh></v>
<v t="karstenw.20230306120530.19"><vh>find</vh></v>
<v t="karstenw.20230306120530.20"><vh>rfind</vh></v>
<v t="karstenw.20230306120530.21"><vh>index</vh></v>
<v t="karstenw.20230306120530.22"><vh>rindex</vh></v>
<v t="karstenw.20230306120530.23"><vh>startswith</vh></v>
<v t="karstenw.20230306120530.24"><vh>endswith</vh></v>
<v t="karstenw.20230306120530.25"><vh>title</vh></v>
<v t="karstenw.20230306120530.26"><vh>format</vh></v>
<v t="karstenw.20230306120530.27"><vh>split</vh></v>
<v t="karstenw.20230306120530.28"><vh>strip</vh></v>
<v t="karstenw.20230306120530.29"><vh>upper</vh></v>
<v t="karstenw.20230306120530.30"><vh>lower</vh></v>
<v t="karstenw.20230306120530.31"><vh>join</vh></v>
<v t="karstenw.20230306120530.32"><vh>replace</vh></v>
</v>
</v>
<v t="karstenw.20230306120247.1"><vh>@clean textblob/np_extractors.py</vh>
<v t="karstenw.20230306120532.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230306120255.1"><vh>@clean textblob/parsers.py</vh>
<v t="karstenw.20230306120534.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230306120300.1"><vh>@clean textblob/sentiments.py</vh>
<v t="karstenw.20230306120536.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230306120307.1"><vh>@clean textblob/tokenizers.py</vh>
<v t="karstenw.20230306120539.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120539.2"><vh>class WordTokenizer</vh>
<v t="karstenw.20230306120539.3"><vh>tokenize</vh></v>
</v>
<v t="karstenw.20230306120539.4"><vh>class SentenceTokenizer</vh>
<v t="karstenw.20230306120539.5"><vh>tokenize</vh></v>
</v>
<v t="karstenw.20230306120539.6"><vh>word_tokenize</vh></v>
</v>
<v t="karstenw.20230306120316.1"><vh>@clean textblob/translate.py</vh>
<v t="karstenw.20230306120541.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120541.2"><vh>class Translator</vh>
<v t="karstenw.20230306120541.3"><vh>translate</vh></v>
<v t="karstenw.20230306120541.4"><vh>detect</vh></v>
<v t="karstenw.20230306120541.5"><vh>_validate_translation</vh></v>
<v t="karstenw.20230306120541.6"><vh>_request</vh></v>
</v>
<v t="karstenw.20230306120541.7"><vh>_unescape</vh></v>
<v t="karstenw.20230306120541.8"><vh>_calculate_tk</vh></v>
</v>
<v t="karstenw.20230306120321.1"><vh>@clean textblob/utils.py</vh>
<v t="karstenw.20230306120543.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120543.2"><vh>strip_punc</vh></v>
<v t="karstenw.20230306120543.3"><vh>lowerstrip</vh></v>
<v t="karstenw.20230306120543.4"><vh>tree2str</vh></v>
<v t="karstenw.20230306120543.5"><vh>filter_insignificant</vh></v>
<v t="karstenw.20230306120543.6"><vh>is_filelike</vh></v>
</v>
<v t="karstenw.20230306120327.1"><vh>@clean textblob/wordnet.py</vh>
<v t="karstenw.20230306120545.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230306120339.1"><vh>en</vh>
<v t="karstenw.20230306120343.1"><vh>@clean textblob/en/__init__.py</vh>
<v t="karstenw.20230306120548.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120548.2"><vh>find_lemmata</vh></v>
<v t="karstenw.20230306120548.3"><vh>class Parser</vh>
<v t="karstenw.20230306120548.4"><vh>find_lemmata</vh></v>
<v t="karstenw.20230306120548.5"><vh>find_tags</vh></v>
</v>
<v t="karstenw.20230306120548.6"><vh>class Sentiment</vh>
<v t="karstenw.20230306120548.7"><vh>load</vh></v>
</v>
<v t="karstenw.20230306120548.8"><vh>tokenize</vh></v>
<v t="karstenw.20230306120548.9"><vh>parse</vh></v>
<v t="karstenw.20230306120548.10"><vh>parsetree</vh></v>
<v t="karstenw.20230306120548.11"><vh>split</vh></v>
<v t="karstenw.20230306120548.12"><vh>tag</vh></v>
<v t="karstenw.20230306120548.13"><vh>suggest</vh></v>
<v t="karstenw.20230306120548.14"><vh>polarity</vh></v>
<v t="karstenw.20230306120548.15"><vh>subjectivity</vh></v>
<v t="karstenw.20230306120548.16"><vh>positive</vh></v>
</v>
<v t="karstenw.20230306120408.1"><vh>@clean textblob/en/inflect.py</vh>
<v t="karstenw.20230306120550.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120550.2"><vh>pluralize</vh></v>
<v t="karstenw.20230306120550.3"><vh>singularize</vh></v>
</v>
<v t="karstenw.20230306120414.1"><vh>@clean textblob/en/np_extractors.py</vh>
<v t="karstenw.20230306120552.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120552.2"><vh>class ChunkParser</vh>
<v t="karstenw.20230306120552.3"><vh>__init__</vh></v>
<v t="karstenw.20230306120552.4"><vh>train</vh></v>
<v t="karstenw.20230306120552.5"><vh>parse</vh></v>
</v>
<v t="karstenw.20230306120552.6"><vh>class ConllExtractor</vh>
<v t="karstenw.20230306120552.7"><vh>__init__</vh></v>
<v t="karstenw.20230306120552.8"><vh>extract</vh></v>
<v t="karstenw.20230306120552.9"><vh>_parse_sentence</vh></v>
</v>
<v t="karstenw.20230306120552.10"><vh>class FastNPExtractor</vh>
<v t="karstenw.20230306120552.11"><vh>__init__</vh></v>
<v t="karstenw.20230306120552.12"><vh>train</vh></v>
<v t="karstenw.20230306120552.13"><vh>_tokenize_sentence</vh></v>
<v t="karstenw.20230306120552.14"><vh>extract</vh></v>
</v>
<v t="karstenw.20230306120552.15"><vh>_normalize_tags</vh></v>
<v t="karstenw.20230306120552.16"><vh>_is_match</vh></v>
</v>
<v t="karstenw.20230306120423.1"><vh>@clean textblob/en/parsers.py</vh>
<v t="karstenw.20230306120554.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120554.2"><vh>class PatternParser</vh>
<v t="karstenw.20230306120554.3"><vh>parse</vh></v>
</v>
</v>
<v t="karstenw.20230306120428.1"><vh>@clean textblob/en/sentiments.py</vh>
<v t="karstenw.20230306120625.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120625.2"><vh>class PatternAnalyzer</vh>
<v t="karstenw.20230306120625.3"><vh>analyze</vh></v>
</v>
<v t="karstenw.20230306120625.4"><vh>_default_feature_extractor</vh></v>
<v t="karstenw.20230306120625.5"><vh>class NaiveBayesAnalyzer</vh>
<v t="karstenw.20230306120625.6"><vh>__init__</vh></v>
<v t="karstenw.20230306120625.7"><vh>train</vh></v>
<v t="karstenw.20230306120625.8"><vh>analyze</vh></v>
</v>
</v>
<v t="karstenw.20230306120439.1"><vh>@clean textblob/en/taggers.py</vh>
<v t="karstenw.20230306120628.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120628.2"><vh>class PatternTagger</vh>
<v t="karstenw.20230306120628.3"><vh>tag</vh></v>
</v>
<v t="karstenw.20230306120628.4"><vh>class NLTKTagger</vh>
<v t="karstenw.20230306120628.5"><vh>tag</vh></v>
</v>
</v>
</v>
<v t="karstenw.20230306120448.1"><vh>unicodecsv</vh>
<v t="karstenw.20230306120455.1"><vh>@clean textblob/unicodecsv/__init__.py</vh>
<v t="karstenw.20230306120631.1"><vh>Declarations</vh></v>
<v t="karstenw.20230306120631.2"><vh>_stringify</vh></v>
<v t="karstenw.20230306120631.3"><vh>_stringify_list</vh></v>
<v t="karstenw.20230306120631.4"><vh>_unicodify</vh></v>
<v t="karstenw.20230306120631.5"><vh>class UnicodeWriter</vh>
<v t="karstenw.20230306120631.6"><vh>__init__</vh></v>
<v t="karstenw.20230306120631.7"><vh>writerow</vh></v>
<v t="karstenw.20230306120631.8"><vh>writerows</vh></v>
<v t="karstenw.20230306120631.9"><vh>dialect</vh></v>
</v>
<v t="karstenw.20230306120631.10"><vh>class UnicodeReader</vh>
<v t="karstenw.20230306120631.11"><vh>__init__</vh></v>
<v t="karstenw.20230306120631.12"><vh>next</vh></v>
<v t="karstenw.20230306120631.13"><vh>__iter__</vh></v>
<v t="karstenw.20230306120631.14"><vh>dialect</vh></v>
<v t="karstenw.20230306120631.15"><vh>line_num</vh></v>
</v>
<v t="karstenw.20230306120631.16"><vh>class DictWriter</vh>
<v t="karstenw.20230306120631.17"><vh>__init__</vh></v>
<v t="karstenw.20230306120631.18"><vh>writeheader</vh></v>
</v>
<v t="karstenw.20230306120631.19"><vh>class DictReader</vh>
<v t="karstenw.20230306120631.20"><vh>__init__</vh></v>
<v t="karstenw.20230306120631.21"><vh>next</vh></v>
</v>
</v>
</v>
</v>
</vnodes>
<tnodes>
<t tx="karstenw.20230306115851.2"></t>
<t tx="karstenw.20230306115915.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230306115933.1">import os
from .blob import TextBlob, Word, Sentence, Blobber, WordList

__version__ = '0.17.1'
__license__ = 'MIT'
__author__ = 'Steven Loria'

PACKAGE_DIR = os.path.dirname(os.path.abspath(__file__))

__all__ = [
    'TextBlob',
    'Word',
    'Sentence',
    'Blobber',
    'WordList',
]
</t>
<t tx="karstenw.20230306120052.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230306120100.1"># -*- coding: utf-8 -*-
"""This file is adapted from the pattern library.

URL: http://www.clips.ua.ac.be/pages/pattern-web
Licence: BSD
"""
from __future__ import unicode_literals
import string
import codecs
from itertools import chain
import types
import os
import re
from xml.etree import cElementTree

from .compat import text_type, basestring, imap, unicode, binary_type, PY2

try:
    MODULE = os.path.dirname(os.path.abspath(__file__))
except:
    MODULE = ""

SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA = \
    "&amp;slash;", "word", "part-of-speech", "chunk", "preposition", "relation", "anchor", "lemma"


# String functions
</t>
<t tx="karstenw.20230306120100.10">def __iter__(self):
    return self._lazy("__iter__")
</t>
<t tx="karstenw.20230306120100.100">def load(self):
    for x in _read(self._path):
        x = x.split()
        dict.__setitem__(self, x[0], int(x[1]))

</t>
<t tx="karstenw.20230306120100.101">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230306120100.102">@property
def language(self):
    return self._language

</t>
<t tx="karstenw.20230306120100.103">@classmethod
def train(self, s, path="spelling.txt"):
    """ Counts the words in the given string and saves the probabilities at the given path.
        This can be used to generate a new model for the Spelling() constructor.
    """
    model = {}
    for w in re.findall("[a-z]+", s.lower()):
        model[w] = w in model and model[w] + 1 or 1
    model = ("%s %s" % (k, v) for k, v in sorted(model.items()))
    model = "\n".join(model)
    f = open(path, "w")
    f.write(model)
    f.close()

</t>
<t tx="karstenw.20230306120100.104">def _edit1(self, w):
    """ Returns a set of words with edit distance 1 from the given word.
    """
    # Of all spelling errors, 80% is covered by edit distance 1.
    # Edit distance 1 = one character deleted, swapped, replaced or inserted.
    split = [(w[:i], w[i:]) for i in range(len(w) + 1)]
    delete, transpose, replace, insert = (
        [a + b[1:] for a, b in split if b],
        [a + b[1] + b[0] + b[2:] for a, b in split if len(b) &gt; 1],
        [a + c + b[1:] for a, b in split for c in Spelling.ALPHA if b],
        [a + c + b[0:] for a, b in split for c in Spelling.ALPHA]
    )
    return set(delete + transpose + replace + insert)

</t>
<t tx="karstenw.20230306120100.105">def _edit2(self, w):
    """ Returns a set of words with edit distance 2 from the given word
    """
    # Of all spelling errors, 99% is covered by edit distance 2.
    # Only keep candidates that are actually known words (20% speedup).
    return set(e2 for e1 in self._edit1(w) for e2 in self._edit1(e1) if e2 in self)

</t>
<t tx="karstenw.20230306120100.106">def _known(self, words=[]):
    """ Returns the given list of words filtered by known words.
    """
    return set(w for w in words if w in self)

</t>
<t tx="karstenw.20230306120100.107">def suggest(self, w):
    """ Return a list of (word, confidence) spelling corrections for the given word,
        based on the probability of known words with edit distance 1-2 from the given word.
    """
    if len(self) == 0:
        self.load()
    if len(w) == 1:
        return [(w, 1.0)] # I
    if w in PUNCTUATION:
        return [(w, 1.0)] # .?!
    if w in string.whitespace:
        return [(w, 1.0)] # \n
    if w.replace(".", "").isdigit():
        return [(w, 1.0)] # 1.5
    candidates = self._known([w]) \
              or self._known(self._edit1(w)) \
              or self._known(self._edit2(w)) \
              or [w]
    candidates = [(self.get(c, 0.0), c) for c in candidates]
    s = float(sum(p for p, word in candidates) or 1)
    candidates = sorted(((p / s, word) for p, word in candidates), reverse=True)
    if w.istitle():  # Preserve capitalization
        candidates = [(word.title(), p) for p, word in candidates]
    else:
        candidates = [(word, p) for p, word in candidates]
    return candidates
</t>
<t tx="karstenw.20230306120100.11">def __contains__(self, *args):
    return self._lazy("__contains__", *args)
</t>
<t tx="karstenw.20230306120100.12">def __getitem__(self, *args):
    return self._lazy("__getitem__", *args)
</t>
<t tx="karstenw.20230306120100.13">def __setitem__(self, *args):
    return self._lazy("__setitem__", *args)
</t>
<t tx="karstenw.20230306120100.14">def setdefault(self, *args):
    return self._lazy("setdefault", *args)
</t>
<t tx="karstenw.20230306120100.15">def get(self, *args, **kwargs):
    return self._lazy("get", *args)
</t>
<t tx="karstenw.20230306120100.16">def items(self):
    return self._lazy("items")
</t>
<t tx="karstenw.20230306120100.17">def keys(self):
    return self._lazy("keys")
</t>
<t tx="karstenw.20230306120100.18">def values(self):
    return self._lazy("values")
</t>
<t tx="karstenw.20230306120100.19">def update(self, *args):
    return self._lazy("update", *args)
</t>
<t tx="karstenw.20230306120100.2">def decode_string(v, encoding="utf-8"):
    """ Returns the given value as a Unicode string (if possible).
    """
    if isinstance(encoding, basestring):
        encoding = ((encoding,),) + (("windows-1252",), ("utf-8", "ignore"))
    if isinstance(v, binary_type):
        for e in encoding:
            try:
                return v.decode(*e)
            except:
                pass
        return v
    return unicode(v)


</t>
<t tx="karstenw.20230306120100.20">def pop(self, *args):
    return self._lazy("pop", *args)
</t>
<t tx="karstenw.20230306120100.21">def popitem(self, *args):
    return self._lazy("popitem", *args)

</t>
<t tx="karstenw.20230306120100.22">class lazylist(list):

    @others
#--- UNIVERSAL TAGSET ------------------------------------------------------------------------------
# The default part-of-speech tagset used in Pattern is Penn Treebank II.
# However, not all languages are well-suited to Penn Treebank (which was developed for English).
# As more languages are implemented, this is becoming more problematic.
#
# A universal tagset is proposed by Slav Petrov (2012):
# http://www.petrovi.de/data/lrec.pdf
#
# Subclasses of Parser should start implementing
# Parser.parse(tagset=UNIVERSAL) with a simplified tagset.
# The names of the constants correspond to Petrov's naming scheme, while
# the value of the constants correspond to Penn Treebank.

UNIVERSAL = "universal"

NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X = \
    "NN", "VB", "JJ", "RB", "PR", "DT", "PP", "PP", "NO", "CJ", "UH", "PT", ".", "X"

</t>
<t tx="karstenw.20230306120100.23">def load(self):
    # Must be overridden in a subclass.
    # Must load data with list.append(self, v) instead of lazylist.append(v).
    pass

</t>
<t tx="karstenw.20230306120100.24">def _lazy(self, method, *args):
    """ If the list is empty, calls lazylist.load().
        Replaces lazylist.method() with list.method() and calls it.
    """
    if list.__len__(self) == 0:
        self.load()
        setattr(self, method, types.MethodType(getattr(list, method), self))
    return getattr(list, method)(self, *args)

</t>
<t tx="karstenw.20230306120100.25">def __repr__(self):
    return self._lazy("__repr__")
</t>
<t tx="karstenw.20230306120100.26">def __len__(self):
    return self._lazy("__len__")
</t>
<t tx="karstenw.20230306120100.27">def __iter__(self):
    return self._lazy("__iter__")
</t>
<t tx="karstenw.20230306120100.28">def __contains__(self, *args):
    return self._lazy("__contains__", *args)
</t>
<t tx="karstenw.20230306120100.29">def insert(self, *args):
    return self._lazy("insert", *args)
</t>
<t tx="karstenw.20230306120100.3">def encode_string(v, encoding="utf-8"):
    """ Returns the given value as a Python byte string (if possible).
    """
    if isinstance(encoding, basestring):
        encoding = ((encoding,),) + (("windows-1252",), ("utf-8", "ignore"))
    if isinstance(v, unicode):
        for e in encoding:
            try:
                return v.encode(*e)
            except:
                pass
        return v
    return str(v)

decode_utf8 = decode_string
encode_utf8 = encode_string


</t>
<t tx="karstenw.20230306120100.30">def append(self, *args):
    return self._lazy("append", *args)
</t>
<t tx="karstenw.20230306120100.31">def extend(self, *args):
    return self._lazy("extend", *args)
</t>
<t tx="karstenw.20230306120100.32">def remove(self, *args):
    return self._lazy("remove", *args)
</t>
<t tx="karstenw.20230306120100.33">def pop(self, *args):
    return self._lazy("pop", *args)

</t>
<t tx="karstenw.20230306120100.34">def penntreebank2universal(token, tag):
    """ Returns a (token, tag)-tuple with a simplified universal part-of-speech tag.
    """
    if tag.startswith(("NNP-", "NNPS-")):
        return (token, "%s-%s" % (NOUN, tag.split("-")[-1]))
    if tag in ("NN", "NNS", "NNP", "NNPS", "NP"):
        return (token, NOUN)
    if tag in ("MD", "VB", "VBD", "VBG", "VBN", "VBP", "VBZ"):
        return (token, VERB)
    if tag in ("JJ", "JJR", "JJS"):
        return (token, ADJ)
    if tag in ("RB", "RBR", "RBS", "WRB"):
        return (token, ADV)
    if tag in ("PRP", "PRP$", "WP", "WP$"):
        return (token, PRON)
    if tag in ("DT", "PDT", "WDT", "EX"):
        return (token, DET)
    if tag in ("IN",):
        return (token, PREP)
    if tag in ("CD",):
        return (token, NUM)
    if tag in ("CC",):
        return (token, CONJ)
    if tag in ("UH",):
        return (token, INTJ)
    if tag in ("POS", "RP", "TO"):
        return (token, PRT)
    if tag in ("SYM", "LS", ".", "!", "?", ",", ":", "(", ")", "\"", "#", "$"):
        return (token, PUNC)
    return (token, X)

#--- TOKENIZER -------------------------------------------------------------------------------------

TOKEN = re.compile(r"(\S+)\s")

# Handle common punctuation marks.
PUNCTUATION = \
punctuation = ".,;:!?()[]{}`''\"@#$^&amp;*+-|=~_"

# Handle common abbreviations.
ABBREVIATIONS = abbreviations = set((
    "a.", "adj.", "adv.", "al.", "a.m.", "c.", "cf.", "comp.", "conf.", "def.",
    "ed.", "e.g.", "esp.", "etc.", "ex.", "f.", "fig.", "gen.", "id.", "i.e.",
    "int.", "l.", "m.", "Med.", "Mil.", "Mr.", "n.", "n.q.", "orig.", "pl.",
    "pred.", "pres.", "p.m.", "ref.", "v.", "vs.", "w/"
))

RE_ABBR1 = re.compile("^[A-Za-z]\.$")       # single letter, "T. De Smedt"
RE_ABBR2 = re.compile("^([A-Za-z]\.)+$")    # alternating letters, "U.S."
RE_ABBR3 = re.compile("^[A-Z][" + "|".join( # capital followed by consonants, "Mr."
        "bcdfghjklmnpqrstvwxz") + "]+.$")

# Handle emoticons.
EMOTICONS = { # (facial expression, sentiment)-keys
    ("love" , +1.00): set(("&lt;3", "♥")),
    ("grin" , +1.00): set(("&gt;:D", ":-D", ":D", "=-D", "=D", "X-D", "x-D", "XD", "xD", "8-D")),
    ("taunt", +0.75): set(("&gt;:P", ":-P", ":P", ":-p", ":p", ":-b", ":b", ":c)", ":o)", ":^)")),
    ("smile", +0.50): set(("&gt;:)", ":-)", ":)", "=)", "=]", ":]", ":}", ":&gt;", ":3", "8)", "8-)")),
    ("wink" , +0.25): set(("&gt;;]", ";-)", ";)", ";-]", ";]", ";D", ";^)", "*-)", "*)")),
    ("gasp" , +0.05): set(("&gt;:o", ":-O", ":O", ":o", ":-o", "o_O", "o.O", "°O°", "°o°")),
    ("worry", -0.25): set(("&gt;:/",  ":-/", ":/", ":\\", "&gt;:\\", ":-.", ":-s", ":s", ":S", ":-S", "&gt;.&gt;")),
    ("frown", -0.75): set(("&gt;:[", ":-(", ":(", "=(", ":-[", ":[", ":{", ":-&lt;", ":c", ":-c", "=/")),
    ("cry"  , -1.00): set((":'(", ":'''(", ";'("))
}

RE_EMOTICONS = [r" ?".join([re.escape(each) for each in e]) for v in EMOTICONS.values() for e in v]
RE_EMOTICONS = re.compile(r"(%s)($|\s)" % "|".join(RE_EMOTICONS))

# Handle sarcasm punctuation (!).
RE_SARCASM = re.compile(r"\( ?\! ?\)")

# Handle common contractions.
replacements = {
     "'d": " 'd",
     "'m": " 'm",
     "'s": " 's",
    "'ll": " 'll",
    "'re": " 're",
    "'ve": " 've",
    "n't": " n't"
}

# Handle paragraph line breaks (\n\n marks end of sentence).
EOS = "END-OF-SENTENCE"

</t>
<t tx="karstenw.20230306120100.35">def find_tokens(string, punctuation=PUNCTUATION, abbreviations=ABBREVIATIONS, replace=replacements, linebreak=r"\n{2,}"):
    """ Returns a list of sentences. Each sentence is a space-separated string of tokens (words).
        Handles common cases of abbreviations (e.g., etc., ...).
        Punctuation marks are split from other words. Periods (or ?!) mark the end of a sentence.
        Headings without an ending period are inferred by line breaks.
    """
    # Handle periods separately.
    punctuation = tuple(punctuation.replace(".", ""))
    # Handle replacements (contractions).
    for a, b in list(replace.items()):
        string = re.sub(a, b, string)
    # Handle Unicode quotes.
    if isinstance(string, unicode):
        string = unicode(string).replace("“", " “ ")\
                                .replace("”", " ” ")\
                                .replace("‘", " ‘ ")\
                                .replace("’", " ’ ")\
                                .replace("'", " ' ")\
                                .replace('"', ' " ')
    # Collapse whitespace.
    string = re.sub("\r\n", "\n", string)
    string = re.sub(linebreak, " %s " % EOS, string)
    string = re.sub(r"\s+", " ", string)
    tokens = []
    for t in TOKEN.findall(string+" "):
        if len(t) &gt; 0:
            tail = []
            while t.startswith(punctuation) and \
              not t in replace:
                # Split leading punctuation.
                if t.startswith(punctuation):
                    tokens.append(t[0]); t=t[1:]
            while t.endswith(punctuation+(".",)) and \
              not t in replace:
                # Split trailing punctuation.
                if t.endswith(punctuation):
                    tail.append(t[-1]); t=t[:-1]
                # Split ellipsis (...) before splitting period.
                if t.endswith("..."):
                    tail.append("..."); t=t[:-3].rstrip(".")
                # Split period (if not an abbreviation).
                if t.endswith("."):
                    if t in abbreviations or \
                      RE_ABBR1.match(t) is not None or \
                      RE_ABBR2.match(t) is not None or \
                      RE_ABBR3.match(t) is not None:
                        break
                    else:
                        tail.append(t[-1]); t=t[:-1]
            if t != "":
                tokens.append(t)
            tokens.extend(reversed(tail))
    sentences, i, j = [[]], 0, 0
    while j &lt; len(tokens):
        if tokens[j] in ("...", ".", "!", "?", EOS):
            # Handle citations, trailing parenthesis, repeated punctuation (!?).
            while j &lt; len(tokens) \
                    and tokens[j] in ("'", "\"", u"”", u"’", "...", ".", "!", "?", ")", EOS):
                if tokens[j] in ("'", "\"") and sentences[-1].count(tokens[j]) % 2 == 0:
                    break  # Balanced quotes.
                j += 1
            sentences[-1].extend(t for t in tokens[i:j] if t != EOS)
            sentences.append([])
            i = j
        j += 1
    sentences[-1].extend(tokens[i:j])
    sentences = (" ".join(s) for s in sentences if len(s) &gt; 0)
    sentences = (RE_SARCASM.sub("(!)", s) for s in sentences)
    sentences = [RE_EMOTICONS.sub(
        lambda m: m.group(1).replace(" ", "") + m.group(2), s) for s in sentences]
    return sentences

#### LEXICON #######################################################################################

#--- LEXICON ---------------------------------------------------------------------------------------
# Pattern's text parsers are based on Brill's algorithm.
# Brill's algorithm automatically acquires a lexicon of known words,
# and a set of rules for tagging unknown words from a training corpus.
# Lexical rules are used to tag unknown words, based on the word morphology (prefix, suffix, ...).
# Contextual rules are used to tag all words, based on the word's role in the sentence.
# Named entity rules are used to discover proper nouns (NNP's).


</t>
<t tx="karstenw.20230306120100.36">def _read(path, encoding="utf-8", comment=";;;"):
    """ Returns an iterator over the lines in the file at the given path,
        stripping comments and decoding each line to Unicode.
    """
    if path:
        if isinstance(path, basestring) and os.path.exists(path):
            # From file path.
            if PY2:
                f = codecs.open(path, 'r', encoding='utf-8')
            else:
                f = open(path, 'r', encoding='utf-8')
        elif isinstance(path, basestring):
            # From string.
            f = path.splitlines()
        elif hasattr(path, "read"):
            # From string buffer.
            f = path.read().splitlines()
        else:
            f = path
        for i, line in enumerate(f):
            line = line.strip(codecs.BOM_UTF8) if i == 0 and isinstance(line, binary_type) else line
            line = line.strip()
            line = decode_utf8(line)
            if not line or (comment and line.startswith(comment)):
                continue
            yield line
    return


</t>
<t tx="karstenw.20230306120100.37">class Lexicon(lazydict):

    @others
#--- MORPHOLOGICAL RULES ---------------------------------------------------------------------------
# Brill's algorithm generates lexical (i.e., morphological) rules in the following format:
# NN s fhassuf 1 NNS x =&gt; unknown words ending in -s and tagged NN change to NNS.
#     ly hassuf 2 RB x =&gt; unknown words ending in -ly change to RB.

</t>
<t tx="karstenw.20230306120100.38">def __init__(self, path="", morphology=None, context=None, entities=None, NNP="NNP", language=None):
    """ A dictionary of words and their part-of-speech tags.
        For unknown words, rules for word morphology, context and named entities can be used.
    """
    self._path = path
    self._language  = language
    self.morphology = Morphology(self, path=morphology)
    self.context    = Context(self, path=context)
    self.entities   = Entities(self, path=entities, tag=NNP)

</t>
<t tx="karstenw.20230306120100.39">def load(self):
    # Arnold NNP x
    dict.update(self, (x.split(" ")[:2] for x in _read(self._path) if x.strip()))

</t>
<t tx="karstenw.20230306120100.4">def isnumeric(strg):
    try:
        float(strg)
    except ValueError:
        return False
    return True

#--- LAZY DICTIONARY -------------------------------------------------------------------------------
# A lazy dictionary is empty until one of its methods is called.
# This way many instances (e.g., lexicons) can be created without using memory until used.


</t>
<t tx="karstenw.20230306120100.40">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230306120100.41">@property
def language(self):
    return self._language


</t>
<t tx="karstenw.20230306120100.42">class Rules:

    @others
</t>
<t tx="karstenw.20230306120100.43">def __init__(self, lexicon={}, cmd={}):
    self.lexicon, self.cmd = lexicon, cmd

</t>
<t tx="karstenw.20230306120100.44">def apply(self, x):
    """ Applies the rule to the given token or list of tokens.
    """
    return x

</t>
<t tx="karstenw.20230306120100.45">class Morphology(lazylist, Rules):

    @others
#--- CONTEXT RULES ---------------------------------------------------------------------------------
# Brill's algorithm generates contextual rules in the following format:
# VBD VB PREVTAG TO =&gt; unknown word tagged VBD changes to VB if preceded by a word tagged TO.

</t>
<t tx="karstenw.20230306120100.46">def __init__(self, lexicon={}, path=""):
    """ A list of rules based on word morphology (prefix, suffix).
    """
    cmd = ("char", # Word contains x.
        "haspref", # Word starts with x.
         "hassuf", # Word end with x.
        "addpref", # x + word is in lexicon.
         "addsuf", # Word + x is in lexicon.
     "deletepref", # Word without x at the start is in lexicon.
      "deletesuf", # Word without x at the end is in lexicon.
       "goodleft", # Word preceded by word x.
      "goodright", # Word followed by word x.
    )
    cmd = dict.fromkeys(cmd, True)
    cmd.update(("f" + k, v) for k, v in list(cmd.items()))
    Rules.__init__(self, lexicon, cmd)
    self._path = path

</t>
<t tx="karstenw.20230306120100.47">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230306120100.48">def load(self):
    # ["NN", "s", "fhassuf", "1", "NNS", "x"]
    list.extend(self, (x.split() for x in _read(self._path)))

</t>
<t tx="karstenw.20230306120100.49">def apply(self, token, previous=(None, None), next=(None, None)):
    """ Applies lexical rules to the given token, which is a [word, tag] list.
    """
    w = token[0]
    for r in self:
        if r[1] in self.cmd: # Rule = ly hassuf 2 RB x
            f, x, pos, cmd = bool(0), r[0], r[-2], r[1].lower()
        if r[2] in self.cmd: # Rule = NN s fhassuf 1 NNS x
            f, x, pos, cmd = bool(1), r[1], r[-2], r[2].lower().lstrip("f")
        if f and token[1] != r[0]:
            continue
        if (cmd == "char"       and x in w) \
        or (cmd == "haspref"    and w.startswith(x)) \
        or (cmd == "hassuf"     and w.endswith(x)) \
        or (cmd == "addpref"    and x + w in self.lexicon) \
        or (cmd == "addsuf"     and w + x in self.lexicon) \
        or (cmd == "deletepref" and w.startswith(x) and w[len(x):] in self.lexicon) \
        or (cmd == "deletesuf"  and w.endswith(x) and w[:-len(x)] in self.lexicon) \
        or (cmd == "goodleft"   and x == next[0]) \
        or (cmd == "goodright"  and x == previous[0]):
            token[1] = pos
    return token

</t>
<t tx="karstenw.20230306120100.5">class lazydict(dict):

    @others
</t>
<t tx="karstenw.20230306120100.50">def insert(self, i, tag, affix, cmd="hassuf", tagged=None):
    """ Inserts a new rule that assigns the given tag to words with the given affix,
        e.g., Morphology.append("RB", "-ly").
    """
    if affix.startswith("-") and affix.endswith("-"):
        affix, cmd = affix[+1:-1], "char"
    if affix.startswith("-"):
        affix, cmd = affix[+1:-0], "hassuf"
    if affix.endswith("-"):
        affix, cmd = affix[+0:-1], "haspref"
    if tagged:
        r = [tagged, affix, "f"+cmd.lstrip("f"), tag, "x"]
    else:
        r = [affix, cmd.lstrip("f"), tag, "x"]
    lazylist.insert(self, i, r)

</t>
<t tx="karstenw.20230306120100.51">def append(self, *args, **kwargs):
    self.insert(len(self)-1, *args, **kwargs)

</t>
<t tx="karstenw.20230306120100.52">def extend(self, rules=[]):
    for r in rules:
        self.append(*r)

</t>
<t tx="karstenw.20230306120100.53">class Context(lazylist, Rules):

    @others
#--- NAMED ENTITY RECOGNIZER -----------------------------------------------------------------------

RE_ENTITY1 = re.compile(r"^http://")                            # http://www.domain.com/path
RE_ENTITY2 = re.compile(r"^www\..*?\.[com|org|net|edu|de|uk]$") # www.domain.com
RE_ENTITY3 = re.compile(r"^[\w\-\.\+]+@(\w[\w\-]+\.)+[\w\-]+$") # name@domain.com

</t>
<t tx="karstenw.20230306120100.54">def __init__(self, lexicon={}, path=""):
    """ A list of rules based on context (preceding and following words).
    """
    cmd = ("prevtag", # Preceding word is tagged x.
           "nexttag", # Following word is tagged x.
          "prev2tag", # Word 2 before is tagged x.
          "next2tag", # Word 2 after is tagged x.
       "prev1or2tag", # One of 2 preceding words is tagged x.
       "next1or2tag", # One of 2 following words is tagged x.
    "prev1or2or3tag", # One of 3 preceding words is tagged x.
    "next1or2or3tag", # One of 3 following words is tagged x.
       "surroundtag", # Preceding word is tagged x and following word is tagged y.
             "curwd", # Current word is x.
            "prevwd", # Preceding word is x.
            "nextwd", # Following word is x.
        "prev1or2wd", # One of 2 preceding words is x.
        "next1or2wd", # One of 2 following words is x.
     "next1or2or3wd", # One of 3 preceding words is x.
     "prev1or2or3wd", # One of 3 following words is x.
         "prevwdtag", # Preceding word is x and tagged y.
         "nextwdtag", # Following word is x and tagged y.
         "wdprevtag", # Current word is y and preceding word is tagged x.
         "wdnexttag", # Current word is x and following word is tagged y.
         "wdand2aft", # Current word is x and word 2 after is y.
      "wdand2tagbfr", # Current word is y and word 2 before is tagged x.
      "wdand2tagaft", # Current word is x and word 2 after is tagged y.
           "lbigram", # Current word is y and word before is x.
           "rbigram", # Current word is x and word after is y.
        "prevbigram", # Preceding word is tagged x and word before is tagged y.
        "nextbigram", # Following word is tagged x and word after is tagged y.
    )
    Rules.__init__(self, lexicon, dict.fromkeys(cmd, True))
    self._path = path

</t>
<t tx="karstenw.20230306120100.55">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230306120100.56">def load(self):
    # ["VBD", "VB", "PREVTAG", "TO"]
    list.extend(self, (x.split() for x in _read(self._path)))

</t>
<t tx="karstenw.20230306120100.57">def apply(self, tokens):
    """ Applies contextual rules to the given list of tokens,
        where each token is a [word, tag] list.
    """
    o = [("STAART", "STAART")] * 3 # Empty delimiters for look ahead/back.
    t = o + tokens + o
    for i, token in enumerate(t):
        for r in self:
            if token[1] == "STAART":
                continue
            if token[1] != r[0] and r[0] != "*":
                continue
            cmd, x, y = r[2], r[3], r[4] if len(r) &gt; 4 else ""
            cmd = cmd.lower()
            if (cmd == "prevtag"        and x ==  t[i-1][1]) \
            or (cmd == "nexttag"        and x ==  t[i+1][1]) \
            or (cmd == "prev2tag"       and x ==  t[i-2][1]) \
            or (cmd == "next2tag"       and x ==  t[i+2][1]) \
            or (cmd == "prev1or2tag"    and x in (t[i-1][1], t[i-2][1])) \
            or (cmd == "next1or2tag"    and x in (t[i+1][1], t[i+2][1])) \
            or (cmd == "prev1or2or3tag" and x in (t[i-1][1], t[i-2][1], t[i-3][1])) \
            or (cmd == "next1or2or3tag" and x in (t[i+1][1], t[i+2][1], t[i+3][1])) \
            or (cmd == "surroundtag"    and x ==  t[i-1][1] and y == t[i+1][1]) \
            or (cmd == "curwd"          and x ==  t[i+0][0]) \
            or (cmd == "prevwd"         and x ==  t[i-1][0]) \
            or (cmd == "nextwd"         and x ==  t[i+1][0]) \
            or (cmd == "prev1or2wd"     and x in (t[i-1][0], t[i-2][0])) \
            or (cmd == "next1or2wd"     and x in (t[i+1][0], t[i+2][0])) \
            or (cmd == "prevwdtag"      and x ==  t[i-1][0] and y == t[i-1][1]) \
            or (cmd == "nextwdtag"      and x ==  t[i+1][0] and y == t[i+1][1]) \
            or (cmd == "wdprevtag"      and x ==  t[i-1][1] and y == t[i+0][0]) \
            or (cmd == "wdnexttag"      and x ==  t[i+0][0] and y == t[i+1][1]) \
            or (cmd == "wdand2aft"      and x ==  t[i+0][0] and y == t[i+2][0]) \
            or (cmd == "wdand2tagbfr"   and x ==  t[i-2][1] and y == t[i+0][0]) \
            or (cmd == "wdand2tagaft"   and x ==  t[i+0][0] and y == t[i+2][1]) \
            or (cmd == "lbigram"        and x ==  t[i-1][0] and y == t[i+0][0]) \
            or (cmd == "rbigram"        and x ==  t[i+0][0] and y == t[i+1][0]) \
            or (cmd == "prevbigram"     and x ==  t[i-2][1] and y == t[i-1][1]) \
            or (cmd == "nextbigram"     and x ==  t[i+1][1] and y == t[i+2][1]):
                t[i] = [t[i][0], r[1]]
    return t[len(o):-len(o)]

</t>
<t tx="karstenw.20230306120100.58">def insert(self, i, tag1, tag2, cmd="prevtag", x=None, y=None):
    """ Inserts a new rule that updates words with tag1 to tag2,
        given constraints x and y, e.g., Context.append("TO &lt; NN", "VB")
    """
    if " &lt; " in tag1 and not x and not y:
        tag1, x = tag1.split(" &lt; "); cmd="prevtag"
    if " &gt; " in tag1 and not x and not y:
        x, tag1 = tag1.split(" &gt; "); cmd="nexttag"
    lazylist.insert(self, i, [tag1, tag2, cmd, x or "", y or ""])

</t>
<t tx="karstenw.20230306120100.59">def append(self, *args, **kwargs):
    self.insert(len(self)-1, *args, **kwargs)

</t>
<t tx="karstenw.20230306120100.6">def load(self):
    # Must be overridden in a subclass.
    # Must load data with dict.__setitem__(self, k, v) instead of lazydict[k] = v.
    pass

</t>
<t tx="karstenw.20230306120100.60">def extend(self, rules=[]):
    for r in rules:
        self.append(*r)
</t>
<t tx="karstenw.20230306120100.61">class Entities(lazydict, Rules):

    @others
### SENTIMENT POLARITY LEXICON #####################################################################
# A sentiment lexicon can be used to discern objective facts from subjective opinions in text.
# Each word in the lexicon has scores for:
# 1)     polarity: negative vs. positive    (-1.0 =&gt; +1.0)
# 2) subjectivity: objective vs. subjective (+0.0 =&gt; +1.0)
# 3)    intensity: modifies next word?      (x0.5 =&gt; x2.0)

# For English, adverbs are used as modifiers (e.g., "very good").
# For Dutch, adverbial adjectives are used as modifiers
# ("hopeloos voorspelbaar", "ontzettend spannend", "verschrikkelijk goed").
# Negation words (e.g., "not") reverse the polarity of the following word.

# Sentiment()(txt) returns an averaged (polarity, subjectivity)-tuple.
# Sentiment().assessments(txt) returns a list of (chunk, polarity, subjectivity, label)-tuples.

# Semantic labels are useful for fine-grained analysis, e.g.,
# negative words + positive emoticons could indicate cynicism.

# Semantic labels:
MOOD  = "mood"  # emoticons, emojis
IRONY = "irony" # sarcasm mark (!)

NOUN, VERB, ADJECTIVE, ADVERB = \
    "NN", "VB", "JJ", "RB"

RE_SYNSET = re.compile(r"^[acdnrv][-_][0-9]+$")

</t>
<t tx="karstenw.20230306120100.62">def __init__(self, lexicon={}, path="", tag="NNP"):
    """ A dictionary of named entities and their labels.
        For domain names and e-mail adresses, regular expressions are used.
    """
    cmd = (
        "pers", # Persons: George/NNP-PERS
         "loc", # Locations: Washington/NNP-LOC
         "org", # Organizations: Google/NNP-ORG
    )
    Rules.__init__(self, lexicon, cmd)
    self._path = path
    self.tag   = tag

</t>
<t tx="karstenw.20230306120100.63">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230306120100.64">def load(self):
    # ["Alexander", "the", "Great", "PERS"]
    # {"alexander": [["alexander", "the", "great", "pers"], ...]}
    for x in _read(self.path):
        x = [x.lower() for x in x.split()]
        dict.setdefault(self, x[0], []).append(x)

</t>
<t tx="karstenw.20230306120100.65">def apply(self, tokens):
    """ Applies the named entity recognizer to the given list of tokens,
        where each token is a [word, tag] list.
    """
    # Note: we could also scan for patterns, e.g.,
    # "my|his|her name is|was *" =&gt; NNP-PERS.
    i = 0
    while i &lt; len(tokens):
        w = tokens[i][0].lower()
        if RE_ENTITY1.match(w) \
        or RE_ENTITY2.match(w) \
        or RE_ENTITY3.match(w):
            tokens[i][1] = self.tag
        if w in self:
            for e in self[w]:
                # Look ahead to see if successive words match the named entity.
                e, tag = (e[:-1], "-"+e[-1].upper()) if e[-1] in self.cmd else (e, "")
                b = True
                for j, e in enumerate(e):
                    if i + j &gt;= len(tokens) or tokens[i+j][0].lower() != e:
                        b = False; break
                if b:
                    for token in tokens[i:i+j+1]:
                        token[1] = (token[1] == "NNPS" and token[1] or self.tag) + tag
                    i += j
                    break
        i += 1
    return tokens

</t>
<t tx="karstenw.20230306120100.66">def append(self, entity, name="pers"):
    """ Appends a named entity to the lexicon,
        e.g., Entities.append("Hooloovoo", "PERS")
    """
    e = [s.lower() for s in entity.split(" ") + [name]]
    self.setdefault(e[0], []).append(e)

</t>
<t tx="karstenw.20230306120100.67">def extend(self, entities):
    for entity, name in entities:
        self.append(entity, name)


</t>
<t tx="karstenw.20230306120100.68">def avg(list):
    return sum(list) / float(len(list) or 1)

</t>
<t tx="karstenw.20230306120100.69">class Score(tuple):

    @others
</t>
<t tx="karstenw.20230306120100.7">def _lazy(self, method, *args):
    """ If the dictionary is empty, calls lazydict.load().
        Replaces lazydict.method() with dict.method() and calls it.
    """
    if dict.__len__(self) == 0:
        self.load()
        setattr(self, method, types.MethodType(getattr(dict, method), self))
    return getattr(dict, method)(self, *args)

</t>
<t tx="karstenw.20230306120100.70">def __new__(self, polarity, subjectivity, assessments=[]):
    """ A (polarity, subjectivity)-tuple with an assessments property.
    """
    return tuple.__new__(self, [polarity, subjectivity])

</t>
<t tx="karstenw.20230306120100.71">def __init__(self, polarity, subjectivity, assessments=[]):
    self.assessments = assessments

</t>
<t tx="karstenw.20230306120100.72">class Sentiment(lazydict):

    @others
#--- PART-OF-SPEECH TAGGER -------------------------------------------------------------------------

# Unknown words are recognized as numbers if they contain only digits and -,.:/%$
CD = re.compile(r"^[0-9\-\,\.\:\/\%\$]+$")

</t>
<t tx="karstenw.20230306120100.73">def __init__(self, path="", language=None, synset=None, confidence=None, **kwargs):
    """ A dictionary of words (adjectives) and polarity scores (positive/negative).
        The value for each word is a dictionary of part-of-speech tags.
        The value for each word POS-tag is a tuple with values for
        polarity (-1.0-1.0), subjectivity (0.0-1.0) and intensity (0.5-2.0).
    """
    self._path       = path   # XML file path.
    self._language   = None   # XML language attribute ("en", "fr", ...)
    self._confidence = None   # XML confidence attribute threshold (&gt;=).
    self._synset     = synset # XML synset attribute ("wordnet_id", "cornetto_id", ...)
    self._synsets    = {}     # {"a-01123879": (1.0, 1.0, 1.0)}
    self.labeler     = {}     # {"dammit": "profanity"}
    self.tokenizer   = kwargs.get("tokenizer", find_tokens)
    self.negations   = kwargs.get("negations", ("no", "not", "n't", "never"))
    self.modifiers   = kwargs.get("modifiers", ("RB",))
    self.modifier    = kwargs.get("modifier" , lambda w: w.endswith("ly"))

</t>
<t tx="karstenw.20230306120100.74">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230306120100.75">@property
def language(self):
    return self._language

</t>
<t tx="karstenw.20230306120100.76">@property
def confidence(self):
    return self._confidence

</t>
<t tx="karstenw.20230306120100.77">def load(self, path=None):
    """ Loads the XML-file (with sentiment annotations) from the given path.
        By default, Sentiment.path is lazily loaded.
    """
    # &lt;word form="great" wordnet_id="a-01123879" pos="JJ" polarity="1.0" subjectivity="1.0" intensity="1.0" /&gt;
    # &lt;word form="damnmit" polarity="-0.75" subjectivity="1.0" label="profanity" /&gt;
    if not path:
        path = self._path
    if not os.path.exists(path):
        return
    words, synsets, labels = {}, {}, {}
    xml = cElementTree.parse(path)
    xml = xml.getroot()
    for w in xml.findall("word"):
        if self._confidence is None \
        or self._confidence &lt;= float(w.attrib.get("confidence", 0.0)):
            w, pos, p, s, i, label, synset = (
                w.attrib.get("form"),
                w.attrib.get("pos"),
                w.attrib.get("polarity", 0.0),
                w.attrib.get("subjectivity", 0.0),
                w.attrib.get("intensity", 1.0),
                w.attrib.get("label"),
                w.attrib.get(self._synset) # wordnet_id, cornetto_id, ...
            )
            psi = (float(p), float(s), float(i))
            if w:
                words.setdefault(w, {}).setdefault(pos, []).append(psi)
            if w and label:
                labels[w] = label
            if synset:
                synsets.setdefault(synset, []).append(psi)
    self._language = xml.attrib.get("language", self._language)
    # Average scores of all word senses per part-of-speech tag.
    for w in words:
        words[w] = dict((pos, [avg(each) for each in zip(*psi)]) for pos, psi in words[w].items())
    # Average scores of all part-of-speech tags.
    for w, pos in list(words.items()):
        words[w][None] = [avg(each) for each in zip(*pos.values())]
    # Average scores of all synonyms per synset.
    for id, psi in synsets.items():
        synsets[id] = [avg(each) for each in zip(*psi)]
    dict.update(self, words)
    dict.update(self.labeler, labels)
    dict.update(self._synsets, synsets)

</t>
<t tx="karstenw.20230306120100.78">def synset(self, id, pos=ADJECTIVE):
    """ Returns a (polarity, subjectivity)-tuple for the given synset id.
        For example, the adjective "horrible" has id 193480 in WordNet:
        Sentiment.synset(193480, pos="JJ") =&gt; (-0.6, 1.0, 1.0).
    """
    id = str(id).zfill(8)
    if not id.startswith(("n-", "v-", "a-", "r-")):
        if pos == NOUN:
            id = "n-" + id
        if pos == VERB:
            id = "v-" + id
        if pos == ADJECTIVE:
            id = "a-" + id
        if pos == ADVERB:
            id = "r-" + id
    if dict.__len__(self) == 0:
        self.load()
    return tuple(self._synsets.get(id, (0.0, 0.0))[:2])

</t>
<t tx="karstenw.20230306120100.79">def __call__(self, s, negation=True, **kwargs):
    """ Returns a (polarity, subjectivity)-tuple for the given sentence,
        with polarity between -1.0 and 1.0 and subjectivity between 0.0 and 1.0.
        The sentence can be a string, Synset, Text, Sentence, Chunk, Word, Document, Vector.
        An optional weight parameter can be given,
        as a function that takes a list of words and returns a weight.
    """
    def avg(assessments, weighted=lambda w: 1):
        s, n = 0, 0
        for words, score in assessments:
            w = weighted(words)
            s += w * score
            n += w
        return s / float(n or 1)
    # A pattern.en.wordnet.Synset.
    # Sentiment(synsets("horrible", "JJ")[0]) =&gt; (-0.6, 1.0)
    if hasattr(s, "gloss"):
        a = [(s.synonyms[0],) + self.synset(s.id, pos=s.pos) + (None,)]
    # A synset id.
    # Sentiment("a-00193480") =&gt; horrible =&gt; (-0.6, 1.0)   (English WordNet)
    # Sentiment("c_267") =&gt; verschrikkelijk =&gt; (-0.9, 1.0) (Dutch Cornetto)
    elif isinstance(s, basestring) and RE_SYNSET.match(s) and hasattr(s, "synonyms"):
        a = [(s.synonyms[0],) + self.synset(s.id, pos=s.pos) + (None,)]
    # A string of words.
    # Sentiment("a horrible movie") =&gt; (-0.6, 1.0)
    elif isinstance(s, basestring):
        a = self.assessments(((w.lower(), None) for w in " ".join(self.tokenizer(s)).split()), negation)
    # A pattern.en.Text.
    elif hasattr(s, "sentences"):
        a = self.assessments(((w.lemma or w.string.lower(), w.pos[:2])
                              for w in chain.from_iterable(s)), negation)
    # A pattern.en.Sentence or pattern.en.Chunk.
    elif hasattr(s, "lemmata"):
        a = self.assessments(((w.lemma or w.string.lower(), w.pos[:2]) for w in s.words), negation)
    # A pattern.en.Word.
    elif hasattr(s, "lemma"):
        a = self.assessments(((s.lemma or s.string.lower(), s.pos[:2]),), negation)
    # A pattern.vector.Document.
    # Average score = weighted average using feature weights.
    # Bag-of words is unordered: inject None between each two words
    # to stop assessments() from scanning for preceding negation &amp; modifiers.
    elif hasattr(s, "terms"):
        a = self.assessments(chain.from_iterable(((w, None), (None, None)) for w in s), negation)
        kwargs.setdefault("weight", lambda w: s.terms[w[0]])
    # A dict of (word, weight)-items.
    elif isinstance(s, dict):
        a = self.assessments(chain.from_iterable(((w, None), (None, None)) for w in s), negation)
        kwargs.setdefault("weight", lambda w: s[w[0]])
    # A list of words.
    elif isinstance(s, list):
        a = self.assessments(((w, None) for w in s), negation)
    else:
        a = []
    weight = kwargs.get("weight", lambda w: 1) # [(w, p) for w, p, s, x in a]
    return Score(polarity = avg( [(w, p) for w, p, s, x in a], weight ),
             subjectivity = avg([(w, s) for w, p, s, x in a], weight),
              assessments = a)

</t>
<t tx="karstenw.20230306120100.8">def __repr__(self):
    return self._lazy("__repr__")
</t>
<t tx="karstenw.20230306120100.80">def assessments(self, words=[], negation=True):
    """ Returns a list of (chunk, polarity, subjectivity, label)-tuples for the given list of words:
        where chunk is a list of successive words: a known word optionally
        preceded by a modifier ("very good") or a negation ("not good").
    """
    a = []
    m = None # Preceding modifier (i.e., adverb or adjective).
    n = None # Preceding negation (e.g., "not beautiful").
    for w, pos in words:
        # Only assess known words, preferably by part-of-speech tag.
        # Including unknown words (polarity 0.0 and subjectivity 0.0) lowers the average.
        if w is None:
            continue
        if w in self and pos in self[w]:
            p, s, i = self[w][pos]
            # Known word not preceded by a modifier ("good").
            if m is None:
                a.append(dict(w=[w], p=p, s=s, i=i, n=1, x=self.labeler.get(w)))
            # Known word preceded by a modifier ("really good").
            if m is not None:
                a[-1]["w"].append(w)
                a[-1]["p"] = max(-1.0, min(p * a[-1]["i"], +1.0))
                a[-1]["s"] = max(-1.0, min(s * a[-1]["i"], +1.0))
                a[-1]["i"] = i
                a[-1]["x"] = self.labeler.get(w)
            # Known word preceded by a negation ("not really good").
            if n is not None:
                a[-1]["w"].insert(0, n)
                a[-1]["i"] = 1.0 / a[-1]["i"]
                a[-1]["n"] = -1
            # Known word may be a negation.
            # Known word may be modifying the next word (i.e., it is a known adverb).
            m = None
            n = None
            if pos and pos in self.modifiers or any(map(self[w].__contains__, self.modifiers)):
                m = (w, pos)
            if negation and w in self.negations:
                n = w
        else:
            # Unknown word may be a negation ("not good").
            if negation and w in self.negations:
                n = w
            # Unknown word. Retain negation across small words ("not a good").
            elif n and len(w.strip("'")) &gt; 1:
                n = None
            # Unknown word may be a negation preceded by a modifier ("really not good").
            if n is not None and m is not None and (pos in self.modifiers or self.modifier(m[0])):
                a[-1]["w"].append(n)
                a[-1]["n"] = -1
                n = None
            # Unknown word. Retain modifier across small words ("really is a good").
            elif m and len(w) &gt; 2:
                m = None
            # Exclamation marks boost previous word.
            if w == "!" and len(a) &gt; 0:
                a[-1]["w"].append("!")
                a[-1]["p"] = max(-1.0, min(a[-1]["p"] * 1.25, +1.0))
            # Exclamation marks in parentheses indicate sarcasm.
            if w == "(!)":
                a.append(dict(w=[w], p=0.0, s=1.0, i=1.0, n=1, x=IRONY))
            # EMOTICONS: {("grin", +1.0): set((":-D", ":D"))}
            if w.isalpha() is False and len(w) &lt;= 5 and w not in PUNCTUATION: # speedup
                for (type, p), e in EMOTICONS.items():
                    if w in imap(lambda e: e.lower(), e):
                        a.append(dict(w=[w], p=p, s=1.0, i=1.0, n=1, x=MOOD))
                        break
    for i in range(len(a)):
        w = a[i]["w"]
        p = a[i]["p"]
        s = a[i]["s"]
        n = a[i]["n"]
        x = a[i]["x"]
        # "not good" = slightly bad, "not bad" = slightly good.
        a[i] = (w, p * -0.5 if n &lt; 0 else p, s, x)
    return a

</t>
<t tx="karstenw.20230306120100.81">def annotate(self, word, pos=None, polarity=0.0, subjectivity=0.0, intensity=1.0, label=None):
    """ Annotates the given word with polarity, subjectivity and intensity scores,
        and optionally a semantic label (e.g., MOOD for emoticons, IRONY for "(!)").
    """
    w = self.setdefault(word, {})
    w[pos] = w[None] = (polarity, subjectivity, intensity)
    if label:
        self.labeler[word] = label

</t>
<t tx="karstenw.20230306120100.82">def _suffix_rules(token, tag="NN"):
    """ Default morphological tagging rules for English, based on word suffixes.
    """
    if isinstance(token, (list, tuple)):
        token, tag = token
    if token.endswith("ing"):
        tag = "VBG"
    if token.endswith("ly"):
        tag = "RB"
    if token.endswith("s") and not token.endswith(("is", "ous", "ss")):
        tag = "NNS"
    if token.endswith(("able", "al", "ful", "ible", "ient", "ish", "ive", "less", "tic", "ous")) or "-" in token:
        tag = "JJ"
    if token.endswith("ed"):
        tag = "VBN"
    if token.endswith(("ate", "ify", "ise", "ize")):
        tag = "VBP"
    return [token, tag]

</t>
<t tx="karstenw.20230306120100.83">def find_tags(tokens, lexicon={}, model=None, morphology=None, context=None, entities=None, default=("NN", "NNP", "CD"), language="en", map=None, **kwargs):
    """ Returns a list of [token, tag]-items for the given list of tokens:
        ["The", "cat", "purs"] =&gt; [["The", "DT"], ["cat", "NN"], ["purs", "VB"]]
        Words are tagged using the given lexicon of (word, tag)-items.
        Unknown words are tagged NN by default.
        Unknown words that start with a capital letter are tagged NNP (unless language="de").
        Unknown words that consist only of digits and punctuation marks are tagged CD.
        Unknown words are then improved with morphological rules.
        All words are improved with contextual rules.
        If a model is given, uses model for unknown words instead of morphology and context.
        If map is a function, it is applied to each (token, tag) after applying all rules.
    """
    tagged = []
    # Tag known words.
    for i, token in enumerate(tokens):
        tagged.append([token, lexicon.get(token, i == 0 and lexicon.get(token.lower()) or None)])
    # Tag unknown words.
    for i, (token, tag) in enumerate(tagged):
        prev, next = (None, None), (None, None)
        if i &gt; 0:
            prev = tagged[i-1]
        if i &lt; len(tagged) - 1:
            next = tagged[i+1]
        if tag is None or token in (model is not None and model.unknown or ()):
            # Use language model (i.e., SLP).
            if model is not None:
                tagged[i] = model.apply([token, None], prev, next)
            # Use NNP for capitalized words (except in German).
            elif token.istitle() and language != "de":
                tagged[i] = [token, default[1]]
            # Use CD for digits and numbers.
            elif CD.match(token) is not None:
                tagged[i] = [token, default[2]]
            # Use suffix rules (e.g., -ly = RB).
            elif morphology is not None:
                tagged[i] = morphology.apply([token, default[0]], prev, next)
            # Use suffix rules (English default).
            elif language == "en":
                tagged[i] = _suffix_rules([token, default[0]])
            # Use most frequent tag (NN).
            else:
                tagged[i] = [token, default[0]]
    # Tag words by context.
    if context is not None and model is None:
        tagged = context.apply(tagged)
    # Tag named entities.
    if entities is not None:
        tagged = entities.apply(tagged)
    # Map tags with a custom function.
    if map is not None:
        tagged = [list(map(token, tag)) or [token, default[0]] for token, tag in tagged]
    return tagged

#--- PHRASE CHUNKER --------------------------------------------------------------------------------

SEPARATOR = "/"

NN = r"NN|NNS|NNP|NNPS|NNPS?\-[A-Z]{3,4}|PR|PRP|PRP\$"
VB = r"VB|VBD|VBG|VBN|VBP|VBZ"
JJ = r"JJ|JJR|JJS"
RB = r"(?&lt;!W)RB|RBR|RBS"

# Chunking rules.
# CHUNKS[0] = Germanic: RB + JJ precedes NN ("the round table").
# CHUNKS[1] = Romance: RB + JJ precedes or follows NN ("la table ronde", "une jolie fille").
CHUNKS = [[
    # Germanic languages: en, de, nl, ...
    (  "NP", re.compile(r"(("+NN+")/)*((DT|CD|CC|CJ)/)*(("+RB+"|"+JJ+")/)*(("+NN+")/)+")),
    (  "VP", re.compile(r"(((MD|"+RB+")/)*(("+VB+")/)+)+")),
    (  "VP", re.compile(r"((MD)/)")),
    (  "PP", re.compile(r"((IN|PP|TO)/)+")),
    ("ADJP", re.compile(r"((CC|CJ|"+RB+"|"+JJ+")/)*(("+JJ+")/)+")),
    ("ADVP", re.compile(r"(("+RB+"|WRB)/)+")),
], [
    # Romance languages: es, fr, it, ...
    (  "NP", re.compile(r"(("+NN+")/)*((DT|CD|CC|CJ)/)*(("+RB+"|"+JJ+")/)*(("+NN+")/)+(("+RB+"|"+JJ+")/)*")),
    (  "VP", re.compile(r"(((MD|"+RB+")/)*(("+VB+")/)+(("+RB+")/)*)+")),
    (  "VP", re.compile(r"((MD)/)")),
    (  "PP", re.compile(r"((IN|PP|TO)/)+")),
    ("ADJP", re.compile(r"((CC|CJ|"+RB+"|"+JJ+")/)*(("+JJ+")/)+")),
    ("ADVP", re.compile(r"(("+RB+"|WRB)/)+")),
]]

# Handle ADJP before VP, so that
# RB prefers next ADJP over previous VP.
CHUNKS[0].insert(1, CHUNKS[0].pop(3))
CHUNKS[1].insert(1, CHUNKS[1].pop(3))

</t>
<t tx="karstenw.20230306120100.84">def find_chunks(tagged, language="en"):
    """ The input is a list of [token, tag]-items.
        The output is a list of [token, tag, chunk]-items:
        The/DT nice/JJ fish/NN is/VBZ dead/JJ ./. =&gt;
        The/DT/B-NP nice/JJ/I-NP fish/NN/I-NP is/VBZ/B-VP dead/JJ/B-ADJP ././O
    """
    chunked = [x for x in tagged]
    tags = "".join("%s%s" % (tag, SEPARATOR) for token, tag in tagged)
    # Use Germanic or Romance chunking rules according to given language.
    for tag, rule in CHUNKS[int(language in ("ca", "es", "pt", "fr", "it", "pt", "ro"))]:
        for m in rule.finditer(tags):
            # Find the start of chunks inside the tags-string.
            # Number of preceding separators = number of preceding tokens.
            i = m.start()
            j = tags[:i].count(SEPARATOR)
            n = m.group(0).count(SEPARATOR)
            for k in range(j, j+n):
                if len(chunked[k]) == 3:
                    continue
                if len(chunked[k]) &lt; 3:
                    # A conjunction can not be start of a chunk.
                    if k == j and chunked[k][1] in ("CC", "CJ", "KON", "Conj(neven)"):
                        j += 1
                    # Mark first token in chunk with B-.
                    elif k == j:
                        chunked[k].append("B-"+tag)
                    # Mark other tokens in chunk with I-.
                    else:
                        chunked[k].append("I-"+tag)
    # Mark chinks (tokens outside of a chunk) with O-.
    for chink in filter(lambda x: len(x) &lt; 3, chunked):
        chink.append("O")
    # Post-processing corrections.
    for i, (word, tag, chunk) in enumerate(chunked):
        if tag.startswith("RB") and chunk == "B-NP":
            # "Very nice work" (NP) &lt;=&gt; "Perhaps" (ADVP) + "you" (NP).
            if i &lt; len(chunked)-1 and not chunked[i+1][1].startswith("JJ"):
                chunked[i+0][2] = "B-ADVP"
                chunked[i+1][2] = "B-NP"
    return chunked

</t>
<t tx="karstenw.20230306120100.85">def find_prepositions(chunked):
    """ The input is a list of [token, tag, chunk]-items.
        The output is a list of [token, tag, chunk, preposition]-items.
        PP-chunks followed by NP-chunks make up a PNP-chunk.
    """
    # Tokens that are not part of a preposition just get the O-tag.
    for ch in chunked:
        ch.append("O")
    for i, chunk in enumerate(chunked):
        if chunk[2].endswith("PP") and chunk[-1] == "O":
            # Find PP followed by other PP, NP with nouns and pronouns, VP with a gerund.
            if i &lt; len(chunked)-1 and \
             (chunked[i+1][2].endswith(("NP", "PP")) or \
              chunked[i+1][1] in ("VBG", "VBN")):
                chunk[-1] = "B-PNP"
                pp = True
                for ch in chunked[i+1:]:
                    if not (ch[2].endswith(("NP", "PP")) or ch[1] in ("VBG", "VBN")):
                        break
                    if ch[2].endswith("PP") and pp:
                        ch[-1] = "I-PNP"
                    if not ch[2].endswith("PP"):
                        ch[-1] = "I-PNP"
                        pp = False
    return chunked

#### PARSER ########################################################################################

#--- PARSER ----------------------------------------------------------------------------------------
# A shallow parser can be used to retrieve syntactic-semantic information from text
# in an efficient way (usually at the expense of deeper configurational syntactic information).
# The shallow parser in Pattern is meant to handle the following tasks:
# 1)  Tokenization: split punctuation marks from words and find sentence periods.
# 2)       Tagging: find the part-of-speech tag of each word (noun, verb, ...) in a sentence.
# 3)      Chunking: find words that belong together in a phrase.
# 4) Role labeling: find the subject and object of the sentence.
# 5) Lemmatization: find the base form of each word ("was" =&gt; "is").

#    WORD     TAG     CHUNK      PNP        ROLE        LEMMA
#------------------------------------------------------------------
#     The      DT      B-NP        O        NP-SBJ-1      the
#   black      JJ      I-NP        O        NP-SBJ-1      black
#     cat      NN      I-NP        O        NP-SBJ-1      cat
#     sat      VB      B-VP        O        VP-1          sit
#      on      IN      B-PP      B-PNP      PP-LOC        on
#     the      DT      B-NP      I-PNP      NP-OBJ-1      the
#     mat      NN      I-NP      I-PNP      NP-OBJ-1      mat
#       .      .        O          O          O           .

# The example demonstrates what information can be retrieved:
#
# - the period is split from "mat." = the end of the sentence,
# - the words are annotated: NN (noun), VB (verb), JJ (adjective), DT (determiner), ...
# - the phrases are annotated: NP (noun phrase), VP (verb phrase), PNP (preposition), ...
# - the phrases are labeled: SBJ (subject), OBJ (object), LOC (location), ...
# - the phrase start is marked: B (begin), I (inside), O (outside),
# - the past tense "sat" is lemmatized =&gt; "sit".
# By default, the English parser uses the Penn Treebank II tagset:
# http://www.clips.ua.ac.be/pages/penn-treebank-tagset
PTB = PENN = "penn"

</t>
<t tx="karstenw.20230306120100.86">class Parser:

    @others
#--- TAGGED STRING ---------------------------------------------------------------------------------
# Pattern.parse() returns a TaggedString: a Unicode string with "tags" and "language" attributes.
# The pattern.text.tree.Text class uses this attribute to determine the token format and
# transform the tagged string to a parse tree of nested Sentence, Chunk and Word objects.

TOKENS = "tokens"

</t>
<t tx="karstenw.20230306120100.87">def __init__(self, lexicon={}, default=("NN", "NNP", "CD"), language=None):
    """ A simple shallow parser using a Brill-based part-of-speech tagger.
        The given lexicon is a dictionary of known words and their part-of-speech tag.
        The given default tags are used for unknown words.
        Unknown words that start with a capital letter are tagged NNP (except for German).
        Unknown words that contain only digits and punctuation are tagged CD.
        The given language can be used to discern between
        Germanic and Romance languages for phrase chunking.
    """
    self.lexicon  = lexicon
    self.default  = default
    self.language = language

</t>
<t tx="karstenw.20230306120100.88">def find_tokens(self, string, **kwargs):
    """ Returns a list of sentences from the given string.
        Punctuation marks are separated from each word by a space.
    """
    # "The cat purs." =&gt; ["The cat purs ."]
    return find_tokens(text_type(string),
            punctuation = kwargs.get(  "punctuation", PUNCTUATION),
          abbreviations = kwargs.get("abbreviations", ABBREVIATIONS),
                replace = kwargs.get(      "replace", replacements),
              linebreak = r"\n{2,}")

</t>
<t tx="karstenw.20230306120100.89">def find_tags(self, tokens, **kwargs):
    """ Annotates the given list of tokens with part-of-speech tags.
        Returns a list of tokens, where each token is now a [word, tag]-list.
    """
    # ["The", "cat", "purs"] =&gt; [["The", "DT"], ["cat", "NN"], ["purs", "VB"]]
    return find_tags(tokens,
               language = kwargs.get("language", self.language),
                lexicon = kwargs.get( "lexicon", self.lexicon),
                default = kwargs.get( "default", self.default),
                    map = kwargs.get(     "map", None))

</t>
<t tx="karstenw.20230306120100.9">def __len__(self):
    return self._lazy("__len__")
</t>
<t tx="karstenw.20230306120100.90">def find_chunks(self, tokens, **kwargs):
    """ Annotates the given list of tokens with chunk tags.
        Several tags can be added, for example chunk + preposition tags.
    """
    # [["The", "DT"], ["cat", "NN"], ["purs", "VB"]] =&gt;
    # [["The", "DT", "B-NP"], ["cat", "NN", "I-NP"], ["purs", "VB", "B-VP"]]
    return find_prepositions(
           find_chunks(tokens,
               language = kwargs.get("language", self.language)))

</t>
<t tx="karstenw.20230306120100.91">def find_prepositions(self, tokens, **kwargs):
    """ Annotates the given list of tokens with prepositional noun phrase tags.
    """
    return find_prepositions(tokens) # See also Parser.find_chunks().

</t>
<t tx="karstenw.20230306120100.92">def find_labels(self, tokens, **kwargs):
    """ Annotates the given list of tokens with verb/predicate tags.
    """
    return find_relations(tokens)

</t>
<t tx="karstenw.20230306120100.93">def find_lemmata(self, tokens, **kwargs):
    """ Annotates the given list of tokens with word lemmata.
    """
    return [token + [token[0].lower()] for token in tokens]

</t>
<t tx="karstenw.20230306120100.94">def parse(self, s, tokenize=True, tags=True, chunks=True, relations=False, lemmata=False, encoding="utf-8", **kwargs):
    """ Takes a string (sentences) and returns a tagged Unicode string (TaggedString).
        Sentences in the output are separated by newlines.
        With tokenize=True, punctuation is split from words and sentences are separated by \n.
        With tags=True, part-of-speech tags are parsed (NN, VB, IN, ...).
        With chunks=True, phrase chunk tags are parsed (NP, VP, PP, PNP, ...).
        With relations=True, semantic role labels are parsed (SBJ, OBJ).
        With lemmata=True, word lemmata are parsed.
        Optional parameters are passed to
        the tokenizer, tagger, chunker, labeler and lemmatizer.
    """
    # Tokenizer.
    if tokenize:
        s = self.find_tokens(s, **kwargs)
    if isinstance(s, (list, tuple)):
        s = [isinstance(s, basestring) and s.split(" ") or s for s in s]
    if isinstance(s, basestring):
        s = [s.split(" ") for s in s.split("\n")]
    # Unicode.
    for i in range(len(s)):
        for j in range(len(s[i])):
            if isinstance(s[i][j], binary_type):
                s[i][j] = decode_string(s[i][j], encoding)
        # Tagger (required by chunker, labeler &amp; lemmatizer).
        if tags or chunks or relations or lemmata:
            s[i] = self.find_tags(s[i], **kwargs)
        else:
            s[i] = [[w] for w in s[i]]
        # Chunker.
        if chunks or relations:
            s[i] = self.find_chunks(s[i], **kwargs)
        # Labeler.
        if relations:
            s[i] = self.find_labels(s[i], **kwargs)
        # Lemmatizer.
        if lemmata:
            s[i] = self.find_lemmata(s[i], **kwargs)
    # Slash-formatted tagged string.
    # With collapse=False (or split=True), returns raw list
    # (this output is not usable by tree.Text).
    if not kwargs.get("collapse", True) \
        or kwargs.get("split", False):
        return s
    # Construct TaggedString.format.
    # (this output is usable by tree.Text).
    format = ["word"]
    if tags:
        format.append("part-of-speech")
    if chunks:
        format.extend(("chunk", "preposition"))
    if relations:
        format.append("relation")
    if lemmata:
        format.append("lemma")
    # Collapse raw list.
    # Sentences are separated by newlines, tokens by spaces, tags by slashes.
    # Slashes in words are encoded with &amp;slash;
    for i in range(len(s)):
        for j in range(len(s[i])):
            s[i][j][0] = s[i][j][0].replace("/", "&amp;slash;")
            s[i][j] = "/".join(s[i][j])
        s[i] = " ".join(s[i])
    s = "\n".join(s)
    s = TaggedString(unicode(s), format, language=kwargs.get("language", self.language))
    return s


</t>
<t tx="karstenw.20230306120100.95">class TaggedString(unicode):

    @others
#### SPELLING CORRECTION ###########################################################################
# Based on: Peter Norvig, "How to Write a Spelling Corrector", http://norvig.com/spell-correct.html

</t>
<t tx="karstenw.20230306120100.96">def __new__(self, string, tags=["word"], language=None):
    """ Unicode string with tags and language attributes.
        For example: TaggedString("cat/NN/NP", tags=["word", "pos", "chunk"]).
    """
    # From a TaggedString:
    if isinstance(string, unicode) and hasattr(string, "tags"):
        tags, language = string.tags, string.language
    # From a TaggedString.split(TOKENS) list:
    if isinstance(string, list):
        string = [[[x.replace("/", "&amp;slash;") for x in token] for token in s] for s in string]
        string = "\n".join(" ".join("/".join(token) for token in s) for s in string)
    s = unicode.__new__(self, string)
    s.tags = list(tags)
    s.language = language
    return s

</t>
<t tx="karstenw.20230306120100.97">def split(self, sep=TOKENS):
    """ Returns a list of sentences, where each sentence is a list of tokens,
        where each token is a list of word + tags.
    """
    if sep != TOKENS:
        return unicode.split(self, sep)
    if len(self) == 0:
        return []
    return [[[x.replace("&amp;slash;", "/") for x in token.split("/")]
        for token in sentence.split(" ")]
            for sentence in unicode.split(self, "\n")]

</t>
<t tx="karstenw.20230306120100.98">class Spelling(lazydict):

    ALPHA = "abcdefghijklmnopqrstuvwxyz"

    @others
</t>
<t tx="karstenw.20230306120100.99">def __init__(self, path=""):
    self._path = path

</t>
<t tx="karstenw.20230306120124.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120140.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120145.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120155.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120200.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230306120207.1">#!/usr/bin/env python
# -*- coding: utf-8 -*-
@others
if __name__ == '__main__':
    main()
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120217.1"># -*- coding: utf-8 -*-

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120231.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120234.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120240.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120247.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120255.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120300.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120307.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120316.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120321.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120327.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120339.1"></t>
<t tx="karstenw.20230306120343.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120408.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120414.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120423.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120428.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120439.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120448.1"></t>
<t tx="karstenw.20230306120455.1"># -*- coding: utf-8 -*-
@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230306120511.1">"""Abstract base classes for models (taggers, noun phrase extractors, etc.)
which define the interface for descendant classes.

.. versionchanged:: 0.7.0
    All base classes are defined in the same module, ``textblob.base``.
"""
from __future__ import absolute_import
from abc import ABCMeta, abstractmethod

import nltk

from textblob.compat import with_metaclass

##### POS TAGGERS #####

</t>
<t tx="karstenw.20230306120511.10">def __init__(self):
    self._trained = False

</t>
<t tx="karstenw.20230306120511.11">def train(self):
    # Train me
    self._trained = True

</t>
<t tx="karstenw.20230306120511.12">@abstractmethod
def analyze(self, text):
    """Return the result of of analysis. Typically returns either a
    tuple, float, or dictionary.
    """
    # Lazily train the classifier
    if not self._trained:
        self.train()
    # Analyze text
    return None

</t>
<t tx="karstenw.20230306120511.13">class BaseParser(with_metaclass(ABCMeta)):
    """Abstract parser class from which all parsers inherit from. All
    descendants must implement a ``parse()`` method.
    """
    @others
</t>
<t tx="karstenw.20230306120511.14">@abstractmethod
def parse(self, text):
    """Parses the text."""
    return
</t>
<t tx="karstenw.20230306120511.2">class BaseTagger(with_metaclass(ABCMeta)):
    """Abstract tagger class from which all taggers
    inherit from. All descendants must implement a
    ``tag()`` method.
    """
    @others
##### NOUN PHRASE EXTRACTORS #####

</t>
<t tx="karstenw.20230306120511.3">@abstractmethod
def tag(self, text, tokenize=True):
    """Return a list of tuples of the form (word, tag)
    for a given set of text or BaseBlob instance.
    """
    return

</t>
<t tx="karstenw.20230306120511.4">class BaseNPExtractor(with_metaclass(ABCMeta)):
    """Abstract base class from which all NPExtractor classes inherit.
    Descendant classes must implement an ``extract(text)`` method
    that returns a list of noun phrases as strings.
    """

    @others
##### TOKENIZERS #####

</t>
<t tx="karstenw.20230306120511.5">@abstractmethod
def extract(self, text):
    """Return a list of noun phrases (strings) for a body of text."""
    return

</t>
<t tx="karstenw.20230306120511.6">class BaseTokenizer(with_metaclass(ABCMeta), nltk.tokenize.api.TokenizerI):
    """Abstract base class from which all Tokenizer classes inherit.
    Descendant classes must implement a ``tokenize(text)`` method
    that returns a list of noun phrases as strings.
    """
    @others
##### SENTIMENT ANALYZERS ####


DISCRETE = 'ds'
CONTINUOUS = 'co'


</t>
<t tx="karstenw.20230306120511.7">@abstractmethod
def tokenize(self, text):
    """Return a list of tokens (strings) for a body of text.

    :rtype: list
    """
    return

</t>
<t tx="karstenw.20230306120511.8">def itokenize(self, text, *args, **kwargs):
    """Return a generator that generates tokens "on-demand".

    .. versionadded:: 0.6.0

    :rtype: generator
    """
    return (t for t in self.tokenize(text, *args, **kwargs))

</t>
<t tx="karstenw.20230306120511.9">class BaseSentimentAnalyzer(with_metaclass(ABCMeta)):
    """Abstract base class from which all sentiment analyzers inherit.
    Should implement an ``analyze(text)`` method which returns either the
    results of analysis.
    """
    kind = DISCRETE

    @others
##### PARSERS #####

</t>
<t tx="karstenw.20230306120514.1">"""Wrappers for various units of text, including the main
:class:`TextBlob &lt;textblob.blob.TextBlob&gt;`, :class:`Word &lt;textblob.blob.Word&gt;`,
and :class:`WordList &lt;textblob.blob.WordList&gt;` classes.
Example usage: ::

    &gt;&gt;&gt; from textblob import TextBlob
    &gt;&gt;&gt; b = TextBlob("Simple is better than complex.")
    &gt;&gt;&gt; b.tags
    [(u'Simple', u'NN'), (u'is', u'VBZ'), (u'better', u'JJR'), (u'than', u'IN'), (u'complex', u'NN')]
    &gt;&gt;&gt; b.noun_phrases
    WordList([u'simple'])
    &gt;&gt;&gt; b.words
    WordList([u'Simple', u'is', u'better', u'than', u'complex'])
    &gt;&gt;&gt; b.sentiment
    (0.06666666666666667, 0.41904761904761906)
    &gt;&gt;&gt; b.words[0].synsets()[0]
    Synset('simple.n.01')

.. versionchanged:: 0.8.0
    These classes are now imported from ``textblob`` rather than ``text.blob``.
"""
from __future__ import unicode_literals, absolute_import
import sys
import json
import warnings
from collections import defaultdict

import nltk

from textblob.decorators import cached_property, requires_nltk_corpus
from textblob.utils import lowerstrip, PUNCTUATION_REGEX
from textblob.inflect import singularize as _singularize, pluralize as _pluralize
from textblob.mixins import BlobComparableMixin, StringlikeMixin
from textblob.compat import unicode, basestring
from textblob.base import (BaseNPExtractor, BaseTagger, BaseTokenizer,
                       BaseSentimentAnalyzer, BaseParser)
from textblob.np_extractors import FastNPExtractor
from textblob.taggers import NLTKTagger
from textblob.tokenizers import WordTokenizer, sent_tokenize, word_tokenize
from textblob.sentiments import PatternAnalyzer
from textblob.parsers import PatternParser
from textblob.translate import Translator
from textblob.en import suggest

# Wordnet interface
# NOTE: textblob.wordnet is not imported so that the wordnet corpus can be lazy-loaded
_wordnet = nltk.corpus.wordnet

</t>
<t tx="karstenw.20230306120514.10">def translate(self, from_lang='auto', to="en"):
    '''Translate the word to another language using Google's
    Translate API.

    .. deprecated:: 0.16.0
        Use the official Google Translate API instead.
    .. versionadded:: 0.5.0
    '''
    warnings.warn(
        'Word.translate is deprecated and will be removed in a future release. '
        'Use the official Google Translate API instead.',
        DeprecationWarning
    )
    return self.translator.translate(self.string,
                                     from_lang=from_lang, to_lang=to)

</t>
<t tx="karstenw.20230306120514.11">def detect_language(self):
    '''Detect the word's language using Google's Translate API.

    .. deprecated:: 0.16.0
        Use the official Google Translate API istead.
    .. versionadded:: 0.5.0
    '''
    warnings.warn(
        'Word.detect_language is deprecated and will be removed in a future release. '
        'Use the official Google Translate API instead.',
        DeprecationWarning
    )
    return self.translator.detect(self.string)

</t>
<t tx="karstenw.20230306120514.12">def spellcheck(self):
    '''Return a list of (word, confidence) tuples of spelling corrections.

    Based on: Peter Norvig, "How to Write a Spelling Corrector"
    (http://norvig.com/spell-correct.html) as implemented in the pattern
    library.

    .. versionadded:: 0.6.0
    '''
    return suggest(self.string)

</t>
<t tx="karstenw.20230306120514.13">def correct(self):
    '''Correct the spelling of the word. Returns the word with the highest
    confidence using the spelling corrector.

    .. versionadded:: 0.6.0
    '''
    return Word(self.spellcheck()[0][0])

</t>
<t tx="karstenw.20230306120514.14">@cached_property
@requires_nltk_corpus
def lemma(self):
    """Return the lemma of this word using Wordnet's morphy function.
    """
    return self.lemmatize(pos=self.pos_tag)

</t>
<t tx="karstenw.20230306120514.15">@requires_nltk_corpus
def lemmatize(self, pos=None):
    """Return the lemma for a word using WordNet's morphy function.

    :param pos: Part of speech to filter upon. If `None`, defaults to
        ``_wordnet.NOUN``.

    .. versionadded:: 0.8.1
    """
    if pos is None:
        tag = _wordnet.NOUN
    elif pos in _wordnet._FILEMAP.keys():
        tag = pos
    else:
        tag = _penn_to_wordnet(pos)
    lemmatizer = nltk.stem.WordNetLemmatizer()
    return lemmatizer.lemmatize(self.string, tag)

PorterStemmer = nltk.stem.porter.PorterStemmer()
LancasterStemmer = nltk.stem.lancaster.LancasterStemmer()
SnowballStemmer = nltk.stem.snowball.SnowballStemmer("english")

#added 'stemmer' on lines of lemmatizer
#based on nltk
</t>
<t tx="karstenw.20230306120514.16">def stem(self, stemmer=PorterStemmer):
    """Stem a word using various NLTK stemmers. (Default: Porter Stemmer)

    .. versionadded:: 0.12.0
    """
    return stemmer.stem(self.string)

</t>
<t tx="karstenw.20230306120514.17">@cached_property
def synsets(self):
    """The list of Synset objects for this Word.

    :rtype: list of Synsets

    .. versionadded:: 0.7.0
    """
    return self.get_synsets(pos=None)

</t>
<t tx="karstenw.20230306120514.18">@cached_property
def definitions(self):
    """The list of definitions for this word. Each definition corresponds
    to a synset.

    .. versionadded:: 0.7.0
    """
    return self.define(pos=None)

</t>
<t tx="karstenw.20230306120514.19">def get_synsets(self, pos=None):
    """Return a list of Synset objects for this word.

    :param pos: A part-of-speech tag to filter upon. If ``None``, all
        synsets for all parts of speech will be loaded.

    :rtype: list of Synsets

    .. versionadded:: 0.7.0
    """
    return _wordnet.synsets(self.string, pos)

</t>
<t tx="karstenw.20230306120514.2">def _penn_to_wordnet(tag):
    """Converts a Penn corpus tag into a Wordnet tag."""
    if tag in ("NN", "NNS", "NNP", "NNPS"):
        return _wordnet.NOUN
    if tag in ("JJ", "JJR", "JJS"):
        return _wordnet.ADJ
    if tag in ("VB", "VBD", "VBG", "VBN", "VBP", "VBZ"):
        return _wordnet.VERB
    if tag in ("RB", "RBR", "RBS"):
        return _wordnet.ADV
    return None

</t>
<t tx="karstenw.20230306120514.20">def define(self, pos=None):
    """Return a list of definitions for this word. Each definition
    corresponds to a synset for this word.

    :param pos: A part-of-speech tag to filter upon. If ``None``, definitions
        for all parts of speech will be loaded.
    :rtype: List of strings

    .. versionadded:: 0.7.0
    """
    return [syn.definition() for syn in self.get_synsets(pos=pos)]


</t>
<t tx="karstenw.20230306120514.21">class WordList(list):
    """A list-like collection of words."""

    @others
</t>
<t tx="karstenw.20230306120514.22">def __init__(self, collection):
    """Initialize a WordList. Takes a collection of strings as
    its only argument.
    """
    super(WordList, self).__init__([Word(w) for w in collection])

</t>
<t tx="karstenw.20230306120514.23">def __str__(self):
    """Returns a string representation for printing."""
    return super(WordList, self).__repr__()

</t>
<t tx="karstenw.20230306120514.24">def __repr__(self):
    """Returns a string representation for debugging."""
    class_name = self.__class__.__name__
    return '{cls}({lst})'.format(cls=class_name, lst=super(WordList, self).__repr__())

</t>
<t tx="karstenw.20230306120514.25">def __getitem__(self, key):
    """Returns a string at the given index."""
    item = super(WordList, self).__getitem__(key)
    if isinstance(key, slice):
        return self.__class__(item)
    else:
        return item

</t>
<t tx="karstenw.20230306120514.26">def __getslice__(self, i, j):
    # This is included for Python 2.* compatibility
    return self.__class__(super(WordList, self).__getslice__(i, j))

</t>
<t tx="karstenw.20230306120514.27">def __setitem__(self, index, obj):
    """Places object at given index, replacing existing item. If the object
    is a string, inserts a :class:`Word &lt;Word&gt;` object.
    """
    if isinstance(obj, basestring):
        super(WordList, self).__setitem__(index, Word(obj))
    else:
        super(WordList, self).__setitem__(index, obj)

</t>
<t tx="karstenw.20230306120514.28">def count(self, strg, case_sensitive=False, *args, **kwargs):
    """Get the count of a word or phrase `s` within this WordList.

    :param strg: The string to count.
    :param case_sensitive: A boolean, whether or not the search is case-sensitive.
    """
    if not case_sensitive:
        return [word.lower() for word in self].count(strg.lower(), *args,
                **kwargs)
    return super(WordList, self).count(strg, *args, **kwargs)

</t>
<t tx="karstenw.20230306120514.29">def append(self, obj):
    """Append an object to end. If the object is a string, appends a
    :class:`Word &lt;Word&gt;` object.
    """
    if isinstance(obj, basestring):
        super(WordList, self).append(Word(obj))
    else:
        super(WordList, self).append(obj)

</t>
<t tx="karstenw.20230306120514.3">class Word(unicode):

    """A simple word representation. Includes methods for inflection,
    translation, and WordNet integration.
    """

    translator = Translator()

    @others
</t>
<t tx="karstenw.20230306120514.30">def extend(self, iterable):
    """Extend WordList by appending elements from ``iterable``. If an element
    is a string, appends a :class:`Word &lt;Word&gt;` object.
    """
    for e in iterable:
        self.append(e)

</t>
<t tx="karstenw.20230306120514.31">def upper(self):
    """Return a new WordList with each word upper-cased."""
    return self.__class__([word.upper() for word in self])

</t>
<t tx="karstenw.20230306120514.32">def lower(self):
    """Return a new WordList with each word lower-cased."""
    return self.__class__([word.lower() for word in self])

</t>
<t tx="karstenw.20230306120514.33">def singularize(self):
    """Return the single version of each word in this WordList."""
    return self.__class__([word.singularize() for word in self])

</t>
<t tx="karstenw.20230306120514.34">def pluralize(self):
    """Return the plural version of each word in this WordList."""
    return self.__class__([word.pluralize() for word in self])

</t>
<t tx="karstenw.20230306120514.35">def lemmatize(self):
    """Return the lemma of each word in this WordList."""
    return self.__class__([word.lemmatize() for word in self])

</t>
<t tx="karstenw.20230306120514.36">def stem(self, *args, **kwargs):
    """Return the stem for each word in this WordList."""
    return self.__class__([word.stem(*args, **kwargs) for word in self])


</t>
<t tx="karstenw.20230306120514.37">def _validated_param(obj, name, base_class, default, base_class_name=None):
    """Validates a parameter passed to __init__. Makes sure that obj is
    the correct class. Return obj if it's not None or falls back to default

    :param obj: The object passed in.
    :param name: The name of the parameter.
    :param base_class: The class that obj must inherit from.
    :param default: The default object to fall back upon if obj is None.
    """
    base_class_name = base_class_name if base_class_name else base_class.__name__
    if obj is not None and not isinstance(obj, base_class):
        raise ValueError('{name} must be an instance of {cls}'
                         .format(name=name, cls=base_class_name))
    return obj or default


</t>
<t tx="karstenw.20230306120514.38">def _initialize_models(obj, tokenizer, pos_tagger,
                       np_extractor, analyzer, parser, classifier):
    """Common initialization between BaseBlob and Blobber classes."""
    # tokenizer may be a textblob or an NLTK tokenizer
    obj.tokenizer = _validated_param(tokenizer, "tokenizer",
                                    base_class=(BaseTokenizer, nltk.tokenize.api.TokenizerI),
                                    default=BaseBlob.tokenizer,
                                    base_class_name="BaseTokenizer")
    obj.np_extractor = _validated_param(np_extractor, "np_extractor",
                                        base_class=BaseNPExtractor,
                                        default=BaseBlob.np_extractor)
    obj.pos_tagger = _validated_param(pos_tagger, "pos_tagger",
                                        BaseTagger, BaseBlob.pos_tagger)
    obj.analyzer = _validated_param(analyzer, "analyzer",
                                     BaseSentimentAnalyzer, BaseBlob.analyzer)
    obj.parser = _validated_param(parser, "parser", BaseParser, BaseBlob.parser)
    obj.classifier = classifier


</t>
<t tx="karstenw.20230306120514.39">class BaseBlob(StringlikeMixin, BlobComparableMixin):
    """An abstract base class that all textblob classes will inherit from.
    Includes words, POS tag, NP, and word count properties. Also includes
    basic dunder and string methods for making objects like Python strings.

    :param text: A string.
    :param tokenizer: (optional) A tokenizer instance. If ``None``,
        defaults to :class:`WordTokenizer() &lt;textblob.tokenizers.WordTokenizer&gt;`.
    :param np_extractor: (optional) An NPExtractor instance. If ``None``,
        defaults to :class:`FastNPExtractor() &lt;textblob.en.np_extractors.FastNPExtractor&gt;`.
    :param pos_tagger: (optional) A Tagger instance. If ``None``,
        defaults to :class:`NLTKTagger &lt;textblob.en.taggers.NLTKTagger&gt;`.
    :param analyzer: (optional) A sentiment analyzer. If ``None``,
        defaults to :class:`PatternAnalyzer &lt;textblob.en.sentiments.PatternAnalyzer&gt;`.
    :param parser: A parser. If ``None``, defaults to
        :class:`PatternParser &lt;textblob.en.parsers.PatternParser&gt;`.
    :param classifier: A classifier.

    .. versionchanged:: 0.6.0
        ``clean_html`` parameter deprecated, as it was in NLTK.
    """
    np_extractor = FastNPExtractor()
    pos_tagger = NLTKTagger()
    tokenizer = WordTokenizer()
    translator = Translator()
    analyzer = PatternAnalyzer()
    parser = PatternParser()

    @others
</t>
<t tx="karstenw.20230306120514.4">def __new__(cls, string, pos_tag=None):
    """Return a new instance of the class. It is necessary to override
    this method in order to handle the extra pos_tag argument in the
    constructor.
    """
    return super(Word, cls).__new__(cls, string)

</t>
<t tx="karstenw.20230306120514.40">def __init__(self, text, tokenizer=None,
            pos_tagger=None, np_extractor=None, analyzer=None,
            parser=None, classifier=None, clean_html=False):
    if not isinstance(text, basestring):
        raise TypeError('The `text` argument passed to `__init__(text)` '
                        'must be a string, not {0}'.format(type(text)))
    if clean_html:
        raise NotImplementedError("clean_html has been deprecated. "
                                "To remove HTML markup, use BeautifulSoup's "
                                "get_text() function")
    self.raw = self.string = text
    self.stripped = lowerstrip(self.raw, all=True)
    _initialize_models(self, tokenizer, pos_tagger, np_extractor, analyzer,
                       parser, classifier)

</t>
<t tx="karstenw.20230306120514.41">@cached_property
def words(self):
    """Return a list of word tokens. This excludes punctuation characters.
    If you want to include punctuation characters, access the ``tokens``
    property.

    :returns: A :class:`WordList &lt;WordList&gt;` of word tokens.
    """
    return WordList(word_tokenize(self.raw, include_punc=False))

</t>
<t tx="karstenw.20230306120514.42">@cached_property
def tokens(self):
    """Return a list of tokens, using this blob's tokenizer object
    (defaults to :class:`WordTokenizer &lt;textblob.tokenizers.WordTokenizer&gt;`).
    """
    return WordList(self.tokenizer.tokenize(self.raw))

</t>
<t tx="karstenw.20230306120514.43">def tokenize(self, tokenizer=None):
    """Return a list of tokens, using ``tokenizer``.

    :param tokenizer: (optional) A tokenizer object. If None, defaults to
        this blob's default tokenizer.
    """
    t = tokenizer if tokenizer is not None else self.tokenizer
    return WordList(t.tokenize(self.raw))

</t>
<t tx="karstenw.20230306120514.44">def parse(self, parser=None):
    """Parse the text.

    :param parser: (optional) A parser instance. If ``None``, defaults to
        this blob's default parser.

    .. versionadded:: 0.6.0
    """
    p = parser if parser is not None else self.parser
    return p.parse(self.raw)

</t>
<t tx="karstenw.20230306120514.45">def classify(self):
    """Classify the blob using the blob's ``classifier``."""
    if self.classifier is None:
        raise NameError("This blob has no classifier. Train one first!")
    return self.classifier.classify(self.raw)

</t>
<t tx="karstenw.20230306120514.46">@cached_property
def sentiment(self):
    """Return a tuple of form (polarity, subjectivity ) where polarity
    is a float within the range [-1.0, 1.0] and subjectivity is a float
    within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is
    very subjective.

    :rtype: namedtuple of the form ``Sentiment(polarity, subjectivity)``
    """
    return self.analyzer.analyze(self.raw)

</t>
<t tx="karstenw.20230306120514.47">@cached_property
def sentiment_assessments(self):
    """Return a tuple of form (polarity, subjectivity, assessments ) where
    polarity is a float within the range [-1.0, 1.0], subjectivity is a
    float within the range [0.0, 1.0] where 0.0 is very objective and 1.0
    is very subjective, and assessments is a list of polarity and
    subjectivity scores for the assessed tokens.

    :rtype: namedtuple of the form ``Sentiment(polarity, subjectivity,
    assessments)``
    """
    return self.analyzer.analyze(self.raw, keep_assessments=True)

</t>
<t tx="karstenw.20230306120514.48">@cached_property
def polarity(self):
    """Return the polarity score as a float within the range [-1.0, 1.0]

    :rtype: float
    """
    return PatternAnalyzer().analyze(self.raw)[0]

</t>
<t tx="karstenw.20230306120514.49">@cached_property
def subjectivity(self):
    """Return the subjectivity score as a float within the range [0.0, 1.0]
    where 0.0 is very objective and 1.0 is very subjective.

    :rtype: float
    """
    return PatternAnalyzer().analyze(self.raw)[1]

</t>
<t tx="karstenw.20230306120514.5">def __init__(self, string, pos_tag=None):
    self.string = string
    self.pos_tag = pos_tag

</t>
<t tx="karstenw.20230306120514.50">@cached_property
def noun_phrases(self):
    """Returns a list of noun phrases for this blob."""
    return WordList([phrase.strip().lower()
                    for phrase in self.np_extractor.extract(self.raw)
                    if len(phrase) &gt; 1])

</t>
<t tx="karstenw.20230306120514.51">@cached_property
def pos_tags(self):
    """Returns an list of tuples of the form (word, POS tag).

    Example:
    ::

        [('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'),
                ('Thursday', 'NNP'), ('morning', 'NN')]

    :rtype: list of tuples
    """
    if isinstance(self, TextBlob):
        return [val for sublist in [s.pos_tags for s in self.sentences] for val in sublist]
    else:
        return [(Word(unicode(word), pos_tag=t), unicode(t))
                for word, t in self.pos_tagger.tag(self)
                if not PUNCTUATION_REGEX.match(unicode(t))]

tags = pos_tags

</t>
<t tx="karstenw.20230306120514.52">@cached_property
def word_counts(self):
    """Dictionary of word frequencies in this text.
    """
    counts = defaultdict(int)
    stripped_words = [lowerstrip(word) for word in self.words]
    for word in stripped_words:
        counts[word] += 1
    return counts

</t>
<t tx="karstenw.20230306120514.53">@cached_property
def np_counts(self):
    """Dictionary of noun phrase frequencies in this text.
    """
    counts = defaultdict(int)
    for phrase in self.noun_phrases:
        counts[phrase] += 1
    return counts

</t>
<t tx="karstenw.20230306120514.54">def ngrams(self, n=3):
    """Return a list of n-grams (tuples of n successive words) for this
    blob.

    :rtype: List of :class:`WordLists &lt;WordList&gt;`
    """
    if n &lt;= 0:
        return []
    grams = [WordList(self.words[i:i + n])
                        for i in range(len(self.words) - n + 1)]
    return grams

</t>
<t tx="karstenw.20230306120514.55">def translate(self, from_lang="auto", to="en"):
    """Translate the blob to another language.
    Uses the Google Translate API. Returns a new TextBlob.

    Requires an internet connection.

    Usage:
    ::

        &gt;&gt;&gt; b = TextBlob("Simple is better than complex")
        &gt;&gt;&gt; b.translate(to="es")
        TextBlob('Lo simple es mejor que complejo')

    Language code reference:
        https://developers.google.com/translate/v2/using_rest#language-params

    .. deprecated:: 0.16.0
        Use the official Google Translate API instead.
    .. versionadded:: 0.5.0.

    :param str from_lang: Language to translate from. If ``None``, will attempt
        to detect the language.
    :param str to: Language to translate to.
    :rtype: :class:`BaseBlob &lt;BaseBlob&gt;`
    """
    warnings.warn(
        'TextBlob.translate is deprecated and will be removed in a future release. '
        'Use the official Google Translate API instead.',
        DeprecationWarning
    )
    return self.__class__(self.translator.translate(self.raw,
                          from_lang=from_lang, to_lang=to))

</t>
<t tx="karstenw.20230306120514.56">def detect_language(self):
    """Detect the blob's language using the Google Translate API.

    Requires an internet connection.

    Usage:
    ::

        &gt;&gt;&gt; b = TextBlob("bonjour")
        &gt;&gt;&gt; b.detect_language()
        u'fr'

    Language code reference:
        https://developers.google.com/translate/v2/using_rest#language-params

    .. deprecated:: 0.16.0
        Use the official Google Translate API instead.
    .. versionadded:: 0.5.0

    :rtype: str
    """
    warnings.warn(
        'TextBlob.detext_translate is deprecated and will be removed in a future release. '
        'Use the official Google Translate API instead.',
        DeprecationWarning
    )
    return self.translator.detect(self.raw)

</t>
<t tx="karstenw.20230306120514.57">def correct(self):
    """Attempt to correct the spelling of a blob.

    .. versionadded:: 0.6.0

    :rtype: :class:`BaseBlob &lt;BaseBlob&gt;`
    """
    # regex matches: word or punctuation or whitespace
    tokens = nltk.tokenize.regexp_tokenize(self.raw, r"\w+|[^\w\s]|\s")
    corrected = (Word(w).correct() for w in tokens)
    ret = ''.join(corrected)
    return self.__class__(ret)

</t>
<t tx="karstenw.20230306120514.58">def _cmpkey(self):
    """Key used by ComparableMixin to implement all rich comparison
    operators.
    """
    return self.raw

</t>
<t tx="karstenw.20230306120514.59">def _strkey(self):
    """Key used by StringlikeMixin to implement string methods."""
    return self.raw

</t>
<t tx="karstenw.20230306120514.6">def __repr__(self):
    return repr(self.string)

</t>
<t tx="karstenw.20230306120514.60">def __hash__(self):
    return hash(self._cmpkey())

</t>
<t tx="karstenw.20230306120514.61">def __add__(self, other):
    '''Concatenates two text objects the same way Python strings are
    concatenated.

    Arguments:
    - `other`: a string or a text object
    '''
    if isinstance(other, basestring):
        return self.__class__(self.raw + other)
    elif isinstance(other, BaseBlob):
        return self.__class__(self.raw + other.raw)
    else:
        raise TypeError('Operands must be either strings or {0} objects'
            .format(self.__class__.__name__))

</t>
<t tx="karstenw.20230306120514.62">def split(self, sep=None, maxsplit=sys.maxsize):
    """Behaves like the built-in str.split() except returns a
    WordList.

    :rtype: :class:`WordList &lt;WordList&gt;`
    """
    return WordList(self._strkey().split(sep, maxsplit))


</t>
<t tx="karstenw.20230306120514.63">class TextBlob(BaseBlob):
    """A general text block, meant for larger bodies of text (esp. those
    containing sentences). Inherits from :class:`BaseBlob &lt;BaseBlob&gt;`.

    :param str text: A string.
    :param tokenizer: (optional) A tokenizer instance. If ``None``, defaults to
        :class:`WordTokenizer() &lt;textblob.tokenizers.WordTokenizer&gt;`.
    :param np_extractor: (optional) An NPExtractor instance. If ``None``,
        defaults to :class:`FastNPExtractor() &lt;textblob.en.np_extractors.FastNPExtractor&gt;`.
    :param pos_tagger: (optional) A Tagger instance. If ``None``, defaults to
        :class:`NLTKTagger &lt;textblob.en.taggers.NLTKTagger&gt;`.
    :param analyzer: (optional) A sentiment analyzer. If ``None``, defaults to
        :class:`PatternAnalyzer &lt;textblob.en.sentiments.PatternAnalyzer&gt;`.
    :param classifier: (optional) A classifier.
    """

    @others
</t>
<t tx="karstenw.20230306120514.64">@cached_property
def sentences(self):
    """Return list of :class:`Sentence &lt;Sentence&gt;` objects."""
    return self._create_sentence_objects()

</t>
<t tx="karstenw.20230306120514.65">@cached_property
def words(self):
    """Return a list of word tokens. This excludes punctuation characters.
    If you want to include punctuation characters, access the ``tokens``
    property.

    :returns: A :class:`WordList &lt;WordList&gt;` of word tokens.
    """
    return WordList(word_tokenize(self.raw, include_punc=False))

</t>
<t tx="karstenw.20230306120514.66">@property
def raw_sentences(self):
    """List of strings, the raw sentences in the blob."""
    return [sentence.raw for sentence in self.sentences]

</t>
<t tx="karstenw.20230306120514.67">@property
def serialized(self):
    """Returns a list of each sentence's dict representation."""
    return [sentence.dict for sentence in self.sentences]

</t>
<t tx="karstenw.20230306120514.68">def to_json(self, *args, **kwargs):
    '''Return a json representation (str) of this blob.
    Takes the same arguments as json.dumps.

    .. versionadded:: 0.5.1
    '''
    return json.dumps(self.serialized, *args, **kwargs)

</t>
<t tx="karstenw.20230306120514.69">@property
def json(self):
    '''The json representation of this blob.

    .. versionchanged:: 0.5.1
        Made ``json`` a property instead of a method to restore backwards
        compatibility that was broken after version 0.4.0.
    '''
    return self.to_json()

</t>
<t tx="karstenw.20230306120514.7">def __str__(self):
    return self.string

</t>
<t tx="karstenw.20230306120514.70">def _create_sentence_objects(self):
    '''Returns a list of Sentence objects from the raw text.
    '''
    sentence_objects = []
    sentences = sent_tokenize(self.raw)
    char_index = 0  # Keeps track of character index within the blob
    for sent in sentences:
        # Compute the start and end indices of the sentence
        # within the blob
        start_index = self.raw.index(sent, char_index)
        char_index += len(sent)
        end_index = start_index + len(sent)
        # Sentences share the same models as their parent blob
        s = Sentence(sent, start_index=start_index, end_index=end_index,
            tokenizer=self.tokenizer, np_extractor=self.np_extractor,
            pos_tagger=self.pos_tagger, analyzer=self.analyzer,
            parser=self.parser, classifier=self.classifier)
        sentence_objects.append(s)
    return sentence_objects


</t>
<t tx="karstenw.20230306120514.71">class Sentence(BaseBlob):
    """A sentence within a TextBlob. Inherits from :class:`BaseBlob &lt;BaseBlob&gt;`.

    :param sentence: A string, the raw sentence.
    :param start_index: An int, the index where this sentence begins
                        in a TextBlob. If not given, defaults to 0.
    :param end_index: An int, the index where this sentence ends in
                        a TextBlob. If not given, defaults to the
                        length of the sentence - 1.
    """

    @others
</t>
<t tx="karstenw.20230306120514.72">def __init__(self, sentence, start_index=0, end_index=None, *args, **kwargs):
    super(Sentence, self).__init__(sentence, *args, **kwargs)
    #: The start index within a TextBlob
    self.start = self.start_index = start_index
    #: The end index within a textBlob
    self.end = self.end_index = end_index or len(sentence) - 1

</t>
<t tx="karstenw.20230306120514.73">@property
def dict(self):
    '''The dict representation of this sentence.'''
    return {
        'raw': self.raw,
        'start_index': self.start_index,
        'end_index': self.end_index,
        'stripped': self.stripped,
        'noun_phrases': self.noun_phrases,
        'polarity': self.polarity,
        'subjectivity': self.subjectivity,
    }


</t>
<t tx="karstenw.20230306120514.74">class Blobber(object):
    """A factory for TextBlobs that all share the same tagger,
    tokenizer, parser, classifier, and np_extractor.

    Usage:

        &gt;&gt;&gt; from textblob import Blobber
        &gt;&gt;&gt; from textblob.taggers import NLTKTagger
        &gt;&gt;&gt; from textblob.tokenizers import SentenceTokenizer
        &gt;&gt;&gt; tb = Blobber(pos_tagger=NLTKTagger(), tokenizer=SentenceTokenizer())
        &gt;&gt;&gt; blob1 = tb("This is one blob.")
        &gt;&gt;&gt; blob2 = tb("This blob has the same tagger and tokenizer.")
        &gt;&gt;&gt; blob1.pos_tagger is blob2.pos_tagger
        True

    :param tokenizer: (optional) A tokenizer instance. If ``None``,
        defaults to :class:`WordTokenizer() &lt;textblob.tokenizers.WordTokenizer&gt;`.
    :param np_extractor: (optional) An NPExtractor instance. If ``None``,
        defaults to :class:`FastNPExtractor() &lt;textblob.en.np_extractors.FastNPExtractor&gt;`.
    :param pos_tagger: (optional) A Tagger instance. If ``None``,
        defaults to :class:`NLTKTagger &lt;textblob.en.taggers.NLTKTagger&gt;`.
    :param analyzer: (optional) A sentiment analyzer. If ``None``,
        defaults to :class:`PatternAnalyzer &lt;textblob.en.sentiments.PatternAnalyzer&gt;`.
    :param parser: A parser. If ``None``, defaults to
        :class:`PatternParser &lt;textblob.en.parsers.PatternParser&gt;`.
    :param classifier: A classifier.

    .. versionadded:: 0.4.0
    """

    np_extractor = FastNPExtractor()
    pos_tagger = NLTKTagger()
    tokenizer = WordTokenizer()
    analyzer = PatternAnalyzer()
    parser = PatternParser()

    @others
</t>
<t tx="karstenw.20230306120514.75">def __init__(self, tokenizer=None, pos_tagger=None, np_extractor=None,
            analyzer=None, parser=None, classifier=None):
    _initialize_models(self, tokenizer, pos_tagger, np_extractor, analyzer,
                        parser, classifier)

</t>
<t tx="karstenw.20230306120514.76">def __call__(self, text):
    """Return a new TextBlob object with this Blobber's ``np_extractor``,
    ``pos_tagger``, ``tokenizer``, ``analyzer``, and ``classifier``.

    :returns: A new :class:`TextBlob &lt;TextBlob&gt;`.
    """
    return TextBlob(text, tokenizer=self.tokenizer, pos_tagger=self.pos_tagger,
                    np_extractor=self.np_extractor, analyzer=self.analyzer,
                    parser=self.parser,
                    classifier=self.classifier)

</t>
<t tx="karstenw.20230306120514.77">def __repr__(self):
    classifier_name = self.classifier.__class__.__name__ + "()" if self.classifier else "None"
    return ("Blobber(tokenizer={0}(), pos_tagger={1}(), "
                "np_extractor={2}(), analyzer={3}(), parser={4}(), classifier={5})")\
                .format(self.tokenizer.__class__.__name__,
                        self.pos_tagger.__class__.__name__,
                        self.np_extractor.__class__.__name__,
                        self.analyzer.__class__.__name__,
                        self.parser.__class__.__name__,
                        classifier_name)

__str__ = __repr__
</t>
<t tx="karstenw.20230306120514.8">def singularize(self):
    """Return the singular version of the word as a string."""
    return Word(_singularize(self.string))

</t>
<t tx="karstenw.20230306120514.9">def pluralize(self):
    '''Return the plural version of the word as a string.'''
    return Word(_pluralize(self.string))

</t>
<t tx="karstenw.20230306120516.1">"""Various classifier implementations. Also includes basic feature extractor
methods.

Example Usage:
::

    &gt;&gt;&gt; from textblob import TextBlob
    &gt;&gt;&gt; from textblob.classifiers import NaiveBayesClassifier
    &gt;&gt;&gt; train = [
    ...     ('I love this sandwich.', 'pos'),
    ...     ('This is an amazing place!', 'pos'),
    ...     ('I feel very good about these beers.', 'pos'),
    ...     ('I do not like this restaurant', 'neg'),
    ...     ('I am tired of this stuff.', 'neg'),
    ...     ("I can't deal with this", 'neg'),
    ...     ("My boss is horrible.", "neg")
    ... ]
    &gt;&gt;&gt; cl = NaiveBayesClassifier(train)
    &gt;&gt;&gt; cl.classify("I feel amazing!")
    'pos'
    &gt;&gt;&gt; blob = TextBlob("The beer is good. But the hangover is horrible.", classifier=cl)
    &gt;&gt;&gt; for s in blob.sentences:
    ...     print(s)
    ...     print(s.classify())
    ...
    The beer is good.
    pos
    But the hangover is horrible.
    neg

.. versionadded:: 0.6.0
"""
from __future__ import absolute_import
from itertools import chain

import nltk

from textblob.compat import basestring
from textblob.decorators import cached_property
from textblob.exceptions import FormatError
from textblob.tokenizers import word_tokenize
from textblob.utils import strip_punc, is_filelike
import textblob.formats as formats

### Basic feature extractors ###


</t>
<t tx="karstenw.20230306120516.10">def classify(self, text):
    """Classifies a string of text."""
    raise NotImplementedError('Must implement a "classify" method.')

</t>
<t tx="karstenw.20230306120516.11">def train(self, labeled_featureset):
    """Trains the classifier."""
    raise NotImplementedError('Must implement a "train" method.')

</t>
<t tx="karstenw.20230306120516.12">def labels(self):
    """Returns an iterable containing the possible labels."""
    raise NotImplementedError('Must implement a "labels" method.')

</t>
<t tx="karstenw.20230306120516.13">def extract_features(self, text):
    '''Extracts features from a body of text.

    :rtype: dictionary of features
    '''
    # Feature extractor may take one or two arguments
    try:
        return self.feature_extractor(text, self._word_set)
    except (TypeError, AttributeError):
        return self.feature_extractor(text)


</t>
<t tx="karstenw.20230306120516.14">class NLTKClassifier(BaseClassifier):
    """An abstract class that wraps around the nltk.classify module.

    Expects that descendant classes include a class variable ``nltk_class``
    which is the class in the nltk.classify module to be wrapped.

    Example: ::

        class MyClassifier(NLTKClassifier):
            nltk_class = nltk.classify.svm.SvmClassifier
    """

    #: The NLTK class to be wrapped. Must be a class within nltk.classify
    nltk_class = None

    @others
</t>
<t tx="karstenw.20230306120516.15">def __init__(self, train_set,
             feature_extractor=basic_extractor, format=None, **kwargs):
    super(NLTKClassifier, self).__init__(train_set, feature_extractor, format, **kwargs)
    self.train_features = [(self.extract_features(d), c) for d, c in self.train_set]

</t>
<t tx="karstenw.20230306120516.16">def __repr__(self):
    class_name = self.__class__.__name__
    return "&lt;{cls} trained on {n} instances&gt;".format(cls=class_name,
                                                    n=len(self.train_set))

</t>
<t tx="karstenw.20230306120516.17">@cached_property
def classifier(self):
    """The classifier."""
    try:
        return self.train()
    except AttributeError:  # nltk_class has not been defined
        raise ValueError("NLTKClassifier must have a nltk_class"
                        " variable that is not None.")

</t>
<t tx="karstenw.20230306120516.18">def train(self, *args, **kwargs):
    """Train the classifier with a labeled feature set and return
    the classifier. Takes the same arguments as the wrapped NLTK class.
    This method is implicitly called when calling ``classify`` or
    ``accuracy`` methods and is included only to allow passing in arguments
    to the ``train`` method of the wrapped NLTK class.

    .. versionadded:: 0.6.2

    :rtype: A classifier
    """
    try:
        self.classifier = self.nltk_class.train(self.train_features,
                                                *args, **kwargs)
        return self.classifier
    except AttributeError:
        raise ValueError("NLTKClassifier must have a nltk_class"
                        " variable that is not None.")

</t>
<t tx="karstenw.20230306120516.19">def labels(self):
    """Return an iterable of possible labels."""
    return self.classifier.labels()

</t>
<t tx="karstenw.20230306120516.2">def _get_words_from_dataset(dataset):
    """Return a set of all words in a dataset.

    :param dataset: A list of tuples of the form ``(words, label)`` where
        ``words`` is either a string of a list of tokens.
    """
    # Words may be either a string or a list of tokens. Return an iterator
    # of tokens accordingly
    def tokenize(words):
        if isinstance(words, basestring):
            return word_tokenize(words, include_punc=False)
        else:
            return words
    all_words = chain.from_iterable(tokenize(words) for words, _ in dataset)
    return set(all_words)

</t>
<t tx="karstenw.20230306120516.20">def classify(self, text):
    """Classifies the text.

    :param str text: A string of text.
    """
    text_features = self.extract_features(text)
    return self.classifier.classify(text_features)

</t>
<t tx="karstenw.20230306120516.21">def accuracy(self, test_set, format=None):
    """Compute the accuracy on a test set.

    :param test_set: A list of tuples of the form ``(text, label)``, or a
        file pointer.
    :param format: If ``test_set`` is a filename, the file format, e.g.
        ``"csv"`` or ``"json"``. If ``None``, will attempt to detect the
        file format.
    """
    if is_filelike(test_set):
        test_data = self._read_data(test_set, format)
    else:  # test_set is a list of tuples
        test_data = test_set
    test_features = [(self.extract_features(d), c) for d, c in test_data]
    return nltk.classify.accuracy(self.classifier, test_features)

</t>
<t tx="karstenw.20230306120516.22">def update(self, new_data, *args, **kwargs):
    """Update the classifier with new training data and re-trains the
    classifier.

    :param new_data: New data as a list of tuples of the form
        ``(text, label)``.
    """
    self.train_set += new_data
    self._word_set.update(_get_words_from_dataset(new_data))
    self.train_features = [(self.extract_features(d), c)
                            for d, c in self.train_set]
    try:
        self.classifier = self.nltk_class.train(self.train_features,
                                                *args, **kwargs)
    except AttributeError:  # Descendant has not defined nltk_class
        raise ValueError("NLTKClassifier must have a nltk_class"
                        " variable that is not None.")
    return True


</t>
<t tx="karstenw.20230306120516.23">class NaiveBayesClassifier(NLTKClassifier):
    """A classifier based on the Naive Bayes algorithm, as implemented in
    NLTK.

    :param train_set: The training set, either a list of tuples of the form
        ``(text, classification)`` or a filename. ``text`` may be either
        a string or an iterable.
    :param feature_extractor: A feature extractor function that takes one or
        two arguments: ``document`` and ``train_set``.
    :param format: If ``train_set`` is a filename, the file format, e.g.
        ``"csv"`` or ``"json"``. If ``None``, will attempt to detect the
        file format.

    .. versionadded:: 0.6.0
    """

    nltk_class = nltk.classify.NaiveBayesClassifier

    @others
</t>
<t tx="karstenw.20230306120516.24">def prob_classify(self, text):
    """Return the label probability distribution for classifying a string
    of text.

    Example:
    ::

        &gt;&gt;&gt; classifier = NaiveBayesClassifier(train_data)
        &gt;&gt;&gt; prob_dist = classifier.prob_classify("I feel happy this morning.")
        &gt;&gt;&gt; prob_dist.max()
        'positive'
        &gt;&gt;&gt; prob_dist.prob("positive")
        0.7

    :rtype: nltk.probability.DictionaryProbDist
    """
    text_features = self.extract_features(text)
    return self.classifier.prob_classify(text_features)

</t>
<t tx="karstenw.20230306120516.25">def informative_features(self, *args, **kwargs):
    """Return the most informative features as a list of tuples of the
    form ``(feature_name, feature_value)``.

    :rtype: list
    """
    return self.classifier.most_informative_features(*args, **kwargs)

</t>
<t tx="karstenw.20230306120516.26">def show_informative_features(self, *args, **kwargs):
    """Displays a listing of the most informative features for this
    classifier.

    :rtype: None
    """
    return self.classifier.show_most_informative_features(*args, **kwargs)


</t>
<t tx="karstenw.20230306120516.27">class DecisionTreeClassifier(NLTKClassifier):
    """A classifier based on the decision tree algorithm, as implemented in
    NLTK.

    :param train_set: The training set, either a list of tuples of the form
        ``(text, classification)`` or a filename. ``text`` may be either
        a string or an iterable.
    :param feature_extractor: A feature extractor function that takes one or
        two arguments: ``document`` and ``train_set``.
    :param format: If ``train_set`` is a filename, the file format, e.g.
        ``"csv"`` or ``"json"``. If ``None``, will attempt to detect the
        file format.

    .. versionadded:: 0.6.2
    """

    nltk_class = nltk.classify.decisiontree.DecisionTreeClassifier

    @others
</t>
<t tx="karstenw.20230306120516.28">def pretty_format(self, *args, **kwargs):
    """Return a string containing a pretty-printed version of this decision
    tree. Each line in the string corresponds to a single decision tree node
    or leaf, and indentation is used to display the structure of the tree.

    :rtype: str
    """
    return self.classifier.pretty_format(*args, **kwargs)

# Backwards-compat
pprint = pretty_format

</t>
<t tx="karstenw.20230306120516.29">def pseudocode(self, *args, **kwargs):
    """Return a string representation of this decision tree that expresses
    the decisions it makes as a nested set of pseudocode if statements.

    :rtype: str
    """
    return self.classifier.pseudocode(*args, **kwargs)


</t>
<t tx="karstenw.20230306120516.3">def _get_document_tokens(document):
    if isinstance(document, basestring):
        tokens = set((strip_punc(w, all=False)
                    for w in word_tokenize(document, include_punc=False)))
    else:
        tokens = set(strip_punc(w, all=False) for w in document)
    return tokens

</t>
<t tx="karstenw.20230306120516.30">class PositiveNaiveBayesClassifier(NLTKClassifier):
    """A variant of the Naive Bayes Classifier that performs binary
    classification with partially-labeled training sets, i.e. when only
    one class is labeled and the other is not. Assuming a prior distribution
    on the two labels, uses the unlabeled set to estimate the frequencies of
    the features.

    Example usage:
    ::

        &gt;&gt;&gt; from text.classifiers import PositiveNaiveBayesClassifier
        &gt;&gt;&gt; sports_sentences = ['The team dominated the game',
        ...                   'They lost the ball',
        ...                   'The game was intense',
        ...                   'The goalkeeper catched the ball',
        ...                   'The other team controlled the ball']
        &gt;&gt;&gt; various_sentences = ['The President did not comment',
        ...                        'I lost the keys',
        ...                        'The team won the game',
        ...                        'Sara has two kids',
        ...                        'The ball went off the court',
        ...                        'They had the ball for the whole game',
        ...                        'The show is over']
        &gt;&gt;&gt; classifier = PositiveNaiveBayesClassifier(positive_set=sports_sentences,
        ...                                           unlabeled_set=various_sentences)
        &gt;&gt;&gt; classifier.classify("My team lost the game")
        True
        &gt;&gt;&gt; classifier.classify("And now for something completely different.")
        False


    :param positive_set: A collection of strings that have the positive label.
    :param unlabeled_set: A collection of unlabeled strings.
    :param feature_extractor: A feature extractor function.
    :param positive_prob_prior: A prior estimate of the probability of the
        label ``True``.

    .. versionadded:: 0.7.0
    """

    nltk_class = nltk.classify.PositiveNaiveBayesClassifier

    @others
</t>
<t tx="karstenw.20230306120516.31">def __init__(self, positive_set, unlabeled_set,
            feature_extractor=contains_extractor,
            positive_prob_prior=0.5, **kwargs):
    self.feature_extractor = feature_extractor
    self.positive_set = positive_set
    self.unlabeled_set = unlabeled_set
    self.positive_features = [self.extract_features(d)
                                for d in self.positive_set]
    self.unlabeled_features = [self.extract_features(d)
                                for d in self.unlabeled_set]
    self.positive_prob_prior = positive_prob_prior

</t>
<t tx="karstenw.20230306120516.32">def __repr__(self):
    class_name = self.__class__.__name__
    return "&lt;{cls} trained on {n_pos} labeled and {n_unlabeled} unlabeled instances&gt;"\
                    .format(cls=class_name, n_pos=len(self.positive_set),
                            n_unlabeled=len(self.unlabeled_set))

# Override
</t>
<t tx="karstenw.20230306120516.33">def train(self, *args, **kwargs):
    """Train the classifier with a labeled and unlabeled feature sets and return
    the classifier. Takes the same arguments as the wrapped NLTK class.
    This method is implicitly called when calling ``classify`` or
    ``accuracy`` methods and is included only to allow passing in arguments
    to the ``train`` method of the wrapped NLTK class.

    :rtype: A classifier
    """
    self.classifier = self.nltk_class.train(self.positive_features,
                                            self.unlabeled_features,
                                            self.positive_prob_prior)
    return self.classifier

</t>
<t tx="karstenw.20230306120516.34">def update(self, new_positive_data=None,
           new_unlabeled_data=None, positive_prob_prior=0.5,
           *args, **kwargs):
    """Update the classifier with new data and re-trains the
    classifier.

    :param new_positive_data: List of new, labeled strings.
    :param new_unlabeled_data: List of new, unlabeled strings.
    """
    self.positive_prob_prior = positive_prob_prior
    if new_positive_data:
        self.positive_set += new_positive_data
        self.positive_features += [self.extract_features(d)
                                        for d in new_positive_data]
    if new_unlabeled_data:
        self.unlabeled_set += new_unlabeled_data
        self.unlabeled_features += [self.extract_features(d)
                                        for d in new_unlabeled_data]
    self.classifier = self.nltk_class.train(self.positive_features,
                                            self.unlabeled_features,
                                            self.positive_prob_prior,
                                            *args, **kwargs)
    return True


</t>
<t tx="karstenw.20230306120516.35">class MaxEntClassifier(NLTKClassifier):
    __doc__ = nltk.classify.maxent.MaxentClassifier.__doc__
    nltk_class = nltk.classify.maxent.MaxentClassifier

    @others
</t>
<t tx="karstenw.20230306120516.36">def prob_classify(self, text):
    """Return the label probability distribution for classifying a string
    of text.

    Example:
    ::

        &gt;&gt;&gt; classifier = MaxEntClassifier(train_data)
        &gt;&gt;&gt; prob_dist = classifier.prob_classify("I feel happy this morning.")
        &gt;&gt;&gt; prob_dist.max()
        'positive'
        &gt;&gt;&gt; prob_dist.prob("positive")
        0.7

    :rtype: nltk.probability.DictionaryProbDist
    """
    feats = self.extract_features(text)
    return self.classifier.prob_classify(feats)
</t>
<t tx="karstenw.20230306120516.4">def basic_extractor(document, train_set):
    """A basic document feature extractor that returns a dict indicating
    what words in ``train_set`` are contained in ``document``.

    :param document: The text to extract features from. Can be a string or an iterable.
    :param list train_set: Training data set, a list of tuples of the form
        ``(words, label)`` OR an iterable of strings.
    """

    try:
        el_zero = next(iter(train_set))  # Infer input from first element.
    except StopIteration:
        return {}
    if isinstance(el_zero, basestring):
        word_features = [w for w in chain([el_zero], train_set)]
    else:
        try:
            assert(isinstance(el_zero[0], basestring))
            word_features = _get_words_from_dataset(chain([el_zero], train_set))
        except Exception:
            raise ValueError('train_set is probably malformed.')

    tokens = _get_document_tokens(document)
    features = dict(((u'contains({0})'.format(word), (word in tokens))
                                            for word in word_features))
    return features


</t>
<t tx="karstenw.20230306120516.5">def contains_extractor(document):
    """A basic document feature extractor that returns a dict of words that
    the document contains.
    """
    tokens = _get_document_tokens(document)
    features = dict((u'contains({0})'.format(w), True) for w in tokens)
    return features

##### CLASSIFIERS #####

</t>
<t tx="karstenw.20230306120516.6">class BaseClassifier(object):
    """Abstract classifier class from which all classifers inherit. At a
    minimum, descendant classes must implement a ``classify`` method and have
    a ``classifier`` property.

    :param train_set: The training set, either a list of tuples of the form
        ``(text, classification)`` or a file-like object. ``text`` may be either
        a string or an iterable.
    :param callable feature_extractor: A feature extractor function that takes one or
        two arguments: ``document`` and ``train_set``.
    :param str format: If ``train_set`` is a filename, the file format, e.g.
        ``"csv"`` or ``"json"``. If ``None``, will attempt to detect the
        file format.
    :param kwargs: Additional keyword arguments are passed to the constructor
        of the :class:`Format &lt;textblob.formats.BaseFormat&gt;` class used to
        read the data. Only applies when a file-like object is passed as
        ``train_set``.

    .. versionadded:: 0.6.0
    """

    @others
</t>
<t tx="karstenw.20230306120516.7">def __init__(self, train_set, feature_extractor=basic_extractor, format=None, **kwargs):
    self.format_kwargs = kwargs
    self.feature_extractor = feature_extractor
    if is_filelike(train_set):
        self.train_set = self._read_data(train_set, format)
    else:  # train_set is a list of tuples
        self.train_set = train_set
    self._word_set = _get_words_from_dataset(self.train_set)  # Keep a hidden set of unique words.
    self.train_features = None

</t>
<t tx="karstenw.20230306120516.8">def _read_data(self, dataset, format=None):
    """Reads a data file and returns an iterable that can be used
    as testing or training data.
    """
    # Attempt to detect file format if "format" isn't specified
    if not format:
        format_class = formats.detect(dataset)
        if not format_class:
            raise FormatError('Could not automatically detect format for the given '
                              'data source.')
    else:
        registry = formats.get_registry()
        if format not in registry.keys():
            raise ValueError("'{0}' format not supported.".format(format))
        format_class = registry[format]
    return format_class(dataset, **self.format_kwargs).to_iterable()

</t>
<t tx="karstenw.20230306120516.9">@cached_property
def classifier(self):
    """The classifier object."""
    raise NotImplementedError('Must implement the "classifier" property.')

</t>
<t tx="karstenw.20230306120518.1">import sys

PY2 = int(sys.version[0]) == 2

if PY2:
    from itertools import imap, izip
    import urllib2 as request
    from urllib import quote as urlquote
    from urllib import urlencode
    text_type = unicode
    binary_type = str
    string_types = (str, unicode)
    unicode = unicode
    basestring = basestring
    imap = imap
    izip = izip
    import unicodecsv as csv

    def implements_to_string(cls):
        """Class decorator that renames __str__ to __unicode__ and
        modifies __str__ that returns utf-8.
        """
        cls.__unicode__ = cls.__str__
        cls.__str__ = lambda x: x.__unicode__().encode('utf-8')
        return cls
else:  # PY3
    from urllib import request
    from urllib.parse import quote as urlquote
    from urllib.parse import urlencode
    text_type = str
    binary_type = bytes
    string_types = (str,)
    unicode = str
    basestring = (str, bytes)
    imap = map
    izip = zip
    import csv

    implements_to_string = lambda x: x


# From six
</t>
<t tx="karstenw.20230306120518.2">def with_metaclass(meta, *bases):
    """Create a base class with a metaclass."""
    # This requires a bit of explanation: the basic idea is to make a dummy
    # metaclass for one level of class instantiation that replaces itself with
    # the actual metaclass.
    class metaclass(meta):  # noqa

        def __new__(cls, name, this_bases, d):
            return meta(name, bases, d)
    return type.__new__(metaclass, 'temporary_class', (), {})
</t>
<t tx="karstenw.20230306120520.1"># -*- coding: utf-8 -*-
"""Custom decorators."""

from __future__ import absolute_import
from functools import wraps
from textblob.exceptions import MissingCorpusError


</t>
<t tx="karstenw.20230306120520.2">class cached_property(object):
    """A property that is only computed once per instance and then replaces
    itself with an ordinary attribute. Deleting the attribute resets the
    property.

    Credit to Marcel Hellkamp, author of bottle.py.
    """

    @others
</t>
<t tx="karstenw.20230306120520.3">def __init__(self, func):
    self.__doc__ = getattr(func, '__doc__')
    self.func = func

</t>
<t tx="karstenw.20230306120520.4">def __get__(self, obj, cls):
    if obj is None:
        return self
    value = obj.__dict__[self.func.__name__] = self.func(obj)
    return value


</t>
<t tx="karstenw.20230306120520.5">def requires_nltk_corpus(func):
    """Wraps a function that requires an NLTK corpus. If the corpus isn't found,
    raise a :exc:`MissingCorpusError`.
    """
    @wraps(func)
    def decorated(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except LookupError as err:
            print(err)
            raise MissingCorpusError()
    return decorated
</t>
<t tx="karstenw.20230306120523.1">"""Downloads the necessary NLTK corpora for TextBlob.

Usage: ::

    $ python -m textblob.download_corpora

If you only intend to use TextBlob's default models, you can use the "lite"
option: ::

    $ python -m textblob.download_corpora lite

"""
import sys, os


# nodebox fix
filepath = os.path.abspath(__file__)
folder, _ = os.path.split( filepath )
parent, _ = os.path.split( folder )
download_dir = os.path.join( parent, "nltk-data" )


import nltk

MIN_CORPORA = [
    'brown',  # Required for FastNPExtractor
    'punkt',  # Required for WordTokenizer
    'wordnet',  # Required for lemmatization
    'averaged_perceptron_tagger',  # Required for NLTKTagger
]

ADDITIONAL_CORPORA = [
    'conll2000',  # Required for ConllExtractor
    'movie_reviews',  # Required for NaiveBayesAnalyzer
]

ALL_CORPORA = MIN_CORPORA + ADDITIONAL_CORPORA

</t>
<t tx="karstenw.20230306120523.2">def download_lite():
    for each in MIN_CORPORA:
        nltk.download(each, download_dir=download_dir)


</t>
<t tx="karstenw.20230306120523.3">def download_all():
    for each in ALL_CORPORA:
        nltk.download(each, download_dir=download_dir)


</t>
<t tx="karstenw.20230306120523.4">def main():
    if 'lite' in sys.argv:
        download_lite()
    else:
        download_all()
    print("Finished.")


</t>
<t tx="karstenw.20230306120524.1">MISSING_CORPUS_MESSAGE = """
Looks like you are missing some required data for this feature.

To download the necessary data, simply run

    python -m textblob.download_corpora

or use the NLTK downloader to download the missing data: http://nltk.org/data.html
If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
"""

</t>
<t tx="karstenw.20230306120524.2">class TextBlobError(Exception):
    """A TextBlob-related error."""
    pass


TextBlobException = TextBlobError  # Backwards compat

</t>
<t tx="karstenw.20230306120524.3">class MissingCorpusError(TextBlobError):
    """Exception thrown when a user tries to use a feature that requires a
    dataset or model that the user does not have on their system.
    """

    @others
MissingCorpusException = MissingCorpusError  # Backwards compat

</t>
<t tx="karstenw.20230306120524.4">def __init__(self, message=MISSING_CORPUS_MESSAGE, *args, **kwargs):
    super(MissingCorpusError, self).__init__(message, *args, **kwargs)


</t>
<t tx="karstenw.20230306120524.5">class DeprecationError(TextBlobError):
    """Raised when user uses a deprecated feature."""
    pass

</t>
<t tx="karstenw.20230306120524.6">class TranslatorError(TextBlobError):
    """Raised when an error occurs during language translation or detection."""
    pass

</t>
<t tx="karstenw.20230306120525.1">class NotTranslated(TranslatorError):
    """Raised when text is unchanged after translation. This may be due to the language
    being unsupported by the translator.
    """
    pass

</t>
<t tx="karstenw.20230306120525.2">class FormatError(TextBlobError):
    """Raised if a data file with an unsupported format is passed to a classifier."""
    pass
</t>
<t tx="karstenw.20230306120526.1">"""File formats for training and testing data.

Includes a registry of valid file formats. New file formats can be added to the
registry like so: ::

    from textblob import formats

    class PipeDelimitedFormat(formats.DelimitedFormat):
        delimiter = '|'

    formats.register('psv', PipeDelimitedFormat)

Once a format has been registered, classifiers will be able to read data files with
that format. ::

    from textblob.classifiers import NaiveBayesAnalyzer

    with open('training_data.psv', 'r') as fp:
        cl = NaiveBayesAnalyzer(fp, format='psv')
"""
from __future__ import absolute_import
import json
from collections import OrderedDict

from textblob.compat import PY2, csv
from textblob.utils import is_filelike

DEFAULT_ENCODING = 'utf-8'

</t>
<t tx="karstenw.20230306120526.10">class CSV(DelimitedFormat):
    """CSV format. Assumes each row is of the form ``text,label``.
    ::

        Today is a good day,pos
        I hate this car.,pos
    """
    delimiter = ","


</t>
<t tx="karstenw.20230306120526.11">class TSV(DelimitedFormat):
    """TSV format. Assumes each row is of the form ``text\tlabel``.
    """
    delimiter = "\t"


</t>
<t tx="karstenw.20230306120526.12">class JSON(BaseFormat):
    """JSON format.

    Assumes that JSON is formatted as an array of objects with ``text`` and
    ``label`` properties.
    ::

        [
            {"text": "Today is a good day.", "label": "pos"},
            {"text": "I hate this car.", "label": "neg"}
        ]
    """
    @others
_registry = OrderedDict([
    ('csv', CSV),
    ('json', JSON),
    ('tsv', TSV),
])

</t>
<t tx="karstenw.20230306120526.13">def __init__(self, fp, **kwargs):
    BaseFormat.__init__(self, fp, **kwargs)
    self.dict = json.load(fp)

</t>
<t tx="karstenw.20230306120526.14">def to_iterable(self):
    """Return an iterable object from the JSON data."""
    return [(d['text'], d['label']) for d in self.dict]

</t>
<t tx="karstenw.20230306120526.15">@classmethod
def detect(cls, stream):
    """Return True if stream is valid JSON."""
    try:
        json.loads(stream)
        return True
    except ValueError:
        return False


</t>
<t tx="karstenw.20230306120526.16">def detect(fp, max_read=1024):
    """Attempt to detect a file's format, trying each of the supported
    formats. Return the format class that was detected. If no format is
    detected, return ``None``.
    """
    if not is_filelike(fp):
        return None
    for Format in _registry.values():
        if Format.detect(fp.read(max_read)):
            fp.seek(0)
            return Format
        fp.seek(0)
    return None

</t>
<t tx="karstenw.20230306120526.17">def get_registry():
    """Return a dictionary of registered formats."""
    return _registry

</t>
<t tx="karstenw.20230306120526.18">def register(name, format_class):
    """Register a new format.

    :param str name: The name that will be used to refer to the format, e.g. 'csv'
    :param type format_class: The format class to register.
    """
    get_registry()[name] = format_class
</t>
<t tx="karstenw.20230306120526.2">class BaseFormat(object):
    """Interface for format classes. Individual formats can decide on the
    composition and meaning of ``**kwargs``.

    :param File fp: A file-like object.

    .. versionchanged:: 0.9.0
        Constructor receives a file pointer rather than a file path.
    """
    @others
</t>
<t tx="karstenw.20230306120526.3">def __init__(self, fp, **kwargs):
    pass

</t>
<t tx="karstenw.20230306120526.4">def to_iterable(self):
    """Return an iterable object from the data."""
    raise NotImplementedError('Must implement a "to_iterable" method.')

</t>
<t tx="karstenw.20230306120526.5">@classmethod
def detect(cls, stream):
    """Detect the file format given a filename.
    Return True if a stream is this file format.

    .. versionchanged:: 0.9.0
        Changed from a static method to a class method.
    """
    raise NotImplementedError('Must implement a "detect" class method.')

</t>
<t tx="karstenw.20230306120526.6">class DelimitedFormat(BaseFormat):
    """A general character-delimited format."""

    delimiter = ","

    @others
</t>
<t tx="karstenw.20230306120526.7">def __init__(self, fp, **kwargs):
    BaseFormat.__init__(self, fp, **kwargs)
    if PY2:
        reader = csv.reader(fp, delimiter=self.delimiter,
                            encoding=DEFAULT_ENCODING)
    else:
        reader = csv.reader(fp, delimiter=self.delimiter)
    self.data = [row for row in reader]

</t>
<t tx="karstenw.20230306120526.8">def to_iterable(self):
    """Return an iterable object from the data."""
    return self.data

</t>
<t tx="karstenw.20230306120526.9">@classmethod
def detect(cls, stream):
    """Return True if stream is valid."""
    try:
        csv.Sniffer().sniff(stream, delimiters=cls.delimiter)
        return True
    except (csv.Error, TypeError):
        return False


</t>
<t tx="karstenw.20230306120528.1">'''Make word inflection default to English. This allows for backwards
compatibility so you can still import text.inflect.

    &gt;&gt;&gt; from textblob.inflect import singularize

is equivalent to

    &gt;&gt;&gt; from textblob.en.inflect import singularize
'''
from __future__ import absolute_import
from textblob.en.inflect import singularize, pluralize

__all__ = [
    'singularize',
    'pluralize',
]
</t>
<t tx="karstenw.20230306120530.1">from __future__ import absolute_import
import sys
from textblob.compat import basestring, implements_to_string, PY2, binary_type


</t>
<t tx="karstenw.20230306120530.10">class BlobComparableMixin(ComparableMixin):

    '''Allow blob objects to be comparable with both strings and blobs.'''

    @others
</t>
<t tx="karstenw.20230306120530.11">def _compare(self, other, method):
    if isinstance(other, basestring):
        # Just compare with the other string
        return method(self._cmpkey(), other)
    return super(BlobComparableMixin, self)._compare(other, method)


</t>
<t tx="karstenw.20230306120530.12">@implements_to_string
class StringlikeMixin(object):

    '''Make blob objects behave like Python strings.

    Expects that classes that use this mixin to have a _strkey() method that
    returns the string to apply string methods to. Using _strkey() instead
    of __str__ ensures consistent behavior between Python 2 and 3.
    '''

    @others
</t>
<t tx="karstenw.20230306120530.13">def __repr__(self):
    '''Returns a string representation for debugging.'''
    class_name = self.__class__.__name__
    text = self.__unicode__().encode("utf-8") if PY2 else str(self)
    ret = '{cls}("{text}")'.format(cls=class_name,
                                    text=text)
    return binary_type(ret) if PY2 else ret

</t>
<t tx="karstenw.20230306120530.14">def __str__(self):
    '''Returns a string representation used in print statements
    or str(my_blob).'''
    return self._strkey()

</t>
<t tx="karstenw.20230306120530.15">def __len__(self):
    '''Returns the length of the raw text.'''
    return len(self._strkey())

</t>
<t tx="karstenw.20230306120530.16">def __iter__(self):
    '''Makes the object iterable as if it were a string,
    iterating through the raw string's characters.
    '''
    return iter(self._strkey())

</t>
<t tx="karstenw.20230306120530.17">def __contains__(self, sub):
    '''Implements the `in` keyword like a Python string.'''
    return sub in self._strkey()

</t>
<t tx="karstenw.20230306120530.18">def __getitem__(self, index):
    '''Returns a  substring. If index is an integer, returns a Python
    string of a single character. If a range is given, e.g. `blob[3:5]`,
    a new instance of the class is returned.
    '''
    if isinstance(index, int):
        return self._strkey()[index]  # Just return a single character
    else:
        # Return a new blob object
        return self.__class__(self._strkey()[index])

</t>
<t tx="karstenw.20230306120530.19">def find(self, sub, start=0, end=sys.maxsize):
    '''Behaves like the built-in str.find() method. Returns an integer,
    the index of the first occurrence of the substring argument sub in the
    sub-string given by [start:end].
    '''
    return self._strkey().find(sub, start, end)

</t>
<t tx="karstenw.20230306120530.2">class ComparableMixin(object):

    '''Implements rich operators for an object.'''

    @others
</t>
<t tx="karstenw.20230306120530.20">def rfind(self, sub, start=0, end=sys.maxsize):
    '''Behaves like the built-in str.rfind() method. Returns an integer,
    the index of he last (right-most) occurence of the substring argument
    sub in the sub-sequence given by [start:end].
    '''
    return self._strkey().rfind(sub, start, end)

</t>
<t tx="karstenw.20230306120530.21">def index(self, sub, start=0, end=sys.maxsize):
    '''Like blob.find() but raise ValueError when the substring
    is not found.
    '''
    return self._strkey().index(sub, start, end)

</t>
<t tx="karstenw.20230306120530.22">def rindex(self, sub, start=0, end=sys.maxsize):
    '''Like blob.rfind() but raise ValueError when substring is not
    found.
    '''
    return self._strkey().rindex(sub, start, end)

</t>
<t tx="karstenw.20230306120530.23">def startswith(self, prefix, start=0, end=sys.maxsize):
    """Returns True if the blob starts with the given prefix."""
    return self._strkey().startswith(prefix, start, end)

</t>
<t tx="karstenw.20230306120530.24">def endswith(self, suffix, start=0, end=sys.maxsize):
    """Returns True if the blob ends with the given suffix."""
    return self._strkey().endswith(suffix, start, end)

# PEP8 aliases
starts_with = startswith
ends_with = endswith

</t>
<t tx="karstenw.20230306120530.25">def title(self):
    """Returns a blob object with the text in title-case."""
    return self.__class__(self._strkey().title())

</t>
<t tx="karstenw.20230306120530.26">def format(self, *args, **kwargs):
    """Perform a string formatting operation, like the built-in
    `str.format(*args, **kwargs)`. Returns a blob object.
    """
    return self.__class__(self._strkey().format(*args, **kwargs))

</t>
<t tx="karstenw.20230306120530.27">def split(self, sep=None, maxsplit=sys.maxsize):
    """Behaves like the built-in str.split().
    """
    return self._strkey().split(sep, maxsplit)

</t>
<t tx="karstenw.20230306120530.28">def strip(self, chars=None):
    """Behaves like the built-in str.strip([chars]) method. Returns
    an object with leading and trailing whitespace removed.
    """
    return self.__class__(self._strkey().strip(chars))

</t>
<t tx="karstenw.20230306120530.29">def upper(self):
    """Like str.upper(), returns new object with all upper-cased characters.
    """
    return self.__class__(self._strkey().upper())

</t>
<t tx="karstenw.20230306120530.3">def _compare(self, other, method):
    try:
        return method(self._cmpkey(), other._cmpkey())
    except (AttributeError, TypeError):
        # _cmpkey not implemented, or return different type,
        # so I can't compare with "other". Try the reverse comparison
        return NotImplemented

</t>
<t tx="karstenw.20230306120530.30">def lower(self):
    """Like str.lower(), returns new object with all lower-cased characters.
    """
    return self.__class__(self._strkey().lower())

</t>
<t tx="karstenw.20230306120530.31">def join(self, iterable):
    """Behaves like the built-in `str.join(iterable)` method, except
    returns a blob object.

    Returns a blob which is the concatenation of the strings or blobs
    in the iterable.
    """
    return self.__class__(self._strkey().join(iterable))

</t>
<t tx="karstenw.20230306120530.32">def replace(self, old, new, count=sys.maxsize):
    """Return a new blob object with all the occurence of `old` replaced
    by `new`.
    """
    return self.__class__(self._strkey().replace(old, new, count))
</t>
<t tx="karstenw.20230306120530.4">def __lt__(self, other):
    return self._compare(other, lambda s, o: s &lt; o)

</t>
<t tx="karstenw.20230306120530.5">def __le__(self, other):
    return self._compare(other, lambda s, o: s &lt;= o)

</t>
<t tx="karstenw.20230306120530.6">def __eq__(self, other):
    return self._compare(other, lambda s, o: s == o)

</t>
<t tx="karstenw.20230306120530.7">def __ge__(self, other):
    return self._compare(other, lambda s, o: s &gt;= o)

</t>
<t tx="karstenw.20230306120530.8">def __gt__(self, other):
    return self._compare(other, lambda s, o: s &gt; o)

</t>
<t tx="karstenw.20230306120530.9">def __ne__(self, other):
    return self._compare(other, lambda s, o: s != o)


</t>
<t tx="karstenw.20230306120532.1">"""Default noun phrase extractors are for English to maintain backwards
compatibility, so you can still do

&gt;&gt;&gt; from textblob.np_extractors import ConllExtractor

which is equivalent to

&gt;&gt;&gt; from textblob.en.np_extractors import ConllExtractor
"""
from __future__ import absolute_import
from textblob.base import BaseNPExtractor
from textblob.en.np_extractors import ConllExtractor, FastNPExtractor

__all__ = [
    'BaseNPExtractor',
    'ConllExtractor',
    'FastNPExtractor',
]
</t>
<t tx="karstenw.20230306120534.1">'''Default parsers to English for backwards compatibility so you can still do

&gt;&gt;&gt; from textblob.parsers import PatternParser

which is equivalent to

&gt;&gt;&gt; from textblob.en.parsers import PatternParser
'''
from __future__ import absolute_import
from textblob.base import BaseParser
from textblob.en.parsers import PatternParser

__all__ = [
    'BaseParser',
    'PatternParser',
]
</t>
<t tx="karstenw.20230306120536.1">'''Default sentiment analyzers are English for backwards compatibility, so
you can still do

&gt;&gt;&gt; from textblob.sentiments import PatternAnalyzer

which is equivalent to

&gt;&gt;&gt; from textblob.en.sentiments import PatternAnalyzer
'''
from __future__ import absolute_import
from textblob.base import BaseSentimentAnalyzer
from textblob.en.sentiments import (DISCRETE, CONTINUOUS,
                                PatternAnalyzer, NaiveBayesAnalyzer)

__all__ = [
    'BaseSentimentAnalyzer',
    'DISCRETE',
    'CONTINUOUS',
    'PatternAnalyzer',
    'NaiveBayesAnalyzer',
]
</t>
<t tx="karstenw.20230306120539.1">'''Various tokenizer implementations.

.. versionadded:: 0.4.0
'''
from __future__ import absolute_import
from itertools import chain

import nltk

from textblob.utils import strip_punc
from textblob.base import BaseTokenizer
from textblob.decorators import requires_nltk_corpus


</t>
<t tx="karstenw.20230306120539.2">class WordTokenizer(BaseTokenizer):
    """NLTK's recommended word tokenizer (currently the TreeBankTokenizer).
    Uses regular expressions to tokenize text. Assumes text has already been
    segmented into sentences.

    Performs the following steps:

    * split standard contractions, e.g. don't -&gt; do n't
    * split commas and single quotes
    * separate periods that appear at the end of line
    """

    @others
</t>
<t tx="karstenw.20230306120539.3">def tokenize(self, text, include_punc=True):
    '''Return a list of word tokens.

    :param text: string of text.
    :param include_punc: (optional) whether to include punctuation as separate tokens. Default to True.
    '''
    tokens = nltk.tokenize.word_tokenize(text)
    if include_punc:
        return tokens
    else:
        # Return each word token
        # Strips punctuation unless the word comes from a contraction
        # e.g. "Let's" =&gt; ["Let", "'s"]
        # e.g. "Can't" =&gt; ["Ca", "n't"]
        # e.g. "home." =&gt; ['home']
        return [word if word.startswith("'") else strip_punc(word, all=False)
                for word in tokens if strip_punc(word, all=False)]


</t>
<t tx="karstenw.20230306120539.4">class SentenceTokenizer(BaseTokenizer):
    """NLTK's sentence tokenizer (currently PunktSentenceTokenizer).
    Uses an unsupervised algorithm to build a model for abbreviation words,
    collocations, and words that start sentences,
    then uses that to find sentence boundaries.
    """

    @others
#: Convenience function for tokenizing sentences
sent_tokenize = SentenceTokenizer().itokenize

_word_tokenizer = WordTokenizer()  # Singleton word tokenizer
</t>
<t tx="karstenw.20230306120539.5">@requires_nltk_corpus
def tokenize(self, text):
    '''Return a list of sentences.'''
    return nltk.tokenize.sent_tokenize(text)


</t>
<t tx="karstenw.20230306120539.6">def word_tokenize(text, include_punc=True, *args, **kwargs):
    """Convenience function for tokenizing text into words.

    NOTE: NLTK's word tokenizer expects sentences as input, so the text will be
    tokenized to sentences before being tokenized to words.
    """
    words = chain.from_iterable(
        _word_tokenizer.itokenize(sentence, include_punc=include_punc,
                                *args, **kwargs)
        for sentence in sent_tokenize(text))
    return words
</t>
<t tx="karstenw.20230306120541.1">"""
Translator module that uses the Google Translate API.

Adapted from Terry Yin's google-translate-python.
Language detection added by Steven Loria.
"""
from __future__ import absolute_import

import codecs
import json
import re

from textblob.compat import PY2, request, urlencode
from textblob.exceptions import TranslatorError, NotTranslated


</t>
<t tx="karstenw.20230306120541.2">class Translator(object):

    """A language translator and detector.

    Usage:
    ::
        &gt;&gt;&gt; from textblob.translate import Translator
        &gt;&gt;&gt; t = Translator()
        &gt;&gt;&gt; t.translate('hello', from_lang='en', to_lang='fr')
        u'bonjour'
        &gt;&gt;&gt; t.detect("hola")
        u'es'
    """

    url = "http://translate.google.com/translate_a/t?client=webapp&amp;dt=bd&amp;dt=ex&amp;dt=ld&amp;dt=md&amp;dt=qca&amp;dt=rw&amp;dt=rm&amp;dt=ss&amp;dt=t&amp;dt=at&amp;ie=UTF-8&amp;oe=UTF-8&amp;otf=2&amp;ssel=0&amp;tsel=0&amp;kc=1"

    headers = {
        'Accept': '*/*',
        'Connection': 'keep-alive',
        'User-Agent': (
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) '
            'AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.168 Safari/535.19')
    }

    @others
</t>
<t tx="karstenw.20230306120541.3">def translate(self, source, from_lang='auto', to_lang='en', host=None, type_=None):
    """Translate the source text from one language to another."""
    if PY2:
        source = source.encode('utf-8')
    data = {"q": source}
    url = u'{url}&amp;sl={from_lang}&amp;tl={to_lang}&amp;hl={to_lang}&amp;tk={tk}&amp;client={client}'.format(
        url=self.url,
        from_lang=from_lang,
        to_lang=to_lang,
        tk=_calculate_tk(source),
        client="te",
    )
    response = self._request(url, host=host, type_=type_, data=data)
    result = json.loads(response)
    if isinstance(result, list):
        try:
            result = result[0]  # ignore detected language
        except IndexError:
            pass
    self._validate_translation(source, result)
    return result

</t>
<t tx="karstenw.20230306120541.4">def detect(self, source, host=None, type_=None):
    """Detect the source text's language."""
    if PY2:
        source = source.encode('utf-8')
    if len(source) &lt; 3:
        raise TranslatorError('Must provide a string with at least 3 characters.')
    data = {"q": source}
    url = u'{url}&amp;sl=auto&amp;tk={tk}&amp;client={client}'.format(
        url=self.url,
        tk=_calculate_tk(source),
        client="te",
    )
    response = self._request(url, host=host, type_=type_, data=data)
    result, language = json.loads(response)
    return language

</t>
<t tx="karstenw.20230306120541.5">def _validate_translation(self, source, result):
    """Validate API returned expected schema, and that the translated text
    is different than the original string.
    """
    if not result:
        raise NotTranslated('Translation API returned and empty response.')
    if PY2:
        result = result.encode('utf-8')
    if result.strip() == source.strip():
        raise NotTranslated('Translation API returned the input string unchanged.')

</t>
<t tx="karstenw.20230306120541.6">def _request(self, url, host=None, type_=None, data=None):
    encoded_data = urlencode(data).encode('utf-8')
    req = request.Request(url=url, headers=self.headers, data=encoded_data)
    if host or type_:
        req.set_proxy(host=host, type=type_)
    resp = request.urlopen(req)
    content = resp.read()
    return content.decode('utf-8')


</t>
<t tx="karstenw.20230306120541.7">def _unescape(text):
    """Unescape unicode character codes within a string.
    """
    pattern = r'\\{1,2}u[0-9a-fA-F]{4}'
    return re.sub(pattern, lambda x: codecs.getdecoder('unicode_escape')(x.group())[0], text)


</t>
<t tx="karstenw.20230306120541.8">def _calculate_tk(source):
    """Reverse engineered cross-site request protection."""
    # Source: https://github.com/soimort/translate-shell/issues/94#issuecomment-165433715
    # Source: http://www.liuxiatool.com/t.php

    def c_int(x, nbits=32):
        """ C cast to int32, int16, int8... """
        return (x &amp; ((1 &lt;&lt; (nbits - 1)) - 1)) - (x &amp; (1 &lt;&lt; (nbits - 1)))

    def c_uint(x, nbits=32):
        """ C cast to uint32, uint16, uint8... """
        return x &amp; ((1 &lt;&lt; nbits) - 1)

    tkk = [406398, 561666268 + 1526272306]
    b = tkk[0]

    if PY2:
        d = map(ord, source)
    else:
        d = source.encode('utf-8')

    def RL(a, b):
        for c in range(0, len(b) - 2, 3):
            d = b[c + 2]
            d = ord(d) - 87 if d &gt;= 'a' else int(d)
            xa = c_uint(a)
            d = xa &gt;&gt; d if b[c + 1] == '+' else xa &lt;&lt; d
            a = a + d &amp; 4294967295 if b[c] == '+' else a ^ d
        return c_int(a)

    a = b

    for di in d:
        a = RL(a + di, "+-a^+6")

    a = RL(a, "+-3^+b+-f")
    a ^= tkk[1]
    a = a if a &gt;= 0 else ((a &amp; 2147483647) + 2147483648)
    a %= pow(10, 6)

    tk = '{0:d}.{1:d}'.format(a, a ^ b)
    return tk
</t>
<t tx="karstenw.20230306120543.1">import re
import string

PUNCTUATION_REGEX = re.compile('[{0}]'.format(re.escape(string.punctuation)))


</t>
<t tx="karstenw.20230306120543.2">def strip_punc(s, all=False):
    """Removes punctuation from a string.

    :param s: The string.
    :param all: Remove all punctuation. If False, only removes punctuation from
        the ends of the string.
    """
    if all:
        return PUNCTUATION_REGEX.sub('', s.strip())
    else:
        return s.strip().strip(string.punctuation)


</t>
<t tx="karstenw.20230306120543.3">def lowerstrip(s, all=False):
    """Makes text all lowercase and strips punctuation and whitespace.

    :param s: The string.
    :param all: Remove all punctuation. If False, only removes punctuation from
        the ends of the string.
    """
    return strip_punc(s.lower().strip(), all=all)


</t>
<t tx="karstenw.20230306120543.4">def tree2str(tree, concat=' '):
    """Convert a nltk.tree.Tree to a string.

    For example:
        (NP a/DT beautiful/JJ new/JJ dashboard/NN) -&gt; "a beautiful dashboard"
    """
    return concat.join([word for (word, tag) in tree])


</t>
<t tx="karstenw.20230306120543.5">def filter_insignificant(chunk, tag_suffixes=('DT', 'CC', 'PRP$', 'PRP')):
    """Filter out insignificant (word, tag) tuples from a chunk of text."""
    good = []
    for word, tag in chunk:
        ok = True
        for suffix in tag_suffixes:
            if tag.endswith(suffix):
                ok = False
                break
        if ok:
            good.append((word, tag))
    return good


</t>
<t tx="karstenw.20230306120543.6">def is_filelike(obj):
    """Return whether ``obj`` is a file-like object."""
    return hasattr(obj, 'read')
</t>
<t tx="karstenw.20230306120545.1">"""Wordnet interface. Contains classes for creating Synsets and Lemmas
directly.

.. versionadded:: 0.7.0

"""
import nltk

#: wordnet module from nltk
wordnet = nltk.corpus.wordnet
#: Synset constructor
Synset = nltk.corpus.wordnet.synset
#: Lemma constructor
Lemma = nltk.corpus.wordnet.lemma
# Part of speech constants
VERB, NOUN, ADJ, ADV = wordnet.VERB, wordnet.NOUN, wordnet.ADJ, wordnet.ADV
</t>
<t tx="karstenw.20230306120548.1">'''This file is based on pattern.en. See the bundled NOTICE file for
license information.
'''
from __future__ import absolute_import
import os

from textblob._text import (Parser as _Parser, Sentiment as _Sentiment, Lexicon,
    WORD, POS, CHUNK, PNP, PENN, UNIVERSAL, Spelling)

from textblob.compat import text_type, unicode

try:
    MODULE = os.path.dirname(os.path.abspath(__file__))
except:
    MODULE = ""

spelling = Spelling(
        path = os.path.join(MODULE, "en-spelling.txt")
)

#--- ENGLISH PARSER --------------------------------------------------------------------------------

</t>
<t tx="karstenw.20230306120548.10">def parsetree(s, *args, **kwargs):
    """ Returns a parsed Text from the given string.
    """
    return Text(parse(unicode(s), *args, **kwargs))

</t>
<t tx="karstenw.20230306120548.11">def split(s, token=[WORD, POS, CHUNK, PNP]):
    """ Returns a parsed Text from the given parsed string.
    """
    return Text(text_type(s), token)

</t>
<t tx="karstenw.20230306120548.12">def tag(s, tokenize=True, encoding="utf-8"):
    """ Returns a list of (token, tag)-tuples from the given string.
    """
    tags = []
    for sentence in parse(s, tokenize, True, False, False, False, encoding).split():
        for token in sentence:
            tags.append((token[0], token[1]))
    return tags

</t>
<t tx="karstenw.20230306120548.13">def suggest(w):
    """ Returns a list of (word, confidence)-tuples of spelling corrections.
    """
    return spelling.suggest(w)

</t>
<t tx="karstenw.20230306120548.14">def polarity(s, **kwargs):
    """ Returns the sentence polarity (positive/negative) between -1.0 and 1.0.
    """
    return sentiment(unicode(s), **kwargs)[0]

</t>
<t tx="karstenw.20230306120548.15">def subjectivity(s, **kwargs):
    """ Returns the sentence subjectivity (objective/subjective) between 0.0 and 1.0.
    """
    return sentiment(unicode(s), **kwargs)[1]

</t>
<t tx="karstenw.20230306120548.16">def positive(s, threshold=0.1, **kwargs):
    """ Returns True if the given sentence has a positive sentiment (polarity &gt;= threshold).
    """
    return polarity(unicode(s), **kwargs) &gt;= threshold

</t>
<t tx="karstenw.20230306120548.2">def find_lemmata(tokens):
    """ Annotates the tokens with lemmata for plural nouns and conjugated verbs,
        where each token is a [word, part-of-speech] list.
    """
    for token in tokens:
        word, pos, lemma = token[0], token[1], token[0]
        # cats =&gt; cat
        if pos == "NNS":
            lemma = singularize(word)
        # sat =&gt; sit
        if pos.startswith(("VB", "MD")):
            lemma = conjugate(word, INFINITIVE) or word
        token.append(lemma.lower())
    return tokens

</t>
<t tx="karstenw.20230306120548.3">class Parser(_Parser):

    @others
</t>
<t tx="karstenw.20230306120548.4">def find_lemmata(self, tokens, **kwargs):
    return find_lemmata(tokens)

</t>
<t tx="karstenw.20230306120548.5">def find_tags(self, tokens, **kwargs):
    if kwargs.get("tagset") in (PENN, None):
        kwargs.setdefault("map", lambda token, tag: (token, tag))
    if kwargs.get("tagset") == UNIVERSAL:
        kwargs.setdefault("map", lambda token, tag: penntreebank2universal(token, tag))
    return _Parser.find_tags(self, tokens, **kwargs)

</t>
<t tx="karstenw.20230306120548.6">class Sentiment(_Sentiment):

    @others
lexicon = Lexicon(
        path = os.path.join(MODULE, "en-lexicon.txt"),
  morphology = os.path.join(MODULE, "en-morphology.txt"),
     context = os.path.join(MODULE, "en-context.txt"),
    entities = os.path.join(MODULE, "en-entities.txt"),
    language = "en"
)
parser = Parser(
     lexicon = lexicon,
     default = ("NN", "NNP", "CD"),
    language = "en"
)

sentiment = Sentiment(
        path = os.path.join(MODULE, "en-sentiment.xml"),
      synset = "wordnet_id",
   negations = ("no", "not", "n't", "never"),
   modifiers = ("RB",),
   modifier  = lambda w: w.endswith("ly"),
   tokenizer = parser.find_tokens,
    language = "en"
)


</t>
<t tx="karstenw.20230306120548.7">def load(self, path=None):
    _Sentiment.load(self, path)
    # Map "terrible" to adverb "terribly" (+1% accuracy)
    if not path:
        for w, pos in list(dict.items(self)):
            if "JJ" in pos:
                if w.endswith("y"):
                    w = w[:-1] + "i"
                if w.endswith("le"):
                    w = w[:-2]
                p, s, i = pos["JJ"]
                self.annotate(w + "ly", "RB", p, s, i)


</t>
<t tx="karstenw.20230306120548.8">def tokenize(s, *args, **kwargs):
    """ Returns a list of sentences, where punctuation marks have been split from words.
    """
    return parser.find_tokens(text_type(s), *args, **kwargs)

</t>
<t tx="karstenw.20230306120548.9">def parse(s, *args, **kwargs):
    """ Returns a tagged Unicode string.
    """
    return parser.parse(unicode(s), *args, **kwargs)

</t>
<t tx="karstenw.20230306120550.1">'''The pluralize and singular methods from the pattern library.

Licenced under the BSD.
See here https://github.com/clips/pattern/blob/master/LICENSE.txt for
complete license information.
'''
import re

VERB, NOUN, ADJECTIVE, ADVERB = "VB", "NN", "JJ", "RB"

#### PLURALIZE #####################################################################################
# Based on "An Algorithmic Approach to English Pluralization" by Damian Conway:
# http://www.csse.monash.edu.au/~damian/papers/HTML/Plurals.html

# Prepositions are used to solve things like
# "mother-in-law" or "man at arms"
plural_prepositions = [
    "about", "above", "across", "after", "among", "around", "at", "athwart", "before", "behind",
    "below", "beneath", "beside", "besides", "between", "betwixt", "beyond", "but", "by", "during",
    "except", "for", "from", "in", "into", "near", "of", "off", "on", "onto", "out", "over",
    "since", "till", "to", "under", "until", "unto", "upon", "with"
]

# Inflection rules that are either general,
# or apply to a certain category of words,
# or apply to a certain category of words only in classical mode,
# or apply only in classical mode.
# Each rule consists of:
# suffix, inflection, category and classic flag.
plural_rules = [
    # 0) Indefinite articles and demonstratives.
    [["^a$|^an$", "some", None, False],
     ["^this$", "these", None, False],
     ["^that$", "those", None, False],
     ["^any$", "all", None, False]
    ],
    # 1) Possessive adjectives.
    # Overlaps with 1/ for "his" and "its".
    # Overlaps with 2/ for "her".
    [["^my$", "our", None, False],
     ["^your$|^thy$", "your", None, False],
     ["^her$|^his$|^its$|^their$", "their", None, False]
    ],
    # 2) Possessive pronouns.
    [["^mine$", "ours", None, False],
     ["^yours$|^thine$", "yours", None, False],
     ["^hers$|^his$|^its$|^theirs$", "theirs", None, False]
    ],
    # 3) Personal pronouns.
    [["^I$", "we", None, False],
     ["^me$", "us", None, False],
     ["^myself$", "ourselves", None, False],
     ["^you$", "you", None, False],
     ["^thou$|^thee$", "ye", None, False],
     ["^yourself$|^thyself$", "yourself", None, False],
     ["^she$|^he$|^it$|^they$", "they", None, False],
     ["^her$|^him$|^it$|^them$", "them", None, False],
     ["^herself$|^himself$|^itself$|^themself$", "themselves", None, False],
     ["^oneself$", "oneselves", None, False]
    ],
    # 4) Words that do not inflect.
    [["$", "", "uninflected", False],
     ["$", "", "uncountable", False],
     ["fish$", "fish", None, False],
     ["([- ])bass$", "\\1bass", None, False],
     ["ois$", "ois", None, False],
     ["sheep$", "sheep", None, False],
     ["deer$", "deer", None, False],
     ["pox$", "pox", None, False],
     ["([A-Z].*)ese$", "\\1ese", None, False],
     ["itis$", "itis", None, False],
     ["(fruct|gluc|galact|lact|ket|malt|rib|sacchar|cellul)ose$", "\\1ose", None, False]
    ],
    # 5) Irregular plurals (mongoose, oxen).
    [["atlas$", "atlantes", None, True],
     ["atlas$", "atlases", None, False],
     ["beef$", "beeves", None, True],
     ["brother$", "brethren", None, True],
     ["child$", "children", None, False],
     ["corpus$", "corpora", None, True],
     ["corpus$", "corpuses", None, False],
     ["^cow$", "kine", None, True],
     ["ephemeris$", "ephemerides", None, False],
     ["ganglion$", "ganglia", None, True],
     ["genie$", "genii", None, True],
     ["genus$", "genera", None, False],
     ["graffito$", "graffiti", None, False],
     ["loaf$", "loaves", None, False],
     ["money$", "monies", None, True],
     ["mongoose$", "mongooses", None, False],
     ["mythos$", "mythoi", None, False],
     ["octopus$", "octopodes", None, True],
     ["opus$", "opera", None, True],
     ["opus$", "opuses", None, False],
     ["^ox$", "oxen", None, False],
     ["penis$", "penes", None, True],
     ["penis$", "penises", None, False],
     ["soliloquy$", "soliloquies", None, False],
     ["testis$", "testes", None, False],
     ["trilby$", "trilbys", None, False],
     ["turf$", "turves", None, True],
     ["numen$", "numena", None, False],
     ["occiput$", "occipita", None, True]
    ],
    # 6) Irregular inflections for common suffixes (synopses, mice, men).
    [["man$", "men", None, False],
     ["person$", "people", None, False],
     ["([lm])ouse$", "\\1ice", None, False],
     ["tooth$", "teeth", None, False],
     ["goose$", "geese", None, False],
     ["foot$", "feet", None, False],
     ["zoon$", "zoa", None, False],
     ["([csx])is$", "\\1es", None, False]
    ],
    # 7) Fully assimilated classical inflections (vertebrae, codices).
    [["ex$", "ices", "ex-ices", False],
     ["ex$", "ices", "ex-ices-classical", True],
     ["um$", "a", "um-a", False],
     ["um$", "a", "um-a-classical", True],
     ["on$", "a", "on-a", False],
     ["a$", "ae", "a-ae", False],
     ["a$", "ae", "a-ae-classical", True]
    ],
    # 8) Classical variants of modern inflections (stigmata, soprani).
    [["trix$", "trices", None, True],
     ["eau$", "eaux", None, True],
     ["ieu$", "ieu", None, True],
     ["([iay])nx$", "\\1nges", None, True],
     ["en$", "ina", "en-ina-classical", True],
     ["a$", "ata", "a-ata-classical", True],
     ["is$", "ides", "is-ides-classical", True],
     ["us$", "i", "us-i-classical", True],
     ["us$", "us", "us-us-classical", True],
     ["o$", "i", "o-i-classical", True],
     ["$", "i", "-i-classical", True],
     ["$", "im", "-im-classical", True]
    ],
    # 9) -ch, -sh and -ss and the s-singular group take -es in the plural (churches, classes, lenses).
    [["([cs])h$", "\\1hes", None, False],
     ["ss$", "sses", None, False],
     ["x$", "xes", None, False],
     ["s$", "ses", "s-singular", False]
    ],
    # 10) Certain words ending in -f or -fe take -ves in the plural (lives, wolves).
    [["([aeo]l)f$", "\\1ves", None, False],
     ["([^d]ea)f$", "\\1ves", None, False],
     ["arf$", "arves", None, False],
     ["([nlw]i)fe$", "\\1ves", None, False],
    ],
    # 11) -y takes -ys if preceded by a vowel or when a proper noun,
    # but -ies if preceded by a consonant (storeys, Marys, stories).
    [["([aeiou])y$", "\\1ys", None, False],
     ["([A-Z].*)y$", "\\1ys", None, False],
     ["y$", "ies", None, False]
    ],
    # 12) Some words ending in -o take -os, the rest take -oes.
    # Words in which the -o is preceded by a vowel always take -os (lassos, potatoes, bamboos).
    [["o$", "os", "o-os", False],
     ["([aeiou])o$", "\\1os", None, False],
     ["o$", "oes", None, False]
    ],
    # 13) Miltary stuff (Major Generals).
    [["l$", "ls", "general-generals", False]
    ],
    # 14) Otherwise, assume that the plural just adds -s (cats, programmes).
    [["$", "s", None, False]
    ],
]

# For performance, compile the regular expressions only once:
for ruleset in plural_rules:
    for rule in ruleset:
        rule[0] = re.compile(rule[0])

# Suffix categories.
plural_categories = {
    "uninflected": [
        "aircraft", "antelope", "bison", "bream", "breeches", "britches", "carp", "cattle", "chassis",
        "clippers", "cod", "contretemps", "corps", "debris", "diabetes", "djinn", "eland", "elk",
        "flounder", "gallows", "graffiti", "headquarters", "herpes", "high-jinks", "homework", "innings",
        "jackanapes", "mackerel", "measles", "mews", "moose", "mumps", "offspring", "news", "pincers",
        "pliers", "proceedings", "rabies", "salmon", "scissors", "series", "shears", "species", "swine",
        "trout", "tuna", "whiting", "wildebeest"],
    "uncountable": [
        "advice", "bread", "butter", "cannabis", "cheese", "electricity", "equipment", "fruit", "furniture",
        "garbage", "gravel", "happiness", "information", "ketchup", "knowledge", "love", "luggage",
        "mathematics", "mayonnaise", "meat", "mustard", "news", "progress", "research", "rice",
        "sand", "software", "understanding", "water"],
    "s-singular": [
        "acropolis", "aegis", "alias", "asbestos", "bathos", "bias", "bus", "caddis", "canvas",
        "chaos", "christmas", "cosmos", "dais", "digitalis", "epidermis", "ethos", "gas", "glottis",
        "ibis", "lens", "mantis", "marquis", "metropolis", "pathos", "pelvis", "polis", "rhinoceros",
        "sassafras", "trellis"],
    "ex-ices": ["codex", "murex", "silex"],
    "ex-ices-classical": [
        "apex", "cortex", "index", "latex", "pontifex", "simplex", "vertex", "vortex"],
    "um-a": [
        "agendum", "bacterium", "candelabrum", "datum", "desideratum", "erratum", "extremum",
        "ovum", "stratum"],
    "um-a-classical": [
        "aquarium", "compendium", "consortium", "cranium", "curriculum", "dictum", "emporium",
        "enconium", "gymnasium", "honorarium", "interregnum", "lustrum", "maximum", "medium",
        "memorandum", "millenium", "minimum", "momentum", "optimum", "phylum", "quantum", "rostrum",
        "spectrum", "speculum", "stadium", "trapezium", "ultimatum", "vacuum", "velum"],
    "on-a": [
        "aphelion", "asyndeton", "criterion", "hyperbaton", "noumenon", "organon", "perihelion",
        "phenomenon", "prolegomenon"],
    "a-ae": ["alga", "alumna", "vertebra"],
    "a-ae-classical": [
        "abscissa", "amoeba", "antenna", "aurora", "formula", "hydra", "hyperbola", "lacuna",
        "medusa", "nebula", "nova", "parabola"],
    "en-ina-classical": ["foramen", "lumen", "stamen"],
    "a-ata-classical": [
        "anathema", "bema", "carcinoma", "charisma", "diploma", "dogma", "drama", "edema", "enema",
        "enigma", "gumma", "lemma", "lymphoma", "magma", "melisma", "miasma", "oedema", "sarcoma",
        "schema", "soma", "stigma", "stoma", "trauma"],
    "is-ides-classical": ["clitoris", "iris"],
    "us-i-classical": [
        "focus", "fungus", "genius", "incubus", "nimbus", "nucleolus", "radius", "stylus", "succubus",
        "torus", "umbilicus", "uterus"],
    "us-us-classical": [
        "apparatus", "cantus", "coitus", "hiatus", "impetus", "nexus", "plexus", "prospectus",
        "sinus", "status"],
    "o-i-classical": ["alto", "basso", "canto", "contralto", "crescendo", "solo", "soprano", "tempo"],
    "-i-classical": ["afreet", "afrit", "efreet"],
    "-im-classical": ["cherub", "goy", "seraph"],
    "o-os": [
        "albino", "archipelago", "armadillo", "commando", "ditto", "dynamo", "embryo", "fiasco",
        "generalissimo", "ghetto", "guano", "inferno", "jumbo", "lingo", "lumbago", "magneto",
        "manifesto", "medico", "octavo", "photo", "pro", "quarto", "rhino", "stylo"],
    "general-generals": [
        "Adjutant", "Brigadier", "Lieutenant", "Major", "Quartermaster",
        "adjutant", "brigadier", "lieutenant", "major", "quartermaster"],
}

</t>
<t tx="karstenw.20230306120550.2">def pluralize(word, pos=NOUN, custom={}, classical=True):
    """ Returns the plural of a given word.
        For example: child -&gt; children.
        Handles nouns and adjectives, using classical inflection by default
        (e.g. where "matrix" pluralizes to "matrices" instead of "matrixes").
        The custom dictionary is for user-defined replacements.
    """

    if word in custom:
        return custom[word]

    # Recursion of genitives.
    # Remove the apostrophe and any trailing -s,
    # form the plural of the resultant noun, and then append an apostrophe (dog's -&gt; dogs').
    if word.endswith("'") or word.endswith("'s"):
        owner = word.rstrip("'s")
        owners = pluralize(owner, pos, custom, classical)
        if owners.endswith("s"):
            return owners + "'"
        else:
            return owners + "'s"

    # Recursion of compound words
    # (Postmasters General, mothers-in-law, Roman deities).
    words = word.replace("-", " ").split(" ")
    if len(words) &gt; 1:
        if words[1] == "general" or words[1] == "General" and \
            words[0] not in plural_categories["general-generals"]:
            return word.replace(words[0], pluralize(words[0], pos, custom, classical))
        elif words[1] in plural_prepositions:
            return word.replace(words[0], pluralize(words[0], pos, custom, classical))
        else:
            return word.replace(words[-1], pluralize(words[-1], pos, custom, classical))

    # Only a very few number of adjectives inflect.
    n = list(range(len(plural_rules)))
    if pos.startswith(ADJECTIVE):
        n = [0, 1]

    # Apply pluralization rules.
    for i in n:
        ruleset = plural_rules[i]
        for rule in ruleset:
            suffix, inflection, category, classic = rule
            # A general rule, or a classic rule in classical mode.
            if category == None:
                if not classic or (classic and classical):
                    if suffix.search(word) is not None:
                        return suffix.sub(inflection, word)
            # A rule relating to a specific category of words.
            if category != None:
                if word in plural_categories[category] and (not classic or (classic and classical)):
                    if suffix.search(word) is not None:
                        return suffix.sub(inflection, word)

#### SINGULARIZE ###################################################################################
# Adapted from Bermi Ferrer's Inflector for Python:
# http://www.bermi.org/inflector/

# Copyright (c) 2006 Bermi Ferrer Martinez
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software to deal in this software without restriction, including
# without limitation the rights to use, copy, modify, merge, publish,
# distribute, sublicense, and/or sell copies of this software, and to permit
# persons to whom this software is furnished to do so, subject to the following
# condition:
#
# THIS SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THIS SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THIS SOFTWARE.

singular_rules = [
    ['(?i)(.)ae$', '\\1a'],
    ['(?i)(.)itis$', '\\1itis'],
    ['(?i)(.)eaux$', '\\1eau'],
    ['(?i)(quiz)zes$', '\\1'],
    ['(?i)(matr)ices$', '\\1ix'],
    ['(?i)(ap|vert|ind)ices$', '\\1ex'],
    ['(?i)^(ox)en', '\\1'],
    ['(?i)(alias|status)es$', '\\1'],
    ['(?i)([octop|vir])i$', '\\1us'],
    ['(?i)(cris|ax|test)es$', '\\1is'],
    ['(?i)(shoe)s$', '\\1'],
    ['(?i)(o)es$', '\\1'],
    ['(?i)(bus)es$', '\\1'],
    ['(?i)([m|l])ice$', '\\1ouse'],
    ['(?i)(x|ch|ss|sh)es$', '\\1'],
    ['(?i)(m)ovies$', '\\1ovie'],
    ['(?i)(.)ombies$', '\\1ombie'],
    ['(?i)(s)eries$', '\\1eries'],
    ['(?i)([^aeiouy]|qu)ies$', '\\1y'],
    # Certain words ending in -f or -fe take -ves in the plural (lives, wolves).
    ["([aeo]l)ves$", "\\1f"],
    ["([^d]ea)ves$", "\\1f"],
    ["arves$", "arf"],
    ["erves$", "erve"],
    ["([nlw]i)ves$", "\\1fe"],
    ['(?i)([lr])ves$', '\\1f'],
    ["([aeo])ves$", "\\1ve"],
    ['(?i)(sive)s$', '\\1'],
    ['(?i)(tive)s$', '\\1'],
    ['(?i)(hive)s$', '\\1'],
    ['(?i)([^f])ves$', '\\1fe'],
    # -es suffix.
    ['(?i)(^analy)ses$', '\\1sis'],
    ['(?i)((a)naly|(b)a|(d)iagno|(p)arenthe|(p)rogno|(s)ynop|(t)he)ses$', '\\1\\2sis'],
    ['(?i)(.)opses$', '\\1opsis'],
    ['(?i)(.)yses$', '\\1ysis'],
    ['(?i)(h|d|r|o|n|b|cl|p)oses$', '\\1ose'],
    ['(?i)(fruct|gluc|galact|lact|ket|malt|rib|sacchar|cellul)ose$', '\\1ose'],
    ['(?i)(.)oses$', '\\1osis'],
    # -a
    ['(?i)([ti])a$', '\\1um'],
    ['(?i)(n)ews$', '\\1ews'],
    ['(?i)s$', ''],
]

# For performance, compile the regular expressions only once:
for rule in singular_rules:
    rule[0] = re.compile(rule[0])

singular_uninflected = [
    "aircraft", "antelope", "bison", "bream", "breeches", "britches", "carp", "cattle", "chassis",
    "clippers", "cod", "contretemps", "corps", "debris", "diabetes", "djinn", "eland",
    "elk", "flounder", "gallows", "georgia", "graffiti", "headquarters", "herpes", "high-jinks",
    "homework", "innings", "jackanapes", "mackerel", "measles", "mews", "moose", "mumps", "news",
    "offspring", "pincers", "pliers", "proceedings", "rabies", "salmon", "scissors", "series",
    "shears", "species", "swine", "swiss", "trout", "tuna", "whiting", "wildebeest"
]
singular_uncountable = [
    "advice", "bread", "butter", "cannabis", "cheese", "electricity", "equipment", "fruit", "furniture",
    "garbage", "gravel", "happiness", "information", "ketchup", "knowledge", "love", "luggage",
    "mathematics", "mayonnaise", "meat", "mustard", "news", "progress", "research", "rice", "sand",
    "software", "understanding", "water"
]
singular_ie = [
    "algerie", "auntie", "beanie", "birdie", "bogie", "bombie", "bookie", "collie", "cookie", "cutie",
    "doggie", "eyrie", "freebie", "goonie", "groupie", "hankie", "hippie", "hoagie", "hottie",
    "indie", "junkie", "laddie", "laramie", "lingerie", "meanie", "nightie", "oldie", "^pie",
    "pixie", "quickie", "reverie", "rookie", "softie", "sortie", "stoolie", "sweetie", "techie",
    "^tie", "toughie", "valkyrie", "veggie", "weenie", "yuppie", "zombie"
]
singular_s = plural_categories['s-singular']

# key plural, value singular
singular_irregular = {
            "men": "man",
         "people": "person",
       "children": "child",
          "sexes": "sex",
           "axes": "axe",
          "moves": "move",
          "teeth": "tooth",
          "geese": "goose",
           "feet": "foot",
            "zoa": "zoon",
       "atlantes": "atlas",
        "atlases": "atlas",
         "beeves": "beef",
       "brethren": "brother",
       "children": "child",
        "corpora": "corpus",
       "corpuses": "corpus",
           "kine": "cow",
    "ephemerides": "ephemeris",
        "ganglia": "ganglion",
          "genii": "genie",
         "genera": "genus",
       "graffiti": "graffito",
         "helves": "helve",
         "leaves": "leaf",
         "loaves": "loaf",
         "monies": "money",
      "mongooses": "mongoose",
         "mythoi": "mythos",
      "octopodes": "octopus",
          "opera": "opus",
         "opuses": "opus",
           "oxen": "ox",
          "penes": "penis",
        "penises": "penis",
    "soliloquies": "soliloquy",
         "testes": "testis",
        "trilbys": "trilby",
         "turves": "turf",
         "numena": "numen",
       "occipita": "occiput",
            "our": "my",
}

</t>
<t tx="karstenw.20230306120550.3">def singularize(word, pos=NOUN, custom={}):

    if word in list(custom.keys()):
        return custom[word]

    # Recursion of compound words (e.g. mothers-in-law).
    if "-" in word:
        words = word.split("-")
        if len(words) &gt; 1 and words[1] in plural_prepositions:
            return singularize(words[0], pos, custom)+"-"+"-".join(words[1:])
    # dogs' =&gt; dog's
    if word.endswith("'"):
        return singularize(word[:-1]) + "'s"

    lower = word.lower()
    for w in singular_uninflected:
        if w.endswith(lower):
            return word
    for w in singular_uncountable:
        if w.endswith(lower):
            return word
    for w in singular_ie:
        if lower.endswith(w+"s"):
            return w
    for w in singular_s:
        if lower.endswith(w + 'es'):
            return w
    for w in list(singular_irregular.keys()):
        if lower.endswith(w):
            return re.sub('(?i)'+w+'$', singular_irregular[w], word)

    for rule in singular_rules:
        suffix, inflection = rule
        match = suffix.search(word)
        if match:
            groups = match.groups()
            for k in range(0, len(groups)):
                if groups[k] == None:
                    inflection = inflection.replace('\\'+str(k+1), '')
            return suffix.sub(inflection, word)

    return word
</t>
<t tx="karstenw.20230306120552.1">'''Various noun phrase extractors.'''
from __future__ import unicode_literals, absolute_import

import nltk

from textblob.taggers import PatternTagger
from textblob.decorators import requires_nltk_corpus
from textblob.utils import tree2str, filter_insignificant
from textblob.base import BaseNPExtractor


</t>
<t tx="karstenw.20230306120552.10">class FastNPExtractor(BaseNPExtractor):

    '''A fast and simple noun phrase extractor.

    Credit to Shlomi Babluk. Link to original blog post:

        http://thetokenizer.com/2013/05/09/efficient-way-to-extract-the-main-topics-of-a-sentence/
    '''

    CFG = {
        ('NNP', 'NNP'): 'NNP',
        ('NN', 'NN'): 'NNI',
        ('NNI', 'NN'): 'NNI',
        ('JJ', 'JJ'): 'JJ',
        ('JJ', 'NN'): 'NNI',
        }

    @others
### Utility methods ###

</t>
<t tx="karstenw.20230306120552.11">def __init__(self):
    self._trained = False

</t>
<t tx="karstenw.20230306120552.12">@requires_nltk_corpus
def train(self):
    train_data = nltk.corpus.brown.tagged_sents(categories='news')
    regexp_tagger = nltk.RegexpTagger([
        (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),
        (r'(-|:|;)$', ':'),
        (r'\'*$', 'MD'),
        (r'(The|the|A|a|An|an)$', 'AT'),
        (r'.*able$', 'JJ'),
        (r'^[A-Z].*$', 'NNP'),
        (r'.*ness$', 'NN'),
        (r'.*ly$', 'RB'),
        (r'.*s$', 'NNS'),
        (r'.*ing$', 'VBG'),
        (r'.*ed$', 'VBD'),
        (r'.*', 'NN'),
        ])
    unigram_tagger = nltk.UnigramTagger(train_data, backoff=regexp_tagger)
    self.tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)
    self._trained = True
    return None


</t>
<t tx="karstenw.20230306120552.13">def _tokenize_sentence(self, sentence):
    '''Split the sentence into single words/tokens'''
    tokens = nltk.word_tokenize(sentence)
    return tokens

</t>
<t tx="karstenw.20230306120552.14">def extract(self, sentence):
    '''Return a list of noun phrases (strings) for body of text.'''
    if not self._trained:
        self.train()
    tokens = self._tokenize_sentence(sentence)
    tagged = self.tagger.tag(tokens)
    tags = _normalize_tags(tagged)
    merge = True
    while merge:
        merge = False
        for x in range(0, len(tags) - 1):
            t1 = tags[x]
            t2 = tags[x + 1]
            key = t1[1], t2[1]
            value = self.CFG.get(key, '')
            if value:
                merge = True
                tags.pop(x)
                tags.pop(x)
                match = '%s %s' % (t1[0], t2[0])
                pos = value
                tags.insert(x, (match, pos))
                break

    matches = [t[0] for t in tags if t[1] in ['NNP', 'NNI']]
    return matches


</t>
<t tx="karstenw.20230306120552.15">def _normalize_tags(chunk):
    '''Normalize the corpus tags.
    ("NN", "NN-PL", "NNS") -&gt; "NN"
    '''
    ret = []
    for word, tag in chunk:
        if tag == 'NP-TL' or tag == 'NP':
            ret.append((word, 'NNP'))
            continue
        if tag.endswith('-TL'):
            ret.append((word, tag[:-3]))
            continue
        if tag.endswith('S'):
            ret.append((word, tag[:-1]))
            continue
        ret.append((word, tag))
    return ret


</t>
<t tx="karstenw.20230306120552.16">def _is_match(tagged_phrase, cfg):
    '''Return whether or not a tagged phrases matches a context-free grammar.
    '''
    copy = list(tagged_phrase)  # A copy of the list
    merge = True
    while merge:
        merge = False
        for i in range(len(copy) - 1):
            first, second = copy[i], copy[i + 1]
            key = first[1], second[1]  # Tuple of tags e.g. ('NN', 'JJ')
            value = cfg.get(key, None)
            if value:
                merge = True
                copy.pop(i)
                copy.pop(i)
                match = '{0} {1}'.format(first[0], second[0])
                pos = value
                copy.insert(i, (match, pos))
                break
    match = any([t[1] in ('NNP', 'NNI') for t in copy])
    return match
</t>
<t tx="karstenw.20230306120552.2">class ChunkParser(nltk.ChunkParserI):

    @others
</t>
<t tx="karstenw.20230306120552.3">def __init__(self):
    self._trained = False

</t>
<t tx="karstenw.20230306120552.4">@requires_nltk_corpus
def train(self):
    '''Train the Chunker on the ConLL-2000 corpus.'''
    train_data = [[(t, c) for _, t, c in nltk.chunk.tree2conlltags(sent)]
                  for sent in
                  nltk.corpus.conll2000.chunked_sents('train.txt',
                                                chunk_types=['NP'])]
    unigram_tagger = nltk.UnigramTagger(train_data)
    self.tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)
    self._trained = True

</t>
<t tx="karstenw.20230306120552.5">def parse(self, sentence):
    '''Return the parse tree for the sentence.'''
    if not self._trained:
        self.train()
    pos_tags = [pos for (word, pos) in sentence]
    tagged_pos_tags = self.tagger.tag(pos_tags)
    chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]
    conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in
                 zip(sentence, chunktags)]
    return nltk.chunk.util.conlltags2tree(conlltags)


</t>
<t tx="karstenw.20230306120552.6">class ConllExtractor(BaseNPExtractor):

    '''A noun phrase extractor that uses chunk parsing trained with the
    ConLL-2000 training corpus.
    '''

    POS_TAGGER = PatternTagger()

    # The context-free grammar with which to filter the noun phrases
    CFG = {
        ('NNP', 'NNP'): 'NNP',
        ('NN', 'NN'): 'NNI',
        ('NNI', 'NN'): 'NNI',
        ('JJ', 'JJ'): 'JJ',
        ('JJ', 'NN'): 'NNI',
        }

    # POS suffixes that will be ignored
    INSIGNIFICANT_SUFFIXES = ['DT', 'CC', 'PRP$', 'PRP']

    @others
</t>
<t tx="karstenw.20230306120552.7">def __init__(self, parser=None):
    self.parser = ChunkParser() if not parser else parser

</t>
<t tx="karstenw.20230306120552.8">def extract(self, text):
    '''Return a list of noun phrases (strings) for body of text.'''
    sentences = nltk.tokenize.sent_tokenize(text)
    noun_phrases = []
    for sentence in sentences:
        parsed = self._parse_sentence(sentence)
        # Get the string representation of each subtree that is a
        # noun phrase tree
        phrases = [_normalize_tags(filter_insignificant(each,
                   self.INSIGNIFICANT_SUFFIXES)) for each in parsed
                   if isinstance(each, nltk.tree.Tree) and each.label()
                   == 'NP' and len(filter_insignificant(each)) &gt;= 1
                   and _is_match(each, cfg=self.CFG)]
        nps = [tree2str(phrase) for phrase in phrases]
        noun_phrases.extend(nps)
    return noun_phrases

</t>
<t tx="karstenw.20230306120552.9">def _parse_sentence(self, sentence):
    '''Tag and parse a sentence (a plain, untagged string).'''
    tagged = self.POS_TAGGER.tag(sentence)
    return self.parser.parse(tagged)


</t>
<t tx="karstenw.20230306120554.1">"""Various parser implementations.

.. versionadded:: 0.6.0
"""
from __future__ import absolute_import
from textblob.en import parse as pattern_parse
from textblob.base import BaseParser


</t>
<t tx="karstenw.20230306120554.2">class PatternParser(BaseParser):
    """Parser that uses the implementation in Tom de Smedt's pattern library.
    http://www.clips.ua.ac.be/pages/pattern-en#parser
    """

    @others
</t>
<t tx="karstenw.20230306120554.3">def parse(self, text):
    """Parses the text."""
    return pattern_parse(text)
</t>
<t tx="karstenw.20230306120625.1">"""Sentiment analysis implementations.

.. versionadded:: 0.5.0
"""
from __future__ import absolute_import
from collections import namedtuple

import nltk

from textblob.en import sentiment as pattern_sentiment
from textblob.tokenizers import word_tokenize
from textblob.decorators import requires_nltk_corpus
from textblob.base import BaseSentimentAnalyzer, DISCRETE, CONTINUOUS


</t>
<t tx="karstenw.20230306120625.2">class PatternAnalyzer(BaseSentimentAnalyzer):
    """Sentiment analyzer that uses the same implementation as the
    pattern library. Returns results as a named tuple of the form:

    ``Sentiment(polarity, subjectivity, [assessments])``

    where [assessments] is a list of the assessed tokens and their
    polarity and subjectivity scores
    """
    kind = CONTINUOUS
    # This is only here for backwards-compatibility.
    # The return type is actually determined upon calling analyze()
    RETURN_TYPE = namedtuple('Sentiment', ['polarity', 'subjectivity'])

    @others
</t>
<t tx="karstenw.20230306120625.3">def analyze(self, text, keep_assessments=False):
    """Return the sentiment as a named tuple of the form:
    ``Sentiment(polarity, subjectivity, [assessments])``.
    """
    #: Return type declaration
    if keep_assessments:
        Sentiment = namedtuple('Sentiment', ['polarity', 'subjectivity', 'assessments'])
        assessments = pattern_sentiment(text).assessments
        polarity, subjectivity = pattern_sentiment(text)
        return Sentiment(polarity, subjectivity, assessments)

    else:
        Sentiment = namedtuple('Sentiment', ['polarity', 'subjectivity'])
        return Sentiment(*pattern_sentiment(text))


</t>
<t tx="karstenw.20230306120625.4">def _default_feature_extractor(words):
    """Default feature extractor for the NaiveBayesAnalyzer."""
    return dict(((word, True) for word in words))


</t>
<t tx="karstenw.20230306120625.5">class NaiveBayesAnalyzer(BaseSentimentAnalyzer):
    """Naive Bayes analyzer that is trained on a dataset of movie reviews.
    Returns results as a named tuple of the form:
    ``Sentiment(classification, p_pos, p_neg)``

    :param callable feature_extractor: Function that returns a dictionary of
        features, given a list of words.
    """

    kind = DISCRETE
    #: Return type declaration
    RETURN_TYPE = namedtuple('Sentiment', ['classification', 'p_pos', 'p_neg'])

    @others
</t>
<t tx="karstenw.20230306120625.6">def __init__(self, feature_extractor=_default_feature_extractor):
    super(NaiveBayesAnalyzer, self).__init__()
    self._classifier = None
    self.feature_extractor = feature_extractor

</t>
<t tx="karstenw.20230306120625.7">@requires_nltk_corpus
def train(self):
    """Train the Naive Bayes classifier on the movie review corpus."""
    super(NaiveBayesAnalyzer, self).train()
    neg_ids = nltk.corpus.movie_reviews.fileids('neg')
    pos_ids = nltk.corpus.movie_reviews.fileids('pos')
    neg_feats = [(self.feature_extractor(
        nltk.corpus.movie_reviews.words(fileids=[f])), 'neg') for f in neg_ids]
    pos_feats = [(self.feature_extractor(
        nltk.corpus.movie_reviews.words(fileids=[f])), 'pos') for f in pos_ids]
    train_data = neg_feats + pos_feats
    self._classifier = nltk.classify.NaiveBayesClassifier.train(train_data)

</t>
<t tx="karstenw.20230306120625.8">def analyze(self, text):
    """Return the sentiment as a named tuple of the form:
    ``Sentiment(classification, p_pos, p_neg)``
    """
    # Lazily train the classifier
    super(NaiveBayesAnalyzer, self).analyze(text)
    tokens = word_tokenize(text, include_punc=False)
    filtered = (t.lower() for t in tokens if len(t) &gt;= 3)
    feats = self.feature_extractor(filtered)
    prob_dist = self._classifier.prob_classify(feats)
    return self.RETURN_TYPE(
        classification=prob_dist.max(),
        p_pos=prob_dist.prob('pos'),
        p_neg=prob_dist.prob("neg")
    )
</t>
<t tx="karstenw.20230306120628.1">"""Parts-of-speech tagger implementations."""
from __future__ import absolute_import

import nltk
import textblob.compat

import textblob as tb
from textblob.en import tag as pattern_tag
from textblob.decorators import requires_nltk_corpus
from textblob.base import BaseTagger


</t>
<t tx="karstenw.20230306120628.2">class PatternTagger(BaseTagger):
    """Tagger that uses the implementation in
    Tom de Smedt's pattern library
    (http://www.clips.ua.ac.be/pattern).
    """

    @others
</t>
<t tx="karstenw.20230306120628.3">def tag(self, text, tokenize=True):
    """Tag a string or BaseBlob."""
    if not isinstance(text, textblob.compat.text_type):
        text = text.raw
    return pattern_tag(text, tokenize)


</t>
<t tx="karstenw.20230306120628.4">class NLTKTagger(BaseTagger):
    """Tagger that uses NLTK's standard TreeBank tagger.
    NOTE: Requires numpy. Not yet supported with PyPy.
    """

    @others
</t>
<t tx="karstenw.20230306120628.5">@requires_nltk_corpus
def tag(self, text):
    """Tag a string or BaseBlob."""
    if isinstance(text, textblob.compat.text_type):
        text = tb.TextBlob(text)

    return nltk.tag.pos_tag(text.tokens)
</t>
<t tx="karstenw.20230306120631.1">import csv
from textblob.compat import izip

#http://semver.org/
VERSION = (0, 9, 4)
__version__ = ".".join(map(str,VERSION))

pass_throughs = [
    'register_dialect',
    'unregister_dialect',
    'get_dialect',
    'list_dialects',
    'field_size_limit',
    'Dialect',
    'excel',
    'excel_tab',
    'Sniffer',
    'QUOTE_ALL',
    'QUOTE_MINIMAL',
    'QUOTE_NONNUMERIC',
    'QUOTE_NONE',
    'Error'
]
__all__ = [
    'reader',
    'writer',
    'DictReader',
    'DictWriter',
] + pass_throughs

for prop in pass_throughs:
    globals()[prop]=getattr(csv, prop)

</t>
<t tx="karstenw.20230306120631.10">class UnicodeReader(object):
    @others
reader = UnicodeReader

</t>
<t tx="karstenw.20230306120631.11">def __init__(self, f, dialect=None, encoding='utf-8', errors='strict',
             **kwds):
    format_params = ['delimiter', 'doublequote', 'escapechar', 'lineterminator', 'quotechar', 'quoting', 'skipinitialspace']
    if dialect is None:
        if not any([kwd_name in format_params for kwd_name in kwds.keys()]):
            dialect = csv.excel
    self.reader = csv.reader(f, dialect, **kwds)
    self.encoding = encoding
    self.encoding_errors = errors

</t>
<t tx="karstenw.20230306120631.12">def next(self):
    row = self.reader.next()
    encoding = self.encoding
    encoding_errors = self.encoding_errors
    float_ = float
    unicode_ = unicode
    return [(value if isinstance(value, float_) else
             unicode_(value, encoding, encoding_errors)) for value in row]

</t>
<t tx="karstenw.20230306120631.13">def __iter__(self):
    return self

</t>
<t tx="karstenw.20230306120631.14">@property
def dialect(self):
    return self.reader.dialect

</t>
<t tx="karstenw.20230306120631.15">@property
def line_num(self):
    return self.reader.line_num
</t>
<t tx="karstenw.20230306120631.16">class DictWriter(csv.DictWriter):
    """
    &gt;&gt;&gt; from cStringIO import StringIO
    &gt;&gt;&gt; f = StringIO()
    &gt;&gt;&gt; w = DictWriter(f, ['a', u'ñ', 'b'], restval=u'î')
    &gt;&gt;&gt; w.writerow({'a':'1', u'ñ':'2'})
    &gt;&gt;&gt; w.writerow({'a':'1', u'ñ':'2', 'b':u'ø'})
    &gt;&gt;&gt; w.writerow({'a':u'é', u'ñ':'2'})
    &gt;&gt;&gt; f.seek(0)
    &gt;&gt;&gt; r = DictReader(f, fieldnames=['a', u'ñ'], restkey='r')
    &gt;&gt;&gt; r.next() == {'a': u'1', u'ñ':'2', 'r': [u'î']}
    True
    &gt;&gt;&gt; r.next() == {'a': u'1', u'ñ':'2', 'r': [u'\xc3\xb8']}
    True
    &gt;&gt;&gt; r.next() == {'a': u'\xc3\xa9', u'ñ':'2', 'r': [u'\xc3\xae']}
    True
    """
    @others
</t>
<t tx="karstenw.20230306120631.17">def __init__(self, csvfile, fieldnames, restval='', extrasaction='raise', dialect='excel', encoding='utf-8', errors='strict', *args, **kwds):
    self.encoding = encoding
    csv.DictWriter.__init__(self, csvfile, fieldnames, restval, extrasaction, dialect, *args, **kwds)
    self.writer = UnicodeWriter(csvfile, dialect, encoding=encoding, errors=errors, *args, **kwds)
    self.encoding_errors = errors

</t>
<t tx="karstenw.20230306120631.18">def writeheader(self):
    fieldnames = _stringify_list(self.fieldnames, self.encoding, self.encoding_errors)
    header = dict(zip(self.fieldnames, self.fieldnames))
    self.writerow(header)

</t>
<t tx="karstenw.20230306120631.19">class DictReader(csv.DictReader):
    """
    &gt;&gt;&gt; from cStringIO import StringIO
    &gt;&gt;&gt; f = StringIO()
    &gt;&gt;&gt; w = DictWriter(f, fieldnames=['name', 'place'])
    &gt;&gt;&gt; w.writerow({'name': 'Cary Grant', 'place': 'hollywood'})
    &gt;&gt;&gt; w.writerow({'name': 'Nathan Brillstone', 'place': u'øLand'})
    &gt;&gt;&gt; w.writerow({'name': u'Willam ø. Unicoder', 'place': u'éSpandland'})
    &gt;&gt;&gt; f.seek(0)
    &gt;&gt;&gt; r = DictReader(f, fieldnames=['name', 'place'])
    &gt;&gt;&gt; print r.next() == {'name': 'Cary Grant', 'place': 'hollywood'}
    True
    &gt;&gt;&gt; print r.next() == {'name': 'Nathan Brillstone', 'place': u'øLand'}
    True
    &gt;&gt;&gt; print r.next() == {'name': u'Willam ø. Unicoder', 'place': u'éSpandland'}
    True
    """
    @others
</t>
<t tx="karstenw.20230306120631.2">def _stringify(s, encoding, errors):
    if s is None:
        return ''
    if isinstance(s, unicode):
        return s.encode(encoding, errors)
    elif isinstance(s, (int , float)):
        pass #let csv.QUOTE_NONNUMERIC do its thing.
    elif not isinstance(s, str):
        s=str(s)
    return s

</t>
<t tx="karstenw.20230306120631.20">def __init__(self, csvfile, fieldnames=None, restkey=None, restval=None,
             dialect='excel', encoding='utf-8', errors='strict', *args,
             **kwds):
    if fieldnames is not None:
        fieldnames = _stringify_list(fieldnames, encoding)
    csv.DictReader.__init__(self, csvfile, fieldnames, restkey, restval, dialect, *args, **kwds)
    self.reader = UnicodeReader(csvfile, dialect, encoding=encoding,
                                errors=errors, *args, **kwds)
    if fieldnames is None and not hasattr(csv.DictReader, 'fieldnames'):
        # Python 2.5 fieldnames workaround. (http://bugs.python.org/issue3436)
        reader = UnicodeReader(csvfile, dialect, encoding=encoding, *args, **kwds)
        self.fieldnames = _stringify_list(reader.next(), reader.encoding)
    self.unicode_fieldnames = [_unicodify(f, encoding) for f in
                               self.fieldnames]
    self.unicode_restkey = _unicodify(restkey, encoding)

</t>
<t tx="karstenw.20230306120631.21">def next(self):
    row = csv.DictReader.next(self)
    result = dict((uni_key, row[str_key]) for (str_key, uni_key) in
                  izip(self.fieldnames, self.unicode_fieldnames))
    rest = row.get(self.restkey)
    if rest:
        result[self.unicode_restkey] = rest
    return result
</t>
<t tx="karstenw.20230306120631.3">def _stringify_list(l, encoding, errors='strict'):
    try:
        return [_stringify(s, encoding, errors) for s in iter(l)]
    except TypeError as e:
        raise csv.Error(str(e))

</t>
<t tx="karstenw.20230306120631.4">def _unicodify(s, encoding):
    if s is None:
        return None
    if isinstance(s, (unicode, int, float)):
        return s
    elif isinstance(s, str):
        return s.decode(encoding)
    return s

</t>
<t tx="karstenw.20230306120631.5">class UnicodeWriter(object):
    """
    &gt;&gt;&gt; import unicodecsv
    &gt;&gt;&gt; from cStringIO import StringIO
    &gt;&gt;&gt; f = StringIO()
    &gt;&gt;&gt; w = unicodecsv.writer(f, encoding='utf-8')
    &gt;&gt;&gt; w.writerow((u'é', u'ñ'))
    &gt;&gt;&gt; f.seek(0)
    &gt;&gt;&gt; r = unicodecsv.reader(f, encoding='utf-8')
    &gt;&gt;&gt; row = r.next()
    &gt;&gt;&gt; row[0] == u'é'
    True
    &gt;&gt;&gt; row[1] == u'ñ'
    True
    """
    @others
writer = UnicodeWriter

</t>
<t tx="karstenw.20230306120631.6">def __init__(self, f, dialect=csv.excel, encoding='utf-8', errors='strict',
             *args, **kwds):
    self.encoding = encoding
    self.writer = csv.writer(f, dialect, *args, **kwds)
    self.encoding_errors = errors

</t>
<t tx="karstenw.20230306120631.7">def writerow(self, row):
    self.writer.writerow(_stringify_list(row, self.encoding, self.encoding_errors))

</t>
<t tx="karstenw.20230306120631.8">def writerows(self, rows):
    for row in rows:
      self.writerow(row)

</t>
<t tx="karstenw.20230306120631.9">@property
def dialect(self):
    return self.writer.dialect
</t>
</tnodes>
</leo_file>
