<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: http://leoeditor.com/leo_toc.html -->
<leo_file xmlns:leo="http://leoeditor.com/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2" tnodes="0" max_tnode_index="0" clone_windows="0"/>
<globals body_outline_ratio="0.5" body_secondary_ratio="0.5">
	<global_window_position top="50" left="50" height="500" width="700"/>
	<global_log_window_position top="0" left="0" height="0" width="0"/>
</globals>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="karstenw.20230303121029.2" a="E"><vh>pattern-Library</vh>
<v t="karstenw.20230303122029.1"><vh>@clean pattern/__init__.py</vh>
<v t="karstenw.20230303122044.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230303123040.1"><vh>db</vh>
<v t="karstenw.20230303123043.1"><vh>@clean pattern/db/__init__.py</vh>
<v t="karstenw.20230303123103.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303123103.2"><vh>_import_db</vh></v>
<v t="karstenw.20230303123103.3"><vh>pd</vh></v>
<v t="karstenw.20230303123103.4"><vh>_yyyywwd2yyyymmdd</vh></v>
<v t="karstenw.20230303123103.5"><vh>_strftime1900</vh></v>
<v t="karstenw.20230303123103.6"><vh>class DateError</vh></v>
<v t="karstenw.20230303123103.7"><vh>class Date</vh>
<v t="karstenw.20230303123103.8"><vh>minutes</vh></v>
<v t="karstenw.20230303123103.9"><vh>seconds</vh></v>
<v t="karstenw.20230303123103.10"><vh>microseconds</vh></v>
<v t="karstenw.20230303123103.11"><vh>week</vh></v>
<v t="karstenw.20230303123103.12"><vh>weekday</vh></v>
<v t="karstenw.20230303123103.13"><vh>timestamp</vh></v>
<v t="karstenw.20230303123103.14"><vh>strftime</vh></v>
<v t="karstenw.20230303123103.15"><vh>copy</vh></v>
<v t="karstenw.20230303123103.16"><vh>__str__</vh></v>
<v t="karstenw.20230303123103.17"><vh>__repr__</vh></v>
<v t="karstenw.20230303123103.18"><vh>__iadd__</vh></v>
<v t="karstenw.20230303123103.19"><vh>__isub__</vh></v>
<v t="karstenw.20230303123103.20"><vh>__add__</vh></v>
<v t="karstenw.20230303123103.21"><vh>__sub__</vh></v>
</v>
<v t="karstenw.20230303123103.22"><vh>date</vh></v>
<v t="karstenw.20230303123103.23"><vh>class Time</vh>
<v t="karstenw.20230303123103.24"><vh>__new__</vh></v>
</v>
<v t="karstenw.20230303123103.25"><vh>time</vh></v>
<v t="karstenw.20230303123103.26"><vh>string</vh></v>
<v t="karstenw.20230303123103.27"><vh>class EncryptionError</vh></v>
<v t="karstenw.20230303123103.28"><vh>class DecryptionError</vh></v>
<v t="karstenw.20230303123103.29"><vh>encrypt_string</vh></v>
<v t="karstenw.20230303123103.30"><vh>decrypt_string</vh></v>
<v t="karstenw.20230303123103.31"><vh>encode_entities</vh></v>
<v t="karstenw.20230303123103.32"><vh>decode_entities</vh></v>
<v t="karstenw.20230303123103.33"><vh>class _Binary</vh>
<v t="karstenw.20230303123103.34"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.35"><vh>escape</vh></v>
</v>
<v t="karstenw.20230303123103.36"><vh>_escape</vh></v>
<v t="karstenw.20230303123103.37"><vh>cast</vh></v>
<v t="karstenw.20230303123103.38"><vh>find</vh></v>
<v t="karstenw.20230303123103.39"><vh>order</vh></v>
<v t="karstenw.20230303123103.40"><vh>avg</vh></v>
<v t="karstenw.20230303123103.41"><vh>variance</vh></v>
<v t="karstenw.20230303123103.42"><vh>stdev</vh></v>
<v t="karstenw.20230303123103.43"><vh>class sqlite_first</vh>
<v t="karstenw.20230303123103.44"><vh>step</vh></v>
<v t="karstenw.20230303123103.45"><vh>finalize</vh></v>
</v>
<v t="karstenw.20230303123103.46"><vh>class sqlite_last</vh>
<v t="karstenw.20230303123103.47"><vh>step</vh></v>
<v t="karstenw.20230303123103.48"><vh>finalize</vh></v>
</v>
<v t="karstenw.20230303123103.49"><vh>class sqlite_group_concat</vh>
<v t="karstenw.20230303123103.50"><vh>step</vh></v>
<v t="karstenw.20230303123103.51"><vh>finalize</vh></v>
</v>
<v t="karstenw.20230303123103.52"><vh>sqlite_year</vh></v>
<v t="karstenw.20230303123103.53"><vh>sqlite_month</vh></v>
<v t="karstenw.20230303123103.54"><vh>sqlite_day</vh></v>
<v t="karstenw.20230303123103.55"><vh>sqlite_hour</vh></v>
<v t="karstenw.20230303123103.56"><vh>sqlite_minute</vh></v>
<v t="karstenw.20230303123103.57"><vh>sqlite_second</vh></v>
<v t="karstenw.20230303123103.58"><vh>class DatabaseConnectionError</vh></v>
<v t="karstenw.20230303123103.59"><vh>class Database</vh>
<v t="karstenw.20230303123103.60"><vh>class Tables</vh>
<v t="karstenw.20230303123103.61"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.62"><vh>__getitem__</vh></v>
</v>
<v t="karstenw.20230303123103.63"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.64"><vh>connect</vh></v>
<v t="karstenw.20230303123103.65"><vh>disconnect</vh></v>
<v t="karstenw.20230303123103.66"><vh>connection</vh></v>
<v t="karstenw.20230303123103.67"><vh>connected</vh></v>
<v t="karstenw.20230303123103.68"><vh>__getattr__</vh></v>
<v t="karstenw.20230303123103.69"><vh>__len__</vh></v>
<v t="karstenw.20230303123103.70"><vh>__iter__</vh></v>
<v t="karstenw.20230303123103.71"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123103.72"><vh>__setitem__</vh></v>
<v t="karstenw.20230303123103.73"><vh>__delitem__</vh></v>
<v t="karstenw.20230303123103.74"><vh>__nonzero__</vh></v>
<v t="karstenw.20230303123103.75"><vh>_get_user</vh></v>
<v t="karstenw.20230303123103.76"><vh>_set_user</vh></v>
<v t="karstenw.20230303123103.77"><vh>query</vh></v>
<v t="karstenw.20230303123103.78"><vh>execute</vh></v>
<v t="karstenw.20230303123103.79"><vh>class RowsIterator</vh>
<v t="karstenw.20230303123103.80"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.81"><vh>__next__</vh></v>
<v t="karstenw.20230303123103.82"><vh>__iter__</vh></v>
<v t="karstenw.20230303123103.83"><vh>__del__</vh></v>
</v>
<v t="karstenw.20230303123103.84"><vh>commit</vh></v>
<v t="karstenw.20230303123103.85"><vh>rollback</vh></v>
<v t="karstenw.20230303123103.86"><vh>escape</vh></v>
<v t="karstenw.20230303123103.87"><vh>binary</vh></v>
<v t="karstenw.20230303123103.88"><vh>_field_SQL</vh></v>
<v t="karstenw.20230303123103.89"><vh>create</vh></v>
<v t="karstenw.20230303123103.90"><vh>drop</vh></v>
<v t="karstenw.20230303123103.91"><vh>link</vh></v>
<v t="karstenw.20230303123103.92"><vh>__repr__</vh></v>
<v t="karstenw.20230303123103.93"><vh>_delete</vh></v>
<v t="karstenw.20230303123103.94"><vh>__delete__</vh></v>
</v>
<v t="karstenw.20230303123103.95"><vh>class _String</vh>
<v t="karstenw.20230303123103.96"><vh>__new__</vh></v>
<v t="karstenw.20230303123103.97"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230303123103.98"><vh>field</vh></v>
<v t="karstenw.20230303123103.99"><vh>primary_key</vh></v>
<v t="karstenw.20230303123103.100"><vh>class Schema</vh>
<v t="karstenw.20230303123103.101"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.102"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303123103.103"><vh>class TableError</vh></v>
<v t="karstenw.20230303123103.104"><vh>class Table</vh>
<v t="karstenw.20230303123103.105"><vh>class Fields</vh>
<v t="karstenw.20230303123103.106"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.107"><vh>append</vh></v>
<v t="karstenw.20230303123103.108"><vh>extend</vh></v>
<v t="karstenw.20230303123103.109"><vh>__setitem__</vh></v>
</v>
<v t="karstenw.20230303123103.110"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.111"><vh>_update</vh></v>
<v t="karstenw.20230303123103.112"><vh>_get_name</vh></v>
<v t="karstenw.20230303123103.113"><vh>_set_name</vh></v>
<v t="karstenw.20230303123103.114"><vh>db</vh></v>
<v t="karstenw.20230303123103.115"><vh>pk</vh></v>
<v t="karstenw.20230303123103.116"><vh>count</vh></v>
<v t="karstenw.20230303123103.117"><vh>__len__</vh></v>
<v t="karstenw.20230303123103.118"><vh>__iter__</vh></v>
<v t="karstenw.20230303123103.119"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123103.120"><vh>__setitem__</vh></v>
<v t="karstenw.20230303123103.121"><vh>__delitem__</vh></v>
<v t="karstenw.20230303123103.122"><vh>abs</vh></v>
<v t="karstenw.20230303123103.123"><vh>iterrows</vh></v>
<v t="karstenw.20230303123103.124"><vh>rows</vh></v>
<v t="karstenw.20230303123103.125"><vh>record</vh></v>
<v t="karstenw.20230303123103.126"><vh>class Rows</vh>
<v t="karstenw.20230303123103.127"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.128"><vh>record</vh></v>
</v>
<v t="karstenw.20230303123103.129"><vh>filter</vh></v>
<v t="karstenw.20230303123103.130"><vh>find</vh></v>
<v t="karstenw.20230303123103.131"><vh>search</vh></v>
<v t="karstenw.20230303123103.132"><vh>_insert_id</vh></v>
<v t="karstenw.20230303123103.133"><vh>insert</vh></v>
<v t="karstenw.20230303123103.134"><vh>update</vh></v>
<v t="karstenw.20230303123103.135"><vh>delete</vh></v>
<v t="karstenw.20230303123103.136"><vh>xml</vh></v>
<v t="karstenw.20230303123103.137"><vh>datasheet</vh></v>
<v t="karstenw.20230303123103.138"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303123103.139"><vh>abs</vh></v>
<v t="karstenw.20230303123103.140"><vh>cmp</vh></v>
<v t="karstenw.20230303123103.141"><vh>year</vh></v>
<v t="karstenw.20230303123103.142"><vh>month</vh></v>
<v t="karstenw.20230303123103.143"><vh>day</vh></v>
<v t="karstenw.20230303123103.144"><vh>hour</vh></v>
<v t="karstenw.20230303123103.145"><vh>minute</vh></v>
<v t="karstenw.20230303123103.146"><vh>second</vh></v>
<v t="karstenw.20230303123103.147"><vh>count</vh></v>
<v t="karstenw.20230303123103.148"><vh>sum</vh></v>
<v t="karstenw.20230303123103.149"><vh>class Filter</vh>
<v t="karstenw.20230303123103.150"><vh>__new__</vh></v>
<v t="karstenw.20230303123103.151"><vh>SQL</vh></v>
</v>
<v t="karstenw.20230303123103.152"><vh>filter</vh></v>
<v t="karstenw.20230303123103.153"><vh>eq</vh></v>
<v t="karstenw.20230303123103.154"><vh>eqi</vh></v>
<v t="karstenw.20230303123103.155"><vh>ne</vh></v>
<v t="karstenw.20230303123103.156"><vh>gt</vh></v>
<v t="karstenw.20230303123103.157"><vh>lt</vh></v>
<v t="karstenw.20230303123103.158"><vh>gte</vh></v>
<v t="karstenw.20230303123103.159"><vh>lte</vh></v>
<v t="karstenw.20230303123103.160"><vh>rng</vh></v>
<v t="karstenw.20230303123103.161"><vh>class FilterChain</vh>
<v t="karstenw.20230303123103.162"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.163"><vh>SQL</vh></v>
</v>
<v t="karstenw.20230303123103.164"><vh>all</vh></v>
<v t="karstenw.20230303123103.165"><vh>any</vh></v>
<v t="karstenw.20230303123103.166"><vh>class Relation</vh>
<v t="karstenw.20230303123103.167"><vh>__new__</vh></v>
</v>
<v t="karstenw.20230303123103.168"><vh>relation</vh></v>
<v t="karstenw.20230303123103.169"><vh>class Query</vh>
<v t="karstenw.20230303123103.170"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.171"><vh>table</vh></v>
<v t="karstenw.20230303123103.172"><vh>__len__</vh></v>
<v t="karstenw.20230303123103.173"><vh>__iter__</vh></v>
<v t="karstenw.20230303123103.174"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123103.175"><vh>SQL</vh></v>
<v t="karstenw.20230303123103.176"><vh>execute</vh></v>
<v t="karstenw.20230303123103.177"><vh>iterrows</vh></v>
<v t="karstenw.20230303123103.178"><vh>rows</vh></v>
<v t="karstenw.20230303123103.179"><vh>record</vh></v>
<v t="karstenw.20230303123103.180"><vh>xml</vh></v>
<v t="karstenw.20230303123103.181"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303123103.182"><vh>associative</vh></v>
<v t="karstenw.20230303123103.183"><vh>class View</vh>
<v t="karstenw.20230303123103.184"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.185"><vh>db</vh></v>
<v t="karstenw.20230303123103.186"><vh>table</vh></v>
<v t="karstenw.20230303123103.187"><vh>setup</vh></v>
<v t="karstenw.20230303123103.188"><vh>render</vh></v>
<v t="karstenw.20230303123103.189"><vh>default</vh></v>
</v>
<v t="karstenw.20230303123103.190"><vh>_unpack_fields</vh></v>
<v t="karstenw.20230303123103.191"><vh>xml_format</vh></v>
<v t="karstenw.20230303123103.192"><vh>xml</vh></v>
<v t="karstenw.20230303123103.193"><vh>parse_xml</vh></v>
<v t="karstenw.20230303123103.194"><vh>csv_header_encode</vh></v>
<v t="karstenw.20230303123103.195"><vh>csv_header_decode</vh></v>
<v t="karstenw.20230303123103.196"><vh>class CSV</vh>
<v t="karstenw.20230303123103.197"><vh>__new__</vh></v>
<v t="karstenw.20230303123103.198"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.199"><vh>extend</vh></v>
<v t="karstenw.20230303123103.200"><vh>_set_headers</vh></v>
<v t="karstenw.20230303123103.201"><vh>_get_headers</vh></v>
<v t="karstenw.20230303123103.202"><vh>save</vh></v>
<v t="karstenw.20230303123103.203"><vh>load</vh></v>
</v>
<v t="karstenw.20230303123103.204"><vh>class Datasheet</vh>
<v t="karstenw.20230303123103.205"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.206"><vh>_get_rows</vh></v>
<v t="karstenw.20230303123103.207"><vh>_set_rows</vh></v>
<v t="karstenw.20230303123103.208"><vh>_get_columns</vh></v>
<v t="karstenw.20230303123103.209"><vh>_set_columns</vh></v>
<v t="karstenw.20230303123103.210"><vh>__getattr__</vh></v>
<v t="karstenw.20230303123103.211"><vh>__setattr__</vh></v>
<v t="karstenw.20230303123103.212"><vh>__setitem__</vh></v>
<v t="karstenw.20230303123103.213"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123103.214"><vh>__delitem__</vh></v>
<v t="karstenw.20230303123103.215"><vh>__add__</vh></v>
<v t="karstenw.20230303123103.216"><vh>__radd__</vh></v>
<v t="karstenw.20230303123103.217"><vh>__iadd__</vh></v>
<v t="karstenw.20230303123103.218"><vh>insert</vh></v>
<v t="karstenw.20230303123103.219"><vh>append</vh></v>
<v t="karstenw.20230303123103.220"><vh>extend</vh></v>
<v t="karstenw.20230303123103.221"><vh>group</vh></v>
<v t="karstenw.20230303123103.222"><vh>record</vh></v>
<v t="karstenw.20230303123103.223"><vh>map</vh></v>
<v t="karstenw.20230303123103.224"><vh>slice</vh></v>
<v t="karstenw.20230303123103.225"><vh>copy</vh></v>
<v t="karstenw.20230303123103.226"><vh>array</vh></v>
<v t="karstenw.20230303123103.227"><vh>json</vh></v>
<v t="karstenw.20230303123103.228"><vh>html</vh></v>
</v>
<v t="karstenw.20230303123103.229"><vh>flip</vh></v>
<v t="karstenw.20230303123103.230"><vh>csv</vh></v>
<v t="karstenw.20230303123103.231"><vh>class DatasheetRows</vh>
<v t="karstenw.20230303123103.232"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.233"><vh>__setitem__</vh></v>
<v t="karstenw.20230303123103.234"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123103.235"><vh>__getslice__</vh></v>
<v t="karstenw.20230303123103.236"><vh>__delitem__</vh></v>
<v t="karstenw.20230303123103.237"><vh>__len__</vh></v>
<v t="karstenw.20230303123103.238"><vh>__iter__</vh></v>
<v t="karstenw.20230303123103.239"><vh>__repr__</vh></v>
<v t="karstenw.20230303123103.240"><vh>__add__</vh></v>
<v t="karstenw.20230303123103.241"><vh>__iadd__</vh></v>
<v t="karstenw.20230303123103.242"><vh>__eq__</vh></v>
<v t="karstenw.20230303123103.243"><vh>__ne__</vh></v>
<v t="karstenw.20230303123103.244"><vh>insert</vh></v>
<v t="karstenw.20230303123103.245"><vh>append</vh></v>
<v t="karstenw.20230303123103.246"><vh>extend</vh></v>
<v t="karstenw.20230303123103.247"><vh>remove</vh></v>
<v t="karstenw.20230303123103.248"><vh>pop</vh></v>
<v t="karstenw.20230303123103.249"><vh>count</vh></v>
<v t="karstenw.20230303123103.250"><vh>index</vh></v>
<v t="karstenw.20230303123103.251"><vh>sort</vh></v>
<v t="karstenw.20230303123103.252"><vh>reverse</vh></v>
<v t="karstenw.20230303123103.253"><vh>swap</vh></v>
</v>
<v t="karstenw.20230303123103.254"><vh>class DatasheetColumns</vh>
<v t="karstenw.20230303123103.255"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.256"><vh>__setitem__</vh></v>
<v t="karstenw.20230303123103.257"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123103.258"><vh>__getslice__</vh></v>
<v t="karstenw.20230303123103.259"><vh>__delitem__</vh></v>
<v t="karstenw.20230303123103.260"><vh>__len__</vh></v>
<v t="karstenw.20230303123103.261"><vh>__iter__</vh></v>
<v t="karstenw.20230303123103.262"><vh>__repr__</vh></v>
<v t="karstenw.20230303123103.263"><vh>__add__</vh></v>
<v t="karstenw.20230303123103.264"><vh>__iadd__</vh></v>
<v t="karstenw.20230303123103.265"><vh>__eq__</vh></v>
<v t="karstenw.20230303123103.266"><vh>__ne__</vh></v>
<v t="karstenw.20230303123103.267"><vh>insert</vh></v>
<v t="karstenw.20230303123103.268"><vh>append</vh></v>
<v t="karstenw.20230303123103.269"><vh>extend</vh></v>
<v t="karstenw.20230303123103.270"><vh>remove</vh></v>
<v t="karstenw.20230303123103.271"><vh>pop</vh></v>
<v t="karstenw.20230303123103.272"><vh>count</vh></v>
<v t="karstenw.20230303123103.273"><vh>index</vh></v>
<v t="karstenw.20230303123103.274"><vh>sort</vh></v>
<v t="karstenw.20230303123103.275"><vh>swap</vh></v>
</v>
<v t="karstenw.20230303123103.276"><vh>class DatasheetColumn</vh>
<v t="karstenw.20230303123103.277"><vh>__init__</vh></v>
<v t="karstenw.20230303123103.278"><vh>__getslice__</vh></v>
<v t="karstenw.20230303123103.279"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123103.280"><vh>__setitem__</vh></v>
<v t="karstenw.20230303123103.281"><vh>__len__</vh></v>
<v t="karstenw.20230303123103.282"><vh>__iter__</vh></v>
<v t="karstenw.20230303123103.283"><vh>__reversed__</vh></v>
<v t="karstenw.20230303123103.284"><vh>__repr__</vh></v>
<v t="karstenw.20230303123103.285"><vh>__gt__</vh></v>
<v t="karstenw.20230303123103.286"><vh>__lt__</vh></v>
<v t="karstenw.20230303123103.287"><vh>__ge__</vh></v>
<v t="karstenw.20230303123103.288"><vh>__le__</vh></v>
<v t="karstenw.20230303123103.289"><vh>__eq__</vh></v>
<v t="karstenw.20230303123103.290"><vh>__ne__</vh></v>
<v t="karstenw.20230303123103.291"><vh>__add__</vh></v>
<v t="karstenw.20230303123103.292"><vh>__iadd__</vh></v>
<v t="karstenw.20230303123103.293"><vh>__contains__</vh></v>
<v t="karstenw.20230303123103.294"><vh>count</vh></v>
<v t="karstenw.20230303123103.295"><vh>index</vh></v>
<v t="karstenw.20230303123103.296"><vh>remove</vh></v>
<v t="karstenw.20230303123103.297"><vh>pop</vh></v>
<v t="karstenw.20230303123103.298"><vh>sort</vh></v>
<v t="karstenw.20230303123103.299"><vh>insert</vh></v>
<v t="karstenw.20230303123103.300"><vh>append</vh></v>
<v t="karstenw.20230303123103.301"><vh>extend</vh></v>
<v t="karstenw.20230303123103.302"><vh>map</vh></v>
<v t="karstenw.20230303123103.303"><vh>filter</vh></v>
<v t="karstenw.20230303123103.304"><vh>swap</vh></v>
</v>
<v t="karstenw.20230303123103.305"><vh>uid</vh></v>
<v t="karstenw.20230303123103.306"><vh>truncate</vh></v>
<v t="karstenw.20230303123103.307"><vh>pprint</vh></v>
</v>
</v>
<v t="karstenw.20230303123122.1"><vh>graph</vh>
<v t="karstenw.20230303123127.1"><vh>@clean pattern/graph/__init__.py</vh>
<v t="karstenw.20230303123148.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303123148.2"><vh>unique</vh></v>
<v t="karstenw.20230324114612.1"><vh>newHeadline</vh>
<v t="karstenw.20230303123148.3"><vh>line</vh></v>
<v t="karstenw.20230303123148.4"><vh>ellipse</vh></v>
<v t="karstenw.20230303123148.5"><vh>class Text</vh>
<v t="karstenw.20230303123148.6"><vh>__init__</vh></v>
<v t="karstenw.20230303123148.7"><vh>copy</vh></v>
<v t="karstenw.20230303123148.8"><vh>draw</vh></v>
</v>
<v t="karstenw.20230303123148.9"><vh>class Vector</vh>
<v t="karstenw.20230303123148.10"><vh>__init__</vh></v>
</v>
<v t="karstenw.20230303123148.11"><vh>coordinates</vh></v>
</v>
<v t="karstenw.20230324114641.1"><vh>newHeadline</vh></v>
<v t="karstenw.20230303123148.12"><vh>deepcopy</vh></v>
<v t="karstenw.20230324114654.1"><vh>+ NODE +</vh>
<v t="karstenw.20230303123148.13"><vh>class Node</vh>
<v t="karstenw.20230303123148.14"><vh>__init__</vh></v>
<v t="karstenw.20230303123148.15"><vh>_distance</vh></v>
<v t="karstenw.20230303123148.16"><vh>_get_x</vh></v>
<v t="karstenw.20230303123148.17"><vh>_get_y</vh></v>
<v t="karstenw.20230303123148.18"><vh>_set_x</vh></v>
<v t="karstenw.20230303123148.19"><vh>_set_y</vh></v>
<v t="karstenw.20230303123148.20"><vh>edges</vh></v>
<v t="karstenw.20230303123148.21"><vh>edge</vh></v>
<v t="karstenw.20230303123148.22"><vh>weight</vh></v>
<v t="karstenw.20230303123148.23"><vh>centrality</vh></v>
<v t="karstenw.20230303123148.24"><vh>degree</vh></v>
<v t="karstenw.20230303123148.25"><vh>flatten</vh></v>
<v t="karstenw.20230303123148.26"><vh>draw</vh></v>
<v t="karstenw.20230303123148.27"><vh>contains</vh></v>
<v t="karstenw.20230303123148.28"><vh>__repr__</vh></v>
<v t="karstenw.20230303123148.29"><vh>__eq__</vh></v>
<v t="karstenw.20230303123148.30"><vh>__ne__</vh></v>
</v>
<v t="karstenw.20230303123148.31"><vh>class Links</vh>
<v t="karstenw.20230303123148.32"><vh>__init__</vh></v>
<v t="karstenw.20230303123148.33"><vh>append</vh></v>
<v t="karstenw.20230303123148.34"><vh>remove</vh></v>
<v t="karstenw.20230303123148.35"><vh>edge</vh></v>
</v>
</v>
<v t="karstenw.20230324114735.1"><vh>+ EDGE +</vh>
<v t="karstenw.20230303123148.36"><vh>class Edge</vh>
<v t="karstenw.20230303123148.37"><vh>__init__</vh></v>
<v t="karstenw.20230303123148.38"><vh>_get_weight</vh></v>
<v t="karstenw.20230303123148.39"><vh>_set_weight</vh></v>
<v t="karstenw.20230303123148.40"><vh>draw</vh></v>
<v t="karstenw.20230303123148.41"><vh>draw_arrow</vh></v>
<v t="karstenw.20230303123148.42"><vh>__repr__</vh></v>
</v>
</v>
<v t="karstenw.20230324114833.1"><vh>+ GRAPH +</vh>
<v t="karstenw.20230303123148.43"><vh>class nodedict</vh>
<v t="karstenw.20230303123148.44"><vh>__init__</vh></v>
<v t="karstenw.20230303123148.45"><vh>__contains__</vh></v>
<v t="karstenw.20230303123148.46"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123148.47"><vh>get</vh></v>
</v>
<v t="karstenw.20230303123148.48"><vh>class Graph</vh>
<v t="karstenw.20230303123148.49"><vh>__init__</vh></v>
<v t="karstenw.20230303123148.50"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123148.51"><vh>append</vh></v>
<v t="karstenw.20230303123148.52"><vh>add_node</vh></v>
<v t="karstenw.20230303123148.53"><vh>add_edge</vh></v>
<v t="karstenw.20230303123148.54"><vh>remove</vh></v>
<v t="karstenw.20230303123148.55"><vh>node</vh></v>
<v t="karstenw.20230303123148.56"><vh>edge</vh></v>
<v t="karstenw.20230303123148.57"><vh>paths</vh></v>
<v t="karstenw.20230303123148.58"><vh>shortest_path</vh></v>
<v t="karstenw.20230303123148.59"><vh>shortest_paths</vh></v>
<v t="karstenw.20230303123148.60"><vh>eigenvector_centrality</vh></v>
<v t="karstenw.20230303123148.61"><vh>betweenness_centrality</vh></v>
<v t="karstenw.20230303123148.62"><vh>sorted</vh></v>
<v t="karstenw.20230303123148.63"><vh>prune</vh></v>
<v t="karstenw.20230303123148.64"><vh>fringe</vh></v>
<v t="karstenw.20230303123148.65"><vh>density</vh></v>
<v t="karstenw.20230303123148.66"><vh>is_complete</vh></v>
<v t="karstenw.20230303123148.67"><vh>is_dense</vh></v>
<v t="karstenw.20230303123148.68"><vh>is_sparse</vh></v>
<v t="karstenw.20230303123148.69"><vh>split</vh></v>
<v t="karstenw.20230303123148.70"><vh>update</vh></v>
<v t="karstenw.20230303123148.71"><vh>draw</vh></v>
<v t="karstenw.20230303123148.72"><vh>node_at</vh></v>
<v t="karstenw.20230303123148.73"><vh>_add_node_copy</vh></v>
<v t="karstenw.20230303123148.74"><vh>_add_edge_copy</vh></v>
<v t="karstenw.20230303123148.75"><vh>copy</vh></v>
<v t="karstenw.20230303123148.76"><vh>export</vh></v>
<v t="karstenw.20230303123148.77"><vh>write</vh></v>
<v t="karstenw.20230303123148.78"><vh>serialize</vh></v>
</v>
<v t="karstenw.20230303123148.79"><vh>class GraphLayout</vh>
<v t="karstenw.20230303123148.80"><vh>__init__</vh></v>
<v t="karstenw.20230303123148.81"><vh>update</vh></v>
<v t="karstenw.20230303123148.82"><vh>reset</vh></v>
<v t="karstenw.20230303123148.83"><vh>bounds</vh></v>
<v t="karstenw.20230303123148.84"><vh>copy</vh></v>
</v>
</v>
<v t="karstenw.20230324114855.1"><vh>+ GRAPH ANALYSIS +</vh>
<v t="karstenw.20230303123148.85"><vh>class GraphSpringLayout</vh>
<v t="karstenw.20230303123148.86"><vh>__init__</vh></v>
<v t="karstenw.20230303123148.87"><vh>_distance</vh></v>
<v t="karstenw.20230303123148.88"><vh>_repulse</vh></v>
<v t="karstenw.20230303123148.89"><vh>_attract</vh></v>
<v t="karstenw.20230303123148.90"><vh>update</vh></v>
<v t="karstenw.20230303123148.91"><vh>copy</vh></v>
</v>
<v t="karstenw.20230303123148.92"><vh>depth_first_search</vh></v>
<v t="karstenw.20230303123148.93"><vh>breadth_first_search</vh></v>
<v t="karstenw.20230303123148.94"><vh>paths</vh></v>
<v t="karstenw.20230303123148.95"><vh>edges</vh></v>
<v t="karstenw.20230303123148.96"><vh>adjacency</vh></v>
<v t="karstenw.20230303123148.97"><vh>dijkstra_shortest_path</vh></v>
<v t="karstenw.20230303123148.98"><vh>dijkstra_shortest_paths</vh></v>
<v t="karstenw.20230303123148.99"><vh>floyd_warshall_all_pairs_distance</vh></v>
<v t="karstenw.20230303123148.100"><vh>predecessor_path</vh></v>
<v t="karstenw.20230303123148.101"><vh>brandes_betweenness_centrality</vh></v>
<v t="karstenw.20230303123148.102"><vh>eigenvector_centrality</vh></v>
<v t="karstenw.20230303123148.103"><vh>union</vh></v>
<v t="karstenw.20230303123148.104"><vh>intersection</vh></v>
<v t="karstenw.20230303123148.105"><vh>difference</vh></v>
<v t="karstenw.20230303123148.106"><vh>partition</vh></v>
<v t="karstenw.20230303123148.107"><vh>is_clique</vh></v>
<v t="karstenw.20230303123148.108"><vh>clique</vh></v>
<v t="karstenw.20230303123148.109"><vh>cliques</vh></v>
</v>
<v t="karstenw.20230324114943.1"><vh>+ GRAPH UTILITY FUNCTIONS +</vh>
<v t="karstenw.20230303123148.110"><vh>unlink</vh></v>
<v t="karstenw.20230303123148.111"><vh>redirect</vh></v>
<v t="karstenw.20230303123148.112"><vh>cut</vh></v>
<v t="karstenw.20230303123148.113"><vh>insert</vh></v>
</v>
<v t="karstenw.20230324115018.1"><vh>+ GRAPH EXPORT +</vh>
<v t="karstenw.20230303123148.114"><vh>class GraphRenderer</vh>
<v t="karstenw.20230303123148.115"><vh>__init__</vh></v>
<v t="karstenw.20230303123148.116"><vh>serialize</vh></v>
<v t="karstenw.20230303123148.117"><vh>export</vh></v>
</v>
<v t="karstenw.20230303123148.118"><vh>minify</vh></v>
<v t="karstenw.20230303123148.119"><vh>class HTMLCanvasRenderer</vh>
<v t="karstenw.20230303123148.120"><vh>__init__</vh></v>
<v t="karstenw.20230303123148.121"><vh>_escape</vh></v>
<v t="karstenw.20230303123148.122"><vh>_rgba</vh></v>
<v t="karstenw.20230303123148.123"><vh>data</vh></v>
<v t="karstenw.20230303123148.124"><vh>_data</vh></v>
<v t="karstenw.20230303123148.125"><vh>script</vh></v>
<v t="karstenw.20230303123148.126"><vh>_script</vh></v>
<v t="karstenw.20230303123148.127"><vh>canvas</vh></v>
<v t="karstenw.20230303123148.128"><vh>style</vh></v>
<v t="karstenw.20230303123148.129"><vh>html</vh></v>
<v t="karstenw.20230303123148.130"><vh>serialize</vh></v>
<v t="karstenw.20230303123148.131"><vh>export</vh></v>
</v>
<v t="karstenw.20230303123148.132"><vh>class GraphMLRenderer</vh>
<v t="karstenw.20230303123148.133"><vh>serialize</vh></v>
<v t="karstenw.20230303123148.134"><vh>export</vh></v>
</v>
<v t="karstenw.20230303123148.135"><vh>export</vh></v>
<v t="karstenw.20230303123148.136"><vh>serialize</vh></v>
</v>
</v>
<v t="karstenw.20230303123158.1"><vh>@clean pattern/graph/commonsense.py</vh>
<v t="karstenw.20230303123206.1"><vh>Declarations</vh></v>
<v t="karstenw.20230324115334.1"><vh>+ COMMONSENSE SEMANTIC NETWORK +</vh>
<v t="karstenw.20230303123206.2"><vh>class Concept</vh>
<v t="karstenw.20230303123206.3"><vh>__init__</vh></v>
<v t="karstenw.20230303123206.4"><vh>halo</vh></v>
<v t="karstenw.20230303123206.5"><vh>properties</vh></v>
</v>
<v t="karstenw.20230303123206.6"><vh>halo</vh></v>
<v t="karstenw.20230303123206.7"><vh>properties</vh></v>
<v t="karstenw.20230303123206.8"><vh>class Relation</vh>
<v t="karstenw.20230303123206.9"><vh>__init__</vh></v>
</v>
<v t="karstenw.20230324122147.1"><vh>+ HEURISTICS +</vh></v>
<v t="karstenw.20230303123206.10"><vh>class Commonsense</vh>
<v t="karstenw.20230303123206.11"><vh>__init__</vh></v>
<v t="karstenw.20230303123206.12"><vh>concepts</vh></v>
<v t="karstenw.20230303123206.13"><vh>relations</vh></v>
<v t="karstenw.20230303123206.14"><vh>properties</vh></v>
<v t="karstenw.20230303123206.15"><vh>add_node</vh></v>
<v t="karstenw.20230303123206.16"><vh>add_edge</vh></v>
<v t="karstenw.20230303123206.17"><vh>remove</vh></v>
<v t="karstenw.20230303123206.18"><vh>similarity</vh></v>
<v t="karstenw.20230303123206.19"><vh>nearest_neighbors</vh></v>
<v t="karstenw.20230303123206.20"><vh>taxonomy</vh></v>
</v>
</v>
<v t="karstenw.20230324115357.1"><vh>+ COMMON SENSE DATA +</vh>
<v t="karstenw.20230303123206.21"><vh>download</vh></v>
<v t="karstenw.20230303123206.22"><vh>json</vh></v>
</v>
</v>
</v>
<v t="karstenw.20230303123256.1"><vh>@clean pattern/helpers.py</vh>
<v t="karstenw.20230303123309.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303123309.2"><vh>decode_string</vh></v>
<v t="karstenw.20230303123309.3"><vh>encode_string</vh></v>
</v>
<v t="karstenw.20230303123315.1"><vh>@clean pattern/metrics.py</vh>
<v t="karstenw.20230303123322.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303123322.2"><vh>class freq</vh>
<v t="karstenw.20230303123322.3"><vh>__init__</vh></v>
<v t="karstenw.20230303123322.4"><vh>__missing__</vh></v>
<v t="karstenw.20230303123322.5"><vh>__iter__</vh></v>
<v t="karstenw.20230303123322.6"><vh>items</vh></v>
<v t="karstenw.20230303123322.7"><vh>keys</vh></v>
<v t="karstenw.20230303123322.8"><vh>values</vh></v>
<v t="karstenw.20230303123322.9"><vh>copy</vh></v>
</v>
<v t="karstenw.20230303123322.10"><vh>cumsum</vh></v>
<v t="karstenw.20230303123322.11"><vh>duration</vh></v>
<v t="karstenw.20230303123322.12"><vh>profile</vh></v>
<v t="karstenw.20230303123322.13"><vh>sizeof</vh></v>
<v t="karstenw.20230303123322.14"><vh>kb</vh></v>
<v t="karstenw.20230303123322.15"><vh>confusion_matrix</vh></v>
<v t="karstenw.20230303123322.16"><vh>test</vh></v>
<v t="karstenw.20230303123322.17"><vh>accuracy</vh></v>
<v t="karstenw.20230303123322.18"><vh>precision</vh></v>
<v t="karstenw.20230303123322.19"><vh>recall</vh></v>
<v t="karstenw.20230303123322.20"><vh>F1</vh></v>
<v t="karstenw.20230303123322.21"><vh>F</vh></v>
<v t="karstenw.20230303123322.22"><vh>sensitivity</vh></v>
<v t="karstenw.20230303123322.23"><vh>specificity</vh></v>
<v t="karstenw.20230303123322.24"><vh>roc</vh></v>
<v t="karstenw.20230303123322.25"><vh>auc</vh></v>
<v t="karstenw.20230303123322.26"><vh>fleiss_kappa</vh></v>
<v t="karstenw.20230303123322.27"><vh>levenshtein</vh></v>
<v t="karstenw.20230303123322.28"><vh>levenshtein_similarity</vh></v>
<v t="karstenw.20230303123322.29"><vh>dice_coefficient</vh></v>
<v t="karstenw.20230303123322.30"><vh>similarity</vh></v>
<v t="karstenw.20230303123322.31"><vh>flesch_reading_ease</vh></v>
<v t="karstenw.20230303123322.32"><vh>ngrams</vh></v>
<v t="karstenw.20230303123322.33"><vh>class Weight</vh>
<v t="karstenw.20230303123322.34"><vh>__new__</vh></v>
<v t="karstenw.20230303123322.35"><vh>__init__</vh></v>
<v t="karstenw.20230303123322.36"><vh>__iadd__</vh></v>
<v t="karstenw.20230303123322.37"><vh>__isub__</vh></v>
<v t="karstenw.20230303123322.38"><vh>__imul__</vh></v>
<v t="karstenw.20230303123322.39"><vh>__idiv__</vh></v>
</v>
<v t="karstenw.20230303123322.40"><vh>intertextuality</vh></v>
<v t="karstenw.20230303123322.41"><vh>type_token_ratio</vh></v>
<v t="karstenw.20230303123322.42"><vh>suffixes</vh></v>
<v t="karstenw.20230303123322.43"><vh>class Sentinel</vh></v>
<v t="karstenw.20230303123322.44"><vh>isplit</vh></v>
<v t="karstenw.20230303123322.45"><vh>cooccurrence</vh></v>
<v t="karstenw.20230303123323.1"><vh>lerp</vh></v>
<v t="karstenw.20230303123323.2"><vh>smoothstep</vh></v>
<v t="karstenw.20230303123323.3"><vh>smoothrange</vh></v>
<v t="karstenw.20230303123323.4"><vh>mean</vh></v>
<v t="karstenw.20230303123323.5"><vh>hmean</vh></v>
<v t="karstenw.20230303123323.6"><vh>median</vh></v>
<v t="karstenw.20230303123323.7"><vh>variance</vh></v>
<v t="karstenw.20230303123323.8"><vh>standard_deviation</vh></v>
<v t="karstenw.20230303123323.9"><vh>simple_moving_average</vh></v>
<v t="karstenw.20230303123323.10"><vh>histogram</vh></v>
<v t="karstenw.20230303123323.11"><vh>moment</vh></v>
<v t="karstenw.20230303123323.12"><vh>skewness</vh></v>
<v t="karstenw.20230303123323.13"><vh>kurtosis</vh></v>
<v t="karstenw.20230303123323.14"><vh>quantile</vh></v>
<v t="karstenw.20230303123323.15"><vh>boxplot</vh></v>
<v t="karstenw.20230303123323.16"><vh>fisher_exact_test</vh></v>
<v t="karstenw.20230303123323.17"><vh>_expected</vh></v>
<v t="karstenw.20230303123323.18"><vh>pearson_chi_squared_test</vh></v>
<v t="karstenw.20230303123323.19"><vh>chi2p</vh></v>
<v t="karstenw.20230303123323.20"><vh>pearson_log_likelihood_ratio</vh></v>
<v t="karstenw.20230303123323.21"><vh>kolmogorov_smirnov_two_sample_test</vh></v>
<v t="karstenw.20230303123323.22"><vh>gamma</vh></v>
<v t="karstenw.20230303123323.23"><vh>gammaln</vh></v>
<v t="karstenw.20230303123323.24"><vh>gammai</vh></v>
<v t="karstenw.20230303123323.25"><vh>erf</vh></v>
<v t="karstenw.20230303123323.26"><vh>erfc</vh></v>
<v t="karstenw.20230303123323.27"><vh>cdf</vh></v>
<v t="karstenw.20230303123323.28"><vh>pdf</vh></v>
<v t="karstenw.20230303123323.29"><vh>norm</vh></v>
<v t="karstenw.20230303123323.30"><vh>kolmogorov</vh></v>
</v>
<v t="karstenw.20230303132553.1"><vh>server</vh>
<v t="karstenw.20230303123433.1"><vh>@clean pattern/server/__init__.py</vh>
<v t="karstenw.20230303123446.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303123854.1"><vh>Globals</vh></v>
<v t="karstenw.20230303123446.2"><vh>chown</vh></v>
<v t="karstenw.20230303123446.3"><vh>encode_entities</vh></v>
<v t="karstenw.20230303123446.4"><vh>decode_entities</vh></v>
<v t="karstenw.20230303123446.5"><vh>encode_url</vh></v>
<v t="karstenw.20230303123446.6"><vh>decode_url</vh></v>
<v t="karstenw.20230303123446.7"><vh>openable</vh></v>
<v t="karstenw.20230303123934.1"><vh>+ INTROSPECTION +</vh>
<v t="karstenw.20230303123446.8"><vh>define</vh></v>
</v>
<v t="karstenw.20230303124013.1"><vh>+ DATABASE +</vh>
<v t="karstenw.20230303123446.9"><vh>class Row</vh>
<v t="karstenw.20230303123446.10"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.11"><vh>__getattr__</vh></v>
</v>
<v t="karstenw.20230303123446.12"><vh>class DatabaseError</vh></v>
<v t="karstenw.20230303123446.13"><vh>class Database</vh>
<v t="karstenw.20230303123446.14"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.15"><vh>name</vh></v>
<v t="karstenw.20230303123446.16"><vh>type</vh></v>
<v t="karstenw.20230303123446.17"><vh>host</vh></v>
<v t="karstenw.20230303123446.18"><vh>port</vh></v>
<v t="karstenw.20230303123446.19"><vh>connection</vh></v>
<v t="karstenw.20230303123446.20"><vh>connect</vh></v>
<v t="karstenw.20230303123446.21"><vh>disconnect</vh></v>
<v t="karstenw.20230303123446.22"><vh>execute</vh></v>
<v t="karstenw.20230303123446.23"><vh>commit</vh></v>
<v t="karstenw.20230303123446.24"><vh>rollback</vh></v>
<v t="karstenw.20230303123446.25"><vh>__call__</vh></v>
<v t="karstenw.20230303123446.26"><vh>__repr__</vh></v>
<v t="karstenw.20230303123446.27"><vh>__del__</vh></v>
<v t="karstenw.20230303123446.28"><vh>batch</vh></v>
</v>
<v t="karstenw.20230303123446.29"><vh>class DatabaseTransaction</vh>
<v t="karstenw.20230303123446.30"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.31"><vh>execute</vh></v>
<v t="karstenw.20230303123446.32"><vh>commit</vh></v>
<v t="karstenw.20230303123446.33"><vh>rollback</vh></v>
<v t="karstenw.20230303123446.34"><vh>__len__</vh></v>
<v t="karstenw.20230303123446.35"><vh>__call__</vh></v>
<v t="karstenw.20230303123446.36"><vh>__repr__</vh></v>
<v t="karstenw.20230303123446.37"><vh>batch</vh></v>
</v>
</v>
<v t="karstenw.20230303124122.1"><vh>+ DATABASE SECURITY +</vh>
<v t="karstenw.20230303123446.38"><vh>pbkdf2</vh></v>
<v t="karstenw.20230303123446.39"><vh>streql</vh></v>
<v t="karstenw.20230303123446.40"><vh>encode_password</vh></v>
<v t="karstenw.20230303123446.41"><vh>verify_password</vh></v>
<v t="karstenw.20230303123446.42"><vh>class RateLimitError</vh></v>
<v t="karstenw.20230303123446.43"><vh>class RateLimitExceeded</vh></v>
<v t="karstenw.20230303123446.44"><vh>class RateLimitForbidden</vh></v>
<v t="karstenw.20230303123446.45"><vh>class RateLimit</vh>
<v t="karstenw.20230303123446.46"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.47"><vh>cache</vh></v>
<v t="karstenw.20230303123446.48"><vh>lock</vh></v>
<v t="karstenw.20230303123446.49"><vh>key</vh></v>
<v t="karstenw.20230303123446.50"><vh>reset</vh></v>
<v t="karstenw.20230303123446.51"><vh>load</vh></v>
<v t="karstenw.20230303123446.52"><vh>set</vh></v>
<v t="karstenw.20230303123446.53"><vh>get</vh></v>
<v t="karstenw.20230303123446.54"><vh>delete</vh></v>
<v t="karstenw.20230303123446.55"><vh>__setitem__</vh></v>
<v t="karstenw.20230303123446.56"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123446.57"><vh>__delitem__</vh></v>
<v t="karstenw.20230303123446.58"><vh>__contains__</vh></v>
<v t="karstenw.20230303123446.59"><vh>__call__</vh></v>
<v t="karstenw.20230303123446.60"><vh>count</vh></v>
</v>
</v>
<v t="karstenw.20230303124220.1"><vh>+ ROUTER +</vh>
<v t="karstenw.20230303123446.61"><vh>class RouteError</vh></v>
<v t="karstenw.20230303123446.62"><vh>class Router</vh>
<v t="karstenw.20230303123446.63"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.64"><vh>__setitem__</vh></v>
<v t="karstenw.20230303123446.65"><vh>__call__</vh></v>
</v>
</v>
<v t="karstenw.20230303124242.1"><vh>+ APPLICATION +</vh>
<v t="karstenw.20230303123446.66"><vh>class HTTPRequest</vh>
<v t="karstenw.20230303123446.67"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.68"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303123446.69"><vh>class HTTPRedirect</vh>
<v t="karstenw.20230303123446.70"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.71"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303123446.72"><vh>class HTTPError</vh>
<v t="karstenw.20230303123446.73"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.74"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303123446.75"><vh>_HTTPErrorSubclass</vh></v>
<v t="karstenw.20230303124355.1"><vh>+ HTTP Errors +</vh></v>
</v>
<v t="karstenw.20230303124543.1"><vh>+ APPLICATION THREAD-SAFE DATA +</vh>
<v t="karstenw.20230303123446.76"><vh>class localdict</vh>
<v t="karstenw.20230303123446.77"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.78"><vh>items</vh></v>
<v t="karstenw.20230303123446.79"><vh>keys</vh></v>
<v t="karstenw.20230303123446.80"><vh>values</vh></v>
<v t="karstenw.20230303123446.81"><vh>update</vh></v>
<v t="karstenw.20230303123446.82"><vh>clear</vh></v>
<v t="karstenw.20230303123446.83"><vh>pop</vh></v>
<v t="karstenw.20230303123446.84"><vh>setdefault</vh></v>
<v t="karstenw.20230303123446.85"><vh>set</vh></v>
<v t="karstenw.20230303123446.86"><vh>get</vh></v>
<v t="karstenw.20230303123446.87"><vh>__delitem__</vh></v>
<v t="karstenw.20230303123446.88"><vh>__getitem__</vh></v>
<v t="karstenw.20230303123446.89"><vh>__setitem__</vh></v>
<v t="karstenw.20230303123446.90"><vh>__delattr__</vh></v>
<v t="karstenw.20230303123446.91"><vh>__getattr__</vh></v>
<v t="karstenw.20230303123446.92"><vh>__setattr__</vh></v>
<v t="karstenw.20230303123446.93"><vh>__len__</vh></v>
<v t="karstenw.20230303123446.94"><vh>__iter__</vh></v>
<v t="karstenw.20230303123446.95"><vh>__contains__</vh></v>
<v t="karstenw.20230303123446.96"><vh>__str__</vh></v>
<v t="karstenw.20230303123446.97"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303123446.98"><vh>threadsafe</vh></v>
</v>
<v t="karstenw.20230303124612.1"><vh>+ APPLICATION +</vh>
<v t="karstenw.20230303123446.99"><vh>class ApplicationError</vh></v>
<v t="karstenw.20230303123446.100"><vh>class Application</vh>
<v t="karstenw.20230303123446.101"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.102"><vh>owner</vh></v>
<v t="karstenw.20230303123446.103"><vh>name</vh></v>
<v t="karstenw.20230303123446.104"><vh>host</vh></v>
<v t="karstenw.20230303123446.105"><vh>port</vh></v>
<v t="karstenw.20230303123446.106"><vh>up</vh></v>
<v t="karstenw.20230303123446.107"><vh>path</vh></v>
<v t="karstenw.20230303123446.108"><vh>static</vh></v>
<v t="karstenw.20230303123446.109"><vh>session</vh></v>
<v t="karstenw.20230303123446.110"><vh>request</vh></v>
<v t="karstenw.20230303123446.111"><vh>response</vh></v>
<v t="karstenw.20230303123446.112"><vh>elapsed</vh></v>
<v t="karstenw.20230303123446.113"><vh>_cast</vh></v>
<v t="karstenw.20230303123446.114"><vh>default</vh></v>
<v t="karstenw.20230303123446.115"><vh>route</vh></v>
<v t="karstenw.20230303123446.116"><vh>error</vh></v>
<v t="karstenw.20230303123446.117"><vh>view</vh></v>
<v t="karstenw.20230303123446.118"><vh>class Thread</vh>
<v t="karstenw.20230303123446.119"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.120"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230303123446.121"><vh>rate</vh></v>
<v t="karstenw.20230303123446.122"><vh>bind</vh></v>
<v t="karstenw.20230303123446.123"><vh>__getattr__</vh></v>
<v t="karstenw.20230303123446.124"><vh>cached</vh></v>
<v t="karstenw.20230303123446.125"><vh>task</vh></v>
<v t="karstenw.20230303123446.126"><vh>redirect</vh></v>
<v t="karstenw.20230303123446.127"><vh>run</vh></v>
<v t="karstenw.20230303123446.128"><vh>stop</vh></v>
<v t="karstenw.20230303123446.129"><vh>__call__</vh></v>
</v>
</v>
<v t="karstenw.20230303124836.1"><vh>+ CERTIFICATE +</vh>
<v t="karstenw.20230303123446.130"><vh>certificate</vh></v>
</v>
<v t="karstenw.20230303123446.131"><vh>redirect</vh></v>
<v t="karstenw.20230303123446.132"><vh>static</vh></v>
<v t="karstenw.20230303123446.133"><vh>_register</vh></v>
<v t="karstenw.20230303123446.134"><vh>_request_start</vh></v>
<v t="karstenw.20230303123446.135"><vh>_request_end</vh></v>
<v t="karstenw.20230303123446.136"><vh>class Template</vh>
<v t="karstenw.20230303123446.137"><vh>__init__</vh></v>
<v t="karstenw.20230303123446.138"><vh>_escape</vh></v>
<v t="karstenw.20230303123446.139"><vh>_encode</vh></v>
<v t="karstenw.20230303123446.140"><vh>_dict</vh></v>
<v t="karstenw.20230303123446.141"><vh>_compile</vh></v>
<v t="karstenw.20230303123446.142"><vh>_render</vh></v>
<v t="karstenw.20230303123446.143"><vh>render</vh></v>
</v>
<v t="karstenw.20230303123446.144"><vh>template</vh></v>
<v t="karstenw.20230303123446.145"><vh>class HTML</vh>
<v t="karstenw.20230303123446.146"><vh>_attrs</vh></v>
<v t="karstenw.20230303123446.147"><vh>div</vh></v>
<v t="karstenw.20230303123446.148"><vh>span</vh></v>
<v t="karstenw.20230303123446.149"><vh>table</vh></v>
<v t="karstenw.20230303123446.150"><vh>select</vh></v>
</v>
<v t="karstenw.20230303125114.1"><vh>+ dead code +</vh></v>
</v>
</v>
<v t="karstenw.20230303132600.1"><vh>text</vh>
<v t="karstenw.20230303131659.1"><vh>@clean pattern/text/__init__.py</vh>
<v t="karstenw.20230303131711.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303131711.2"><vh>ngrams</vh></v>
<v t="karstenw.20230303131711.3"><vh>split_document_by_delimeters</vh></v>
<v t="karstenw.20230303131711.4"><vh>train_topmine_ngrammer</vh></v>
<v t="karstenw.20230303131711.5"><vh>topmine_ngramms</vh></v>
<v t="karstenw.20230303131711.6"><vh>class NGrammer</vh>
<v t="karstenw.20230303131711.7"><vh>__init__</vh></v>
<v t="karstenw.20230303131711.8"><vh>delimiters</vh></v>
<v t="karstenw.20230303131711.9"><vh>delimiters</vh></v>
<v t="karstenw.20230303131711.10"><vh>delimiters_regex</vh></v>
<v t="karstenw.20230303131711.11"><vh>delimiters_regex</vh></v>
<v t="karstenw.20230303131711.12"><vh>lengthInWords</vh></v>
<v t="karstenw.20230303131711.13"><vh>lengthInWords</vh></v>
<v t="karstenw.20230303131711.14"><vh>frequentPhraseMining</vh></v>
<v t="karstenw.20230303131712.1"><vh>_significanceScore</vh></v>
<v t="karstenw.20230303131712.2"><vh>ngramm</vh></v>
<v t="karstenw.20230303131712.3"><vh>removeDelimiters</vh></v>
<v t="karstenw.20230303131712.4"><vh>saveAsJson</vh></v>
<v t="karstenw.20230303131712.5"><vh>loadFromJson</vh></v>
</v>
<v t="karstenw.20230303131712.6"><vh>deflood</vh></v>
<v t="karstenw.20230303131712.7"><vh>decamel</vh></v>
<v t="karstenw.20230303131712.8"><vh>pprint</vh></v>
<v t="karstenw.20230303131712.9"><vh>class lazydict</vh>
<v t="karstenw.20230303131712.10"><vh>load</vh></v>
<v t="karstenw.20230303131712.11"><vh>_lazy</vh></v>
<v t="karstenw.20230303131712.12"><vh>__repr__</vh></v>
<v t="karstenw.20230303131712.13"><vh>__len__</vh></v>
<v t="karstenw.20230303131712.14"><vh>__iter__</vh></v>
<v t="karstenw.20230303131712.15"><vh>__contains__</vh></v>
<v t="karstenw.20230303131712.16"><vh>__getitem__</vh></v>
<v t="karstenw.20230303131712.17"><vh>__setitem__</vh></v>
<v t="karstenw.20230303131712.18"><vh>__delitem__</vh></v>
<v t="karstenw.20230303131712.19"><vh>setdefault</vh></v>
<v t="karstenw.20230303131712.20"><vh>get</vh></v>
<v t="karstenw.20230303131712.21"><vh>items</vh></v>
<v t="karstenw.20230303131712.22"><vh>keys</vh></v>
<v t="karstenw.20230303131712.23"><vh>values</vh></v>
<v t="karstenw.20230303131712.24"><vh>update</vh></v>
<v t="karstenw.20230303131712.25"><vh>pop</vh></v>
<v t="karstenw.20230303131712.26"><vh>popitem</vh></v>
</v>
<v t="karstenw.20230303131712.27"><vh>class lazylist</vh>
<v t="karstenw.20230303131712.28"><vh>load</vh></v>
<v t="karstenw.20230303131712.29"><vh>_lazy</vh></v>
<v t="karstenw.20230303131712.30"><vh>__repr__</vh></v>
<v t="karstenw.20230303131712.31"><vh>__len__</vh></v>
<v t="karstenw.20230303131712.32"><vh>__iter__</vh></v>
<v t="karstenw.20230303131712.33"><vh>__contains__</vh></v>
<v t="karstenw.20230303131712.34"><vh>__getitem__</vh></v>
<v t="karstenw.20230303131712.35"><vh>__setitem__</vh></v>
<v t="karstenw.20230303131712.36"><vh>__delitem__</vh></v>
<v t="karstenw.20230303131712.37"><vh>insert</vh></v>
<v t="karstenw.20230303131712.38"><vh>append</vh></v>
<v t="karstenw.20230303131712.39"><vh>extend</vh></v>
<v t="karstenw.20230303131712.40"><vh>remove</vh></v>
<v t="karstenw.20230303131712.41"><vh>pop</vh></v>
<v t="karstenw.20230303131712.42"><vh>index</vh></v>
<v t="karstenw.20230303131712.43"><vh>count</vh></v>
</v>
<v t="karstenw.20230303131712.44"><vh>class lazyset</vh>
<v t="karstenw.20230303131712.45"><vh>load</vh></v>
<v t="karstenw.20230303131712.46"><vh>_lazy</vh></v>
<v t="karstenw.20230303131712.47"><vh>__repr__</vh></v>
<v t="karstenw.20230303131712.48"><vh>__len__</vh></v>
<v t="karstenw.20230303131712.49"><vh>__iter__</vh></v>
<v t="karstenw.20230303131712.50"><vh>__contains__</vh></v>
<v t="karstenw.20230303131712.51"><vh>__sub__</vh></v>
<v t="karstenw.20230303131712.52"><vh>__and__</vh></v>
<v t="karstenw.20230303131712.53"><vh>__or__</vh></v>
<v t="karstenw.20230303131712.54"><vh>__xor__</vh></v>
<v t="karstenw.20230303131712.55"><vh>__isub__</vh></v>
<v t="karstenw.20230303131712.56"><vh>__iand__</vh></v>
<v t="karstenw.20230303131712.57"><vh>__ior__</vh></v>
<v t="karstenw.20230303131712.58"><vh>__ixor__</vh></v>
<v t="karstenw.20230303131712.59"><vh>__gt__</vh></v>
<v t="karstenw.20230303131712.60"><vh>__lt__</vh></v>
<v t="karstenw.20230303131712.61"><vh>__gte__</vh></v>
<v t="karstenw.20230303131712.62"><vh>__lte__</vh></v>
<v t="karstenw.20230303131712.63"><vh>add</vh></v>
<v t="karstenw.20230303131712.64"><vh>pop</vh></v>
<v t="karstenw.20230303131712.65"><vh>remove</vh></v>
<v t="karstenw.20230303131712.66"><vh>discard</vh></v>
<v t="karstenw.20230303131712.67"><vh>isdisjoint</vh></v>
<v t="karstenw.20230303131712.68"><vh>issubset</vh></v>
<v t="karstenw.20230303131712.69"><vh>issuperset</vh></v>
<v t="karstenw.20230303131712.70"><vh>union</vh></v>
<v t="karstenw.20230303131712.71"><vh>intersection</vh></v>
<v t="karstenw.20230303131712.72"><vh>difference</vh></v>
</v>
<v t="karstenw.20230304130553.1"><vh>+PARSER +</vh>
<v t="karstenw.20230303131712.73"><vh>_read</vh></v>
<v t="karstenw.20230303131712.74"><vh>class Lexicon</vh>
<v t="karstenw.20230303131712.75"><vh>__init__</vh></v>
<v t="karstenw.20230303131712.76"><vh>path</vh></v>
<v t="karstenw.20230303131712.77"><vh>load</vh></v>
</v>
</v>
<v t="karstenw.20230303131712.78"><vh>class Frequency</vh>
<v t="karstenw.20230303131712.79"><vh>__init__</vh></v>
<v t="karstenw.20230303131712.80"><vh>path</vh></v>
<v t="karstenw.20230303131712.81"><vh>load</vh></v>
</v>
<v t="karstenw.20230304130628.1"><vh>+ LANGUAGE MODEL +</vh>
<v t="karstenw.20230303131712.82"><vh>class Model</vh>
<v t="karstenw.20230303131712.83"><vh>__init__</vh></v>
<v t="karstenw.20230303131712.84"><vh>path</vh></v>
<v t="karstenw.20230303131712.85"><vh>load</vh></v>
<v t="karstenw.20230303131712.86"><vh>save</vh></v>
<v t="karstenw.20230303131712.87"><vh>train</vh></v>
<v t="karstenw.20230303131712.88"><vh>classify</vh></v>
<v t="karstenw.20230303131712.89"><vh>apply</vh></v>
<v t="karstenw.20230303131712.90"><vh>_v</vh></v>
<v t="karstenw.20230303131712.91"><vh>_get_description</vh></v>
<v t="karstenw.20230303131712.92"><vh>_set_description</vh></v>
</v>
</v>
<v t="karstenw.20230303131712.93"><vh>class Morphology</vh>
<v t="karstenw.20230303131712.94"><vh>__init__</vh></v>
<v t="karstenw.20230303131712.95"><vh>path</vh></v>
<v t="karstenw.20230303131712.96"><vh>load</vh></v>
<v t="karstenw.20230303131712.97"><vh>apply</vh></v>
<v t="karstenw.20230303131712.98"><vh>insert</vh></v>
<v t="karstenw.20230303131712.99"><vh>append</vh></v>
<v t="karstenw.20230303131712.100"><vh>extend</vh></v>
</v>
<v t="karstenw.20230303131712.101"><vh>class Context</vh>
<v t="karstenw.20230303131712.102"><vh>__init__</vh></v>
<v t="karstenw.20230303131712.103"><vh>path</vh></v>
<v t="karstenw.20230303131712.104"><vh>load</vh></v>
<v t="karstenw.20230303131712.105"><vh>apply</vh></v>
<v t="karstenw.20230303131712.106"><vh>insert</vh></v>
<v t="karstenw.20230303131712.107"><vh>append</vh></v>
<v t="karstenw.20230303131712.108"><vh>extend</vh></v>
</v>
<v t="karstenw.20230303131712.109"><vh>class Entities</vh>
<v t="karstenw.20230303131712.110"><vh>__init__</vh></v>
<v t="karstenw.20230303131712.111"><vh>path</vh></v>
<v t="karstenw.20230303131712.112"><vh>load</vh></v>
<v t="karstenw.20230303131712.113"><vh>apply</vh></v>
<v t="karstenw.20230303131712.114"><vh>append</vh></v>
<v t="karstenw.20230303131712.115"><vh>extend</vh></v>
</v>
<v t="karstenw.20230303131712.116"><vh>class Parser</vh>
<v t="karstenw.20230303131712.117"><vh>__init__</vh></v>
<v t="karstenw.20230303131712.118"><vh>find_keywords</vh></v>
<v t="karstenw.20230303131712.119"><vh>find_tokens</vh></v>
<v t="karstenw.20230303131712.120"><vh>find_tags</vh></v>
<v t="karstenw.20230303131712.121"><vh>find_chunks</vh></v>
<v t="karstenw.20230303131712.122"><vh>find_prepositions</vh></v>
<v t="karstenw.20230303131712.123"><vh>find_labels</vh></v>
<v t="karstenw.20230303131712.124"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303131712.125"><vh>parse</vh></v>
</v>
<v t="karstenw.20230303131712.126"><vh>class TaggedString</vh>
<v t="karstenw.20230303131712.127"><vh>__new__</vh></v>
<v t="karstenw.20230303131712.128"><vh>split</vh></v>
</v>
<v t="karstenw.20230303131712.129"><vh>penntreebank2universal</vh></v>
<v t="karstenw.20230303131712.130"><vh>find_tokens</vh></v>
<v t="karstenw.20230303131712.131"><vh>_suffix_rules</vh></v>
<v t="karstenw.20230303131712.132"><vh>find_tags</vh></v>
<v t="karstenw.20230304130812.1"><vh>+ PHRASE CHUNKER +</vh></v>
<v t="karstenw.20230303131712.133"><vh>find_chunks</vh></v>
<v t="karstenw.20230303131712.134"><vh>find_prepositions</vh></v>
<v t="karstenw.20230303131712.135"><vh>find_relations</vh></v>
<v t="karstenw.20230303131712.136"><vh>find_keywords</vh></v>
<v t="karstenw.20230303131712.137"><vh>commandline</vh></v>
<v t="karstenw.20230303131712.138"><vh>tense_id</vh></v>
<v t="karstenw.20230303131712.139"><vh>class Verbs</vh>
<v t="karstenw.20230303131712.140"><vh>__init__</vh></v>
<v t="karstenw.20230303131712.141"><vh>load</vh></v>
<v t="karstenw.20230303131712.142"><vh>path</vh></v>
<v t="karstenw.20230303131712.143"><vh>language</vh></v>
<v t="karstenw.20230303131712.144"><vh>infinitives</vh></v>
<v t="karstenw.20230303131712.145"><vh>inflections</vh></v>
<v t="karstenw.20230303131712.146"><vh>TENSES</vh></v>
<v t="karstenw.20230303131712.147"><vh>lemma</vh></v>
<v t="karstenw.20230303131712.148"><vh>lexeme</vh></v>
<v t="karstenw.20230303131712.149"><vh>conjugate</vh></v>
<v t="karstenw.20230303131712.150"><vh>tenses</vh></v>
<v t="karstenw.20230303131712.151"><vh>find_lemma</vh></v>
<v t="karstenw.20230303131712.152"><vh>find_lexeme</vh></v>
</v>
<v t="karstenw.20230303131712.153"><vh>class Tenses</vh>
<v t="karstenw.20230303131712.154"><vh>__contains__</vh></v>
</v>
<v t="karstenw.20230304130935.1"><vh>+ SENTIMENT POLARITY LEXICON +</vh></v>
<v t="karstenw.20230303131712.155"><vh>avg</vh></v>
<v t="karstenw.20230303131712.156"><vh>class Score</vh>
<v t="karstenw.20230303131712.157"><vh>__new__</vh></v>
<v t="karstenw.20230303131712.158"><vh>__init__</vh></v>
</v>
<v t="karstenw.20230303131712.159"><vh>class Sentiment</vh>
<v t="karstenw.20230303131712.160"><vh>__init__</vh></v>
<v t="karstenw.20230303131712.161"><vh>path</vh></v>
<v t="karstenw.20230303131712.162"><vh>language</vh></v>
<v t="karstenw.20230303131712.163"><vh>confidence</vh></v>
<v t="karstenw.20230303131712.164"><vh>load</vh></v>
<v t="karstenw.20230303131712.165"><vh>synset</vh></v>
<v t="karstenw.20230303131712.166"><vh>__call__</vh></v>
<v t="karstenw.20230303131712.167"><vh>assessments</vh></v>
<v t="karstenw.20230303131712.168"><vh>annotate</vh></v>
<v t="karstenw.20230303131712.169"><vh>save</vh></v>
</v>
<v t="karstenw.20230303131712.170"><vh>class Spelling</vh>
<v t="karstenw.20230303131712.171"><vh>__init__</vh></v>
<v t="karstenw.20230303131712.172"><vh>load</vh></v>
<v t="karstenw.20230303131712.173"><vh>path</vh></v>
<v t="karstenw.20230303131712.174"><vh>language</vh></v>
<v t="karstenw.20230303131712.175"><vh>train</vh></v>
<v t="karstenw.20230303131712.176"><vh>_edit1</vh></v>
<v t="karstenw.20230303131712.177"><vh>_edit2</vh></v>
<v t="karstenw.20230303131712.178"><vh>_known</vh></v>
<v t="karstenw.20230303131712.179"><vh>suggest</vh></v>
</v>
<v t="karstenw.20230304131008.1"><vh>+ MULTILINGUAL +</vh></v>
<v t="karstenw.20230303131712.180"><vh>_module</vh></v>
<v t="karstenw.20230303131712.181"><vh>_multilingual</vh></v>
<v t="karstenw.20230303131712.182"><vh>language</vh></v>
<v t="karstenw.20230303131712.183"><vh>tokenize</vh></v>
<v t="karstenw.20230303131712.184"><vh>parse</vh></v>
<v t="karstenw.20230303131712.185"><vh>parsetree</vh></v>
<v t="karstenw.20230303131712.186"><vh>split</vh></v>
<v t="karstenw.20230303131712.187"><vh>tag</vh></v>
<v t="karstenw.20230303131712.188"><vh>keywords</vh></v>
<v t="karstenw.20230303131712.189"><vh>suggest</vh></v>
<v t="karstenw.20230303131712.190"><vh>sentiment</vh></v>
<v t="karstenw.20230303131712.191"><vh>singularize</vh></v>
<v t="karstenw.20230303131712.192"><vh>pluralize</vh></v>
<v t="karstenw.20230303131712.193"><vh>conjugate</vh></v>
<v t="karstenw.20230303131712.194"><vh>predicative</vh></v>
<v t="karstenw.20230303131712.195"><vh>suggest</vh></v>
</v>
<v t="karstenw.20230303131752.1"><vh>@clean pattern/text/search.py</vh>
<v t="karstenw.20230303131819.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303131819.2"><vh>class Text</vh>
<v t="karstenw.20230303131819.3"><vh>__init__</vh></v>
<v t="karstenw.20230303131819.4"><vh>sentences</vh></v>
<v t="karstenw.20230303131819.5"><vh>words</vh></v>
</v>
<v t="karstenw.20230303131819.6"><vh>class Sentence</vh>
<v t="karstenw.20230303131819.7"><vh>__init__</vh></v>
<v t="karstenw.20230303131819.8"><vh>string</vh></v>
<v t="karstenw.20230303131819.9"><vh>words</vh></v>
<v t="karstenw.20230303131819.10"><vh>chunks</vh></v>
</v>
<v t="karstenw.20230303131819.11"><vh>class Word</vh>
<v t="karstenw.20230303131819.12"><vh>__init__</vh></v>
<v t="karstenw.20230303131819.13"><vh>__repr__</vh></v>
<v t="karstenw.20230303131819.14"><vh>_get_type</vh></v>
<v t="karstenw.20230303131819.15"><vh>_set_type</vh></v>
<v t="karstenw.20230303131819.16"><vh>chunk</vh></v>
<v t="karstenw.20230303131819.17"><vh>lemma</vh></v>
</v>
<v t="karstenw.20230303131819.18"><vh>_match</vh></v>
<v t="karstenw.20230303131819.19"><vh>unique</vh></v>
<v t="karstenw.20230303131819.20"><vh>find</vh></v>
<v t="karstenw.20230303131819.21"><vh>combinations</vh></v>
<v t="karstenw.20230303131819.22"><vh>product</vh></v>
<v t="karstenw.20230303131819.23"><vh>variations</vh></v>
<v t="karstenw.20230303131819.24"><vh>class odict</vh>
<v t="karstenw.20230303131819.25"><vh>__init__</vh></v>
<v t="karstenw.20230303131819.26"><vh>fromkeys</vh></v>
<v t="karstenw.20230303131819.27"><vh>push</vh></v>
<v t="karstenw.20230303131819.28"><vh>__iter__</vh></v>
<v t="karstenw.20230303131819.29"><vh>__setitem__</vh></v>
<v t="karstenw.20230303131819.30"><vh>__delitem__</vh></v>
<v t="karstenw.20230303131819.31"><vh>update</vh></v>
<v t="karstenw.20230303131819.32"><vh>setdefault</vh></v>
<v t="karstenw.20230303131819.33"><vh>pop</vh></v>
<v t="karstenw.20230303131819.34"><vh>popitem</vh></v>
<v t="karstenw.20230303131819.35"><vh>clear</vh></v>
<v t="karstenw.20230303131819.36"><vh>iterkeys</vh></v>
<v t="karstenw.20230303131819.37"><vh>itervalues</vh></v>
<v t="karstenw.20230303131819.38"><vh>iteritems</vh></v>
<v t="karstenw.20230303131819.39"><vh>keys</vh></v>
<v t="karstenw.20230303131819.40"><vh>values</vh></v>
<v t="karstenw.20230303131819.41"><vh>items</vh></v>
<v t="karstenw.20230303131819.42"><vh>copy</vh></v>
<v t="karstenw.20230303131819.43"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303131819.44"><vh>class Taxonomy</vh>
<v t="karstenw.20230303131819.45"><vh>__init__</vh></v>
<v t="karstenw.20230303131819.46"><vh>_normalize</vh></v>
<v t="karstenw.20230303131819.47"><vh>__contains__</vh></v>
<v t="karstenw.20230303131819.48"><vh>append</vh></v>
<v t="karstenw.20230303131819.49"><vh>classify</vh></v>
<v t="karstenw.20230303131819.50"><vh>parents</vh></v>
<v t="karstenw.20230303131819.51"><vh>children</vh></v>
<v t="karstenw.20230303131819.52"><vh>value</vh></v>
<v t="karstenw.20230303131819.53"><vh>remove</vh></v>
</v>
<v t="karstenw.20230303131819.54"><vh>class Classifier</vh>
<v t="karstenw.20230303131819.55"><vh>__init__</vh></v>
</v>
<v t="karstenw.20230303131819.56"><vh>class WordNetClassifier</vh>
<v t="karstenw.20230303131819.57"><vh>__init__</vh></v>
<v t="karstenw.20230303131819.58"><vh>_children</vh></v>
<v t="karstenw.20230303131819.59"><vh>_parents</vh></v>
</v>
<v t="karstenw.20230303131819.60"><vh>class Constraint</vh>
<v t="karstenw.20230303131819.61"><vh>__init__</vh></v>
<v t="karstenw.20230303131819.62"><vh>fromstring</vh></v>
<v t="karstenw.20230303131819.63"><vh>_append</vh></v>
<v t="karstenw.20230303131819.64"><vh>match</vh></v>
<v t="karstenw.20230303131819.65"><vh>__repr__</vh></v>
<v t="karstenw.20230303131819.66"><vh>string</vh></v>
</v>
<v t="karstenw.20230303131819.67"><vh>class Pattern</vh>
<v t="karstenw.20230303131819.68"><vh>__init__</vh></v>
<v t="karstenw.20230303131819.69"><vh>__iter__</vh></v>
<v t="karstenw.20230303131819.70"><vh>__len__</vh></v>
<v t="karstenw.20230303131819.71"><vh>__getitem__</vh></v>
<v t="karstenw.20230303131819.72"><vh>fromstring</vh></v>
<v t="karstenw.20230303131819.73"><vh>scan</vh></v>
<v t="karstenw.20230303131819.74"><vh>search</vh></v>
<v t="karstenw.20230303131819.75"><vh>match</vh></v>
<v t="karstenw.20230303131819.76"><vh>_variations</vh></v>
<v t="karstenw.20230303131819.77"><vh>_match</vh></v>
<v t="karstenw.20230303131819.78"><vh>string</vh></v>
</v>
<v t="karstenw.20230303131819.79"><vh>compile</vh></v>
<v t="karstenw.20230303131819.80"><vh>scan</vh></v>
<v t="karstenw.20230303131819.81"><vh>match</vh></v>
<v t="karstenw.20230303131819.82"><vh>search</vh></v>
<v t="karstenw.20230303131819.83"><vh>escape</vh></v>
<v t="karstenw.20230303131819.84"><vh>class Match</vh>
<v t="karstenw.20230303131819.85"><vh>__init__</vh></v>
<v t="karstenw.20230303131819.86"><vh>__len__</vh></v>
<v t="karstenw.20230303131819.87"><vh>__iter__</vh></v>
<v t="karstenw.20230303131819.88"><vh>__getitem__</vh></v>
<v t="karstenw.20230303131819.89"><vh>start</vh></v>
<v t="karstenw.20230303131819.90"><vh>stop</vh></v>
<v t="karstenw.20230303131819.91"><vh>constraint</vh></v>
<v t="karstenw.20230303131819.92"><vh>constraints</vh></v>
<v t="karstenw.20230303131819.93"><vh>constituents</vh></v>
<v t="karstenw.20230303131819.94"><vh>group</vh></v>
<v t="karstenw.20230303131819.95"><vh>string</vh></v>
<v t="karstenw.20230303131819.96"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303131819.97"><vh>class Group</vh>
<v t="karstenw.20230303131819.98"><vh>__init__</vh></v>
<v t="karstenw.20230303131819.99"><vh>words</vh></v>
<v t="karstenw.20230303131819.100"><vh>start</vh></v>
<v t="karstenw.20230303131819.101"><vh>stop</vh></v>
<v t="karstenw.20230303131819.102"><vh>string</vh></v>
</v>
</v>
<v t="karstenw.20230303131805.1"><vh>@clean pattern/text/tree.py</vh>
<v t="karstenw.20230303131812.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303131812.2"><vh>find</vh></v>
<v t="karstenw.20230303131812.3"><vh>intersects</vh></v>
<v t="karstenw.20230303131812.4"><vh>unique</vh></v>
<v t="karstenw.20230303131812.5"><vh>zip</vh></v>
<v t="karstenw.20230303131812.6"><vh>unzip</vh></v>
<v t="karstenw.20230303131812.7"><vh>class Map</vh>
<v t="karstenw.20230303131812.8"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.9"><vh>items</vh></v>
<v t="karstenw.20230303131812.10"><vh>__repr__</vh></v>
<v t="karstenw.20230303131812.11"><vh>__getitem__</vh></v>
<v t="karstenw.20230303131812.12"><vh>__len__</vh></v>
<v t="karstenw.20230303131812.13"><vh>__iter__</vh></v>
</v>
<v t="karstenw.20230303131812.14"><vh>class Word</vh>
<v t="karstenw.20230303131812.15"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.16"><vh>copy</vh></v>
<v t="karstenw.20230303131812.17"><vh>_get_tag</vh></v>
<v t="karstenw.20230303131812.18"><vh>_set_tag</vh></v>
<v t="karstenw.20230303131812.19"><vh>phrase</vh></v>
<v t="karstenw.20230303131812.20"><vh>prepositional_phrase</vh></v>
<v t="karstenw.20230303131812.21"><vh>tags</vh></v>
<v t="karstenw.20230303131812.22"><vh>custom_tags</vh></v>
<v t="karstenw.20230303131812.23"><vh>next</vh></v>
<v t="karstenw.20230303131812.24"><vh>previous</vh></v>
<v t="karstenw.20230303131812.25"><vh>__getattr__</vh></v>
<v t="karstenw.20230303131812.26"><vh>__str__</vh></v>
<v t="karstenw.20230303131812.27"><vh>__repr__</vh></v>
<v t="karstenw.20230303131812.28"><vh>__eq__</vh></v>
<v t="karstenw.20230303131812.29"><vh>__ne__</vh></v>
</v>
<v t="karstenw.20230303131812.30"><vh>class Tags</vh>
<v t="karstenw.20230303131812.31"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.32"><vh>__setitem__</vh></v>
<v t="karstenw.20230303131812.33"><vh>setdefault</vh></v>
</v>
<v t="karstenw.20230303131812.34"><vh>class Chunk</vh>
<v t="karstenw.20230303131812.35"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.36"><vh>extend</vh></v>
<v t="karstenw.20230303131812.37"><vh>append</vh></v>
<v t="karstenw.20230303131812.38"><vh>__getitem__</vh></v>
<v t="karstenw.20230303131812.39"><vh>__len__</vh></v>
<v t="karstenw.20230303131812.40"><vh>__iter__</vh></v>
<v t="karstenw.20230303131812.41"><vh>_get_tag</vh></v>
<v t="karstenw.20230303131812.42"><vh>_set_tag</vh></v>
<v t="karstenw.20230303131812.43"><vh>start</vh></v>
<v t="karstenw.20230303131812.44"><vh>stop</vh></v>
<v t="karstenw.20230303131812.45"><vh>range</vh></v>
<v t="karstenw.20230303131812.46"><vh>span</vh></v>
<v t="karstenw.20230303131812.47"><vh>lemmata</vh></v>
<v t="karstenw.20230303131812.48"><vh>tagged</vh></v>
<v t="karstenw.20230303131812.49"><vh>head</vh></v>
<v t="karstenw.20230303131812.50"><vh>relation</vh></v>
<v t="karstenw.20230303131812.51"><vh>role</vh></v>
<v t="karstenw.20230303131812.52"><vh>subject</vh></v>
<v t="karstenw.20230303131812.53"><vh>object</vh></v>
<v t="karstenw.20230303131812.54"><vh>verb</vh></v>
<v t="karstenw.20230303131812.55"><vh>related</vh></v>
<v t="karstenw.20230303131812.56"><vh>prepositional_phrase</vh></v>
<v t="karstenw.20230303131812.57"><vh>anchor_id</vh></v>
<v t="karstenw.20230303131812.58"><vh>conjunctions</vh></v>
<v t="karstenw.20230303131812.59"><vh>modifiers</vh></v>
<v t="karstenw.20230303131812.60"><vh>nearest</vh></v>
<v t="karstenw.20230303131812.61"><vh>next</vh></v>
<v t="karstenw.20230303131812.62"><vh>previous</vh></v>
<v t="karstenw.20230303131812.63"><vh>string</vh></v>
<v t="karstenw.20230303131812.64"><vh>__str__</vh></v>
<v t="karstenw.20230303131812.65"><vh>__repr__</vh></v>
<v t="karstenw.20230303131812.66"><vh>__eq__</vh></v>
<v t="karstenw.20230303131812.67"><vh>__ne__</vh></v>
</v>
<v t="karstenw.20230303131812.68"><vh>class Chink</vh>
<v t="karstenw.20230303131812.69"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303131812.70"><vh>class PNPChunk</vh>
<v t="karstenw.20230303131812.71"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.72"><vh>append</vh></v>
<v t="karstenw.20230303131812.73"><vh>preposition</vh></v>
<v t="karstenw.20230303131812.74"><vh>phrases</vh></v>
<v t="karstenw.20230303131812.75"><vh>guess_anchor</vh></v>
</v>
<v t="karstenw.20230303131812.76"><vh>class Conjunctions</vh>
<v t="karstenw.20230303131812.77"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.78"><vh>append</vh></v>
</v>
<v t="karstenw.20230303131812.79"><vh>_uid</vh></v>
<v t="karstenw.20230303131812.80"><vh>_is_tokenstring</vh></v>
<v t="karstenw.20230303131812.81"><vh>class Sentence</vh>
<v t="karstenw.20230303131812.82"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.83"><vh>word</vh></v>
<v t="karstenw.20230303131812.84"><vh>lemmata</vh></v>
<v t="karstenw.20230303131812.85"><vh>parts_of_speech</vh></v>
<v t="karstenw.20230303131812.86"><vh>tagged</vh></v>
<v t="karstenw.20230303131812.87"><vh>phrases</vh></v>
<v t="karstenw.20230303131812.88"><vh>prepositional_phrases</vh></v>
<v t="karstenw.20230303131812.89"><vh>start</vh></v>
<v t="karstenw.20230303131812.90"><vh>stop</vh></v>
<v t="karstenw.20230303131812.91"><vh>nouns</vh></v>
<v t="karstenw.20230303131812.92"><vh>verbs</vh></v>
<v t="karstenw.20230303131812.93"><vh>adjectives</vh></v>
<v t="karstenw.20230303131812.94"><vh>subjects</vh></v>
<v t="karstenw.20230303131812.95"><vh>objects</vh></v>
<v t="karstenw.20230303131812.96"><vh>verbs</vh></v>
<v t="karstenw.20230303131812.97"><vh>anchors</vh></v>
<v t="karstenw.20230303131812.98"><vh>is_question</vh></v>
<v t="karstenw.20230303131812.99"><vh>is_exclamation</vh></v>
<v t="karstenw.20230303131812.100"><vh>__getitem__</vh></v>
<v t="karstenw.20230303131812.101"><vh>__len__</vh></v>
<v t="karstenw.20230303131812.102"><vh>__iter__</vh></v>
<v t="karstenw.20230303131812.103"><vh>append</vh></v>
<v t="karstenw.20230303131812.104"><vh>parse_token</vh></v>
<v t="karstenw.20230303131812.105"><vh>_parse_relation</vh></v>
<v t="karstenw.20230303131812.106"><vh>_do_word</vh></v>
<v t="karstenw.20230303131812.107"><vh>_do_chunk</vh></v>
<v t="karstenw.20230303131812.108"><vh>_do_relation</vh></v>
<v t="karstenw.20230303131812.109"><vh>_do_pnp</vh></v>
<v t="karstenw.20230303131812.110"><vh>_do_anchor</vh></v>
<v t="karstenw.20230303131812.111"><vh>_do_custom</vh></v>
<v t="karstenw.20230303131812.112"><vh>_do_conjunction</vh></v>
<v t="karstenw.20230303131812.113"><vh>get</vh></v>
<v t="karstenw.20230303131812.114"><vh>loop</vh></v>
<v t="karstenw.20230303131812.115"><vh>indexof</vh></v>
<v t="karstenw.20230303131812.116"><vh>slice</vh></v>
<v t="karstenw.20230303131812.117"><vh>copy</vh></v>
<v t="karstenw.20230303131812.118"><vh>chunked</vh></v>
<v t="karstenw.20230303131812.119"><vh>constituents</vh></v>
<v t="karstenw.20230303131812.120"><vh>string</vh></v>
<v t="karstenw.20230303131812.121"><vh>__str__</vh></v>
<v t="karstenw.20230303131812.122"><vh>__repr__</vh></v>
<v t="karstenw.20230303131812.123"><vh>__eq__</vh></v>
<v t="karstenw.20230303131812.124"><vh>xml</vh></v>
<v t="karstenw.20230303131812.125"><vh>from_xml</vh></v>
<v t="karstenw.20230303131812.126"><vh>nltk_tree</vh></v>
</v>
<v t="karstenw.20230303131812.127"><vh>class Slice</vh>
<v t="karstenw.20230303131812.128"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.129"><vh>start</vh></v>
<v t="karstenw.20230303131812.130"><vh>stop</vh></v>
</v>
<v t="karstenw.20230303131812.131"><vh>chunked</vh></v>
<v t="karstenw.20230303131812.132"><vh>class Text</vh>
<v t="karstenw.20230303131812.133"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.134"><vh>insert</vh></v>
<v t="karstenw.20230303131812.135"><vh>append</vh></v>
<v t="karstenw.20230303131812.136"><vh>extend</vh></v>
<v t="karstenw.20230303131812.137"><vh>remove</vh></v>
<v t="karstenw.20230303131812.138"><vh>pop</vh></v>
<v t="karstenw.20230303131812.139"><vh>sentences</vh></v>
<v t="karstenw.20230303131812.140"><vh>words</vh></v>
<v t="karstenw.20230303131812.141"><vh>copy</vh></v>
<v t="karstenw.20230303131812.142"><vh>string</vh></v>
<v t="karstenw.20230303131812.143"><vh>__str__</vh></v>
<v t="karstenw.20230303131812.144"><vh>xml</vh></v>
<v t="karstenw.20230303131812.145"><vh>from_xml</vh></v>
</v>
<v t="karstenw.20230303131812.146"><vh>tree</vh></v>
<v t="karstenw.20230303131812.147"><vh>xml</vh></v>
<v t="karstenw.20230303131812.148"><vh>xml_encode</vh></v>
<v t="karstenw.20230303131812.149"><vh>xml_decode</vh></v>
<v t="karstenw.20230303131812.150"><vh>parse_xml</vh></v>
<v t="karstenw.20230303131812.151"><vh>class XML</vh>
<v t="karstenw.20230303131812.152"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.153"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230303131812.154"><vh>class XMLNode</vh>
<v t="karstenw.20230303131812.155"><vh>__init__</vh></v>
<v t="karstenw.20230303131812.156"><vh>tag</vh></v>
<v t="karstenw.20230303131812.157"><vh>value</vh></v>
<v t="karstenw.20230303131812.158"><vh>__iter__</vh></v>
<v t="karstenw.20230303131812.159"><vh>__getitem__</vh></v>
<v t="karstenw.20230303131812.160"><vh>get</vh></v>
</v>
<v t="karstenw.20230303131812.161"><vh>class TaggedString</vh>
<v t="karstenw.20230303131812.162"><vh>__new__</vh></v>
</v>
<v t="karstenw.20230303131812.163"><vh>parse_string</vh></v>
<v t="karstenw.20230303131812.164"><vh>_parse_tokens</vh></v>
<v t="karstenw.20230303131812.165"><vh>_parse_relation</vh></v>
<v t="karstenw.20230303131812.166"><vh>_parse_token</vh></v>
<v t="karstenw.20230303131812.167"><vh>nltk_tree</vh></v>
<v t="karstenw.20230303131812.168"><vh>_colorize</vh></v>
<v t="karstenw.20230303131812.169"><vh>graphviz_dot</vh></v>
<v t="karstenw.20230303131812.170"><vh>table</vh></v>
</v>
<v t="karstenw.20230303132610.1"><vh>de</vh>
<v t="karstenw.20230303132508.1"><vh>@clean pattern/text/de/__init__.py</vh>
<v t="karstenw.20230303132547.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303132547.2"><vh>stts2penntreebank</vh></v>
<v t="karstenw.20230303132547.3"><vh>stts2universal</vh></v>
<v t="karstenw.20230303132547.4"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303132547.5"><vh>class Parser</vh>
<v t="karstenw.20230303132547.6"><vh>find_tokens</vh></v>
<v t="karstenw.20230303132547.7"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303132547.8"><vh>find_tags</vh></v>
</v>
<v t="karstenw.20230303132547.9"><vh>tokenize</vh></v>
<v t="karstenw.20230303132547.10"><vh>parse</vh></v>
<v t="karstenw.20230303132547.11"><vh>parsetree</vh></v>
<v t="karstenw.20230303132547.12"><vh>tree</vh></v>
<v t="karstenw.20230303132547.13"><vh>tag</vh></v>
<v t="karstenw.20230303132547.14"><vh>keywords</vh></v>
<v t="karstenw.20230303132547.15"><vh>suggest</vh></v>
</v>
<v t="karstenw.20230303132624.1"><vh>@clean pattern/text/de/__main__.py</vh>
<v t="karstenw.20230303132635.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230303132715.1"><vh>@clean pattern/text/de/inflect.py</vh>
<v t="karstenw.20230303132723.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303132723.2"><vh>definite_article</vh></v>
<v t="karstenw.20230303132723.3"><vh>indefinite_article</vh></v>
<v t="karstenw.20230303132723.4"><vh>article</vh></v>
<v t="karstenw.20230303132723.5"><vh>referenced</vh></v>
<v t="karstenw.20230303132723.6"><vh>gender</vh></v>
<v t="karstenw.20230303132723.7"><vh>pluralize</vh></v>
<v t="karstenw.20230303132723.8"><vh>singularize</vh></v>
<v t="karstenw.20230303132723.9"><vh>encode_sz</vh></v>
<v t="karstenw.20230303132723.10"><vh>decode_sz</vh></v>
<v t="karstenw.20230303132723.11"><vh>class Verbs</vh>
<v t="karstenw.20230303132723.12"><vh>__init__</vh></v>
<v t="karstenw.20230303132723.13"><vh>find_lemma</vh></v>
<v t="karstenw.20230303132723.14"><vh>find_lexeme</vh></v>
<v t="karstenw.20230303132723.15"><vh>tenses</vh></v>
</v>
<v t="karstenw.20230303132723.16"><vh>attributive</vh></v>
<v t="karstenw.20230303132723.17"><vh>predicative</vh></v>
<v t="karstenw.20230303132723.18"><vh>grade</vh></v>
<v t="karstenw.20230303132723.19"><vh>comparative</vh></v>
<v t="karstenw.20230303132723.20"><vh>superlative</vh></v>
</v>
</v>
<v t="karstenw.20230303132745.1"><vh>en</vh>
<v t="karstenw.20230303132747.1"><vh>@clean pattern/text/en/__init__.py</vh>
<v t="karstenw.20230303132805.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303132805.2"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303132805.3"><vh>class Parser</vh>
<v t="karstenw.20230303132805.4"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303132805.5"><vh>find_tags</vh></v>
</v>
<v t="karstenw.20230303132805.6"><vh>class Sentiment</vh>
<v t="karstenw.20230303132805.7"><vh>load</vh></v>
</v>
<v t="karstenw.20230310124020.1"><vh>setup Parser</vh></v>
<v t="karstenw.20230303132805.8"><vh>tokenize</vh></v>
<v t="karstenw.20230303132805.9"><vh>parse</vh></v>
<v t="karstenw.20230303132805.10"><vh>parsetree</vh></v>
<v t="karstenw.20230303132805.11"><vh>tree</vh></v>
<v t="karstenw.20230303132805.12"><vh>tag</vh></v>
<v t="karstenw.20230303132805.13"><vh>keywords</vh></v>
<v t="karstenw.20230303132805.14"><vh>suggest</vh></v>
<v t="karstenw.20230303132805.15"><vh>polarity</vh></v>
<v t="karstenw.20230303132805.16"><vh>subjectivity</vh></v>
<v t="karstenw.20230303132805.17"><vh>positive</vh></v>
</v>
<v t="karstenw.20230303132812.1"><vh>@clean pattern/text/en/__main__.py</vh>
<v t="karstenw.20230303132818.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230303132841.1"><vh>@clean pattern/text/en/inflect.py</vh>
<v t="karstenw.20230303132850.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303132850.2"><vh>definite_article</vh></v>
<v t="karstenw.20230303132850.3"><vh>indefinite_article</vh></v>
<v t="karstenw.20230303132850.4"><vh>article</vh></v>
<v t="karstenw.20230303132850.5"><vh>referenced</vh></v>
<v t="karstenw.20230303132850.6"><vh>pluralize</vh></v>
<v t="karstenw.20230303132850.7"><vh>singularize</vh></v>
<v t="karstenw.20230303132850.8"><vh>class Verbs</vh>
<v t="karstenw.20230303132850.9"><vh>__init__</vh></v>
<v t="karstenw.20230303132850.10"><vh>find_lemma</vh></v>
<v t="karstenw.20230303132850.11"><vh>find_lexeme</vh></v>
</v>
<v t="karstenw.20230303132850.12"><vh>_count_syllables</vh></v>
<v t="karstenw.20230303132850.13"><vh>grade</vh></v>
<v t="karstenw.20230303132850.14"><vh>comparative</vh></v>
<v t="karstenw.20230303132850.15"><vh>superlative</vh></v>
<v t="karstenw.20230303132850.16"><vh>attributive</vh></v>
<v t="karstenw.20230303132850.17"><vh>predicative</vh></v>
</v>
<v t="karstenw.20230303132820.1"><vh>@clean pattern/text/en/inflect_quantify.py</vh>
<v t="karstenw.20230303132853.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303132853.2"><vh>zshift</vh></v>
<v t="karstenw.20230303132853.3"><vh>number</vh></v>
<v t="karstenw.20230303132853.4"><vh>numerals</vh></v>
<v t="karstenw.20230303132853.5"><vh>approximate</vh></v>
<v t="karstenw.20230303132853.6"><vh>count</vh></v>
<v t="karstenw.20230303132853.7"><vh>reflect</vh></v>
</v>
<v t="karstenw.20230303132915.1"><vh>@clean pattern/text/en/modality.py</vh>
<v t="karstenw.20230303132921.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303132921.2"><vh>find</vh></v>
<v t="karstenw.20230303132921.3"><vh>s</vh></v>
<v t="karstenw.20230303132921.4"><vh>join</vh></v>
<v t="karstenw.20230303132921.5"><vh>question</vh></v>
<v t="karstenw.20230303132921.6"><vh>verb</vh></v>
<v t="karstenw.20230303132921.7"><vh>verbs</vh></v>
<v t="karstenw.20230303132921.8"><vh>imperative</vh></v>
<v t="karstenw.20230303132921.9"><vh>conditional</vh></v>
<v t="karstenw.20230303132921.10"><vh>subjunctive</vh></v>
<v t="karstenw.20230303132921.11"><vh>negated</vh></v>
<v t="karstenw.20230303132921.12"><vh>mood</vh></v>
<v t="karstenw.20230303132921.13"><vh>d</vh></v>
<v t="karstenw.20230303132921.14"><vh>modality</vh></v>
<v t="karstenw.20230303132921.15"><vh>uncertain</vh></v>
</v>
<v t="karstenw.20230303132932.1"><vh>wordlist</vh>
<v t="karstenw.20230303132938.1"><vh>@clean pattern/text/en/wordlist/__init__.py</vh>
<v t="karstenw.20230303132956.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303132956.2"><vh>class Wordlist</vh>
<v t="karstenw.20230303132956.3"><vh>__init__</vh></v>
<v t="karstenw.20230303132956.4"><vh>_load</vh></v>
<v t="karstenw.20230303132956.5"><vh>__repr__</vh></v>
<v t="karstenw.20230303132956.6"><vh>__iter__</vh></v>
<v t="karstenw.20230303132956.7"><vh>__len__</vh></v>
<v t="karstenw.20230303132956.8"><vh>__contains__</vh></v>
<v t="karstenw.20230303132956.9"><vh>__add__</vh></v>
<v t="karstenw.20230303132956.10"><vh>__getitem__</vh></v>
<v t="karstenw.20230303132956.11"><vh>__setitem__</vh></v>
<v t="karstenw.20230303132956.12"><vh>insert</vh></v>
<v t="karstenw.20230303132956.13"><vh>append</vh></v>
<v t="karstenw.20230303132956.14"><vh>extend</vh></v>
</v>
</v>
</v>
<v t="karstenw.20230303133026.1"><vh>wordnet</vh>
<v t="karstenw.20230303133031.1"><vh>@clean pattern/text/en/wordnet/__init__.py</vh>
<v t="karstenw.20230303133045.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303133045.2"><vh>normalize</vh></v>
<v t="karstenw.20230310152332.1"><vh>Synsets</vh>
<v t="karstenw.20230303133045.3"><vh>synsets</vh></v>
<v t="karstenw.20230303133045.4"><vh>class _synset</vh>
<v t="karstenw.20230303133045.5"><vh>__getitem__</vh></v>
</v>
<v t="karstenw.20230303133045.6"><vh>class Synset</vh>
<v t="karstenw.20230303133045.7"><vh>__init__</vh></v>
<v t="karstenw.20230303133045.8"><vh>__iter__</vh></v>
<v t="karstenw.20230303133045.9"><vh>__len__</vh></v>
<v t="karstenw.20230303133045.10"><vh>__getitem__</vh></v>
<v t="karstenw.20230303133045.11"><vh>__eq__</vh></v>
<v t="karstenw.20230303133045.12"><vh>__ne__</vh></v>
<v t="karstenw.20230303133045.13"><vh>id</vh></v>
<v t="karstenw.20230303133045.14"><vh>pos</vh></v>
<v t="karstenw.20230303133045.15"><vh>synonyms</vh></v>
<v t="karstenw.20230303133045.16"><vh>gloss</vh></v>
<v t="karstenw.20230303133045.17"><vh>lexname</vh></v>
<v t="karstenw.20230303133045.18"><vh>antonym</vh></v>
<v t="karstenw.20230303133045.19"><vh>meronyms</vh></v>
<v t="karstenw.20230303133045.20"><vh>holonyms</vh></v>
<v t="karstenw.20230303133045.21"><vh>hyponyms</vh></v>
<v t="karstenw.20230303133045.22"><vh>hypernyms</vh></v>
<v t="karstenw.20230303133045.23"><vh>hypernym</vh></v>
<v t="karstenw.20230303133045.24"><vh>similar</vh></v>
<v t="karstenw.20230303133045.25"><vh>similarity</vh></v>
<v t="karstenw.20230303133045.26"><vh>ic</vh></v>
<v t="karstenw.20230303133045.27"><vh>weight</vh></v>
</v>
<v t="karstenw.20230303133045.28"><vh>similarity</vh></v>
<v t="karstenw.20230303133045.29"><vh>ancestor</vh></v>
</v>
<v t="karstenw.20230310152428.1"><vh>Information Content</vh></v>
<v t="karstenw.20230310152508.1"><vh>WordNet3 to WordNet2</vh>
<v t="karstenw.20230303133045.30"><vh>map32</vh></v>
</v>
<v t="karstenw.20230310152553.1"><vh>SentiWordNet</vh>
<v t="karstenw.20230303133045.31"><vh>class SentiWordNet</vh>
<v t="karstenw.20230303133045.32"><vh>__init__</vh></v>
<v t="karstenw.20230303133045.33"><vh>load</vh></v>
<v t="karstenw.20230303133045.34"><vh>synset</vh></v>
<v t="karstenw.20230303133045.35"><vh>__getitem__</vh></v>
<v t="karstenw.20230303133045.36"><vh>assessments</vh></v>
<v t="karstenw.20230303133045.37"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230303133045.38"><vh>class sentiment</vh>
<v t="karstenw.20230303133045.39"><vh>load</vh></v>
<v t="karstenw.20230303133045.40"><vh>__getitem__</vh></v>
<v t="karstenw.20230303133045.41"><vh>__contains__</vh></v>
</v>
</v>
</v>
</v>
</v>
<v t="karstenw.20230303134347.1"><vh>es</vh>
<v t="karstenw.20230303134359.1"><vh>@clean pattern/text/es/__init__.py</vh>
<v t="karstenw.20230303134406.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303134406.2"><vh>parole2penntreebank</vh></v>
<v t="karstenw.20230303134406.3"><vh>parole2universal</vh></v>
<v t="karstenw.20230303134406.4"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303134406.5"><vh>class Parser</vh>
<v t="karstenw.20230303134406.6"><vh>find_tokens</vh></v>
<v t="karstenw.20230303134406.7"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303134406.8"><vh>find_tags</vh></v>
</v>
<v t="karstenw.20230303134406.9"><vh>tokenize</vh></v>
<v t="karstenw.20230303134406.10"><vh>parse</vh></v>
<v t="karstenw.20230303134406.11"><vh>parsetree</vh></v>
<v t="karstenw.20230303134406.12"><vh>tree</vh></v>
<v t="karstenw.20230303134406.13"><vh>tag</vh></v>
<v t="karstenw.20230303134406.14"><vh>keywords</vh></v>
<v t="karstenw.20230303134406.15"><vh>suggest</vh></v>
</v>
<v t="karstenw.20230303134421.1"><vh>@clean pattern/text/es/__main__.py</vh>
<v t="karstenw.20230303134428.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230303134433.1"><vh>@clean pattern/text/es/inflect.py</vh>
<v t="karstenw.20230303134440.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303134440.2"><vh>normalize</vh></v>
<v t="karstenw.20230303134440.3"><vh>definite_article</vh></v>
<v t="karstenw.20230303134440.4"><vh>indefinite_article</vh></v>
<v t="karstenw.20230303134440.5"><vh>article</vh></v>
<v t="karstenw.20230303134440.6"><vh>referenced</vh></v>
<v t="karstenw.20230303134440.7"><vh>pluralize</vh></v>
<v t="karstenw.20230303134440.8"><vh>singularize</vh></v>
<v t="karstenw.20230303134440.9"><vh>class Verbs</vh>
<v t="karstenw.20230303134440.10"><vh>__init__</vh></v>
<v t="karstenw.20230303134440.11"><vh>find_lemma</vh></v>
<v t="karstenw.20230303134440.12"><vh>find_lexeme</vh></v>
</v>
<v t="karstenw.20230303134440.13"><vh>attributive</vh></v>
<v t="karstenw.20230303134440.14"><vh>predicative</vh></v>
</v>
</v>
<v t="karstenw.20230303135132.1"><vh>fr</vh>
<v t="karstenw.20230303135134.1"><vh>@clean pattern/text/fr/__init__.py</vh>
<v t="karstenw.20230303135140.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303135140.2"><vh>penntreebank2universal</vh></v>
<v t="karstenw.20230303135140.3"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303135140.4"><vh>class Parser</vh>
<v t="karstenw.20230303135140.5"><vh>find_tokens</vh></v>
<v t="karstenw.20230303135140.6"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303135140.7"><vh>find_tags</vh></v>
</v>
<v t="karstenw.20230303135140.8"><vh>class Sentiment</vh>
<v t="karstenw.20230303135140.9"><vh>load</vh></v>
</v>
<v t="karstenw.20230303135140.10"><vh>tokenize</vh></v>
<v t="karstenw.20230303135140.11"><vh>parse</vh></v>
<v t="karstenw.20230303135140.12"><vh>parsetree</vh></v>
<v t="karstenw.20230303135140.13"><vh>tree</vh></v>
<v t="karstenw.20230303135140.14"><vh>tag</vh></v>
<v t="karstenw.20230303135140.15"><vh>keywords</vh></v>
<v t="karstenw.20230303135140.16"><vh>suggest</vh></v>
<v t="karstenw.20230303135140.17"><vh>polarity</vh></v>
<v t="karstenw.20230303135140.18"><vh>subjectivity</vh></v>
<v t="karstenw.20230303135140.19"><vh>positive</vh></v>
</v>
<v t="karstenw.20230303135501.1"><vh>@clean pattern/text/fr/__main__.py</vh>
<v t="karstenw.20230303135509.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230303135327.1"><vh>@clean pattern/text/fr/inflect.py</vh>
<v t="karstenw.20230303135336.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303135336.2"><vh>pluralize</vh></v>
<v t="karstenw.20230303135336.3"><vh>singularize</vh></v>
<v t="karstenw.20230303135336.4"><vh>class Verbs</vh>
<v t="karstenw.20230303135336.5"><vh>__init__</vh></v>
<v t="karstenw.20230303135336.6"><vh>find_lemma</vh></v>
<v t="karstenw.20230303135336.7"><vh>find_lexeme</vh></v>
</v>
<v t="karstenw.20230303135336.8"><vh>attributive</vh></v>
<v t="karstenw.20230303135336.9"><vh>predicative</vh></v>
</v>
</v>
<v t="karstenw.20230303135415.1"><vh>it</vh>
<v t="karstenw.20230303135420.1"><vh>@clean pattern/text/it/__init__.py</vh>
<v t="karstenw.20230303135427.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303135427.2"><vh>penntreebank2universal</vh></v>
<v t="karstenw.20230303135427.3"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303135427.4"><vh>class Parser</vh>
<v t="karstenw.20230303135427.5"><vh>find_tokens</vh></v>
<v t="karstenw.20230303135427.6"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303135427.7"><vh>find_tags</vh></v>
</v>
<v t="karstenw.20230303135427.8"><vh>class Sentiment</vh>
<v t="karstenw.20230303135427.9"><vh>load</vh></v>
</v>
<v t="karstenw.20230303135427.10"><vh>tokenize</vh></v>
<v t="karstenw.20230303135427.11"><vh>parse</vh></v>
<v t="karstenw.20230303135427.12"><vh>parsetree</vh></v>
<v t="karstenw.20230303135427.13"><vh>tree</vh></v>
<v t="karstenw.20230303135427.14"><vh>tag</vh></v>
<v t="karstenw.20230303135427.15"><vh>keywords</vh></v>
<v t="karstenw.20230303135427.16"><vh>suggest</vh></v>
<v t="karstenw.20230303135427.17"><vh>polarity</vh></v>
<v t="karstenw.20230303135427.18"><vh>subjectivity</vh></v>
<v t="karstenw.20230303135427.19"><vh>positive</vh></v>
</v>
<v t="karstenw.20230303135514.1"><vh>@clean pattern/text/it/__main__.py</vh>
<v t="karstenw.20230303135522.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230303135430.1"><vh>@clean pattern/text/it/inflect.py</vh>
<v t="karstenw.20230303135437.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303135437.2"><vh>definite_article</vh></v>
<v t="karstenw.20230303135437.3"><vh>indefinite_article</vh></v>
<v t="karstenw.20230303135437.4"><vh>article</vh></v>
<v t="karstenw.20230303135437.5"><vh>referenced</vh></v>
<v t="karstenw.20230303135437.6"><vh>gender</vh></v>
<v t="karstenw.20230303135437.7"><vh>pluralize</vh></v>
<v t="karstenw.20230303135437.8"><vh>singularize</vh></v>
<v t="karstenw.20230303135437.9"><vh>class Verbs</vh>
<v t="karstenw.20230303135437.10"><vh>__init__</vh></v>
<v t="karstenw.20230303135437.11"><vh>find_lemma</vh></v>
<v t="karstenw.20230303135437.12"><vh>find_lexeme</vh></v>
</v>
<v t="karstenw.20230303135437.13"><vh>attributive</vh></v>
<v t="karstenw.20230303135437.14"><vh>predicative</vh></v>
</v>
</v>
<v t="karstenw.20230303135616.1"><vh>nl</vh>
<v t="karstenw.20230303135620.1"><vh>@clean pattern/text/nl/__init__.py</vh>
<v t="karstenw.20230303135652.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303135652.2"><vh>wotan2penntreebank</vh></v>
<v t="karstenw.20230303135652.3"><vh>wotan2universal</vh></v>
<v t="karstenw.20230303135652.4"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303135652.5"><vh>class Parser</vh>
<v t="karstenw.20230303135652.6"><vh>find_tokens</vh></v>
<v t="karstenw.20230303135652.7"><vh>find_lemmata</vh></v>
<v t="karstenw.20230303135652.8"><vh>find_tags</vh></v>
</v>
<v t="karstenw.20230303135652.9"><vh>class Sentiment</vh>
<v t="karstenw.20230303135652.10"><vh>load</vh></v>
</v>
<v t="karstenw.20230303135652.11"><vh>tokenize</vh></v>
<v t="karstenw.20230303135652.12"><vh>parse</vh></v>
<v t="karstenw.20230303135652.13"><vh>parsetree</vh></v>
<v t="karstenw.20230303135652.14"><vh>tree</vh></v>
<v t="karstenw.20230303135652.15"><vh>tag</vh></v>
<v t="karstenw.20230303135652.16"><vh>keywords</vh></v>
<v t="karstenw.20230303135652.17"><vh>suggest</vh></v>
<v t="karstenw.20230303135652.18"><vh>polarity</vh></v>
<v t="karstenw.20230303135652.19"><vh>subjectivity</vh></v>
<v t="karstenw.20230303135652.20"><vh>positive</vh></v>
</v>
<v t="karstenw.20230303135631.1"><vh>@clean pattern/text/nl/__main__.py</vh>
<v t="karstenw.20230303135655.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230303135643.1"><vh>@clean pattern/text/nl/inflect.py</vh>
<v t="karstenw.20230303135658.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303135658.2"><vh>pluralize</vh></v>
<v t="karstenw.20230303135658.3"><vh>singularize</vh></v>
<v t="karstenw.20230303135658.4"><vh>class Verbs</vh>
<v t="karstenw.20230303135658.5"><vh>__init__</vh></v>
<v t="karstenw.20230303135658.6"><vh>load</vh></v>
<v t="karstenw.20230303135658.7"><vh>find_lemma</vh></v>
<v t="karstenw.20230303135658.8"><vh>find_lexeme</vh></v>
</v>
<v t="karstenw.20230303135658.9"><vh>attributive</vh></v>
<v t="karstenw.20230303135658.10"><vh>predicative</vh></v>
</v>
</v>
<v t="karstenw.20230303135727.1"><vh>ru</vh>
<v t="karstenw.20230303135735.1"><vh>@clean pattern/text/ru/__init__.py</vh>
<v t="karstenw.20230303135800.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303135800.2"><vh>class Parser</vh>
<v t="karstenw.20230303135800.3"><vh>find_tags</vh></v>
</v>
<v t="karstenw.20230303135800.4"><vh>tokenize</vh></v>
<v t="karstenw.20230303135800.5"><vh>parse</vh></v>
<v t="karstenw.20230303135800.6"><vh>parsetree</vh></v>
<v t="karstenw.20230303135800.7"><vh>suggest</vh></v>
</v>
<v t="karstenw.20230303135744.1"><vh>@clean pattern/text/ru/__main__.py</vh>
<v t="karstenw.20230303135803.1"><vh>Declarations</vh></v>
</v>
</v>
</v>
<v t="karstenw.20230303140243.1" a="E"><vh>vector</vh>
<v t="karstenw.20230303140259.1" a="E"><vh>@clean pattern/vector/__init__.py</vh>
<v t="karstenw.20230303140319.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140319.2"><vh>shi</vh></v>
<v t="karstenw.20230303140319.3"><vh>shuffled</vh></v>
<v t="karstenw.20230303140319.4"><vh>chunk</vh></v>
<v t="karstenw.20230303140319.5"><vh>mix</vh></v>
<v t="karstenw.20230303140319.6"><vh>bin</vh></v>
<v t="karstenw.20230303140319.7"><vh>pimap</vh></v>
<v t="karstenw.20230303140319.8"><vh>class ReadOnlyError</vh></v>
<v t="karstenw.20230303140319.9"><vh>class readonlyodict</vh>
<v t="karstenw.20230303140319.10"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.11"><vh>fromkeys</vh></v>
<v t="karstenw.20230303140319.12"><vh>__setitem__</vh></v>
<v t="karstenw.20230303140319.13"><vh>__delitem__</vh></v>
<v t="karstenw.20230303140319.14"><vh>pop</vh></v>
<v t="karstenw.20230303140319.15"><vh>popitem</vh></v>
<v t="karstenw.20230303140319.16"><vh>update</vh></v>
</v>
<v t="karstenw.20230303140319.17"><vh>class readonlydict</vh>
<v t="karstenw.20230303140319.18"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.19"><vh>fromkeys</vh></v>
<v t="karstenw.20230303140319.20"><vh>copy</vh></v>
<v t="karstenw.20230303140319.21"><vh>__setitem__</vh></v>
<v t="karstenw.20230303140319.22"><vh>__delitem__</vh></v>
<v t="karstenw.20230303140319.23"><vh>pop</vh></v>
<v t="karstenw.20230303140319.24"><vh>popitem</vh></v>
<v t="karstenw.20230303140319.25"><vh>clear</vh></v>
<v t="karstenw.20230303140319.26"><vh>update</vh></v>
<v t="karstenw.20230303140319.27"><vh>setdefault</vh></v>
</v>
<v t="karstenw.20230303140319.28"><vh>class readonlylist</vh>
<v t="karstenw.20230303140319.29"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.30"><vh>__setitem__</vh></v>
<v t="karstenw.20230303140319.31"><vh>__delitem__</vh></v>
<v t="karstenw.20230303140319.32"><vh>append</vh></v>
<v t="karstenw.20230303140319.33"><vh>extend</vh></v>
<v t="karstenw.20230303140319.34"><vh>insert</vh></v>
<v t="karstenw.20230303140319.35"><vh>remove</vh></v>
<v t="karstenw.20230303140319.36"><vh>pop</vh></v>
</v>
<v t="karstenw.20230303140319.37"><vh>words</vh></v>
<v t="karstenw.20230303140319.38"><vh>stem</vh></v>
<v t="karstenw.20230303140319.39"><vh>count</vh></v>
<v t="karstenw.20230303140319.40"><vh>character_ngrams</vh></v>
<v t="karstenw.20230303140319.41"><vh>_uid</vh></v>
<v t="karstenw.20230303140319.42"><vh>class Document</vh>
<v t="karstenw.20230303140319.43"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.44"><vh>load</vh></v>
<v t="karstenw.20230303140319.45"><vh>save</vh></v>
<v t="karstenw.20230303140319.46"><vh>_get_model</vh></v>
<v t="karstenw.20230303140319.47"><vh>_set_model</vh></v>
<v t="karstenw.20230303140319.48"><vh>id</vh></v>
<v t="karstenw.20230303140319.49"><vh>name</vh></v>
<v t="karstenw.20230303140319.50"><vh>type</vh></v>
<v t="karstenw.20230303140319.51"><vh>label</vh></v>
<v t="karstenw.20230303140319.52"><vh>language</vh></v>
<v t="karstenw.20230303140319.53"><vh>description</vh></v>
<v t="karstenw.20230303140319.54"><vh>terms</vh></v>
<v t="karstenw.20230303140319.55"><vh>words</vh></v>
<v t="karstenw.20230303140319.56"><vh>features</vh></v>
<v t="karstenw.20230303140319.57"><vh>count</vh></v>
<v t="karstenw.20230303140319.58"><vh>wordcount</vh></v>
<v t="karstenw.20230303140319.59"><vh>__len__</vh></v>
<v t="karstenw.20230303140319.60"><vh>__iter__</vh></v>
<v t="karstenw.20230303140319.61"><vh>__contains__</vh></v>
<v t="karstenw.20230303140319.62"><vh>__getitem__</vh></v>
<v t="karstenw.20230303140319.63"><vh>get</vh></v>
<v t="karstenw.20230303140319.64"><vh>term_frequency</vh></v>
<v t="karstenw.20230303140319.65"><vh>term_frequency_inverse_document_frequency</vh></v>
<v t="karstenw.20230303140319.66"><vh>information_gain</vh></v>
<v t="karstenw.20230303140319.67"><vh>gain_ratio</vh></v>
<v t="karstenw.20230303140319.68"><vh>vector</vh></v>
<v t="karstenw.20230303140319.69"><vh>concepts</vh></v>
<v t="karstenw.20230303140319.70"><vh>keywords</vh></v>
<v t="karstenw.20230303140319.71"><vh>cosine_similarity</vh></v>
<v t="karstenw.20230303140319.72"><vh>copy</vh></v>
<v t="karstenw.20230303140319.73"><vh>__eq__</vh></v>
<v t="karstenw.20230303140319.74"><vh>__ne__</vh></v>
<v t="karstenw.20230303140319.75"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303140319.76"><vh>class Vector</vh>
<v t="karstenw.20230303140319.77"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.78"><vh>fromkeys</vh></v>
<v t="karstenw.20230303140319.79"><vh>features</vh></v>
<v t="karstenw.20230303140319.80"><vh>l2_norm</vh></v>
<v t="karstenw.20230303140319.81"><vh>copy</vh></v>
<v t="karstenw.20230303140319.82"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230303140319.83"><vh>features</vh></v>
<v t="karstenw.20230303140319.84"><vh>sparse</vh></v>
<v t="karstenw.20230303140319.85"><vh>relative</vh></v>
<v t="karstenw.20230303140319.86"><vh>l2_norm</vh></v>
<v t="karstenw.20230303140319.87"><vh>cosine_similarity</vh></v>
<v t="karstenw.20230303140319.88"><vh>tf_idf</vh></v>
<v t="karstenw.20230303140319.89"><vh>distance</vh></v>
<v t="karstenw.20230303140319.90"><vh>entropy</vh></v>
<v t="karstenw.20230303140319.91"><vh>class Model</vh>
<v t="karstenw.20230303140319.92"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.93"><vh>documents</vh></v>
<v t="karstenw.20230303140319.94"><vh>terms</vh></v>
<v t="karstenw.20230303140319.95"><vh>classes</vh></v>
<v t="karstenw.20230303140319.96"><vh>classifier</vh></v>
<v t="karstenw.20230303140319.97"><vh>distribution</vh></v>
<v t="karstenw.20230303140319.98"><vh>_get_lsa</vh></v>
<v t="karstenw.20230303140319.99"><vh>_set_lsa</vh></v>
<v t="karstenw.20230303140319.100"><vh>_get_weight</vh></v>
<v t="karstenw.20230303140319.101"><vh>_set_weight</vh></v>
<v t="karstenw.20230303140319.102"><vh>load</vh></v>
<v t="karstenw.20230303140319.103"><vh>save</vh></v>
<v t="karstenw.20230303140319.104"><vh>export</vh></v>
<v t="karstenw.20230303140319.105"><vh>_update</vh></v>
<v t="karstenw.20230303140319.106"><vh>__len__</vh></v>
<v t="karstenw.20230303140319.107"><vh>__iter__</vh></v>
<v t="karstenw.20230303140319.108"><vh>__getitem__</vh></v>
<v t="karstenw.20230303140319.109"><vh>__delitem__</vh></v>
<v t="karstenw.20230303140319.110"><vh>clear</vh></v>
<v t="karstenw.20230303140319.111"><vh>append</vh></v>
<v t="karstenw.20230303140319.112"><vh>extend</vh></v>
<v t="karstenw.20230303140319.113"><vh>remove</vh></v>
<v t="karstenw.20230303140319.114"><vh>document</vh></v>
<v t="karstenw.20230303140319.115"><vh>keywords</vh></v>
<v t="karstenw.20230303140319.116"><vh>document_frequency</vh></v>
<v t="karstenw.20230303140319.117"><vh>inverse_document_frequency</vh></v>
<v t="karstenw.20230303140319.118"><vh>inverted_index</vh></v>
<v t="karstenw.20230303140319.119"><vh>vector</vh></v>
<v t="karstenw.20230303140319.120"><vh>vectors</vh></v>
<v t="karstenw.20230303140319.121"><vh>density</vh></v>
<v t="karstenw.20230303140319.122"><vh>frequent_concept_sets</vh></v>
<v t="karstenw.20230303140319.123"><vh>cosine_similarity</vh></v>
<v t="karstenw.20230303140319.124"><vh>nearest_neighbors</vh></v>
<v t="karstenw.20230303140319.125"><vh>vector_space_search</vh></v>
<v t="karstenw.20230303140319.126"><vh>distance</vh></v>
</v>
<v t="karstenw.20230303140319.127"><vh>class Apriori</vh>
<v t="karstenw.20230303140319.128"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.129"><vh>C1</vh></v>
<v t="karstenw.20230303140319.130"><vh>Ck</vh></v>
<v t="karstenw.20230303140319.131"><vh>Lk</vh></v>
<v t="karstenw.20230303140319.132"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230303140319.133"><vh>class LSA</vh>
<v t="karstenw.20230303140319.134"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.135"><vh>terms</vh></v>
<v t="karstenw.20230303140319.136"><vh>concepts</vh></v>
<v t="karstenw.20230303140319.137"><vh>vectors</vh></v>
<v t="karstenw.20230303140319.138"><vh>vector</vh></v>
<v t="karstenw.20230303140319.139"><vh>__getitem__</vh></v>
<v t="karstenw.20230303140319.140"><vh>__contains__</vh></v>
<v t="karstenw.20230303140319.141"><vh>__iter__</vh></v>
<v t="karstenw.20230303140319.142"><vh>__len__</vh></v>
<v t="karstenw.20230303140319.143"><vh>transform</vh></v>
</v>
<v t="karstenw.20230303140319.144"><vh>mean</vh></v>
<v t="karstenw.20230303140319.145"><vh>centroid</vh></v>
<v t="karstenw.20230303140319.146"><vh>class DistanceMap</vh>
<v t="karstenw.20230303140319.147"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.148"><vh>__call__</vh></v>
<v t="karstenw.20230303140319.149"><vh>distance</vh></v>
</v>
<v t="karstenw.20230303140319.150"><vh>cluster</vh></v>
<v t="karstenw.20230303140319.151"><vh>k_means</vh></v>
<v t="karstenw.20230303140319.152"><vh>kmpp</vh></v>
<v t="karstenw.20230303140319.153"><vh>class Cluster</vh>
<v t="karstenw.20230303140319.154"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.155"><vh>depth</vh></v>
<v t="karstenw.20230303140319.156"><vh>flatten</vh></v>
<v t="karstenw.20230303140319.157"><vh>traverse</vh></v>
<v t="karstenw.20230303140319.158"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303140319.159"><vh>sequence</vh></v>
<v t="karstenw.20230303140319.160"><vh>hierarchical</vh></v>
<v t="karstenw.20230303140319.161" a="E"><vh>class Classifier</vh>
<v t="karstenw.20230303140319.162"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.163"><vh>features</vh></v>
<v t="karstenw.20230303140319.164"><vh>classes</vh></v>
<v t="karstenw.20230303140319.165"><vh>binary</vh></v>
<v t="karstenw.20230303140319.166"><vh>distribution</vh></v>
<v t="karstenw.20230303140319.167"><vh>majority</vh></v>
<v t="karstenw.20230303140319.168"><vh>minority</vh></v>
<v t="karstenw.20230303140319.169"><vh>baseline</vh></v>
<v t="karstenw.20230303140319.170"><vh>weighted_random_baseline</vh></v>
<v t="karstenw.20230303140319.171"><vh>skewness</vh></v>
<v t="karstenw.20230303140319.172"><vh>train</vh></v>
<v t="karstenw.20230303140319.173"><vh>classify</vh></v>
<v t="karstenw.20230303140319.174"><vh>_vector</vh></v>
<v t="karstenw.20230303140319.175"><vh>k_fold_cross_validation</vh></v>
<v t="karstenw.20230303140319.176"><vh>test</vh></v>
<v t="karstenw.20230303140319.177"><vh>_test</vh></v>
<v t="karstenw.20230303140319.178"><vh>auc</vh></v>
<v t="karstenw.20230303140319.179"><vh>confusion_matrix</vh></v>
<v t="karstenw.20230303140319.180"><vh>save</vh></v>
<v t="karstenw.20230303140319.181"><vh>load</vh></v>
<v t="karstenw.20230303140319.182"><vh>_on_load</vh></v>
<v t="karstenw.20230303140319.183"><vh>finalize</vh></v>
</v>
<v t="karstenw.20230303140319.184"><vh>class Probabilities</vh>
<v t="karstenw.20230303140319.185"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.186"><vh>max</vh></v>
</v>
<v t="karstenw.20230303140319.187"><vh>class ConfusionMatrix</vh>
<v t="karstenw.20230303140319.188"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.189"><vh>split</vh></v>
<v t="karstenw.20230303140319.190"><vh>__call__</vh></v>
<v t="karstenw.20230303140319.191"><vh>test</vh></v>
<v t="karstenw.20230303140319.192"><vh>auc</vh></v>
<v t="karstenw.20230303140319.193"><vh>table</vh></v>
<v t="karstenw.20230303140319.194"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303140319.195"><vh>K_fold_cross_validation</vh></v>
<v t="karstenw.20230303140319.196"><vh>folds</vh></v>
<v t="karstenw.20230303140319.197"><vh>gridsearch</vh></v>
<v t="karstenw.20230303140319.198"><vh>feature_selection</vh></v>
<v t="karstenw.20230303140319.199"><vh>class NB</vh>
<v t="karstenw.20230303140319.200"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.201"><vh>method</vh></v>
<v t="karstenw.20230303140319.202"><vh>features</vh></v>
<v t="karstenw.20230303140319.203"><vh>train</vh></v>
<v t="karstenw.20230303140319.204"><vh>classify</vh></v>
</v>
<v t="karstenw.20230303140319.205"><vh>class KNN</vh>
<v t="karstenw.20230303140319.206"><vh>__init__</vh></v>
<v t="karstenw.20230303140319.207"><vh>train</vh></v>
<v t="karstenw.20230303140319.208"><vh>classify</vh></v>
</v>
<v t="karstenw.20230303140320.1"><vh>class IGTreeNode</vh>
<v t="karstenw.20230303140320.2"><vh>__init__</vh></v>
<v t="karstenw.20230303140320.3"><vh>children</vh></v>
<v t="karstenw.20230303140320.4"><vh>leaf</vh></v>
</v>
<v t="karstenw.20230303140320.5"><vh>class IGTree</vh>
<v t="karstenw.20230303140320.6"><vh>__init__</vh></v>
<v t="karstenw.20230303140320.7"><vh>method</vh></v>
<v t="karstenw.20230303140320.8"><vh>_tree</vh></v>
<v t="karstenw.20230303140320.9"><vh>_search</vh></v>
<v t="karstenw.20230303140320.10"><vh>train</vh></v>
<v t="karstenw.20230303140320.11"><vh>_train</vh></v>
<v t="karstenw.20230303140320.12"><vh>classify</vh></v>
<v t="karstenw.20230303140320.13"><vh>finalize</vh></v>
</v>
<v t="karstenw.20230303140320.14"><vh>softmax</vh></v>
<v t="karstenw.20230303140320.15"><vh>class SLP</vh>
<v t="karstenw.20230303140320.16"><vh>__init__</vh></v>
<v t="karstenw.20230303140320.17"><vh>iterations</vh></v>
<v t="karstenw.20230303140320.18"><vh>features</vh></v>
<v t="karstenw.20230303140320.19"><vh>train</vh></v>
<v t="karstenw.20230303140320.20"><vh>classify</vh></v>
<v t="karstenw.20230303140320.21"><vh>finalize</vh></v>
</v>
<v t="karstenw.20230303140320.22"><vh>matrix</vh></v>
<v t="karstenw.20230303140320.23"><vh>sigmoid</vh></v>
<v t="karstenw.20230303140320.24"><vh>sigmoid_derivative</vh></v>
<v t="karstenw.20230303140320.25"><vh>class BPNN</vh>
<v t="karstenw.20230303140320.26"><vh>__init__</vh></v>
<v t="karstenw.20230303140320.27"><vh>layers</vh></v>
<v t="karstenw.20230303140320.28"><vh>iterations</vh></v>
<v t="karstenw.20230303140320.29"><vh>rate</vh></v>
<v t="karstenw.20230303140320.30"><vh>momentum</vh></v>
<v t="karstenw.20230303140320.31"><vh>_weight_initialization</vh></v>
<v t="karstenw.20230303140320.32"><vh>_propagate_forward</vh></v>
<v t="karstenw.20230303140320.33"><vh>_propagate_backward</vh></v>
<v t="karstenw.20230303140320.34"><vh>_train</vh></v>
<v t="karstenw.20230303140320.35"><vh>_classify</vh></v>
<v t="karstenw.20230303140320.36"><vh>train</vh></v>
<v t="karstenw.20230303140320.37"><vh>classify</vh></v>
<v t="karstenw.20230303140320.38"><vh>finalize</vh></v>
</v>
<v t="karstenw.20230303140320.39"><vh>class SVM</vh>
<v t="karstenw.20230303140320.40"><vh>__init__</vh></v>
<v t="karstenw.20230303140320.41"><vh>extension</vh></v>
<v t="karstenw.20230303140320.42"><vh>_extension</vh></v>
<v t="karstenw.20230303140320.43"><vh>type</vh></v>
<v t="karstenw.20230303140320.44"><vh>kernel</vh></v>
<v t="karstenw.20230303140320.45"><vh>solver</vh></v>
<v t="karstenw.20230303140320.46"><vh>degree</vh></v>
<v t="karstenw.20230303140320.47"><vh>gamma</vh></v>
<v t="karstenw.20230303140320.48"><vh>coeff0</vh></v>
<v t="karstenw.20230303140320.49"><vh>cost</vh></v>
<v t="karstenw.20230303140320.50"><vh>epsilon</vh></v>
<v t="karstenw.20230303140320.51"><vh>nu</vh></v>
<v t="karstenw.20230303140320.52"><vh>cache</vh></v>
<v t="karstenw.20230303140320.53"><vh>shrinking</vh></v>
<v t="karstenw.20230303140320.54"><vh>support_vectors</vh></v>
<v t="karstenw.20230303140320.55"><vh>_train</vh></v>
<v t="karstenw.20230303140320.56"><vh>_classify</vh></v>
<v t="karstenw.20230303140320.57"><vh>train</vh></v>
<v t="karstenw.20230303140320.58"><vh>classify</vh></v>
<v t="karstenw.20230303140320.59"><vh>save</vh></v>
<v t="karstenw.20230303140320.60"><vh>load</vh></v>
<v t="karstenw.20230303140320.61"><vh>_on_load</vh></v>
<v t="karstenw.20230303140320.62"><vh>finalize</vh></v>
</v>
<v t="karstenw.20230303140320.63"><vh>class LR</vh>
<v t="karstenw.20230303140320.64"><vh>__init__</vh></v>
<v t="karstenw.20230303140320.65"><vh>iterations</vh></v>
<v t="karstenw.20230303140320.66"><vh>_train</vh></v>
<v t="karstenw.20230303140320.67"><vh>_classify</vh></v>
<v t="karstenw.20230303140320.68"><vh>_gradient_descent</vh></v>
<v t="karstenw.20230303140320.69"><vh>train</vh></v>
<v t="karstenw.20230303140320.70"><vh>classify</vh></v>
<v t="karstenw.20230303140320.71"><vh>save</vh></v>
<v t="karstenw.20230303140320.72"><vh>load</vh></v>
<v t="karstenw.20230303140320.73"><vh>_on_load</vh></v>
<v t="karstenw.20230303140320.74"><vh>finalize</vh></v>
</v>
<v t="karstenw.20230303140320.75"><vh>class GeneticAlgorithm</vh>
<v t="karstenw.20230303140320.76"><vh>__init__</vh></v>
<v t="karstenw.20230303140320.77"><vh>fitness</vh></v>
<v t="karstenw.20230303140320.78"><vh>combine</vh></v>
<v t="karstenw.20230303140320.79"><vh>mutate</vh></v>
<v t="karstenw.20230303140320.80"><vh>update</vh></v>
<v t="karstenw.20230303140320.81"><vh>avg</vh></v>
</v>
</v>
<v t="karstenw.20230303140323.1"><vh>@clean pattern/vector/stemmer.py</vh>
<v t="karstenw.20230303141303.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303141303.2"><vh>is_vowel</vh></v>
<v t="karstenw.20230303141303.3"><vh>is_consonant</vh></v>
<v t="karstenw.20230303141303.4"><vh>is_double_consonant</vh></v>
<v t="karstenw.20230303141303.5"><vh>is_short_syllable</vh></v>
<v t="karstenw.20230303141303.6"><vh>is_short</vh></v>
<v t="karstenw.20230303141303.7"><vh>R1</vh></v>
<v t="karstenw.20230303141303.8"><vh>R2</vh></v>
<v t="karstenw.20230303141303.9"><vh>find_vowel</vh></v>
<v t="karstenw.20230303141303.10"><vh>has_vowel</vh></v>
<v t="karstenw.20230303141303.11"><vh>vowel_consonant_pairs</vh></v>
<v t="karstenw.20230303141303.12"><vh>step_1a</vh></v>
<v t="karstenw.20230303141303.13"><vh>step_1b</vh></v>
<v t="karstenw.20230303141303.14"><vh>step_1c</vh></v>
<v t="karstenw.20230303141303.15"><vh>step_2</vh></v>
<v t="karstenw.20230303141303.16"><vh>step_3</vh></v>
<v t="karstenw.20230303141303.17"><vh>step_4</vh></v>
<v t="karstenw.20230303141303.18"><vh>step_5a</vh></v>
<v t="karstenw.20230303141303.19"><vh>step_5b</vh></v>
<v t="karstenw.20230303141303.20"><vh>case_sensitive</vh></v>
<v t="karstenw.20230303141303.21"><vh>upper_consonant_y</vh></v>
<v t="karstenw.20230303141303.22"><vh>stem</vh></v>
</v>
<v t="karstenw.20230303140338.1"><vh>svm</vh>
<v t="karstenw.20230303140342.1"><vh>@clean pattern/vector/svm/__init__.py</vh>
<v t="karstenw.20230303140351.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230303140359.1"><vh>@clean pattern/vector/svm/liblinear.py</vh>
<v t="karstenw.20230303141307.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303141307.2"><vh>print_null</vh></v>
<v t="karstenw.20230303141307.3"><vh>genFields</vh></v>
<v t="karstenw.20230303141307.4"><vh>fillprototype</vh></v>
<v t="karstenw.20230303141307.5"><vh>class feature_node</vh>
<v t="karstenw.20230303141307.6"><vh>__str__</vh></v>
</v>
<v t="karstenw.20230303141307.7"><vh>gen_feature_nodearray</vh></v>
<v t="karstenw.20230303141307.8"><vh>csr_to_problem_jit</vh></v>
<v t="karstenw.20230303141307.9"><vh>csr_to_problem_nojit</vh></v>
<v t="karstenw.20230303141307.10"><vh>csr_to_problem</vh></v>
<v t="karstenw.20230303141307.11"><vh>class problem</vh>
<v t="karstenw.20230303141307.12"><vh>__init__</vh></v>
<v t="karstenw.20230303141307.13"><vh>set_bias</vh></v>
</v>
<v t="karstenw.20230303141307.14"><vh>class parameter</vh>
<v t="karstenw.20230303141307.15"><vh>__init__</vh></v>
<v t="karstenw.20230303141307.16"><vh>__str__</vh></v>
<v t="karstenw.20230303141307.17"><vh>set_to_default_values</vh></v>
<v t="karstenw.20230303141307.18"><vh>parse_options</vh></v>
</v>
<v t="karstenw.20230303141307.19"><vh>class model</vh>
<v t="karstenw.20230303141307.20"><vh>__init__</vh></v>
<v t="karstenw.20230303141307.21"><vh>__del__</vh></v>
<v t="karstenw.20230303141307.22"><vh>get_nr_feature</vh></v>
<v t="karstenw.20230303141307.23"><vh>get_nr_class</vh></v>
<v t="karstenw.20230303141307.24"><vh>get_labels</vh></v>
<v t="karstenw.20230303141307.25"><vh>get_decfun_coef</vh></v>
<v t="karstenw.20230303141307.26"><vh>get_decfun_bias</vh></v>
<v t="karstenw.20230303141307.27"><vh>get_decfun</vh></v>
<v t="karstenw.20230303141307.28"><vh>is_probability_model</vh></v>
<v t="karstenw.20230303141307.29"><vh>is_regression_model</vh></v>
</v>
<v t="karstenw.20230303141307.30"><vh>toPyModel</vh></v>
</v>
<v t="karstenw.20230303140415.1"><vh>@clean pattern/vector/svm/liblinearutil.py</vh>
<v t="karstenw.20230303141310.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303141310.2"><vh>svm_read_problem</vh></v>
<v t="karstenw.20230303141310.3"><vh>load_model</vh></v>
<v t="karstenw.20230303141310.4"><vh>save_model</vh></v>
<v t="karstenw.20230303141310.5"><vh>evaluations_scipy</vh></v>
<v t="karstenw.20230303141310.6"><vh>evaluations</vh></v>
<v t="karstenw.20230303141310.7"><vh>train</vh></v>
<v t="karstenw.20230303141310.8"><vh>predict</vh></v>
</v>
<v t="karstenw.20230303140421.1"><vh>@clean pattern/vector/svm/libsvm.py</vh>
<v t="karstenw.20230303141312.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303141312.2"><vh>print_null</vh></v>
<v t="karstenw.20230303141312.3"><vh>genFields</vh></v>
<v t="karstenw.20230303141312.4"><vh>fillprototype</vh></v>
<v t="karstenw.20230303141312.5"><vh>class svm_node</vh>
<v t="karstenw.20230303141312.6"><vh>__str__</vh></v>
</v>
<v t="karstenw.20230303141312.7"><vh>gen_svm_nodearray</vh></v>
<v t="karstenw.20230303141312.8"><vh>class svm_problem</vh>
<v t="karstenw.20230303141312.9"><vh>__init__</vh></v>
</v>
<v t="karstenw.20230303141312.10"><vh>class svm_parameter</vh>
<v t="karstenw.20230303141312.11"><vh>__init__</vh></v>
<v t="karstenw.20230303141312.12"><vh>__str__</vh></v>
<v t="karstenw.20230303141312.13"><vh>set_to_default_values</vh></v>
<v t="karstenw.20230303141312.14"><vh>parse_options</vh></v>
</v>
<v t="karstenw.20230303141312.15"><vh>class svm_model</vh>
<v t="karstenw.20230303141312.16"><vh>__init__</vh></v>
<v t="karstenw.20230303141312.17"><vh>__del__</vh></v>
<v t="karstenw.20230303141312.18"><vh>get_svm_type</vh></v>
<v t="karstenw.20230303141312.19"><vh>get_nr_class</vh></v>
<v t="karstenw.20230303141312.20"><vh>get_svr_probability</vh></v>
<v t="karstenw.20230303141312.21"><vh>get_labels</vh></v>
<v t="karstenw.20230303141312.22"><vh>get_sv_indices</vh></v>
<v t="karstenw.20230303141312.23"><vh>get_nr_sv</vh></v>
<v t="karstenw.20230303141312.24"><vh>is_probability_model</vh></v>
<v t="karstenw.20230303141312.25"><vh>get_sv_coef</vh></v>
<v t="karstenw.20230303141312.26"><vh>get_SV</vh></v>
</v>
<v t="karstenw.20230303141312.27"><vh>toPyModel</vh></v>
</v>
<v t="karstenw.20230303140429.1"><vh>@clean pattern/vector/svm/libsvmutil.py</vh>
<v t="karstenw.20230303141314.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303141314.2"><vh>svm_read_problem</vh></v>
<v t="karstenw.20230303141314.3"><vh>svm_load_model</vh></v>
<v t="karstenw.20230303141314.4"><vh>svm_save_model</vh></v>
<v t="karstenw.20230303141314.5"><vh>evaluations</vh></v>
<v t="karstenw.20230303141314.6"><vh>svm_train</vh></v>
<v t="karstenw.20230303141314.7"><vh>svm_predict</vh></v>
</v>
<v t="karstenw.20230303140440.1"><vh>macos</vh></v>
<v t="karstenw.20230303140443.1"><vh>ubuntu</vh></v>
<v t="karstenw.20230303140457.1"><vh>windows</vh></v>
</v>
</v>
<v t="karstenw.20230303140246.1"><vh>web</vh>
<v t="karstenw.20230303141215.1"><vh>@clean pattern/web/__init__.py</vh>
<v t="karstenw.20230303141229.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303141229.2"><vh>fix</vh></v>
<v t="karstenw.20230303141229.3"><vh>latin</vh></v>
<v t="karstenw.20230303141229.4"><vh>class AsynchronousRequest</vh>
<v t="karstenw.20230303141229.5"><vh>__init__</vh></v>
<v t="karstenw.20230303141229.6"><vh>_fetch</vh></v>
<v t="karstenw.20230303141229.7"><vh>now</vh></v>
<v t="karstenw.20230303141229.8"><vh>elapsed</vh></v>
<v t="karstenw.20230303141229.9"><vh>done</vh></v>
<v t="karstenw.20230303141229.10"><vh>value</vh></v>
<v t="karstenw.20230303141229.11"><vh>error</vh></v>
<v t="karstenw.20230303141229.12"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141229.13"><vh>asynchronous</vh></v>
<v t="karstenw.20230303141229.14"><vh>extension</vh></v>
<v t="karstenw.20230303141229.15"><vh>urldecode</vh></v>
<v t="karstenw.20230303141229.16"><vh>proxy</vh></v>
<v t="karstenw.20230303141229.17"><vh>class Error</vh>
<v t="karstenw.20230303141229.18"><vh>__init__</vh></v>
<v t="karstenw.20230303141229.19"><vh>headers</vh></v>
</v>
<v t="karstenw.20230303141229.20"><vh>class URLError</vh></v>
<v t="karstenw.20230303141229.21"><vh>class URLTimeout</vh></v>
<v t="karstenw.20230303141229.22"><vh>class HTTPError</vh></v>
<v t="karstenw.20230303141229.23"><vh>class HTTP301Redirect</vh></v>
<v t="karstenw.20230303141230.1"><vh>class HTTP400BadRequest</vh></v>
<v t="karstenw.20230303141230.2"><vh>class HTTP401Authentication</vh></v>
<v t="karstenw.20230303141230.3"><vh>class HTTP403Forbidden</vh></v>
<v t="karstenw.20230303141230.4"><vh>class HTTP404NotFound</vh></v>
<v t="karstenw.20230303141230.5"><vh>class HTTP414RequestURITooLong</vh></v>
<v t="karstenw.20230303141230.6"><vh>class HTTP420Error</vh></v>
<v t="karstenw.20230303141230.7"><vh>class HTTP429TooMayRequests</vh></v>
<v t="karstenw.20230303141230.8"><vh>class HTTP500InternalServerError</vh></v>
<v t="karstenw.20230303141230.9"><vh>class HTTP503ServiceUnavailable</vh></v>
<v t="karstenw.20230303141230.10"><vh>class URL</vh>
<v t="karstenw.20230303141230.11"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.12"><vh>_parse</vh></v>
<v t="karstenw.20230303141230.13"><vh>_get_string</vh></v>
<v t="karstenw.20230303141230.14"><vh>_set_string</vh></v>
<v t="karstenw.20230303141230.15"><vh>parts</vh></v>
<v t="karstenw.20230303141230.16"><vh>querystring</vh></v>
<v t="karstenw.20230303141230.17"><vh>__getattr__</vh></v>
<v t="karstenw.20230303141230.18"><vh>__setattr__</vh></v>
<v t="karstenw.20230303141230.19"><vh>open</vh></v>
<v t="karstenw.20230303141230.20"><vh>download</vh></v>
<v t="karstenw.20230303141230.21"><vh>read</vh></v>
<v t="karstenw.20230303141230.22"><vh>exists</vh></v>
<v t="karstenw.20230303141230.23"><vh>mimetype</vh></v>
<v t="karstenw.20230303141230.24"><vh>headers</vh></v>
<v t="karstenw.20230303141230.25"><vh>redirect</vh></v>
<v t="karstenw.20230303141230.26"><vh>__str__</vh></v>
<v t="karstenw.20230303141230.27"><vh>__repr__</vh></v>
<v t="karstenw.20230303141230.28"><vh>copy</vh></v>
</v>
<v t="karstenw.20230303141230.29"><vh>download</vh></v>
<v t="karstenw.20230303141230.30"><vh>bind</vh></v>
<v t="karstenw.20230303141230.31"><vh>class Stream</vh>
<v t="karstenw.20230303141230.32"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.33"><vh>update</vh></v>
<v t="karstenw.20230303141230.34"><vh>parse</vh></v>
<v t="karstenw.20230303141230.35"><vh>clear</vh></v>
</v>
<v t="karstenw.20230303141230.36"><vh>stream</vh></v>
<v t="karstenw.20230303141230.37"><vh>find_urls</vh></v>
<v t="karstenw.20230303141230.38"><vh>find_email</vh></v>
<v t="karstenw.20230303141230.39"><vh>find_between</vh></v>
<v t="karstenw.20230303141230.40"><vh>class HTMLParser</vh>
<v t="karstenw.20230303141230.41"><vh>clean</vh></v>
</v>
<v t="karstenw.20230303141230.42"><vh>class HTMLTagstripper</vh>
<v t="karstenw.20230303141230.43"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.44"><vh>strip</vh></v>
<v t="karstenw.20230303141230.45"><vh>handle_starttag</vh></v>
<v t="karstenw.20230303141230.46"><vh>handle_endtag</vh></v>
<v t="karstenw.20230303141230.47"><vh>handle_data</vh></v>
<v t="karstenw.20230303141230.48"><vh>handle_comment</vh></v>
</v>
<v t="karstenw.20230303141230.49"><vh>strip_element</vh></v>
<v t="karstenw.20230303141230.50"><vh>strip_between</vh></v>
<v t="karstenw.20230303141230.51"><vh>strip_javascript</vh></v>
<v t="karstenw.20230303141230.52"><vh>strip_inline_css</vh></v>
<v t="karstenw.20230303141230.53"><vh>strip_comments</vh></v>
<v t="karstenw.20230303141230.54"><vh>strip_forms</vh></v>
<v t="karstenw.20230303141230.55"><vh>encode_entities</vh></v>
<v t="karstenw.20230303141230.56"><vh>decode_entities</vh></v>
<v t="karstenw.20230303141230.57"><vh>encode_url</vh></v>
<v t="karstenw.20230303141230.58"><vh>decode_url</vh></v>
<v t="karstenw.20230303141230.59"><vh>collapse_spaces</vh></v>
<v t="karstenw.20230303141230.60"><vh>collapse_tabs</vh></v>
<v t="karstenw.20230303141230.61"><vh>collapse_linebreaks</vh></v>
<v t="karstenw.20230303141230.62"><vh>plaintext</vh></v>
<v t="karstenw.20230303141230.63"><vh>class Result</vh>
<v t="karstenw.20230303141230.64"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.65"><vh>txt</vh></v>
<v t="karstenw.20230303141230.66"><vh>description</vh></v>
<v t="karstenw.20230303141230.67"><vh>likes</vh></v>
<v t="karstenw.20230303141230.68"><vh>retweets</vh></v>
<v t="karstenw.20230303141230.69"><vh>download</vh></v>
<v t="karstenw.20230303141230.70"><vh>_format</vh></v>
<v t="karstenw.20230303141230.71"><vh>__getattr__</vh></v>
<v t="karstenw.20230303141230.72"><vh>__getitem__</vh></v>
<v t="karstenw.20230303141230.73"><vh>__setattr__</vh></v>
<v t="karstenw.20230303141230.74"><vh>__setitem__</vh></v>
<v t="karstenw.20230303141230.75"><vh>setdefault</vh></v>
<v t="karstenw.20230303141230.76"><vh>update</vh></v>
<v t="karstenw.20230303141230.77"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.78"><vh>class Results</vh>
<v t="karstenw.20230303141230.79"><vh>__init__</vh></v>
</v>
<v t="karstenw.20230303141230.80"><vh>class SearchEngine</vh>
<v t="karstenw.20230303141230.81"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.82"><vh>search</vh></v>
</v>
<v t="karstenw.20230303141230.83"><vh>class SearchEngineError</vh></v>
<v t="karstenw.20230303141230.84"><vh>class SearchEngineTypeError</vh></v>
<v t="karstenw.20230303141230.85"><vh>class SearchEngineLimitError</vh></v>
<v t="karstenw.20230303141230.86"><vh>class Google</vh>
<v t="karstenw.20230303141230.87"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.88"><vh>search</vh></v>
<v t="karstenw.20230303141230.89"><vh>translate</vh></v>
<v t="karstenw.20230303141230.90"><vh>identify</vh></v>
</v>
<v t="karstenw.20230303141230.91"><vh>class Yahoo</vh>
<v t="karstenw.20230303141230.92"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.93"><vh>_authenticate</vh></v>
<v t="karstenw.20230303141230.94"><vh>search</vh></v>
</v>
<v t="karstenw.20230303141230.95"><vh>class Bing</vh>
<v t="karstenw.20230303141230.96"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.97"><vh>search</vh></v>
</v>
<v t="karstenw.20230303141230.98"><vh>class DuckDuckGo</vh>
<v t="karstenw.20230303141230.99"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.100"><vh>search</vh></v>
<v t="karstenw.20230303141230.101"><vh>answer</vh></v>
<v t="karstenw.20230303141230.102"><vh>spelling</vh></v>
<v t="karstenw.20230303141230.103"><vh>definition</vh></v>
</v>
<v t="karstenw.20230303141230.104"><vh>class Faroo</vh>
<v t="karstenw.20230303141230.105"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.106"><vh>search</vh></v>
</v>
<v t="karstenw.20230303141230.107"><vh>class VKAPI</vh>
<v t="karstenw.20230303141230.108"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.109"><vh>__getattr__</vh></v>
<v t="karstenw.20230303141230.110"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230303141230.111"><vh>class VKRequest</vh>
<v t="karstenw.20230303141230.112"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.113"><vh>__getattr__</vh></v>
<v t="karstenw.20230303141230.114"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230303141230.115"><vh>class VK</vh>
<v t="karstenw.20230303141230.116"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.117"><vh>_get_city_name_by_id</vh></v>
<v t="karstenw.20230303141230.118"><vh>_get_country_name_by_id</vh></v>
<v t="karstenw.20230303141230.119"><vh>_get_sex_by_id</vh></v>
<v t="karstenw.20230303141230.120"><vh>get_users_info</vh></v>
<v t="karstenw.20230303141230.121"><vh>get_user_posts</vh></v>
<v t="karstenw.20230303141230.122"><vh>get_newsfeed_posts</vh></v>
</v>
<v t="karstenw.20230303141230.123"><vh>class VkException</vh></v>
<v t="karstenw.20230303141230.124"><vh>class VkAuthError</vh></v>
<v t="karstenw.20230303141230.125"><vh>class VkAPIError</vh>
<v t="karstenw.20230303141230.126"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.127"><vh>get_pretty_request_params</vh></v>
<v t="karstenw.20230303141230.128"><vh>__str__</vh></v>
</v>
<v t="karstenw.20230303141230.129"><vh>class Session</vh>
<v t="karstenw.20230303141230.130"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.131"><vh>make_request</vh></v>
<v t="karstenw.20230303141230.132"><vh>send_api_request</vh></v>
</v>
<v t="karstenw.20230303141230.133"><vh>class Twitter</vh>
<v t="karstenw.20230303141230.134"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.135"><vh>_authenticate</vh></v>
<v t="karstenw.20230303141230.136"><vh>search</vh></v>
<v t="karstenw.20230303141230.137"><vh>profile</vh></v>
<v t="karstenw.20230303141230.138"><vh>trends</vh></v>
<v t="karstenw.20230303141230.139"><vh>stream</vh></v>
</v>
<v t="karstenw.20230303141230.140"><vh>class TwitterStream</vh>
<v t="karstenw.20230303141230.141"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.142"><vh>parse</vh></v>
</v>
<v t="karstenw.20230303141230.143"><vh>author</vh></v>
<v t="karstenw.20230303141230.144"><vh>hashtags</vh></v>
<v t="karstenw.20230303141230.145"><vh>mentions</vh></v>
<v t="karstenw.20230303141230.146"><vh>retweets</vh></v>
<v t="karstenw.20230303141230.147"><vh>class MediaWiki</vh>
<v t="karstenw.20230303141230.148"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.149"><vh>_url</vh></v>
<v t="karstenw.20230303141230.150"><vh>MediaWikiArticle</vh></v>
<v t="karstenw.20230303141230.151"><vh>MediaWikiSection</vh></v>
<v t="karstenw.20230303141230.152"><vh>MediaWikiTable</vh></v>
<v t="karstenw.20230303141230.153"><vh>__iter__</vh></v>
<v t="karstenw.20230303141230.154"><vh>articles</vh></v>
<v t="karstenw.20230303141230.155"><vh>index</vh></v>
<v t="karstenw.20230303141230.156"><vh>search</vh></v>
<v t="karstenw.20230303141230.157"><vh>article</vh></v>
<v t="karstenw.20230303141230.158"><vh>_parse_article</vh></v>
<v t="karstenw.20230303141230.159"><vh>_parse_article_sections</vh></v>
<v t="karstenw.20230303141230.160"><vh>_parse_article_section_structure</vh></v>
</v>
<v t="karstenw.20230303141230.161"><vh>class MediaWikiArticle</vh>
<v t="karstenw.20230303141230.162"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.163"><vh>_plaintext</vh></v>
<v t="karstenw.20230303141230.164"><vh>plaintext</vh></v>
<v t="karstenw.20230303141230.165"><vh>html</vh></v>
<v t="karstenw.20230303141230.166"><vh>src</vh></v>
<v t="karstenw.20230303141230.167"><vh>string</vh></v>
<v t="karstenw.20230303141230.168"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.169"><vh>class MediaWikiSection</vh>
<v t="karstenw.20230303141230.170"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.171"><vh>plaintext</vh></v>
<v t="karstenw.20230303141230.172"><vh>source</vh></v>
<v t="karstenw.20230303141230.173"><vh>html</vh></v>
<v t="karstenw.20230303141230.174"><vh>src</vh></v>
<v t="karstenw.20230303141230.175"><vh>string</vh></v>
<v t="karstenw.20230303141230.176"><vh>content</vh></v>
<v t="karstenw.20230303141230.177"><vh>links</vh></v>
<v t="karstenw.20230303141230.178"><vh>tables</vh></v>
<v t="karstenw.20230303141230.179"><vh>level</vh></v>
<v t="karstenw.20230303141230.180"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.181"><vh>class MediaWikiTable</vh>
<v t="karstenw.20230303141230.182"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.183"><vh>plaintext</vh></v>
<v t="karstenw.20230303141230.184"><vh>html</vh></v>
<v t="karstenw.20230303141230.185"><vh>src</vh></v>
<v t="karstenw.20230303141230.186"><vh>string</vh></v>
<v t="karstenw.20230303141230.187"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.188"><vh>class Wikipedia</vh>
<v t="karstenw.20230303141230.189"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.190"><vh>_url</vh></v>
<v t="karstenw.20230303141230.191"><vh>MediaWikiArticle</vh></v>
<v t="karstenw.20230303141230.192"><vh>MediaWikiSection</vh></v>
<v t="karstenw.20230303141230.193"><vh>MediaWikiTable</vh></v>
</v>
<v t="karstenw.20230303141230.194"><vh>class WikipediaArticle</vh>
<v t="karstenw.20230303141230.195"><vh>download</vh></v>
<v t="karstenw.20230303141230.196"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.197"><vh>class WikipediaSection</vh>
<v t="karstenw.20230303141230.198"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.199"><vh>class WikipediaTable</vh>
<v t="karstenw.20230303141230.200"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.201"><vh>class Wiktionary</vh>
<v t="karstenw.20230303141230.202"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.203"><vh>_url</vh></v>
<v t="karstenw.20230303141230.204"><vh>MediaWikiArticle</vh></v>
<v t="karstenw.20230303141230.205"><vh>MediaWikiSection</vh></v>
<v t="karstenw.20230303141230.206"><vh>MediaWikiTable</vh></v>
</v>
<v t="karstenw.20230303141230.207"><vh>class WiktionaryArticle</vh>
<v t="karstenw.20230303141230.208"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.209"><vh>class WiktionarySection</vh>
<v t="karstenw.20230303141230.210"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.211"><vh>class WiktionaryTable</vh>
<v t="karstenw.20230303141230.212"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.213"><vh>class Wikia</vh>
<v t="karstenw.20230303141230.214"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.215"><vh>_url</vh></v>
<v t="karstenw.20230303141230.216"><vh>MediaWikiArticle</vh></v>
<v t="karstenw.20230303141230.217"><vh>MediaWikiSection</vh></v>
<v t="karstenw.20230303141230.218"><vh>MediaWikiTable</vh></v>
<v t="karstenw.20230303141230.219"><vh>articles</vh></v>
</v>
<v t="karstenw.20230303141230.220"><vh>class WikiaArticle</vh>
<v t="karstenw.20230303141230.221"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.222"><vh>class WikiaSection</vh>
<v t="karstenw.20230303141230.223"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.224"><vh>class WikiaTable</vh>
<v t="karstenw.20230303141230.225"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.226"><vh>class DBPediaQueryError</vh></v>
<v t="karstenw.20230303141230.227"><vh>class DBPediaResource</vh>
<v t="karstenw.20230303141230.228"><vh>name</vh></v>
</v>
<v t="karstenw.20230303141230.229"><vh>class DBPedia</vh>
<v t="karstenw.20230303141230.230"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.231"><vh>search</vh></v>
</v>
<v t="karstenw.20230303141230.232"><vh>class Flickr</vh>
<v t="karstenw.20230303141230.233"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.234"><vh>search</vh></v>
</v>
<v t="karstenw.20230303141230.235"><vh>class FlickrResult</vh>
<v t="karstenw.20230303141230.236"><vh>url</vh></v>
</v>
<v t="karstenw.20230303141230.237"><vh>class FacebookResult</vh>
<v t="karstenw.20230303141230.238"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.239"><vh>class Facebook</vh>
<v t="karstenw.20230303141230.240"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.241"><vh>_token</vh></v>
<v t="karstenw.20230303141230.242"><vh>search</vh></v>
<v t="karstenw.20230303141230.243"><vh>profile</vh></v>
</v>
<v t="karstenw.20230303141230.244"><vh>class ProductWiki</vh>
<v t="karstenw.20230303141230.245"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.246"><vh>search</vh></v>
</v>
<v t="karstenw.20230303141230.247"><vh>class Newsfeed</vh>
<v t="karstenw.20230303141230.248"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.249"><vh>search</vh></v>
</v>
<v t="karstenw.20230303141230.250"><vh>query</vh></v>
<v t="karstenw.20230303141230.251"><vh>sort</vh></v>
<v t="karstenw.20230303141230.252"><vh>class Node</vh>
<v t="karstenw.20230303141230.253"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.254"><vh>_beautifulSoup</vh></v>
<v t="karstenw.20230303141230.255"><vh>__eq__</vh></v>
<v t="karstenw.20230303141230.256"><vh>_wrap</vh></v>
<v t="karstenw.20230303141230.257"><vh>parent</vh></v>
<v t="karstenw.20230303141230.258"><vh>children</vh></v>
<v t="karstenw.20230303141230.259"><vh>html</vh></v>
<v t="karstenw.20230303141230.260"><vh>source</vh></v>
<v t="karstenw.20230303141230.261"><vh>next_sibling</vh></v>
<v t="karstenw.20230303141230.262"><vh>previous_sibling</vh></v>
<v t="karstenw.20230303141230.263"><vh>traverse</vh></v>
<v t="karstenw.20230303141230.264"><vh>remove</vh></v>
<v t="karstenw.20230303141230.265"><vh>__bool__</vh></v>
<v t="karstenw.20230303141230.266"><vh>__len__</vh></v>
<v t="karstenw.20230303141230.267"><vh>__iter__</vh></v>
<v t="karstenw.20230303141230.268"><vh>__getitem__</vh></v>
<v t="karstenw.20230303141230.269"><vh>__repr__</vh></v>
<v t="karstenw.20230303141230.270"><vh>__str__</vh></v>
<v t="karstenw.20230303141230.271"><vh>__call__</vh></v>
</v>
<v t="karstenw.20230303141230.272"><vh>class Text</vh>
<v t="karstenw.20230303141230.273"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.274"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.275"><vh>class Comment</vh>
<v t="karstenw.20230303141230.276"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.277"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.278"><vh>class Element</vh>
<v t="karstenw.20230303141230.279"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.280"><vh>tagname</vh></v>
<v t="karstenw.20230303141230.281"><vh>attributes</vh></v>
<v t="karstenw.20230303141230.282"><vh>id</vh></v>
<v t="karstenw.20230303141230.283"><vh>content</vh></v>
<v t="karstenw.20230303141230.284"><vh>source</vh></v>
<v t="karstenw.20230303141230.285"><vh>get_elements_by_tagname</vh></v>
<v t="karstenw.20230303141230.286"><vh>get_element_by_id</vh></v>
<v t="karstenw.20230303141230.287"><vh>get_elements_by_classname</vh></v>
<v t="karstenw.20230303141230.288"><vh>get_elements_by_attribute</vh></v>
<v t="karstenw.20230303141230.289"><vh>__call__</vh></v>
<v t="karstenw.20230303141230.290"><vh>__getattr__</vh></v>
<v t="karstenw.20230303141230.291"><vh>__contains__</vh></v>
<v t="karstenw.20230303141230.292"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.293"><vh>class Document</vh>
<v t="karstenw.20230303141230.294"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.295"><vh>declaration</vh></v>
<v t="karstenw.20230303141230.296"><vh>head</vh></v>
<v t="karstenw.20230303141230.297"><vh>body</vh></v>
<v t="karstenw.20230303141230.298"><vh>tagname</vh></v>
<v t="karstenw.20230303141230.299"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.300"><vh>_encode_space</vh></v>
<v t="karstenw.20230303141230.301"><vh>_decode_space</vh></v>
<v t="karstenw.20230303141230.302"><vh>class Selector</vh>
<v t="karstenw.20230303141230.303"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.304"><vh>_parse_attribute</vh></v>
<v t="karstenw.20230303141230.305"><vh>_first_child</vh></v>
<v t="karstenw.20230303141230.306"><vh>_next_sibling</vh></v>
<v t="karstenw.20230303141230.307"><vh>_previous_sibling</vh></v>
<v t="karstenw.20230303141230.308"><vh>_contains</vh></v>
<v t="karstenw.20230303141230.309"><vh>match</vh></v>
<v t="karstenw.20230303141230.310"><vh>search</vh></v>
<v t="karstenw.20230303141230.311"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303141230.312"><vh>class SelectorChain</vh>
<v t="karstenw.20230303141230.313"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.314"><vh>search</vh></v>
</v>
<v t="karstenw.20230303141230.315"><vh>class Link</vh>
<v t="karstenw.20230303141230.316"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.317"><vh>description</vh></v>
<v t="karstenw.20230303141230.318"><vh>__repr__</vh></v>
<v t="karstenw.20230303141230.319"><vh>__eq__</vh></v>
<v t="karstenw.20230303141230.320"><vh>__ne__</vh></v>
<v t="karstenw.20230303141230.321"><vh>__lt__</vh></v>
<v t="karstenw.20230303141230.322"><vh>__gt__</vh></v>
</v>
<v t="karstenw.20230303141230.323"><vh>class HTMLLinkParser</vh>
<v t="karstenw.20230303141230.324"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.325"><vh>parse</vh></v>
<v t="karstenw.20230303141230.326"><vh>handle_starttag</vh></v>
</v>
<v t="karstenw.20230303141230.327"><vh>base</vh></v>
<v t="karstenw.20230303141230.328"><vh>abs</vh></v>
<v t="karstenw.20230303141230.329"><vh>class Crawler</vh>
<v t="karstenw.20230303141230.330"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.331"><vh>done</vh></v>
<v t="karstenw.20230303141230.332"><vh>push</vh></v>
<v t="karstenw.20230303141230.333"><vh>pop</vh></v>
<v t="karstenw.20230303141230.334"><vh>next</vh></v>
<v t="karstenw.20230303141230.335"><vh>crawl</vh></v>
<v t="karstenw.20230303141230.336"><vh>normalize</vh></v>
<v t="karstenw.20230303141230.337"><vh>follow</vh></v>
<v t="karstenw.20230303141230.338"><vh>priority</vh></v>
<v t="karstenw.20230303141230.339"><vh>visit</vh></v>
<v t="karstenw.20230303141230.340"><vh>fail</vh></v>
</v>
<v t="karstenw.20230303141230.341"><vh>crawl</vh></v>
<v t="karstenw.20230303141230.342"><vh>class DocumentParserError</vh></v>
<v t="karstenw.20230303141230.343"><vh>class DocumentParser</vh>
<v t="karstenw.20230303141230.344"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.345"><vh>_open</vh></v>
<v t="karstenw.20230303141230.346"><vh>_parse</vh></v>
<v t="karstenw.20230303141230.347"><vh>string</vh></v>
<v t="karstenw.20230303141230.348"><vh>__str__</vh></v>
</v>
<v t="karstenw.20230303141230.349"><vh>class PDFError</vh></v>
<v t="karstenw.20230303141230.350"><vh>class PDF</vh>
<v t="karstenw.20230303141230.351"><vh>__init__</vh></v>
<v t="karstenw.20230303141230.352"><vh>_parse</vh></v>
</v>
<v t="karstenw.20230303141230.353"><vh>class DOCXError</vh></v>
<v t="karstenw.20230303141230.354"><vh>class DOCX</vh>
<v t="karstenw.20230303141230.355"><vh>_parse</vh></v>
</v>
<v t="karstenw.20230303141230.356"><vh>parsepdf</vh></v>
<v t="karstenw.20230303141230.357"><vh>parsedocx</vh></v>
<v t="karstenw.20230303141230.358"><vh>parsehtml</vh></v>
<v t="karstenw.20230303141230.359"><vh>parsedoc</vh></v>
</v>
<v t="karstenw.20230303141240.1"><vh>@clean pattern/web/api.py</vh>
<v t="karstenw.20230303141253.1"><vh>Declarations</vh></v>
</v>
<v t="karstenw.20230303141246.1"><vh>@clean pattern/web/utils.py</vh>
<v t="karstenw.20230303141255.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303141255.2"><vh>json_iter_parse</vh></v>
<v t="karstenw.20230303141255.3"><vh>stringify_values</vh></v>
<v t="karstenw.20230303141255.4"><vh>get_url_query</vh></v>
<v t="karstenw.20230303141255.5"><vh>get_form_action</vh></v>
<v t="karstenw.20230303141255.6"><vh>censor_access_token</vh></v>
</v>
<v t="karstenw.20230508125637.1"><vh>cache</vh>
<v t="karstenw.20230508125640.1"><vh>@clean pattern/web/cache/__init__.py</vh>
<v t="karstenw.20230508125701.1"><vh>Declarations</vh></v>
<v t="karstenw.20230508125701.2"><vh>date_now</vh></v>
<v t="karstenw.20230508125701.3"><vh>date_modified</vh></v>
<v t="karstenw.20230508125701.4"><vh>class Cache</vh>
<v t="karstenw.20230508125701.5"><vh>__init__</vh></v>
<v t="karstenw.20230508125701.6"><vh>_get_path</vh></v>
<v t="karstenw.20230508125701.7"><vh>_set_path</vh></v>
<v t="karstenw.20230508125701.8"><vh>_hash</vh></v>
<v t="karstenw.20230508125701.9"><vh>__len__</vh></v>
<v t="karstenw.20230508125701.10"><vh>__contains__</vh></v>
<v t="karstenw.20230508125701.11"><vh>__getitem__</vh></v>
<v t="karstenw.20230508125701.12"><vh>__setitem__</vh></v>
<v t="karstenw.20230508125701.13"><vh>__delitem__</vh></v>
<v t="karstenw.20230508125701.14"><vh>get</vh></v>
<v t="karstenw.20230508125701.15"><vh>age</vh></v>
<v t="karstenw.20230508125701.16"><vh>clear</vh></v>
</v>
</v>
</v>
<v t="karstenw.20230303142559.1"><vh>imap</vh>
<v t="karstenw.20230303142620.1"><vh>@clean pattern/web/imap/__init__.py</vh>
<v t="karstenw.20230303142634.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303142634.2"><vh>class IMAP4</vh></v>
<v t="karstenw.20230303142634.3"><vh>class IMAP4_SSL</vh></v>
<v t="karstenw.20230303142634.4"><vh>_basename</vh></v>
<v t="karstenw.20230303142634.5"><vh>class MailError</vh></v>
<v t="karstenw.20230303142634.6"><vh>class MailServiceError</vh></v>
<v t="karstenw.20230303142634.7"><vh>class MailLoginError</vh></v>
<v t="karstenw.20230303142634.8"><vh>class MailNotLoggedIn</vh></v>
<v t="karstenw.20230303142634.9"><vh>class Mail</vh>
<v t="karstenw.20230303142634.10"><vh>__init__</vh></v>
<v t="karstenw.20230303142634.11"><vh>_id</vh></v>
<v t="karstenw.20230303142634.12"><vh>imap4</vh></v>
<v t="karstenw.20230303142634.13"><vh>login</vh></v>
<v t="karstenw.20230303142634.14"><vh>logout</vh></v>
<v t="karstenw.20230303142634.15"><vh>__del__</vh></v>
<v t="karstenw.20230303142634.16"><vh>folders</vh></v>
<v t="karstenw.20230303142634.17"><vh>__getattr__</vh></v>
</v>
<v t="karstenw.20230303142634.18"><vh>_decode</vh></v>
<v t="karstenw.20230303142634.19"><vh>class MailFolder</vh>
<v t="karstenw.20230303142634.20"><vh>__init__</vh></v>
<v t="karstenw.20230303142634.21"><vh>parent</vh></v>
<v t="karstenw.20230303142634.22"><vh>name</vh></v>
<v t="karstenw.20230303142634.23"><vh>count</vh></v>
<v t="karstenw.20230303142634.24"><vh>search</vh></v>
<v t="karstenw.20230303142634.25"><vh>read</vh></v>
<v t="karstenw.20230303142634.26"><vh>__getitem__</vh></v>
<v t="karstenw.20230303142634.27"><vh>__iter__</vh></v>
<v t="karstenw.20230303142634.28"><vh>__len__</vh></v>
<v t="karstenw.20230303142634.29"><vh>__repr__</vh></v>
</v>
<v t="karstenw.20230303142634.30"><vh>class Message</vh>
<v t="karstenw.20230303142634.31"><vh>author</vh></v>
<v t="karstenw.20230303142634.32"><vh>date</vh></v>
<v t="karstenw.20230303142634.33"><vh>subject</vh></v>
<v t="karstenw.20230303142634.34"><vh>body</vh></v>
<v t="karstenw.20230303142634.35"><vh>attachments</vh></v>
<v t="karstenw.20230303142634.36"><vh>email_address</vh></v>
<v t="karstenw.20230303142634.37"><vh>__repr__</vh></v>
</v>
</v>
</v>
<v t="karstenw.20230303142603.1"><vh>locale</vh>
<v t="karstenw.20230303142647.1"><vh>@clean pattern/web/locale/__init__.py</vh>
<v t="karstenw.20230303142655.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303142655.2"><vh>encode_language</vh></v>
<v t="karstenw.20230303142655.3"><vh>decode_language</vh></v>
<v t="karstenw.20230303142655.4"><vh>encode_region</vh></v>
<v t="karstenw.20230303142655.5"><vh>decode_region</vh></v>
<v t="karstenw.20230303142655.6"><vh>languages</vh></v>
<v t="karstenw.20230303142655.7"><vh>regions</vh></v>
<v t="karstenw.20230303142655.8"><vh>regionalize</vh></v>
<v t="karstenw.20230303142655.9"><vh>market</vh></v>
<v t="karstenw.20230303142655.10"><vh>geocode</vh></v>
</v>
</v>
<v t="karstenw.20230303142608.1"><vh>oauth</vh>
<v t="karstenw.20230303142657.1"><vh>@clean pattern/web/oauth/__init__.py</vh>
<v t="karstenw.20230303142704.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303142704.2"><vh>HMAC_SHA1</vh></v>
<v t="karstenw.20230303142704.3"><vh>nonce</vh></v>
<v t="karstenw.20230303142704.4"><vh>timestamp</vh></v>
<v t="karstenw.20230303142704.5"><vh>escape</vh></v>
<v t="karstenw.20230303142704.6"><vh>utf8</vh></v>
<v t="karstenw.20230303142704.7"><vh>normalize</vh></v>
<v t="karstenw.20230303142704.8"><vh>base</vh></v>
<v t="karstenw.20230303142704.9"><vh>sign</vh></v>
</v>
</v>
</v>
<v t="karstenw.20230303135940.1"><vh>test</vh>
<v t="karstenw.20230303140000.1"><vh>@clean pattern/test/test_db.py</vh>
<v t="karstenw.20230303140159.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140159.2"><vh>create_db_mysql</vh></v>
<v t="karstenw.20230303140159.3"><vh>create_db_sqlite</vh></v>
<v t="karstenw.20230303140159.4"><vh>class TestUnicode</vh>
<v t="karstenw.20230303140159.5"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.6"><vh>test_decode_utf8</vh></v>
<v t="karstenw.20230303140159.7"><vh>test_encode_utf8</vh></v>
<v t="karstenw.20230303140159.8"><vh>test_string</vh></v>
</v>
<v t="karstenw.20230303140159.9"><vh>class TestEntities</vh>
<v t="karstenw.20230303140159.10"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.11"><vh>test_encode_entities</vh></v>
<v t="karstenw.20230303140159.12"><vh>test_decode_entities</vh></v>
</v>
<v t="karstenw.20230303140159.13"><vh>class TestDate</vh>
<v t="karstenw.20230303140159.14"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.15"><vh>test_date</vh></v>
<v t="karstenw.20230303140159.16"><vh>test_format</vh></v>
<v t="karstenw.20230303140159.17"><vh>test_timestamp</vh></v>
<v t="karstenw.20230303140159.18"><vh>test_time</vh></v>
</v>
<v t="karstenw.20230303140159.19"><vh>class TestUtilityFunctions</vh>
<v t="karstenw.20230303140159.20"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.21"><vh>test_encryption</vh></v>
<v t="karstenw.20230303140159.22"><vh>test_json</vh></v>
<v t="karstenw.20230303140159.23"><vh>test_order</vh></v>
<v t="karstenw.20230303140159.24"><vh>test_avg</vh></v>
<v t="karstenw.20230303140159.25"><vh>test_variance</vh></v>
<v t="karstenw.20230303140159.26"><vh>test_stdev</vh></v>
<v t="karstenw.20230303140159.27"><vh>test_sqlite_functions</vh></v>
</v>
<v t="karstenw.20230303140159.28"><vh>class _TestDatabase</vh>
<v t="karstenw.20230303140159.29"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.30"><vh>tearDown</vh></v>
<v t="karstenw.20230303140159.31"><vh>test_escape</vh></v>
<v t="karstenw.20230303140159.32"><vh>test_database</vh></v>
<v t="karstenw.20230303140159.33"><vh>test_create_table</vh></v>
</v>
<v t="karstenw.20230303140159.34"><vh>class TestDeleteMySQLDatabase</vh>
<v t="karstenw.20230303140159.35"><vh>runTest</vh></v>
</v>
<v t="karstenw.20230303140159.36"><vh>class TestDeleteSQLiteDatabase</vh>
<v t="karstenw.20230303140159.37"><vh>runTest</vh></v>
</v>
<v t="karstenw.20230303140159.38"><vh>class TestMySQLDatabase</vh>
<v t="karstenw.20230303140159.39"><vh>setUp</vh></v>
</v>
<v t="karstenw.20230303140159.40"><vh>class TestSQLiteDatabase</vh>
<v t="karstenw.20230303140159.41"><vh>setUp</vh></v>
</v>
<v t="karstenw.20230303140159.42"><vh>class TestSchema</vh>
<v t="karstenw.20230303140159.43"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.44"><vh>test_string</vh></v>
<v t="karstenw.20230303140159.45"><vh>test_field</vh></v>
<v t="karstenw.20230303140159.46"><vh>test_schema</vh></v>
</v>
<v t="karstenw.20230303140159.47"><vh>class _TestTable</vh>
<v t="karstenw.20230303140159.48"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.49"><vh>tearDown</vh></v>
<v t="karstenw.20230303140159.50"><vh>test_table</vh></v>
<v t="karstenw.20230303140159.51"><vh>test_rename</vh></v>
<v t="karstenw.20230303140159.52"><vh>test_fields</vh></v>
<v t="karstenw.20230303140159.53"><vh>test_insert_update_delete</vh></v>
<v t="karstenw.20230303140159.54"><vh>test_filter</vh></v>
<v t="karstenw.20230303140159.55"><vh>test_search</vh></v>
<v t="karstenw.20230303140159.56"><vh>test_datasheet</vh></v>
</v>
<v t="karstenw.20230303140159.57"><vh>class TestMySQLTable</vh>
<v t="karstenw.20230303140159.58"><vh>setUp</vh></v>
</v>
<v t="karstenw.20230303140159.59"><vh>class TestSQLiteTable</vh>
<v t="karstenw.20230303140159.60"><vh>setUp</vh></v>
</v>
<v t="karstenw.20230303140159.61"><vh>class _TestQuery</vh>
<v t="karstenw.20230303140159.62"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.63"><vh>tearDown</vh></v>
<v t="karstenw.20230303140159.64"><vh>_query</vh></v>
<v t="karstenw.20230303140159.65"><vh>test_abs</vh></v>
<v t="karstenw.20230303140159.66"><vh>test_cmp</vh></v>
<v t="karstenw.20230303140159.67"><vh>test_filterchain</vh></v>
<v t="karstenw.20230303140159.68"><vh>test_query</vh></v>
<v t="karstenw.20230303140159.69"><vh>test_xml</vh></v>
</v>
<v t="karstenw.20230303140159.70"><vh>class TestMySQLQuery</vh>
<v t="karstenw.20230303140159.71"><vh>setUp</vh></v>
</v>
<v t="karstenw.20230303140159.72"><vh>class TestSQLiteQuery</vh>
<v t="karstenw.20230303140159.73"><vh>setUp</vh></v>
</v>
<v t="karstenw.20230303140159.74"><vh>class _TestView</vh>
<v t="karstenw.20230303140159.75"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.76"><vh>tearDown</vh></v>
<v t="karstenw.20230303140159.77"><vh>test_view</vh></v>
</v>
<v t="karstenw.20230303140159.78"><vh>class TestMySQLView</vh>
<v t="karstenw.20230303140159.79"><vh>setUp</vh></v>
</v>
<v t="karstenw.20230303140159.80"><vh>class TestSQLiteView</vh>
<v t="karstenw.20230303140159.81"><vh>setUp</vh></v>
</v>
<v t="karstenw.20230303140159.82"><vh>class TestCSV</vh>
<v t="karstenw.20230303140159.83"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.84"><vh>test_csv_header</vh></v>
<v t="karstenw.20230303140159.85"><vh>test_csv</vh></v>
<v t="karstenw.20230303140159.86"><vh>test_file</vh></v>
</v>
<v t="karstenw.20230303140159.87"><vh>class TestDatasheet</vh>
<v t="karstenw.20230303140159.88"><vh>setUp</vh></v>
<v t="karstenw.20230303140159.89"><vh>test_rows</vh></v>
<v t="karstenw.20230303140159.90"><vh>test_columns</vh></v>
<v t="karstenw.20230303140159.91"><vh>test_column</vh></v>
<v t="karstenw.20230303140159.92"><vh>test_fields</vh></v>
<v t="karstenw.20230303140159.93"><vh>test_group</vh></v>
<v t="karstenw.20230303140159.94"><vh>test_slice</vh></v>
<v t="karstenw.20230303140159.95"><vh>test_copy</vh></v>
<v t="karstenw.20230303140159.96"><vh>test_map</vh></v>
<v t="karstenw.20230303140159.97"><vh>test_json</vh></v>
<v t="karstenw.20230303140159.98"><vh>test_flip</vh></v>
<v t="karstenw.20230303140159.99"><vh>test_truncate</vh></v>
<v t="karstenw.20230303140159.100"><vh>test_pprint</vh></v>
</v>
<v t="karstenw.20230303140159.101"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140017.1"><vh>@clean pattern/test/test_de.py</vh>
<v t="karstenw.20230303140202.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140202.2"><vh>class TestInflection</vh>
<v t="karstenw.20230303140202.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140202.4"><vh>test_gender</vh></v>
<v t="karstenw.20230303140202.5"><vh>test_pluralize</vh></v>
<v t="karstenw.20230303140202.6"><vh>test_singularize</vh></v>
<v t="karstenw.20230303140202.7"><vh>test_attributive</vh></v>
<v t="karstenw.20230303140202.8"><vh>test_predicative</vh></v>
<v t="karstenw.20230303140202.9"><vh>test_find_lemma</vh></v>
<v t="karstenw.20230303140202.10"><vh>test_find_lexeme</vh></v>
<v t="karstenw.20230303140202.11"><vh>test_conjugate</vh></v>
<v t="karstenw.20230303140202.12"><vh>test_lexeme</vh></v>
<v t="karstenw.20230303140202.13"><vh>test_tenses</vh></v>
</v>
<v t="karstenw.20230303140202.14"><vh>class TestParser</vh>
<v t="karstenw.20230303140202.15"><vh>setUp</vh></v>
<v t="karstenw.20230303140202.16"><vh>test_find_lemmata</vh></v>
<v t="karstenw.20230303140202.17"><vh>test_parse</vh></v>
<v t="karstenw.20230303140202.18"><vh>test_tag</vh></v>
<v t="karstenw.20230303140202.19"><vh>test_command_line</vh></v>
</v>
<v t="karstenw.20230303140202.20"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140022.1"><vh>@clean pattern/test/test_en.py</vh>
<v t="karstenw.20230303140204.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140204.2"><vh>class TestInflection</vh>
<v t="karstenw.20230303140204.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140204.4"><vh>test_indefinite_article</vh></v>
<v t="karstenw.20230303140204.5"><vh>test_pluralize</vh></v>
<v t="karstenw.20230303140204.6"><vh>test_singularize</vh></v>
<v t="karstenw.20230303140204.7"><vh>test_find_lemma</vh></v>
<v t="karstenw.20230303140204.8"><vh>test_find_lexeme</vh></v>
<v t="karstenw.20230303140204.9"><vh>test_conjugate</vh></v>
<v t="karstenw.20230303140204.10"><vh>test_lemma</vh></v>
<v t="karstenw.20230303140204.11"><vh>test_lexeme</vh></v>
<v t="karstenw.20230303140204.12"><vh>test_tenses</vh></v>
<v t="karstenw.20230303140204.13"><vh>test_comparative</vh></v>
<v t="karstenw.20230303140204.14"><vh>test_superlative</vh></v>
</v>
<v t="karstenw.20230303140204.15"><vh>class TestQuantification</vh>
<v t="karstenw.20230303140204.16"><vh>setUp</vh></v>
<v t="karstenw.20230303140204.17"><vh>test_extract_leading_zeros</vh></v>
<v t="karstenw.20230303140204.18"><vh>test_numerals</vh></v>
<v t="karstenw.20230303140204.19"><vh>test_number</vh></v>
<v t="karstenw.20230303140204.20"><vh>test_quantify</vh></v>
<v t="karstenw.20230303140204.21"><vh>test_reflect</vh></v>
</v>
<v t="karstenw.20230303140204.22"><vh>class TestSpelling</vh>
<v t="karstenw.20230303140204.23"><vh>test_spelling</vh></v>
</v>
<v t="karstenw.20230303140204.24"><vh>class TestParser</vh>
<v t="karstenw.20230303140204.25"><vh>setUp</vh></v>
<v t="karstenw.20230303140204.26"><vh>test_tokenize</vh></v>
<v t="karstenw.20230303140204.27"><vh>_test_morphological_rules</vh></v>
<v t="karstenw.20230303140204.28"><vh>test_default_suffix_rules</vh></v>
<v t="karstenw.20230303140204.29"><vh>test_apply_morphological_rules</vh></v>
<v t="karstenw.20230303140204.30"><vh>test_apply_context_rules</vh></v>
<v t="karstenw.20230303140204.31"><vh>test_find_tags</vh></v>
<v t="karstenw.20230303140204.32"><vh>test_find_chunks</vh></v>
<v t="karstenw.20230303140204.33"><vh>test_find_labels</vh></v>
<v t="karstenw.20230303140204.34"><vh>test_find_prepositions</vh></v>
<v t="karstenw.20230303140204.35"><vh>test_find_lemmata</vh></v>
<v t="karstenw.20230303140204.36"><vh>test_named_entity_recognition</vh></v>
<v t="karstenw.20230303140204.37"><vh>test_parse</vh></v>
<v t="karstenw.20230303140204.38"><vh>test_tagged_string</vh></v>
<v t="karstenw.20230303140204.39"><vh>test_parsetree</vh></v>
<v t="karstenw.20230303140204.40"><vh>test_split</vh></v>
<v t="karstenw.20230303140204.41"><vh>test_tag</vh></v>
<v t="karstenw.20230303140204.42"><vh>test_ngrams</vh></v>
<v t="karstenw.20230303140204.43"><vh>test_command_line</vh></v>
</v>
<v t="karstenw.20230303140204.44"><vh>class TestParseTree</vh>
<v t="karstenw.20230303140204.45"><vh>setUp</vh></v>
<v t="karstenw.20230303140204.46"><vh>test_copy</vh></v>
<v t="karstenw.20230303140204.47"><vh>test_xml</vh></v>
<v t="karstenw.20230303140204.48"><vh>test_text</vh></v>
<v t="karstenw.20230303140204.49"><vh>test_sentence</vh></v>
<v t="karstenw.20230303140204.50"><vh>test_sentence_constituents</vh></v>
<v t="karstenw.20230303140204.51"><vh>test_slice</vh></v>
<v t="karstenw.20230303140204.52"><vh>test_chunk</vh></v>
<v t="karstenw.20230303140204.53"><vh>test_chunk_conjunctions</vh></v>
<v t="karstenw.20230303140204.54"><vh>test_chunk_modifiers</vh></v>
<v t="karstenw.20230303140204.55"><vh>test_pnp</vh></v>
<v t="karstenw.20230303140204.56"><vh>test_word</vh></v>
<v t="karstenw.20230303140204.57"><vh>test_word_custom_tags</vh></v>
<v t="karstenw.20230303140204.58"><vh>test_find</vh></v>
<v t="karstenw.20230303140204.59"><vh>test_zip</vh></v>
<v t="karstenw.20230303140204.60"><vh>test_unzip</vh></v>
<v t="karstenw.20230303140204.61"><vh>test_unique</vh></v>
<v t="karstenw.20230303140204.62"><vh>test_map</vh></v>
</v>
<v t="karstenw.20230303140204.63"><vh>class TestModality</vh>
<v t="karstenw.20230303140204.64"><vh>setUp</vh></v>
<v t="karstenw.20230303140204.65"><vh>test_imperative</vh></v>
<v t="karstenw.20230303140204.66"><vh>test_conditional</vh></v>
<v t="karstenw.20230303140204.67"><vh>test_subjunctive</vh></v>
<v t="karstenw.20230303140204.68"><vh>test_negated</vh></v>
<v t="karstenw.20230303140204.69"><vh>test_mood</vh></v>
<v t="karstenw.20230303140204.70"><vh>test_modality</vh></v>
</v>
<v t="karstenw.20230303140204.71"><vh>class TestSentiment</vh>
<v t="karstenw.20230303140204.72"><vh>setUp</vh></v>
<v t="karstenw.20230303140204.73"><vh>test_sentiment_avg</vh></v>
<v t="karstenw.20230303140204.74"><vh>test_sentiment</vh></v>
<v t="karstenw.20230303140204.75"><vh>test_sentiment_twitter</vh></v>
<v t="karstenw.20230303140204.76"><vh>test_sentiment_assessment</vh></v>
<v t="karstenw.20230303140204.77"><vh>test_polarity</vh></v>
<v t="karstenw.20230303140204.78"><vh>test_subjectivity</vh></v>
<v t="karstenw.20230303140204.79"><vh>test_positive</vh></v>
<v t="karstenw.20230303140204.80"><vh>test_sentiwordnet</vh></v>
</v>
<v t="karstenw.20230303140204.81"><vh>class TestWordNet</vh>
<v t="karstenw.20230303140204.82"><vh>setUp</vh></v>
<v t="karstenw.20230303140204.83"><vh>test_normalize</vh></v>
<v t="karstenw.20230303140204.84"><vh>test_version</vh></v>
<v t="karstenw.20230303140204.85"><vh>test_synsets</vh></v>
<v t="karstenw.20230303140204.86"><vh>test_synset</vh></v>
<v t="karstenw.20230303140204.87"><vh>test_ancenstor</vh></v>
<v t="karstenw.20230303140204.88"><vh>test_map32</vh></v>
<v t="karstenw.20230303140204.89"><vh>test_sentiwordnet</vh></v>
</v>
<v t="karstenw.20230303140204.90"><vh>class TestWordlists</vh>
<v t="karstenw.20230303140204.91"><vh>setUp</vh></v>
<v t="karstenw.20230303140204.92"><vh>test_wordlist</vh></v>
</v>
<v t="karstenw.20230303140204.93"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140025.1"><vh>@clean pattern/test/test_es.py</vh>
<v t="karstenw.20230303140206.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140206.2"><vh>class TestInflection</vh>
<v t="karstenw.20230303140206.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140206.4"><vh>test_pluralize</vh></v>
<v t="karstenw.20230303140206.5"><vh>test_singularize</vh></v>
<v t="karstenw.20230303140206.6"><vh>test_attributive</vh></v>
<v t="karstenw.20230303140206.7"><vh>test_predicative</vh></v>
<v t="karstenw.20230303140206.8"><vh>test_find_lemma</vh></v>
<v t="karstenw.20230303140206.9"><vh>test_find_lexeme</vh></v>
<v t="karstenw.20230303140206.10"><vh>test_conjugate</vh></v>
<v t="karstenw.20230303140206.11"><vh>test_lexeme</vh></v>
<v t="karstenw.20230303140206.12"><vh>test_tenses</vh></v>
</v>
<v t="karstenw.20230303140206.13"><vh>class TestParser</vh>
<v t="karstenw.20230303140206.14"><vh>setUp</vh></v>
<v t="karstenw.20230303140206.15"><vh>test_find_lemmata</vh></v>
<v t="karstenw.20230303140206.16"><vh>test_parse</vh></v>
<v t="karstenw.20230303140206.17"><vh>test_tag</vh></v>
<v t="karstenw.20230303140206.18"><vh>test_command_line</vh></v>
</v>
<v t="karstenw.20230303140206.19"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140030.1"><vh>@clean pattern/test/test_fr.py</vh>
<v t="karstenw.20230303140209.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140209.2"><vh>class TestInflection</vh>
<v t="karstenw.20230303140209.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140209.4"><vh>test_predicative</vh></v>
<v t="karstenw.20230303140209.5"><vh>test_find_lemma</vh></v>
<v t="karstenw.20230303140209.6"><vh>test_find_lexeme</vh></v>
<v t="karstenw.20230303140209.7"><vh>test_conjugate</vh></v>
<v t="karstenw.20230303140209.8"><vh>test_lexeme</vh></v>
<v t="karstenw.20230303140209.9"><vh>test_tenses</vh></v>
</v>
<v t="karstenw.20230303140209.10"><vh>class TestParser</vh>
<v t="karstenw.20230303140209.11"><vh>setUp</vh></v>
<v t="karstenw.20230303140209.12"><vh>test_find_prepositions</vh></v>
<v t="karstenw.20230303140209.13"><vh>test_find_lemmata</vh></v>
<v t="karstenw.20230303140209.14"><vh>test_parse</vh></v>
<v t="karstenw.20230303140209.15"><vh>test_tag</vh></v>
<v t="karstenw.20230303140209.16"><vh>test_command_line</vh></v>
</v>
<v t="karstenw.20230303140209.17"><vh>class TestSentiment</vh>
<v t="karstenw.20230303140209.18"><vh>setUp</vh></v>
<v t="karstenw.20230303140209.19"><vh>test_sentiment</vh></v>
<v t="karstenw.20230303140209.20"><vh>test_tokenizer</vh></v>
</v>
<v t="karstenw.20230303140209.21"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140045.1"><vh>@clean pattern/test/test_it.py</vh>
<v t="karstenw.20230303140211.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140211.2"><vh>class TestInflection</vh>
<v t="karstenw.20230303140211.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140211.4"><vh>test_article</vh></v>
<v t="karstenw.20230303140211.5"><vh>test_gender</vh></v>
<v t="karstenw.20230303140211.6"><vh>test_pluralize</vh></v>
<v t="karstenw.20230303140211.7"><vh>test_singularize</vh></v>
<v t="karstenw.20230303140211.8"><vh>test_predicative</vh></v>
<v t="karstenw.20230303140211.9"><vh>test_find_lemma</vh></v>
<v t="karstenw.20230303140211.10"><vh>test_find_lexeme</vh></v>
<v t="karstenw.20230303140211.11"><vh>test_conjugate</vh></v>
<v t="karstenw.20230303140211.12"><vh>test_lexeme</vh></v>
<v t="karstenw.20230303140211.13"><vh>test_tenses</vh></v>
</v>
<v t="karstenw.20230303140211.14"><vh>class TestParser</vh>
<v t="karstenw.20230303140211.15"><vh>setUp</vh></v>
<v t="karstenw.20230303140211.16"><vh>test_find_lemmata</vh></v>
<v t="karstenw.20230303140211.17"><vh>test_parse</vh></v>
<v t="karstenw.20230303140211.18"><vh>test_tag</vh></v>
<v t="karstenw.20230303140211.19"><vh>test_command_line</vh></v>
</v>
<v t="karstenw.20230303140211.20"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140103.1"><vh>@clean pattern/test/test_nl.py</vh>
<v t="karstenw.20230303140212.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140212.2"><vh>class TestInflection</vh>
<v t="karstenw.20230303140212.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140212.4"><vh>test_pluralize</vh></v>
<v t="karstenw.20230303140212.5"><vh>test_singularize</vh></v>
<v t="karstenw.20230303140212.6"><vh>test_attributive</vh></v>
<v t="karstenw.20230303140212.7"><vh>test_predicative</vh></v>
<v t="karstenw.20230303140212.8"><vh>test_find_lemma</vh></v>
<v t="karstenw.20230303140212.9"><vh>test_find_lexeme</vh></v>
<v t="karstenw.20230303140212.10"><vh>test_conjugate</vh></v>
<v t="karstenw.20230303140212.11"><vh>test_lexeme</vh></v>
<v t="karstenw.20230303140212.12"><vh>test_tenses</vh></v>
</v>
<v t="karstenw.20230303140212.13"><vh>class TestParser</vh>
<v t="karstenw.20230303140212.14"><vh>setUp</vh></v>
<v t="karstenw.20230303140212.15"><vh>test_wotan2penntreebank</vh></v>
<v t="karstenw.20230303140212.16"><vh>test_find_lemmata</vh></v>
<v t="karstenw.20230303140212.17"><vh>test_parse</vh></v>
<v t="karstenw.20230303140213.1"><vh>test_tag</vh></v>
<v t="karstenw.20230303140213.2"><vh>test_command_line</vh></v>
</v>
<v t="karstenw.20230303140213.3"><vh>class TestSentiment</vh>
<v t="karstenw.20230303140213.4"><vh>setUp</vh></v>
<v t="karstenw.20230303140213.5"><vh>test_sentiment</vh></v>
</v>
<v t="karstenw.20230303140213.6"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140111.1"><vh>@clean pattern/test/test_ru.py</vh>
<v t="karstenw.20230303140214.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140214.2"><vh>class TestSpelling</vh>
<v t="karstenw.20230303140214.3"><vh>test_spelling</vh></v>
</v>
<v t="karstenw.20230303140214.4"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140056.1"><vh>@clean pattern/test/test_metrics.py</vh>
<v t="karstenw.20230303140216.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140216.2"><vh>class TestProfiling</vh>
<v t="karstenw.20230303140216.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140216.4"><vh>test_duration</vh></v>
<v t="karstenw.20230303140216.5"><vh>test_confustion_matrix</vh></v>
<v t="karstenw.20230303140216.6"><vh>test_accuracy</vh></v>
<v t="karstenw.20230303140216.7"><vh>test_precision</vh></v>
<v t="karstenw.20230303140216.8"><vh>test_recall</vh></v>
<v t="karstenw.20230303140216.9"><vh>test_F1</vh></v>
<v t="karstenw.20230303140216.10"><vh>test_agreement</vh></v>
</v>
<v t="karstenw.20230303140216.11"><vh>class TestTextMetrics</vh>
<v t="karstenw.20230303140216.12"><vh>setUp</vh></v>
<v t="karstenw.20230303140216.13"><vh>test_levenshtein</vh></v>
<v t="karstenw.20230303140216.14"><vh>test_levenshtein_similarity</vh></v>
<v t="karstenw.20230303140216.15"><vh>test_dice_coefficient</vh></v>
<v t="karstenw.20230303140216.16"><vh>test_similarity</vh></v>
<v t="karstenw.20230303140216.17"><vh>test_readability</vh></v>
<v t="karstenw.20230303140216.18"><vh>test_intertextuality</vh></v>
<v t="karstenw.20230303140216.19"><vh>test_ttr</vh></v>
<v t="karstenw.20230303140216.20"><vh>test_suffixes</vh></v>
<v t="karstenw.20230303140216.21"><vh>test_isplit</vh></v>
<v t="karstenw.20230303140216.22"><vh>test_cooccurrence</vh></v>
</v>
<v t="karstenw.20230303140216.23"><vh>class TestInterpolation</vh>
<v t="karstenw.20230303140216.24"><vh>setUp</vh></v>
<v t="karstenw.20230303140216.25"><vh>test_lerp</vh></v>
<v t="karstenw.20230303140216.26"><vh>test_smoothstep</vh></v>
<v t="karstenw.20230303140216.27"><vh>test_smoothrange</vh></v>
</v>
<v t="karstenw.20230303140216.28"><vh>class TestStatistics</vh>
<v t="karstenw.20230303140216.29"><vh>setUp</vh></v>
<v t="karstenw.20230303140216.30"><vh>test_mean</vh></v>
<v t="karstenw.20230303140216.31"><vh>test_median</vh></v>
<v t="karstenw.20230303140216.32"><vh>test_variance</vh></v>
<v t="karstenw.20230303140216.33"><vh>test_standard_deviation</vh></v>
<v t="karstenw.20230303140216.34"><vh>test_histogram</vh></v>
<v t="karstenw.20230303140216.35"><vh>test_moment</vh></v>
<v t="karstenw.20230303140216.36"><vh>test_skewness</vh></v>
<v t="karstenw.20230303140216.37"><vh>test_kurtosis</vh></v>
<v t="karstenw.20230303140216.38"><vh>test_quantile</vh></v>
<v t="karstenw.20230303140216.39"><vh>test_boxplot</vh></v>
</v>
<v t="karstenw.20230303140216.40"><vh>class TestStatisticalTests</vh>
<v t="karstenw.20230303140216.41"><vh>setUp</vh></v>
<v t="karstenw.20230303140216.42"><vh>test_fisher_test</vh></v>
<v t="karstenw.20230303140216.43"><vh>test_chi_squared</vh></v>
<v t="karstenw.20230303140216.44"><vh>test_chi_squared_p</vh></v>
<v t="karstenw.20230303140216.45"><vh>test_kolmogorov_smirnov</vh></v>
</v>
<v t="karstenw.20230303140216.46"><vh>class TestSpecialFunctions</vh>
<v t="karstenw.20230303140216.47"><vh>setUp</vh></v>
<v t="karstenw.20230303140216.48"><vh>test_gamma</vh></v>
<v t="karstenw.20230303140216.49"><vh>test_gammai</vh></v>
<v t="karstenw.20230303140216.50"><vh>test_erfc</vh></v>
<v t="karstenw.20230303140216.51"><vh>test_kolmogorov</vh></v>
</v>
<v t="karstenw.20230303140216.52"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140035.1"><vh>@clean pattern/test/test_graph.py</vh>
<v t="karstenw.20230303140218.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140218.2"><vh>class TestUtilityFunctions</vh>
<v t="karstenw.20230303140218.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140218.4"><vh>test_deepcopy</vh></v>
<v t="karstenw.20230303140218.5"><vh>test_unique</vh></v>
<v t="karstenw.20230303140218.6"><vh>test_coordinates</vh></v>
</v>
<v t="karstenw.20230303140218.7"><vh>class TestNode</vh>
<v t="karstenw.20230303140218.8"><vh>setUp</vh></v>
<v t="karstenw.20230303140218.9"><vh>test_node</vh></v>
<v t="karstenw.20230303140218.10"><vh>test_edge</vh></v>
<v t="karstenw.20230303140218.11"><vh>test_flatten</vh></v>
<v t="karstenw.20230303140218.12"><vh>test_text</vh></v>
</v>
<v t="karstenw.20230303140218.13"><vh>class TestEdge</vh>
<v t="karstenw.20230303140218.14"><vh>setUp</vh></v>
<v t="karstenw.20230303140218.15"><vh>test_edge</vh></v>
</v>
<v t="karstenw.20230303140218.16"><vh>class TestGraph</vh>
<v t="karstenw.20230303140218.17"><vh>setUp</vh></v>
<v t="karstenw.20230303140218.18"><vh>test_graph</vh></v>
<v t="karstenw.20230303140218.19"><vh>test_graph_nodes</vh></v>
<v t="karstenw.20230303140218.20"><vh>test_graph_edges</vh></v>
<v t="karstenw.20230303140218.21"><vh>test_cache</vh></v>
<v t="karstenw.20230303140218.22"><vh>test_paths</vh></v>
<v t="karstenw.20230303140218.23"><vh>test_eigenvector_centrality</vh></v>
<v t="karstenw.20230303140218.24"><vh>test_betweenness_centrality</vh></v>
<v t="karstenw.20230303140218.25"><vh>test_sorted</vh></v>
<v t="karstenw.20230303140218.26"><vh>test_prune</vh></v>
<v t="karstenw.20230303140218.27"><vh>test_fringe</vh></v>
<v t="karstenw.20230303140218.28"><vh>test_split</vh></v>
<v t="karstenw.20230303140218.29"><vh>test_update</vh></v>
<v t="karstenw.20230303140218.30"><vh>test_copy</vh></v>
</v>
<v t="karstenw.20230303140218.31"><vh>class TestGraphLayout</vh>
<v t="karstenw.20230303140218.32"><vh>setUp</vh></v>
<v t="karstenw.20230303140218.33"><vh>test_layout</vh></v>
</v>
<v t="karstenw.20230303140218.34"><vh>class TestGraphSpringLayout</vh>
<v t="karstenw.20230303140218.35"><vh>test_layout</vh></v>
<v t="karstenw.20230303140218.36"><vh>test_distance</vh></v>
<v t="karstenw.20230303140218.37"><vh>test_repulsion</vh></v>
<v t="karstenw.20230303140218.38"><vh>test_attraction</vh></v>
</v>
<v t="karstenw.20230303140218.39"><vh>class TestGraphTraversal</vh>
<v t="karstenw.20230303140218.40"><vh>setUp</vh></v>
<v t="karstenw.20230303140218.41"><vh>test_search</vh></v>
<v t="karstenw.20230303140218.42"><vh>test_paths</vh></v>
<v t="karstenw.20230303140218.43"><vh>test_edges</vh></v>
<v t="karstenw.20230303140218.44"><vh>test_adjacency</vh></v>
<v t="karstenw.20230303140218.45"><vh>test_dijkstra_shortest_path</vh></v>
<v t="karstenw.20230303140218.46"><vh>test_dijkstra_shortest_paths</vh></v>
<v t="karstenw.20230303140218.47"><vh>test_floyd_warshall_all_pairs_distance</vh></v>
</v>
<v t="karstenw.20230303140218.48"><vh>class TestGraphPartitioning</vh>
<v t="karstenw.20230303140218.49"><vh>setUp</vh></v>
<v t="karstenw.20230303140218.50"><vh>test_union</vh></v>
<v t="karstenw.20230303140218.51"><vh>test_intersection</vh></v>
<v t="karstenw.20230303140218.52"><vh>test_difference</vh></v>
<v t="karstenw.20230303140218.53"><vh>test_partition</vh></v>
<v t="karstenw.20230303140218.54"><vh>test_clique</vh></v>
</v>
<v t="karstenw.20230303140218.55"><vh>class TestGraphMaintenance</vh>
<v t="karstenw.20230303140218.56"><vh>setUp</vh></v>
<v t="karstenw.20230303140218.57"><vh>test_unlink</vh></v>
<v t="karstenw.20230303140218.58"><vh>test_redirect</vh></v>
<v t="karstenw.20230303140218.59"><vh>test_cut</vh></v>
<v t="karstenw.20230303140218.60"><vh>test_insert</vh></v>
</v>
<v t="karstenw.20230303140218.61"><vh>class TestGraphCommonsense</vh>
<v t="karstenw.20230303140218.62"><vh>setUp</vh></v>
<v t="karstenw.20230303140218.63"><vh>test_halo</vh></v>
<v t="karstenw.20230303140218.64"><vh>test_field</vh></v>
<v t="karstenw.20230303140218.65"><vh>test_similarity</vh></v>
</v>
<v t="karstenw.20230303140218.66"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140120.1"><vh>@clean pattern/test/test_search.py</vh>
<v t="karstenw.20230303140220.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140220.2"><vh>class TestUtilityFunctions</vh>
<v t="karstenw.20230303140220.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140220.4"><vh>test_match</vh></v>
<v t="karstenw.20230303140220.5"><vh>test_unique</vh></v>
<v t="karstenw.20230303140220.6"><vh>test_find</vh></v>
<v t="karstenw.20230303140220.7"><vh>test_product</vh></v>
<v t="karstenw.20230303140220.8"><vh>test_variations</vh></v>
<v t="karstenw.20230303140220.9"><vh>test_odict</vh></v>
</v>
<v t="karstenw.20230303140220.10"><vh>class TestTaxonomy</vh>
<v t="karstenw.20230303140220.11"><vh>setUp</vh></v>
<v t="karstenw.20230303140220.12"><vh>test_taxonomy</vh></v>
<v t="karstenw.20230303140220.13"><vh>test_classifier</vh></v>
<v t="karstenw.20230303140220.14"><vh>test_wordnet_classifier</vh></v>
</v>
<v t="karstenw.20230303140220.15"><vh>class TestConstraint</vh>
<v t="karstenw.20230303140220.16"><vh>setUp</vh></v>
<v t="karstenw.20230303140220.17"><vh>_test_constraint</vh></v>
<v t="karstenw.20230303140220.18"><vh>test_fromstring</vh></v>
<v t="karstenw.20230303140220.19"><vh>test_match</vh></v>
<v t="karstenw.20230303140220.20"><vh>test_string</vh></v>
</v>
<v t="karstenw.20230303140220.21"><vh>class TestPattern</vh>
<v t="karstenw.20230303140220.22"><vh>setUp</vh></v>
<v t="karstenw.20230303140220.23"><vh>test_pattern</vh></v>
<v t="karstenw.20230303140220.24"><vh>test_fromstring</vh></v>
<v t="karstenw.20230303140220.25"><vh>test_match</vh></v>
<v t="karstenw.20230303140220.26"><vh>test_search</vh></v>
<v t="karstenw.20230303140220.27"><vh>test_convergence</vh></v>
<v t="karstenw.20230303140220.28"><vh>test_compile_function</vh></v>
<v t="karstenw.20230303140220.29"><vh>test_match_function</vh></v>
<v t="karstenw.20230303140220.30"><vh>test_search_function</vh></v>
<v t="karstenw.20230303140220.31"><vh>test_escape</vh></v>
</v>
<v t="karstenw.20230303140220.32"><vh>class TestMatch</vh>
<v t="karstenw.20230303140220.33"><vh>setUp</vh></v>
<v t="karstenw.20230303140220.34"><vh>test_match</vh></v>
<v t="karstenw.20230303140220.35"><vh>test_group</vh></v>
<v t="karstenw.20230303140220.36"><vh>test_group_ordering</vh></v>
</v>
<v t="karstenw.20230303140220.37"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140126.1"><vh>@clean pattern/test/test_text.py</vh>
<v t="karstenw.20230303140222.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140222.2"><vh>class TestLexicon</vh>
<v t="karstenw.20230303140222.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140222.4"><vh>test_lazydict</vh></v>
<v t="karstenw.20230303140222.5"><vh>test_lazylist</vh></v>
<v t="karstenw.20230303140222.6"><vh>test_lexicon</vh></v>
</v>
<v t="karstenw.20230303140222.7"><vh>class TestFrequency</vh>
<v t="karstenw.20230303140222.8"><vh>setUp</vh></v>
<v t="karstenw.20230303140222.9"><vh>test_frequency</vh></v>
</v>
<v t="karstenw.20230303140222.10"><vh>class TestModel</vh>
<v t="karstenw.20230303140222.11"><vh>setUp</vh></v>
<v t="karstenw.20230303140222.12"><vh>test_model</vh></v>
</v>
<v t="karstenw.20230303140222.13"><vh>class TestMorphology</vh>
<v t="karstenw.20230303140222.14"><vh>setUp</vh></v>
<v t="karstenw.20230303140222.15"><vh>test_morphology</vh></v>
</v>
<v t="karstenw.20230303140222.16"><vh>class TestContext</vh>
<v t="karstenw.20230303140222.17"><vh>setUp</vh></v>
<v t="karstenw.20230303140222.18"><vh>test_context</vh></v>
</v>
<v t="karstenw.20230303140222.19"><vh>class TestEntities</vh>
<v t="karstenw.20230303140222.20"><vh>setUp</vh></v>
<v t="karstenw.20230303140222.21"><vh>test_entities</vh></v>
</v>
<v t="karstenw.20230303140222.22"><vh>class TestParser</vh>
<v t="karstenw.20230303140222.23"><vh>setUp</vh></v>
<v t="karstenw.20230303140222.24"><vh>test_stringio</vh></v>
<v t="karstenw.20230303140222.25"><vh>test_find_keywords</vh></v>
<v t="karstenw.20230303140222.26"><vh>test_find_tokens</vh></v>
<v t="karstenw.20230303140222.27"><vh>test_find_tags</vh></v>
<v t="karstenw.20230303140222.28"><vh>test_find_chunks</vh></v>
</v>
<v t="karstenw.20230303140222.29"><vh>class TestSentiment</vh>
<v t="karstenw.20230303140222.30"><vh>setUp</vh></v>
<v t="karstenw.20230303140222.31"><vh>test_dict</vh></v>
<v t="karstenw.20230303140222.32"><vh>test_bag_of_words</vh></v>
<v t="karstenw.20230303140222.33"><vh>test_annotate</vh></v>
</v>
<v t="karstenw.20230303140222.34"><vh>class TestMultilingual</vh>
<v t="karstenw.20230303140222.35"><vh>setUp</vh></v>
<v t="karstenw.20230303140222.36"><vh>test_language</vh></v>
<v t="karstenw.20230303140222.37"><vh>test_deflood</vh></v>
</v>
<v t="karstenw.20230303140222.38"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140131.1"><vh>@clean pattern/test/test_vector.py</vh>
<v t="karstenw.20230303140224.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140224.2"><vh>model</vh></v>
<v t="karstenw.20230303140224.3"><vh>class TestUnicode</vh>
<v t="karstenw.20230303140224.4"><vh>setUp</vh></v>
<v t="karstenw.20230303140224.5"><vh>test_decode_utf8</vh></v>
<v t="karstenw.20230303140224.6"><vh>test_encode_utf8</vh></v>
</v>
<v t="karstenw.20230303140224.7"><vh>class TestUtilityFunctions</vh>
<v t="karstenw.20230303140224.8"><vh>setUp</vh></v>
<v t="karstenw.20230303140224.9"><vh>test_shi</vh></v>
<v t="karstenw.20230303140224.10"><vh>test_shuffled</vh></v>
<v t="karstenw.20230303140224.11"><vh>test_chunk</vh></v>
<v t="karstenw.20230303140224.12"><vh>test_readonlydict</vh></v>
<v t="karstenw.20230303140224.13"><vh>test_readonlylist</vh></v>
</v>
<v t="karstenw.20230303140224.14"><vh>class TestStemmer</vh>
<v t="karstenw.20230303140224.15"><vh>setUp</vh></v>
<v t="karstenw.20230303140224.16"><vh>test_stem</vh></v>
<v t="karstenw.20230303140224.17"><vh>test_stem_case_sensitive</vh></v>
</v>
<v t="karstenw.20230303140224.18"><vh>class TestDocument</vh>
<v t="karstenw.20230303140224.19"><vh>setUp</vh></v>
<v t="karstenw.20230303140224.20"><vh>tearDown</vh></v>
<v t="karstenw.20230303140224.21"><vh>test_stopwords</vh></v>
<v t="karstenw.20230303140224.22"><vh>test_words</vh></v>
<v t="karstenw.20230303140224.23"><vh>test_stem</vh></v>
<v t="karstenw.20230303140224.24"><vh>test_count</vh></v>
<v t="karstenw.20230303140224.25"><vh>test_document</vh></v>
<v t="karstenw.20230303140224.26"><vh>test_document_load</vh></v>
<v t="karstenw.20230303140224.27"><vh>test_document_vector</vh></v>
<v t="karstenw.20230303140224.28"><vh>test_document_keywords</vh></v>
<v t="karstenw.20230303140224.29"><vh>test_tf</vh></v>
<v t="karstenw.20230303140224.30"><vh>test_tfidf</vh></v>
<v t="karstenw.20230303140224.31"><vh>test_cosine_similarity</vh></v>
</v>
<v t="karstenw.20230303140224.32"><vh>class TestModel</vh>
<v t="karstenw.20230303140224.33"><vh>setUp</vh></v>
<v t="karstenw.20230303140224.34"><vh>test_model</vh></v>
<v t="karstenw.20230303140224.35"><vh>test_model_append</vh></v>
<v t="karstenw.20230303140224.36"><vh>test_model_save</vh></v>
<v t="karstenw.20230303140224.37"><vh>test_model_export</vh></v>
<v t="karstenw.20230303140224.38"><vh>test_df</vh></v>
<v t="karstenw.20230303140224.39"><vh>test_idf</vh></v>
<v t="karstenw.20230303140224.40"><vh>test_tfidf</vh></v>
<v t="karstenw.20230303140224.41"><vh>test_frequent_concept_sets</vh></v>
<v t="karstenw.20230303140224.42"><vh>test_cosine_similarity</vh></v>
<v t="karstenw.20230303140224.43"><vh>test_nearest_neighbors</vh></v>
<v t="karstenw.20230303140224.44"><vh>test_search</vh></v>
<v t="karstenw.20230303140224.45"><vh>test_distance</vh></v>
<v t="karstenw.20230303140224.46"><vh>test_cluster</vh></v>
<v t="karstenw.20230303140224.47"><vh>test_centroid</vh></v>
<v t="karstenw.20230303140224.48"><vh>test_lsa</vh></v>
<v t="karstenw.20230303140224.49"><vh>test_feature_selection</vh></v>
<v t="karstenw.20230303140224.50"><vh>test_information_gain</vh></v>
<v t="karstenw.20230303140224.51"><vh>test_entropy</vh></v>
<v t="karstenw.20230303140224.52"><vh>test_condensed_nearest_neighbor</vh></v>
<v t="karstenw.20230303140224.53"><vh>test_classifier</vh></v>
</v>
<v t="karstenw.20230303140224.54"><vh>class TestApriori</vh>
<v t="karstenw.20230303140224.55"><vh>setUp</vh></v>
<v t="karstenw.20230303140224.56"><vh>test_apriori</vh></v>
</v>
<v t="karstenw.20230303140224.57"><vh>class TestLSA</vh>
<v t="karstenw.20230303140224.58"><vh>setUp</vh></v>
<v t="karstenw.20230303140224.59"><vh>tearDown</vh></v>
<v t="karstenw.20230303140224.60"><vh>test_lsa</vh></v>
<v t="karstenw.20230303140224.61"><vh>test_lsa_concepts</vh></v>
<v t="karstenw.20230303140224.62"><vh>test_model_reduce</vh></v>
</v>
<v t="karstenw.20230303140224.63"><vh>class TestClustering</vh>
<v t="karstenw.20230303140224.64"><vh>setUp</vh></v>
<v t="karstenw.20230303140224.65"><vh>tearDown</vh></v>
<v t="karstenw.20230303140224.66"><vh>test_features</vh></v>
<v t="karstenw.20230303140224.67"><vh>test_mean</vh></v>
<v t="karstenw.20230303140224.68"><vh>test_centroid</vh></v>
<v t="karstenw.20230303140224.69"><vh>test_distance</vh></v>
<v t="karstenw.20230303140224.70"><vh>test_distancemap</vh></v>
<v t="karstenw.20230303140224.71"><vh>_test_k_means</vh></v>
<v t="karstenw.20230303140224.72"><vh>test_k_means_random</vh></v>
<v t="karstenw.20230303140224.73"><vh>test_k_means_kmpp</vh></v>
<v t="karstenw.20230303140224.74"><vh>test_hierarchical</vh></v>
</v>
<v t="karstenw.20230303140224.75"><vh>class TestClassifier</vh>
<v t="karstenw.20230303140224.76"><vh>setUp</vh></v>
<v t="karstenw.20230303140224.77"><vh>_test_classifier</vh></v>
<v t="karstenw.20230303140224.78"><vh>test_classifier_vector</vh></v>
<v t="karstenw.20230303140224.79"><vh>test_nb</vh></v>
<v t="karstenw.20230303140224.80"><vh>test_igtree</vh></v>
<v t="karstenw.20230303140224.81"><vh>test_knn</vh></v>
<v t="karstenw.20230303140224.82"><vh>test_slp</vh></v>
<v t="karstenw.20230303140224.83"><vh>test_svm</vh></v>
<v t="karstenw.20230303140224.84"><vh>test_liblinear</vh></v>
</v>
<v t="karstenw.20230303140224.85"><vh>suite</vh></v>
</v>
<v t="karstenw.20230303140139.1"><vh>@clean pattern/test/test_web.py</vh>
<v t="karstenw.20230303140227.1"><vh>Declarations</vh></v>
<v t="karstenw.20230303140227.2"><vh>class TestCache</vh>
<v t="karstenw.20230303140227.3"><vh>setUp</vh></v>
<v t="karstenw.20230303140227.4"><vh>test_cache</vh></v>
</v>
<v t="karstenw.20230303140227.5"><vh>class TestUnicode</vh>
<v t="karstenw.20230303140227.6"><vh>setUp</vh></v>
<v t="karstenw.20230303140227.7"><vh>test_decode_utf8</vh></v>
<v t="karstenw.20230303140227.8"><vh>test_encode_utf8</vh></v>
<v t="karstenw.20230303140227.9"><vh>test_fix</vh></v>
</v>
<v t="karstenw.20230303140227.10"><vh>class TestURL</vh>
<v t="karstenw.20230303140227.11"><vh>setUp</vh></v>
<v t="karstenw.20230303140227.12"><vh>test_asynchrous</vh></v>
<v t="karstenw.20230303140227.13"><vh>test_extension</vh></v>
<v t="karstenw.20230303140227.14"><vh>test_urldecode</vh></v>
<v t="karstenw.20230303140227.15"><vh>test_proxy</vh></v>
<v t="karstenw.20230303140227.16"><vh>test_url_parts</vh></v>
<v t="karstenw.20230303140227.17"><vh>test_url_query</vh></v>
<v t="karstenw.20230303140227.18"><vh>test_url_string</vh></v>
<v t="karstenw.20230303140227.19"><vh>test_url</vh></v>
<v t="karstenw.20230303140227.20"><vh>test_url_open</vh></v>
<v t="karstenw.20230303140227.21"><vh>test_url_download</vh></v>
<v t="karstenw.20230303140227.22"><vh>test_url_mimetype</vh></v>
<v t="karstenw.20230303140227.23"><vh>test_url_headers</vh></v>
<v t="karstenw.20230303140227.24"><vh>test_url_redirect</vh></v>
<v t="karstenw.20230303140227.25"><vh>test_abs</vh></v>
<v t="karstenw.20230303140227.26"><vh>test_base</vh></v>
<v t="karstenw.20230303140227.27"><vh>test_oauth</vh></v>
</v>
<v t="karstenw.20230303140227.28"><vh>class TestPlaintext</vh>
<v t="karstenw.20230303140227.29"><vh>setUp</vh></v>
<v t="karstenw.20230303140227.30"><vh>test_find_urls</vh></v>
<v t="karstenw.20230303140227.31"><vh>test_find_email</vh></v>
<v t="karstenw.20230303140227.32"><vh>test_find_between</vh></v>
<v t="karstenw.20230303140227.33"><vh>test_strip_tags</vh></v>
<v t="karstenw.20230303140227.34"><vh>test_strip_element</vh></v>
<v t="karstenw.20230303140227.35"><vh>test_strip_between</vh></v>
<v t="karstenw.20230303140227.36"><vh>test_strip_javascript</vh></v>
<v t="karstenw.20230303140227.37"><vh>test_strip_inline_css</vh></v>
<v t="karstenw.20230303140227.38"><vh>test_strip_comments</vh></v>
<v t="karstenw.20230303140227.39"><vh>test_strip_forms</vh></v>
<v t="karstenw.20230303140227.40"><vh>test_encode_entities</vh></v>
<v t="karstenw.20230303140227.41"><vh>test_decode_entities</vh></v>
<v t="karstenw.20230303140227.42"><vh>test_collapse_spaces</vh></v>
<v t="karstenw.20230303140227.43"><vh>test_collapse_tabs</vh></v>
<v t="karstenw.20230303140227.44"><vh>test_collapse_linebreaks</vh></v>
<v t="karstenw.20230303140227.45"><vh>test_plaintext</vh></v>
</v>
<v t="karstenw.20230303140227.46"><vh>class TestSearchEngine</vh>
<v t="karstenw.20230303140227.47"><vh>setUp</vh></v>
<v t="karstenw.20230303140227.48"><vh>_test_search_engine</vh></v>
<v t="karstenw.20230303140227.49"><vh>test_search_google</vh></v>
<v t="karstenw.20230303140227.50"><vh>test_search_yahoo</vh></v>
<v t="karstenw.20230303140227.51"><vh>test_search_bing</vh></v>
<v t="karstenw.20230303140227.52"><vh>test_search_twitter</vh></v>
<v t="karstenw.20230303140227.53"><vh>test_search_wikipedia</vh></v>
<v t="karstenw.20230303140227.54"><vh>test_search_wikia</vh></v>
<v t="karstenw.20230303140227.55"><vh>test_search_flickr</vh></v>
<v t="karstenw.20230303140227.56"><vh>test_search_facebook</vh></v>
<v t="karstenw.20230303140227.57"><vh>test_search_productwiki</vh></v>
<v t="karstenw.20230303140227.58"><vh>test_search_newsfeed</vh></v>
<v t="karstenw.20230303140227.59"><vh>_test_results</vh></v>
<v t="karstenw.20230303140227.60"><vh>test_results_google</vh></v>
<v t="karstenw.20230303140227.61"><vh>test_results_yahoo</vh></v>
<v t="karstenw.20230303140227.62"><vh>test_results_yahoo_images</vh></v>
<v t="karstenw.20230303140227.63"><vh>test_results_yahoo_news</vh></v>
<v t="karstenw.20230303140227.64"><vh>test_results_bing</vh></v>
<v t="karstenw.20230303140227.65"><vh>test_results_bing_images</vh></v>
<v t="karstenw.20230303140227.66"><vh>test_results_bing_news</vh></v>
<v t="karstenw.20230303140227.67"><vh>test_results_twitter</vh></v>
<v t="karstenw.20230303140227.68"><vh>test_results_flickr</vh></v>
<v t="karstenw.20230303140227.69"><vh>test_results_facebook</vh></v>
<v t="karstenw.20230303140227.70"><vh>test_google_translate</vh></v>
<v t="karstenw.20230303140227.71"><vh>test_google_identify</vh></v>
<v t="karstenw.20230303140227.72"><vh>test_twitter_author</vh></v>
<v t="karstenw.20230303140227.73"><vh>test_twitter_hashtags</vh></v>
<v t="karstenw.20230303140227.74"><vh>test_twitter_retweets</vh></v>
<v t="karstenw.20230303140227.75"><vh>_test_search_image_size</vh></v>
<v t="karstenw.20230303140227.76"><vh>test_yahoo_image_size</vh></v>
<v t="karstenw.20230303140227.77"><vh>test_bing_image_size</vh></v>
<v t="karstenw.20230303140227.78"><vh>test_flickr_image_size</vh></v>
<v t="karstenw.20230303140227.79"><vh>test_wikipedia_list</vh></v>
<v t="karstenw.20230303140227.80"><vh>test_wikipedia_all</vh></v>
<v t="karstenw.20230303140227.81"><vh>test_wikipedia_article</vh></v>
<v t="karstenw.20230303140227.82"><vh>test_wikipedia_article_sections</vh></v>
<v t="karstenw.20230303140227.83"><vh>test_productwiki</vh></v>
</v>
<v t="karstenw.20230303140227.84"><vh>class TestDOM</vh>
<v t="karstenw.20230303140227.85"><vh>setUp</vh></v>
<v t="karstenw.20230303140227.86"><vh>test_node_document</vh></v>
<v t="karstenw.20230303140227.87"><vh>test_node_traverse</vh></v>
<v t="karstenw.20230303140227.88"><vh>test_element</vh></v>
<v t="karstenw.20230303140227.89"><vh>test_selector</vh></v>
</v>
<v t="karstenw.20230303140227.90"><vh>class TestDocumentParser</vh>
<v t="karstenw.20230303140227.91"><vh>setUp</vh></v>
<v t="karstenw.20230303140227.92"><vh>test_pdf</vh></v>
<v t="karstenw.20230303140227.93"><vh>test_docx</vh></v>
</v>
<v t="karstenw.20230303140227.94"><vh>class TestLocale</vh>
<v t="karstenw.20230303140227.95"><vh>setUp</vh></v>
<v t="karstenw.20230303140227.96"><vh>test_encode_language</vh></v>
<v t="karstenw.20230303140227.97"><vh>test_decode_language</vh></v>
<v t="karstenw.20230303140227.98"><vh>test_encode_region</vh></v>
<v t="karstenw.20230303140227.99"><vh>test_decode_region</vh></v>
<v t="karstenw.20230303140227.100"><vh>test_languages</vh></v>
<v t="karstenw.20230303140227.101"><vh>test_regions</vh></v>
<v t="karstenw.20230303140227.102"><vh>test_regionalize</vh></v>
<v t="karstenw.20230303140227.103"><vh>test_geocode</vh></v>
<v t="karstenw.20230303140227.104"><vh>test_correlation</vh></v>
</v>
<v t="karstenw.20230303140227.105"><vh>class TestMail</vh>
<v t="karstenw.20230303140227.106"><vh>setUp</vh></v>
<v t="karstenw.20230303140227.107"><vh>test_mail</vh></v>
<v t="karstenw.20230303140227.108"><vh>test_mail_message1</vh></v>
<v t="karstenw.20230303140227.109"><vh>test_mail_message2</vh></v>
</v>
<v t="karstenw.20230303140227.110"><vh>class TestCrawler</vh>
<v t="karstenw.20230303140227.111"><vh>setUp</vh></v>
<v t="karstenw.20230303140227.112"><vh>test_link</vh></v>
<v t="karstenw.20230303140227.113"><vh>test_crawler_crawl</vh></v>
<v t="karstenw.20230303140227.114"><vh>test_crawler_delay</vh></v>
<v t="karstenw.20230303140227.115"><vh>test_crawler_breadth</vh></v>
</v>
<v t="karstenw.20230303140227.116"><vh>suite</vh></v>
</v>
</v>
</v>
</vnodes>
<tnodes>
<t tx="karstenw.20230303121029.2"></t>
<t tx="karstenw.20230303122029.1">#### PATTERN ###################################################################

# Authors:
#   Tom De Smedt &lt;tom@organisms.be&gt;,
#   Walter Daelemans &lt;walter.daelemans@ua.ac.be&gt;
# License:
#   BSD License, see LICENSE.txt

#### BSD LICENSE ###############################################################

# Copyright (c) 2010 University of Antwerp, Belgium
# All rights reserved.
#
#   Redistribution and use in source and binary forms, with or without
#   modification, are permitted provided that the following conditions are met:
#
#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above copyright
#       notice, this list of conditions and the following disclaimer in
#       the documentation and/or other materials provided with the
#       distribution.
#     * Neither the name of Pattern nor the names of its
#       contributors may be used to endorse or promote products
#       derived from this software without specific prior written
#       permission.
#
#   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
#   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
#   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
#   FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
#   COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
#   INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
#   BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#   LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
#   CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
#   LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
#   ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
#   POSSIBILITY OF SUCH DAMAGE.
#
# CLiPS Computational Linguistics Group, University of Antwerp, Belgium
# http://www.clips.ua.ac.be/pages/pattern

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303122044.1">from __future__ import unicode_literals

### CREDITS ####################################################################

__author__    = "Tom De Smedt"
__credits__   = "Tom De Smedt, Walter Daelemans"
__version__   = "3.6"
__copyright__ = "Copyright (c) 2010 University of Antwerp (BE)"
__license__   = "BSD"

################################################################################

import os

# Shortcuts to pattern.en, pattern.es, ...
# (instead of pattern.text.en, pattern.text.es, ...)
try:
    __path__.append(os.path.join(__path__[0], "text"))
except:
    pass

from . import db
from . import graph
from . import text
from . import vector
from . import web

</t>
<t tx="karstenw.20230303123040.1"></t>
<t tx="karstenw.20230303123043.1">#### PATTERN | DB ##################################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303123103.1">from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int, chr
from builtins import map, zip, filter
from builtins import object, range, next

import os
import sys
import inspect
import warnings
import re
import urllib
import base64
import json

import csv as csvlib


from codecs import BOM_UTF8
from itertools import islice
from datetime import datetime, timedelta
from calendar import monthrange
from time import mktime, strftime
from math import sqrt
from types import GeneratorType

from functools import cmp_to_key

from io import open, StringIO, BytesIO

BOM_UTF8 = BOM_UTF8.decode("utf-8")

from html.entities import name2codepoint

from email.utils import parsedate_tz, mktime_tz

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

from pattern.helpers import encode_string, decode_string

decode_utf8 = decode_string
encode_utf8 = encode_string

MYSQL = "mysql"
SQLITE = "sqlite"


</t>
<t tx="karstenw.20230303123103.10">@property
def microseconds(self):
    return self.microsecond

</t>
<t tx="karstenw.20230303123103.100">class Schema(object):

    @others
#### TABLE #########################################################################################

ALL = "*"


</t>
<t tx="karstenw.20230303123103.101">def __init__(self, name, type, default=None, index=False, optional=True, extra=None):
    """ Field info returned from a "show columns from table"-query.
        Each table object has a Table.schema{} dictionary describing the fields' structure.
    """
    # Determine field type (NUMBER, STRING, TEXT, BLOB or DATE).
    type, length = type.lower(), None
    if type.startswith(("varchar", "char")):
        length = type.split("(")[-1].strip(")")
        length = int(length)
        type = STRING
    if type.startswith("int"):
        type = INTEGER
    if type.startswith(("real", "double")):
        type = FLOAT
    if type.startswith("time"):
        type = DATE
    if type.startswith("text"):
        type = TEXT
    if type.startswith("blob"):
        type = BLOB
    if type.startswith("tinyint(1)"):
        type = BOOLEAN
    # Determine index type (PRIMARY, UNIQUE, True or False).
    if isinstance(index, str):
        if index.lower().startswith("pri"):
            index = PRIMARY
        if index.lower().startswith("uni"):
            index = UNIQUE
        if index.lower() in ("0", "1", "", "yes", "mul"):
            index = index.lower() in ("1", "yes", "mul")
    # SQLite dumps the date string with quotes around it:
    if isinstance(default, str) and type == DATE:
        default = default.strip("'")
        default = default.replace("current_timestamp", NOW)
        default = default.replace("CURRENT_TIMESTAMP", NOW)
    if default is not None and type == INTEGER:
        default = int(default)
    if default is not None and type == FLOAT:
        default = float(default)
    if not default and default != 0:
        default = None
    self.name     = name                   # Field name.
    self.type     = type                   # Field type: INTEGER | FLOAT | STRING | TEXT | BLOB | DATE.
    self.length   = length                 # Field length for STRING.
    self.default  = default                # Default value.
    self.index    = index                  # PRIMARY | UNIQUE | True | False.
    self.optional = str(optional) in ("0", "True", "YES")
    self.extra    = extra or None

</t>
<t tx="karstenw.20230303123103.102">def __repr__(self):
    return "Schema(name=%s, type=%s, default=%s, index=%s, optional=%s)" % (
        repr(self.name),
        repr(self.type),
        repr(self.default),
        repr(self.index),
        repr(self.optional))

</t>
<t tx="karstenw.20230303123103.103">class TableError(Exception):
    pass


</t>
<t tx="karstenw.20230303123103.104">class Table(object):

    @others
#### QUERY #########################################################################################

#--- QUERY SYNTAX ----------------------------------------------------------------------------------

BETWEEN, LIKE, IN = \
    "between", "like", "in"

sql_functions = \
    "first|last|count|min|max|sum|avg|stdev|group_concat|concatenate|" \
    "year|month|day|hour|minute|second|" \
    "length|lower|upper|substr|substring|replace|trim|round|random|rand|" \
    "strftime|date_format"


</t>
<t tx="karstenw.20230303123103.105">class Fields(list):
    # Table.fields.append() alters the table.
    # New field() with optional=False must have a default value (can not be NOW).
    # New field() can have index=True, but not PRIMARY or UNIQUE.
    @others
</t>
<t tx="karstenw.20230303123103.106">def __init__(self, table, *args, **kwargs):
    list.__init__(self, *args, **kwargs)
    self.table = table

</t>
<t tx="karstenw.20230303123103.107">def append(self, field):
    name, (field, index) = field[0], self.table.db._field_SQL(self.table.name, field)
    self.table.db.execute("alter table `%s` add column %s;" % (self.table.name, field))
    self.table.db.execute(index, commit=True)
    self.table._update()

</t>
<t tx="karstenw.20230303123103.108">def extend(self, fields):
    [self.append(f) for f in fields]

</t>
<t tx="karstenw.20230303123103.109">def __setitem__(self, *args, **kwargs):
    raise NotImplementedError("Table.fields only supports append()")
insert = remove = pop = __setitem__

</t>
<t tx="karstenw.20230303123103.11">@property
def week(self):
    return self.isocalendar()[1]

</t>
<t tx="karstenw.20230303123103.110">def __init__(self, name, database):
    """ A collection of rows consisting of one or more fields (i.e., table columns)
        of a certain type (i.e., strings, numbers).
    """
    self.database = database
    self._name = name
    self.fields = [] # List of field names (i.e., column names).
    self.schema = {} # Dictionary of (field, Schema)-items.
    self.default = {} # Default values for Table.insert().
    self.primary_key = None
    self._update()

</t>
<t tx="karstenw.20230303123103.111">def _update(self):
    # Retrieve table column names.
    # Table column names are available in the Table.fields list.
    # Table column names should not contain unicode because they can also be function parameters.
    # Table column names should avoid " ", ".", "(" and ")".
    # The primary key column is stored in Table.primary_key.
    self.fields = Table.Fields(self)
    if self.name not in self.database.tables:
        raise TableError("table '%s' does not exist" % (self.database.name + "." + self.name))
    if self.db.type == MYSQL:
        q = "show columns from `%s`;" % self.name
    if self.db.type == SQLITE:
        q = "pragma table_info(`%s`);" % self.name
        i = self.db.execute("pragma index_list(`%s`)" % self.name) # look up indices
        i = dict(((v[1].replace(self.name + "_", "", 1), v[2]) for v in i))
    for f in self.db.execute(q):
        # [name, type, default, index, optional, extra]
        if self.db.type == MYSQL:
            f = [f[0], f[1], f[4], f[3], f[2], f[5]]
        if self.db.type == SQLITE:
            f = [f[1], f[2], f[4], f[5], f[3], ""]
            f[3] = f[3] == 1 and "pri" or (f[0] in i and ("1", "uni")[int(i[f[0]])] or "")
        list.append(self.fields, f[0])
        self.schema[f[0]] = Schema(*f)
        if self.schema[f[0]].index == PRIMARY:
            self.primary_key = f[0]

</t>
<t tx="karstenw.20230303123103.112">def _get_name(self):
    return self._name

</t>
<t tx="karstenw.20230303123103.113">def _set_name(self, name):
    # Rename the table in the database and in any Database.relations.
    # SQLite and MySQL will automatically copy indices on the new table.
    self.db.execute("alter table `%s` rename to `%s`;" % (self._name, name))
    self.db.tables.pop(self._name)
    self.db.tables[name] = self
    for i, r in enumerate(self.db.relations):
        if r[0] == self._name:
            self.db.relations = (name, r[1], r[2], r[3])
        if r[2] == self.name:
            self.db.relations = (r[0], r[1], name, r[3])
    self._name = name

name = property(_get_name, _set_name)

</t>
<t tx="karstenw.20230303123103.114">@property
def db(self):
    return self.database

</t>
<t tx="karstenw.20230303123103.115">@property
def pk(self):
    return self.primary_key

</t>
<t tx="karstenw.20230303123103.116">def count(self):
    """ Yields the number of rows in the table.
    """
    return int(list(self.db.execute("select count(*) from `%s`;" % self.name))[0][0])

</t>
<t tx="karstenw.20230303123103.117">def __len__(self):
    return self.count()

</t>
<t tx="karstenw.20230303123103.118">def __iter__(self):
    return self.iterrows()

</t>
<t tx="karstenw.20230303123103.119">def __getitem__(self, id):
    return self.filter(ALL, id=id)

</t>
<t tx="karstenw.20230303123103.12">@property
def weekday(self):
    return self.isocalendar()[2]

</t>
<t tx="karstenw.20230303123103.120">def __setitem__(self, id, row):
    self.delete(id)
    self.update(self.insert(row), {"id": id})

</t>
<t tx="karstenw.20230303123103.121">def __delitem__(self, id):
    self.delete(id)

</t>
<t tx="karstenw.20230303123103.122">def abs(self, field):
    """ Returns the absolute field name (e.g., "name" =&gt; ""persons.name").
    """
    return abs(self.name, field)

</t>
<t tx="karstenw.20230303123103.123">def iterrows(self):
    """ Returns an iterator over the rows in the table.
    """
    return self.db.execute("select * from `%s`;" % self.name)

</t>
<t tx="karstenw.20230303123103.124">def rows(self):
    """ Returns a list of all the rows in the table.
    """
    return list(self.iterrows())

</t>
<t tx="karstenw.20230303123103.125">def record(self, row):
    """ Returns the given row as a dictionary of (field or alias, value)-items.
    """
    return dict(list(zip(self.fields, row)))

</t>
<t tx="karstenw.20230303123103.126">class Rows(list):
    """ A list of results from Table.filter() with a Rows.table property.
        (i.e., like Query.table returned from Table.search()).
    """

    @others
</t>
<t tx="karstenw.20230303123103.127">def __init__(self, table, data):
    list.__init__(self, data)
    self.table = table

</t>
<t tx="karstenw.20230303123103.128">def record(self, row):
    return self.table.record(row) # See assoc().

</t>
<t tx="karstenw.20230303123103.129">def filter(self, *args, **kwargs):
    """ Returns the rows that match the given constraints (using equals + AND):
    """
    # Table.filter(("name","age"), id=1)
    # Table.filter(ALL, type=("cat","dog")) =&gt; "cat" OR "dog"
    # Table.filter(ALL, type="cat", name="Taxi") =&gt; "cat" AND "Taxi"
    # Table.filter({"type":"cat", "name":"Taxi"})
    if len(args) == 0:
        # No parameters: default to ALL fields.
        fields = ALL
    elif len(args) == 1 and not isinstance(args[0], dict):
        # One parameter: field / list of fields + optional keyword filters.
        fields = args[0]
    elif len(args) == 1:
        # One parameter: dict of filters
        fields, kwargs = ALL, args[0]
    elif len(args) &gt;= 2:
        # Two parameters: field(s) and dict of filters.
        fields, kwargs = args[0], args[1]
    fields = isinstance(fields, (list, tuple)) and ", ".join(fields) or fields or ALL
    q = " and ".join(cmp(k, v, "=", self.db.escape) for k, v in kwargs.items())
    q = q and " where %s" % q or ""
    q = "select %s from `%s`%s;" % (fields, self.name, q)
    return self.Rows(self, self.db.execute(q))

</t>
<t tx="karstenw.20230303123103.13">@property
def timestamp(self):

    # In Python 3, years before 1900 are accepted while mktime()
    # raises ValueError in Python 2. Let's stick to this.
    if self.timetuple().tm_year &lt; 1900:
        raise ValueError("year out of range")

    return int(mktime(self.timetuple())) # Seconds elapsed since 1/1/1970.

</t>
<t tx="karstenw.20230303123103.130">def find(self, *args, **kwargs):
    return self.filter(*args, **kwargs)

</t>
<t tx="karstenw.20230303123103.131">def search(self, *args, **kwargs):
    """ Returns a Query object that can be used to construct complex table queries.
    """
    return Query(self, *args, **kwargs)

query = search

</t>
<t tx="karstenw.20230303123103.132">def _insert_id(self):
    # Retrieves the primary key value of the last inserted row.
    if self.db.type == MYSQL:
        return list(self.db.execute("select last_insert_id();"))[0][0] or None
    if self.db.type == SQLITE:
        return list(self.db.execute("select last_insert_rowid();"))[0][0] or None

</t>
<t tx="karstenw.20230303123103.133">def insert(self, *args, **kwargs):
    """ Inserts a new row from the given field parameters, returns id.
    """
    # Table.insert(name="Taxi", age=2, type="cat")
    # Table.insert({"name":"Fricassée", "age":2, "type":"cat"})
    commit = kwargs.pop("commit", True) # As fieldname, use abs(Table.name, "commit").
    if len(args) == 0 and len(kwargs) == 1 and isinstance(kwargs.get("values"), dict):
        kwargs = kwargs["values"]
    elif len(args) == 1 and isinstance(args[0], dict):
        kwargs = dict(args[0], **kwargs)
    elif len(args) == 1 and isinstance(args[0], (list, tuple)):
        kwargs = dict(list(zip((f for f in self.fields if f != self.pk), args[0])))
    if len(self.default) &gt; 0:
        kwargs.update(self.default)
    k = ", ".join("`%s`" % k for k in kwargs.keys())
    v = ", ".join(self.db.escape(v) for v in kwargs.values())
    q = "insert into `%s` (%s) values (%s);" % (self.name, k, v)
    self.db.execute(q, commit)
    return self._insert_id()

</t>
<t tx="karstenw.20230303123103.134">def update(self, id, *args, **kwargs):
    """ Updates the row with the given id.
    """
    # Table.update(1, age=3)
    # Table.update(1, {"age":3})
    # Table.update(all(filter(field="name", value="Taxi")), age=3)
    commit = kwargs.pop("commit", True) # As fieldname, use abs(Table.name, "commit").
    if isinstance(id, (list, tuple)):
        id = FilterChain(*id)
    if len(args) == 0 and len(kwargs) == 1 and isinstance(kwargs.get("values"), dict):
        kwargs = kwargs["values"]
    if len(args) == 1 and isinstance(args[0], dict):
        a = args[0]
        a.update(kwargs)
        kwargs = a
    kv = ", ".join("`%s`=%s" % (k, self.db.escape(v)) for k, v in kwargs.items())
    q = "update `%s` set %s where %s;" % (self.name, kv,
        not isinstance(id, (Filter, FilterChain)) and cmp(self.primary_key, id, "=", self.db.escape) \
         or id.SQL(escape=self.db.escape))
    self.db.execute(q, commit)

</t>
<t tx="karstenw.20230303123103.135">def delete(self, id, commit=True):
    """ Removes the row which primary key equals the given id.
    """
    # Table.delete(1)
    # Table.delete(ALL)
    # Table.delete(all(("type","cat"), ("age",15,"&gt;")))
    if isinstance(id, (list, tuple)):
        id = FilterChain(*id)
    q = "delete from `%s` where %s" % (self.name,
        not isinstance(id, (Filter, FilterChain)) and cmp(self.primary_key, id, "=", self.db.escape) \
         or id.SQL(escape=self.db.escape))
    self.db.execute(q, commit)

append, edit, remove = insert, update, delete

</t>
<t tx="karstenw.20230303123103.136">@property
def xml(self):
    return xml(self)

</t>
<t tx="karstenw.20230303123103.137">def datasheet(self):
    return Datasheet(rows=self.rows(), fields=[(f, self.schema[f].type) for f in self.fields])

</t>
<t tx="karstenw.20230303123103.138">def __repr__(self):
    return "Table(name=%s, count=%s, database=%s)" % (
        repr(self.name),
        repr(self.count()),
        repr(self.db.name))

</t>
<t tx="karstenw.20230303123103.139">def abs(table, field):
    """ For a given &lt;fieldname&gt;, returns the absolute &lt;tablename&gt;.&lt;fieldname&gt;.
        This is useful when constructing queries with relations to other tables.
    """
    def _format(s):
        if "." not in s:
            # Field could be wrapped in a function: year(date) =&gt; year(table.date).
            p = s.endswith(")") and re.match(r"^(" + sql_functions + r")\(", s, re.I) or None
            i = p and len(p.group(0)) or 0
            return "%s%s.%s" % (s[:i], table, s[i:])
        return s
    if isinstance(field, (list, tuple)):
        return [_format(f) for f in field]
    return _format(field)


</t>
<t tx="karstenw.20230303123103.14">def strftime(self, format):
    return _strftime1900(self, format)

</t>
<t tx="karstenw.20230303123103.140">def cmp(field, value, comparison="=", escape=lambda v: _escape(v), table=""):
    """ Returns an SQL WHERE comparison string using =, i=, !=, &gt;, &lt;, &gt;=, &lt;= or BETWEEN.
        Strings may contain wildcards (*) at the start or at the end.
        A list or tuple of values can be given when using =, != or BETWEEN.
    """
    # Use absolute field names if table name is given:
    if table:
        field = abs(table, field)
    # cmp("name", "Mar*") =&gt; "name like 'Mar%'".
    if isinstance(value, str) and (value.startswith(("*", "%")) or value.endswith(("*", "%"))):
        if comparison in ("=", "i=", "==", LIKE):
            return "%s like %s" % (field, escape(value.replace("*", "%")))
        if comparison in ("!=", "&lt;&gt;"):
            return "%s not like %s" % (field, escape(value.replace("*", "%")))
    # cmp("name", "markov") =&gt; "name" like 'markov'" (case-insensitive).
    if isinstance(value, str):
        if comparison == "i=":
            return "%s like %s" % (field, escape(value))
    # cmp("type", ("cat", "dog"), "!=") =&gt; "type not in ('cat','dog')".
    # cmp("amount", (10, 100), ":") =&gt; "amount between 10 and 100".
    if isinstance(value, (list, tuple)):
        if find(lambda v: isinstance(v, str) and (v.startswith("*") or v.endswith("*")), value):
            return "(%s)" % any(*[(field, v) for v in value]).sql(escape=escape)
        if comparison in ("=", "==", IN):
            return "%s in (%s)" % (field, ",".join(escape(v) for v in value))
        if comparison in ("!=", "&lt;&gt;"):
            return "%s not in (%s)" % (field, ",".join(escape(v) for v in value))
        if comparison in (":", BETWEEN):
            return "%s between %s and %s" % (field, escape(value[0]), escape(value[1]))
    # cmp("type", None, "!=") =&gt; "type is not null".
    if isinstance(value, type(None)):
        if comparison in ("=", "=="):
            return "%s is null" % field
        if comparison in ("!=", "&lt;&gt;"):
            return "%s is not null" % field
    # Using a subquery:
    if isinstance(value, Query):
        if comparison in ("=", "==", IN):
            return "%s in %s" % (field, escape(value))
        if comparison in ("!=", "&lt;&gt;"):
            return "%s not in %s" % (field, escape(value))
    return "%s%s%s" % (field, comparison, escape(value))

# Functions for date fields: cmp(year("date"), 1999, "&gt;").


</t>
<t tx="karstenw.20230303123103.141">def year(date):
    return "year(%s)" % date


</t>
<t tx="karstenw.20230303123103.142">def month(date):
    return "month(%s)" % date


</t>
<t tx="karstenw.20230303123103.143">def day(date):
    return "day(%s)" % date


</t>
<t tx="karstenw.20230303123103.144">def hour(date):
    return "hour(%s)" % date


</t>
<t tx="karstenw.20230303123103.145">def minute(date):
    return "minute(%s)" % date


</t>
<t tx="karstenw.20230303123103.146">def second(date):
    return "second(%s)" % date

# Aggregate functions.


</t>
<t tx="karstenw.20230303123103.147">def count(value):
    return "count(%s)" % value


</t>
<t tx="karstenw.20230303123103.148">def sum(value):
    return "sum(%s)" % value

#--- QUERY FILTER ----------------------------------------------------------------------------------

AND, OR = "and", "or"


</t>
<t tx="karstenw.20230303123103.149">class Filter(tuple):
    @others
</t>
<t tx="karstenw.20230303123103.15">def copy(self):
    return date(self.timestamp)

</t>
<t tx="karstenw.20230303123103.150">def __new__(cls, field, value, comparison):
    return tuple.__new__(cls, (field, value, comparison))

</t>
<t tx="karstenw.20230303123103.151">def SQL(self, **kwargs):
    return cmp(*self, **kwargs)


</t>
<t tx="karstenw.20230303123103.152">def filter(field, value, comparison="="):
    return Filter(field, value, comparison)


</t>
<t tx="karstenw.20230303123103.153">def eq(field, value):
    return Filter(field, value, "=")


</t>
<t tx="karstenw.20230303123103.154">def eqi(field, value):
    return Filter(field, value, "i=")


</t>
<t tx="karstenw.20230303123103.155">def ne(field, value):
    return Filter(field, value, "!=")


</t>
<t tx="karstenw.20230303123103.156">def gt(field, value):
    return Filter(field, value, "&gt;")


</t>
<t tx="karstenw.20230303123103.157">def lt(field, value):
    return Filter(field, value, "&lt;")


</t>
<t tx="karstenw.20230303123103.158">def gte(field, value):
    return Filter(field, value, "&gt;=")


</t>
<t tx="karstenw.20230303123103.159">def lte(field, value):
    return Filter(field, value, "&lt;=")


</t>
<t tx="karstenw.20230303123103.16">def __str__(self):
    return self.strftime(self.format)

</t>
<t tx="karstenw.20230303123103.160">def rng(field, value):
    return Filter(field, value, ":")


</t>
<t tx="karstenw.20230303123103.161">class FilterChain(list):

    @others
</t>
<t tx="karstenw.20230303123103.162">def __init__(self, *args, **kwargs):
    """ A list of SQL WHERE filters combined with AND/OR logical operator.
    """
    # FilterChain(filter("type", "cat", "="), filter("age", 5, "="), operator=AND)
    # FilterChain(type="cat", age=5, operator=AND)
    # FilterChain({"type": "cat", "age": 5}, operator=AND)
    if len(args) == 1 and isinstance(args[0], dict):
        args[0].pop("operator", None)
        kwargs = dict(args[0], **kwargs)
        args = []
    else:
        args = list(args)
    self.operator = kwargs.pop("operator", AND)
    args.extend(list(filter(k, v, "=")) for k, v in kwargs.items())
    list.__init__(self, args)

</t>
<t tx="karstenw.20230303123103.163">def SQL(self, **kwargs):
    """ For example, filter for small pets with tails or wings
        (which is not the same as small pets with tails or pets with wings):
        &gt;&gt;&gt; FilterChain(
        &gt;&gt;&gt;     filter("type", "pet"),
        &gt;&gt;&gt;     filter("weight", (4,6), ":"),
        &gt;&gt;&gt;     FilterChain(
        &gt;&gt;&gt;         filter("tail", True),
        &gt;&gt;&gt;         filter("wing", True), operator=OR))
        Yields:
        "type='pet' and weight between 4 and 6 and (tail=1 or wing=1)"
    """
    # Remember to pass the right escape() function as optional parameter.
    a = []
    for filter in self:
        # Traverse subgroups recursively.
        if isinstance(filter, FilterChain):
            a.append("(%s)" % filter.SQL(**kwargs))
            continue
        # Convert filter() to string with cmp() - see above.
        if isinstance(filter, (Filter, list, tuple)):
            a.append(cmp(*filter, **kwargs))
            continue
        raise TypeError("FilterChain can contain other FilterChain or filter(), not %s" % type(filter))
    return (" %s " % self.operator).join(a)

sql = SQL


</t>
<t tx="karstenw.20230303123103.164">def all(*args, **kwargs):
    """ Returns a group of filters combined with AND.
    """
    kwargs["operator"] = AND
    return FilterChain(*args, **kwargs)


</t>
<t tx="karstenw.20230303123103.165">def any(*args, **kwargs):
    """ Returns a group of filters combined with OR.
    """
    kwargs["operator"] = OR
    return FilterChain(*args, **kwargs)

# From a GET-query dict:
# all(*dict.items())

# filter() value can also be a Query with comparison=IN.

#--- QUERY -----------------------------------------------------------------------------------------

# Relations:
INNER = "inner" # The rows for which there is a match in both tables (same as join=None).
LEFT = "left"  # All rows from this table, with field values from the related table when possible.
RIGHT = "right" # All rows from the related table, with field values from this table when possible.
FULL = "full"  # All rows form both tables.


</t>
<t tx="karstenw.20230303123103.166">class Relation(tuple):
    @others
</t>
<t tx="karstenw.20230303123103.167">def __new__(cls, field1, field2, table, join):
    return tuple.__new__(cls, (field1, field2, table, join))


</t>
<t tx="karstenw.20230303123103.168">def relation(field1, field2, table, join=LEFT):
    return Relation(field1, field2, table, join)

rel = relation

# Sorting:
ASCENDING = "asc"
DESCENDING = "desc"

# Grouping:
FIRST, LAST, COUNT, MAX, MIN, SUM, AVG, STDEV, CONCATENATE = \
    "first", "last", "count", "max", "min", "sum", "avg", "stdev", "group_concat"


</t>
<t tx="karstenw.20230303123103.169">class Query(object):

    id, cache = 0, {}

    @others
</t>
<t tx="karstenw.20230303123103.17">def __repr__(self):
    return "Date(%s)" % repr(self.__str__())

</t>
<t tx="karstenw.20230303123103.170">def __init__(self, table, fields=ALL, filters=[], relations=[], sort=None, order=ASCENDING, group=None, function=FIRST, range=None):
    """ A selection of rows from the given table, filtered by any() and all() constraints.
    """
    # Table.search(ALL, filters=any(("type","cat"), ("type","dog")) =&gt; cats and dogs.
    # Table.search(("type", "name")), group="type", function=COUNT) =&gt; all types + amount per type.
    # Table.search(("name", "types.has_tail"), relations=[("types","type","id")]) =&gt; links type to types.id.
    if isinstance(filters, Filter):
        filters = [filters]
    if isinstance(relations, Relation):
        relations = [relations]
    Query.id += 1
    filters = FilterChain(*filters, **dict(operator=getattr(filters, "operator", AND)))
    self._id       = Query.id
    self._table    = table
    self.fields    = fields    # A field name, list of field names or ALL.
    self.aliases   = {}        # A dictionary of field name aliases, used with Query.xml or Query-in-Query.
    self.filters   = filters   # A group of filter() objects.
    self.relations = relations # A list of rel() objects.
    self.sort      = sort      # A field name, list of field names or field index for sorting.
    self.order     = order     # ASCENDING or DESCENDING.
    self.group     = group     # A field name, list of field names or field index for folding.
    self.function  = function  # FIRST, LAST, COUNT, MAX, MIN, SUM, AVG, STDEV or CONCATENATE (or list).
    self.range     = range     # A (index1, index2)-tuple. The first row in the table is 0.

</t>
<t tx="karstenw.20230303123103.171">@property
def table(self):
    return self._table

</t>
<t tx="karstenw.20230303123103.172">def __len__(self):
    return len(list(self.rows()))

</t>
<t tx="karstenw.20230303123103.173">def __iter__(self):
    return self.execute()

</t>
<t tx="karstenw.20230303123103.174">def __getitem__(self, i):
    return self.rows()[i] # poor performance

</t>
<t tx="karstenw.20230303123103.175">def SQL(self):
    """ Yields the SQL syntax of the query, which can be passed to Database.execute().
        The SQL string will be cached for faster reuse.
    """
    #if self._id in Query.cache:
    #    return Query.cache[self._id]
    # Construct the SELECT clause from Query.fields.
    g = not isinstance(self.group, (list, tuple)) and [self.group] or self.group
    g = [abs(self._table.name, f) for f in g if f is not None]
    fields = not isinstance(self.fields, (list, tuple)) and [self.fields] or self.fields
    fields = [f in self.aliases and "%s as %s" % (f, self.aliases[f]) or f for f in fields]
    fields = abs(self._table.name, fields)
    # With a GROUPY BY clause, fields not used for grouping are wrapped in the given function.
    # The function can also be a list of functions for each field (FIRST by default).
    if g and isinstance(self.function, str):
        fields = [f in g and f or "%s(%s)" % (self.function, f) for f in fields]
    if g and isinstance(self.function, (list, tuple)):
        fields = [f in g and f or "%s(%s)" % (F, f) for F, f in zip(self.function + [FIRST] * len(fields), fields)]
    q = []
    q.append("select %s" % ", ".join(fields))
    # Construct the FROM clause from Query.relations.
    # Table relations defined on the database are taken into account,
    # but overridden by relations defined on the query.
    q.append("from `%s`" % self._table.name)
    relations = {}
    for key1, key2, table, join in (rel(*r) for r in self.relations):
        table = isinstance(table, Table) and table.name or table
        relations[table] = (key1, key2, join)
    for table1, key1, table2, key2, join in self._table.db.relations:
        if table1 == self._table.name:
            relations.setdefault(table2, (key1, key2, join))
        if table2 == self._table.name:
            relations.setdefault(table1, (key1, key2, join == LEFT and RIGHT or (join == RIGHT and LEFT or join)))
    # Define relations only for tables whose fields are actually selected.
    for (table, (key1, key2, join)) in relations.items():
        for f in fields:
            if table + "." in f:
                q.append("%sjoin `%s`" % (join and join + " " or "", table))
                q.append("on %s=%s" % (abs(self._table.name, key1), abs(self._table.db[table].name, key2)))
                break
    # Construct the WHERE clause from Query.filters.SQL().
    # Use the database's escape function and absolute field names.
    if len(self.filters) &gt; 0:
        q.append("where %s" % self.filters.SQL(escape=self._table.db.escape, table=self._table.name))
    # Construct the ORDER BY clause from Query.sort and Query.order.
    # Construct the GROUP BY clause from Query.group.
    for clause, value in (("order", self.sort), ("group", self.group)):
        if isinstance(value, str) and value != "":
            q.append("%s by %s" % (clause, abs(self._table.name, value)))
        elif isinstance(value, (list, tuple)) and len(value) &gt; 0:
            q.append("%s by %s" % (clause, ", ".join(abs(self._table.name, value))))
        elif isinstance(value, int):
            q.append("%s by %s" % (clause, abs(self._table.name, self._table.fields[value])))
        if self.sort and clause == "order":
            if self.order in (ASCENDING, DESCENDING):
                q.append("%s" % self.order)
            elif isinstance(self.order, (list, tuple)):
                q[-1] = ",".join(" ".join(v) for v in zip(q[-1].split(","), self.order))
    # Construct the LIMIT clause from Query.range.
    if self.range:
        q.append("limit %s, %s" % (str(self.range[0]), str(self.range[1])))
    q = " ".join(q) + ";"
    # Cache the SQL-string for faster retrieval.
    #if len(Query.cache) &gt; 100:
    #    Query.cache.clear()
    #Query.cache[self._id] = q # XXX cache is not updated when properties change.
    return q

sql = SQL

</t>
<t tx="karstenw.20230303123103.176">def execute(self):
    """ Executes the query and returns an iterator over the matching rows in the table.
    """
    return self._table.db.execute(self.SQL())

</t>
<t tx="karstenw.20230303123103.177">def iterrows(self):
    """ Executes the query and returns an iterator over the matching rows in the table.
    """
    return self.execute()

</t>
<t tx="karstenw.20230303123103.178">def rows(self):
    """ Executes the query and returns the matching rows from the table.
    """
    return list(self.execute())

</t>
<t tx="karstenw.20230303123103.179">def record(self, row):
    """ Returns the given row as a dictionary of (field or alias, value)-items.
    """
    return dict(list(zip((self.aliases.get(f, f) for f in self.fields), row)))

</t>
<t tx="karstenw.20230303123103.18">def __iadd__(self, t):
    return self.__add__(t)

</t>
<t tx="karstenw.20230303123103.180">@property
def xml(self):
    return xml(self)

</t>
<t tx="karstenw.20230303123103.181">def __repr__(self):
    return "Query(sql=%s)" % repr(self.SQL())


</t>
<t tx="karstenw.20230303123103.182">def associative(query):
    """ Yields query rows as dictionaries of (field, value)-items.
    """
    for row in query:
        yield query.record(row)

assoc = associative

#### VIEW ##########################################################################################
# A representation of data based on a table in the database.
# The render() method can be overridden to output data in a certain format (e.g., HTML for a web app).


</t>
<t tx="karstenw.20230303123103.183">class View(object):

    @others
#### XML PARSER ####################################################################################

XML_HEADER = "&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;"


</t>
<t tx="karstenw.20230303123103.184">def __init__(self, database, table, schema=[]):
    """ A representation of data.
        View.render() should be overridden in a subclass.
    """
    self.database = database
    self._table = isinstance(table, Table) and table.name or table
    self.schema = schema # A list of table fields - see field().

</t>
<t tx="karstenw.20230303123103.185">@property
def db(self):
    return self.database

</t>
<t tx="karstenw.20230303123103.186">@property
def table(self):
    # If it doesn't exist, create the table from View.schema.
    if self._table not in self.db:
        self.setup()
    return self.db[self._table]

</t>
<t tx="karstenw.20230303123103.187">def setup(self, overwrite=False):
    """ Creates the database table from View.schema, optionally overwriting the old table.
    """
    if overwrite:
        self.db.drop(self._table)
    if self._table not in self.db:
        self.db.create(self._table, self.schema)

</t>
<t tx="karstenw.20230303123103.188">def render(self, *path, **query):
    """ This method should be overwritten to return formatted table output (XML, HTML, RSS, ...)
        For web apps, the given path should list all parts in the relative URL path,
        and query is a dictionary of all POST and GET variables sent from the client.
        For example: http://books.com/science/new
        =&gt; ["science", "new"]
        =&gt; render() data from db.books.filter(ALL, category="science", new=True).
    """
    pass

# CherryPy-specific.
</t>
<t tx="karstenw.20230303123103.189">def default(self, *path, **query):
    return self.render(*path, **query)
default.exposed = True

</t>
<t tx="karstenw.20230303123103.19">def __isub__(self, t):
    return self.__sub__(t)

</t>
<t tx="karstenw.20230303123103.190">def _unpack_fields(table, fields=[]):
    """ Replaces "*" with the actual field names.
        Fields from related tables keep the "&lt;tablename&gt;." prefix.
    """
    u = []
    for f in fields:
        a, b = "." in f and f.split(".", 1) or (table.name, f)
        if a == table.name and b == ALL:
            # &lt;table&gt;.*
            u.extend(f for f in table.db.tables[a].fields)
        elif a != table.name and b == ALL:
            # &lt;related-table&gt;.*
            u.extend("%s.%s" % (a, f) for f in table.db.tables[a].fields)
        elif a != table.name:
            # &lt;related-table&gt;.&lt;field&gt;
            u.append("%s.%s" % (a, b))
        else:
            # &lt;field&gt;
            u.append(b)
    return u


</t>
<t tx="karstenw.20230303123103.191">def xml_format(a):
    """ Returns the given attribute (string, int, float, bool, None) as a quoted unicode string.
    """
    if isinstance(a, str):
        return "\"%s\"" % encode_entities(a)
    if isinstance(a, bool):
        return "\"%s\"" % ("no", "yes")[int(a)]
    if isinstance(a, int):
        return "\"%s\"" % a
    if isinstance(a, float):
        return "\"%s\"" % round(a, 5)
    if isinstance(a, type(None)):
        return "\"\""
    if isinstance(a, Date):
        return "\"%s\"" % str(a)
    if isinstance(a, datetime):
        return "\"%s\"" % str(date(mktime(a.timetuple())))


</t>
<t tx="karstenw.20230303123103.192">def xml(rows):
    """ Returns the rows in the given Table or Query as an XML-string, for example:
        &lt;?xml version="1.0" encoding="utf-8"?&gt;
        &lt;table name="pets", fields="id, name, type" count="2"&gt;
            &lt;schema&gt;
                &lt;field name="id", type="integer", index="primary", optional="no" /&gt;
                &lt;field name="name", type="string", length="50" /&gt;
                &lt;field name="type", type="string", length="50" /&gt;
            &lt;/schema&gt;
            &lt;rows&gt;
                &lt;row id="1", name="Taxi", type="cat" /&gt;
                &lt;row id="2", name="Hofstadter", type="dog" /&gt;
            &lt;/rows&gt;
        &lt;/table&gt;
    """
    if isinstance(rows, Table):
        root, table, rows, fields, aliases = "table", rows, rows.rows(), rows.fields, {}
    if isinstance(rows, Query):
        root, table, rows, fields, aliases, = "query", rows.table, rows.rows(), rows.fields, rows.aliases
    fields = _unpack_fields(table, fields)
    # &lt;table name="" fields="" count=""&gt;
    # &lt;query table="" fields="" count=""&gt;
    xml = []
    xml.append(XML_HEADER)
    xml.append("&lt;%s %s=%s fields=\"%s\" count=\"%s\"&gt;" % (
        root,
        root != "table" and "table" or "name",
        xml_format(table.name), # Use Query.aliases as field names.
        ", ".join(encode_entities(aliases.get(f, f)) for f in fields),
        len(rows)))
    # &lt;schema&gt;
    # Field information is retrieved from the (related) table schema.
    # If the XML is imported as a Table, the related fields become part of it.
    xml.append("\t&lt;schema&gt;")
    for f in fields:
        if f not in table.schema:
            s = f.split(".")
            s = table.db[s[0]].schema[s[-1]]
        else:
            s = table.schema[f]
        # &lt;field name="" type="" length="" default="" index="" optional="" extra="" /&gt;
        xml.append("\t\t&lt;field name=%s type=%s%s%s%s%s%s /&gt;" % (
            xml_format(aliases.get(f, f)),
            xml_format(s.type),
            s.length is not None and " length=%s" % xml_format(s.length) or "",
            s.default is not None and " default=%s" % xml_format(s.default) or "",
            s.index is not False and " index=%s" % xml_format(s.index) or "",
            s.optional is not True and " optional=%s" % xml_format(s.optional) or "",
            s.extra is not None and " extra=%s" % xml_format(s.extra) or ""))
    xml.append("\t&lt;/schema&gt;")
    xml.append("\t&lt;rows&gt;")
    # &lt;rows&gt;
    for r in rows:
        # &lt;row field="value" /&gt;
        xml.append("\t\t&lt;row %s /&gt;" % " ".join("%s=%s" % (aliases.get(k, k), xml_format(v)) for k, v in zip(fields, r)))
    xml.append("\t&lt;/rows&gt;")
    xml.append("&lt;/%s&gt;" % root)
    xml = "\n".join(xml)
    return xml


</t>
<t tx="karstenw.20230303123103.193">def parse_xml(database, xml, table=None, field=lambda s: s.replace(".", "-")):
    """ Creates a new table in the given database from the given XML-string.
        The XML must be in the format generated by Table.xml.
        If the table already exists, raises a TableError.
        The given table parameter can be used to rename the table.
        The given field function can be used to rename field names.
    """
    def _attr(node, attribute, default=""):
        return node.getAttribute(attribute) or default
    # parseString() will decode entities, no need for decode_entities().
    from xml.dom.minidom import parseString
    dom = parseString(encode_utf8(xml))
    a = dom.getElementsByTagName("table")
    b = dom.getElementsByTagName("query")
    if len(a) &gt; 0:
        table = table or _attr(a[0], "name", "")
    if len(b) &gt; 0:
        table = table or _attr(b[0], "table", "")
    # Parse field information (i.e., field name, field type, etc.)
    fields, schema, rows = [], [], []
    for f in dom.getElementsByTagName("field"):
        fields.append(_attr(f, "name"))
        schema.append(_field(
            name = field(_attr(f, "name")),
            type = _attr(f, "type") == STRING and STRING(int(_attr(f, "length", 255))) or _attr(f, "type"),
         default = _attr(f, "default", None),
           index = _attr(f, "index", False),
        optional = _attr(f, "optional", True) != "no"
        ))
        # Integer primary key is always auto-increment.
        # The id's in the new table will differ from those in the XML.
        if _attr(f, "index") == PRIMARY and _attr(f, "type") == INTEGER:
            fields.pop()
    # Parse row data.
    for r in dom.getElementsByTagName("row"):
        rows.append({})
        for i, f in enumerate(fields):
            v = _attr(r, f, None)
            if schema[i][1] == BOOLEAN:
                rows[-1][f] = (0, 1)[v != "no"]
            else:
                rows[-1][f] = v
    # Create table if not exists and insert rows.
    if database.connected is False:
        database.connect()
    if table in database:
        raise TableError("table '%s' already exists" % table)
    database.create(table, fields=schema)
    for r in rows:
        database[table].insert(r, commit=False)
    database.commit()
    return database[table]

#db = Database("test")
#db.create("persons", (pk(), field("data", TEXT)))
#db.persons.append((json.dumps({"name": u"Schrödinger", "type": "cat"}),))
#
#for id, data in db.persons:
#    print(id, json.loads(data))

#### DATASHEET #####################################################################################

#--- CSV -------------------------------------------------------------------------------------------

# Raise the default field size limit:
if sys.platform == 'win32':
    csvlib.field_size_limit(min(sys.maxsize, 2147483647))
else:
    csvlib.field_size_limit(sys.maxsize)


</t>
<t tx="karstenw.20230303123103.194">def csv_header_encode(field, type=STRING):
    # csv_header_encode("age", INTEGER) =&gt; "age (INTEGER)".
    t = re.sub(r"^varchar\(.*?\)", "string", (type or ""))
    t = t and " (%s)" % t or ""
    s = "%s%s" % (field or "", t.upper())
    return s


</t>
<t tx="karstenw.20230303123103.195">def csv_header_decode(s):
    # csv_header_decode("age (INTEGER)") =&gt; ("age", INTEGER).
    p = r"STRING|INTEGER|FLOAT|TEXT|BLOB|BOOLEAN|DATE|"
    p = re.match(r"(.*?) \((" + p + ")\)", s)
    s = s.endswith(" ()") and s[:-3] or s
    return p and (string(p.group(1), default=None), p.group(2).lower()) or (string(s) or None, None)


</t>
<t tx="karstenw.20230303123103.196">class CSV(list):

    @others
</t>
<t tx="karstenw.20230303123103.197">def __new__(cls, rows=[], fields=None, **kwargs):
    """ A list of lists that can be imported and exported as a comma-separated text file (CSV).
    """
    if isinstance(rows, str) and os.path.exists(rows):
        csv = cls.load(rows, **kwargs)
    else:
        csv = list.__new__(cls)
    return csv

</t>
<t tx="karstenw.20230303123103.198">def __init__(self, rows=[], fields=None, **kwargs):
    # List of (name, type)-tuples (STRING, INTEGER, FLOAT, DATE, BOOLEAN).
    fields = fields or kwargs.pop("headers", None)
    fields = fields and [tuple(f) if isinstance(f, (tuple, list)) else (f, None) for f in fields] or None
    self.__dict__["fields"] = fields
    if hasattr(rows, "__iter__"):
        self.extend(rows, **kwargs)

</t>
<t tx="karstenw.20230303123103.199">def extend(self, rows, **kwargs):
    list.extend(self, rows)

</t>
<t tx="karstenw.20230303123103.2">def _import_db(engine=SQLITE):
    """ Lazy import called from Database() or Database.new().
        Depending on the type of database we either import MySQLdb or SQLite.
        Note: 64-bit Python needs 64-bit MySQL, 32-bit the 32-bit version.
    """
    global MySQLdb
    global sqlite
    if engine == MYSQL:
        import MySQLdb
        warnings.simplefilter("ignore", MySQLdb.Warning)
    if engine == SQLITE:
        import sqlite3.dbapi2 as sqlite


</t>
<t tx="karstenw.20230303123103.20">def __add__(self, t):
    d = self
    if getattr(t, "years", 0) \
    or getattr(t, "months", 0):
        # January 31 + 1 month = February 28.
        y = (d.month + t.months - 1) // 12 + d.year + t.years
        m = (d.month + t.months + 0) % 12 or 12
        r = monthrange(y, m)
        d = date(y, m, min(d.day, r[1]), d.hour, d.minute, d.second, d.microsecond)
    d = datetime.__add__(d, t)
    return date(d.year, d.month, d.day, d.hour, d.minute, d.second, d.microsecond, self.format)

</t>
<t tx="karstenw.20230303123103.200">def _set_headers(self, v):
    self.__dict__["fields"] = v

</t>
<t tx="karstenw.20230303123103.201">def _get_headers(self):
    return self.__dict__["fields"]

headers = property(_get_headers, _set_headers)

</t>
<t tx="karstenw.20230303123103.202">def save(self, path, separator=",", encoder=lambda v: v, headers=False, password=None, **kwargs):
    """ Exports the table to a unicode text file at the given path.
        Rows in the file are separated with a newline.
        Columns in a row are separated with the given separator (by default, comma).
        For data types other than string, int, float, bool or None, a custom string encoder can be given.
    """
    # Optional parameters include all arguments for csv.writer(), see:
    # http://docs.python.org/library/csv.html#csv.writer
    kwargs.setdefault("delimiter", separator )
    kwargs.setdefault("quoting", csvlib.QUOTE_ALL)
    # csv.writer will handle str, int, float and bool:
    s = StringIO()
    w = csvlib.writer(s, **kwargs)
    if headers and self.fields is not None:
        w.writerows([[csv_header_encode(name, type) for name, type in self.fields]])
    w.writerows([[encoder(v) for v in row] for row in self])
    s = s.getvalue()
    s = s.strip()
    s = re.sub("([^\"]|^)\"None\"", "\\1None", s)
    s = s if not password else encrypt_string(s, password)
    f = open(path, "w", encoding="utf-8")
    f.write(BOM_UTF8)
    f.write(s)
    f.close()

</t>
<t tx="karstenw.20230303123103.203">@classmethod
def load(cls, path, separator=",", decoder=lambda v: v, headers=False, preprocess=None, password=None, encoding="utf-8", **kwargs):
    """ Returns a table from the data in the given text file.
        Rows are expected to be separated by a newline.
        Columns are expected to be separated by the given separator (by default, comma).
        Strings will be converted to int, float, bool, date or None if headers are parsed.
        For other data types, a custom string decoder can be given.
        A preprocess(str) function can be given to change the file content before parsing.
    """
    # Date objects are saved and loaded as strings, but it is easy to convert these back to dates:
    # - set a DATE field type for the column,
    # - or do Table.columns[x].map(lambda s: date(s))
    data = open(path, "r", encoding=encoding)
    data = data if not password else decrypt_string(data.read(), password)
    data.seek(data.readline().startswith(BOM_UTF8) and 3 or 0)
    data = data if not password else BytesIO(data.replace("\r\n", "\n").replace("\r", "\n"))
    data = data if not preprocess else BytesIO(preprocess(data.read()))
    data = csvlib.reader(data, delimiter=separator)
    i, n = kwargs.get("start"), kwargs.get("count")
    if i is not None and n is not None:
        data = list(islice(data, i, i + n))
    elif i is not None:
        data = list(islice(data, i, None))
    elif n is not None:
        data = list(islice(data, n))
    else:
        data = list(data)
    if headers:
        fields = [csv_header_decode(field) for field in data.pop(0)]
        fields += [(None, None)] * (max([0] + [len(row) for row in data]) - len(fields))
    else:
        fields = []
    if not fields:
        # Cast fields using the given decoder (by default, all strings + None).
        data = [[decoder(decode_utf8(v) if v != "None" else None) for v in row] for row in data]
    else:
        # Cast fields to their defined field type (STRING, INTEGER, ...)
        for i, row in enumerate(data):
            for j, v in enumerate(row):
                type = fields[j][1]
                if row[j] == "None":
                    row[j] = decoder(None)
                elif type is None:
                    row[j] = decoder(decode_utf8(v))
                elif type in (STRING, TEXT):
                    row[j] = decode_utf8(v)
                elif type == INTEGER:
                    row[j] = int(row[j])
                elif type == FLOAT:
                    row[j] = float(row[j])
                elif type == BOOLEAN:
                    row[j] = bool(row[j])
                elif type == DATE:
                    row[j] = date(row[j])
                elif type == BLOB:
                    row[j] = v
                else:
                    row[j] = decoder(decode_utf8(v))
    return cls(rows=data, fields=fields, **kwargs)

</t>
<t tx="karstenw.20230303123103.204">#--- DATASHEET -------------------------------------------------------------------------------------


class Datasheet(CSV):

    @others
</t>
<t tx="karstenw.20230303123103.205">def __init__(self, rows=[], fields=None, **kwargs):
    """ A matrix of rows and columns, where each row and column can be retrieved as a list.
        Values can be any kind of Python object.
    """
    # NumPy array, convert to list of int/float/str/bool.
    if rows.__class__.__name__ == "ndarray":
        rows = rows.tolist()
    self.__dict__["_rows"] = DatasheetRows(self)
    self.__dict__["_columns"] = DatasheetColumns(self)
    self.__dict__["_m"] = 0 # Number of columns per row, see Datasheet.insert().
    list.__init__(self)
    CSV.__init__(self, rows, fields, **kwargs)

</t>
<t tx="karstenw.20230303123103.206">def _get_rows(self):
    return self._rows

</t>
<t tx="karstenw.20230303123103.207">def _set_rows(self, rows):
    # Datasheet.rows property can't be set, except in special case Datasheet.rows += row.
    if isinstance(rows, DatasheetRows) and rows._datasheet == self:
        self._rows = rows
        return
    raise AttributeError("can't set attribute")
rows = property(_get_rows, _set_rows)

</t>
<t tx="karstenw.20230303123103.208">def _get_columns(self):
    return self._columns

</t>
<t tx="karstenw.20230303123103.209">def _set_columns(self, columns):
    # Datasheet.columns property can't be set, except in special case Datasheet.columns += column.
    if isinstance(columns, DatasheetColumns) and columns._datasheet == self:
        self._columns = columns
        return
    raise AttributeError("can't set attribute")
columns = cols = property(_get_columns, _set_columns)

</t>
<t tx="karstenw.20230303123103.21">def __sub__(self, t):
    if isinstance(t, (Date, datetime)):
        # Subtracting two dates returns a Time.
        t = datetime.__sub__(self, t)
        return Time(+t.days, +t.seconds,
            microseconds = +t.microseconds)
    if isinstance(t, (Time, timedelta)):
        return self + Time(-t.days, -t.seconds,
            microseconds = -t.microseconds,
                  months = -getattr(t, "months", 0),
                   years = -getattr(t, "years", 0))


</t>
<t tx="karstenw.20230303123103.210">def __getattr__(self, k):
    """ Columns can be retrieved by field name, e.g., Datasheet.date.
    """
    #print("Datasheet.__getattr__", k)
    if k in self.__dict__:
        return self.__dict__[k]
    for i, f in enumerate(f[0] for f in self.__dict__["fields"] or []):
        if f == k:
            return self.__dict__["_columns"][i]
    raise AttributeError("'Datasheet' object has no attribute '%s'" % k)

</t>
<t tx="karstenw.20230303123103.211">def __setattr__(self, k, v):
    """ Columns can be set by field name, e.g., Datasheet.date = [...].
    """
    #print("Datasheet.__setattr__", k)
    if k in self.__dict__:
        self.__dict__[k] = v
        return
    if k == "rows":
        self._set_rows(v)
        return
    if k == "columns":
        self._set_columns(v)
        return
    if k == "headers":
        self._set_headers(v)
        return
    for i, f in enumerate(f[0] for f in self.__dict__["fields"] or []):
        if f == k:
            self.__dict__["_columns"].__setitem__(i, v)
            return
    raise AttributeError("'Datasheet' object has no attribute '%s'" % k)

</t>
<t tx="karstenw.20230303123103.212">def __setitem__(self, index, value):
    """ Sets an item or row in the matrix.
        For Datasheet[i] = v, sets the row at index i to v.
        For Datasheet[i,j] = v, sets the value in row i and column j to v.
    """
    if isinstance(index, tuple):
        list.__getitem__(self, index[0])[index[1]] = value
    elif isinstance(index, int):
        self.pop(index)
        self.insert(index, value)
    else:
        raise TypeError("Datasheet indices must be int or tuple")

</t>
<t tx="karstenw.20230303123103.213">def __getitem__(self, index):
    """ Returns an item, row or slice from the matrix.
        For Datasheet[i], returns the row at the given index.
        For Datasheet[i,j], returns the value in row i and column j.
    """
    if isinstance(index, int):
        # Datasheet[i] =&gt; row i.
        return list.__getitem__(self, index)
    elif isinstance(index, slice):
        return Datasheet(rows = list.__getitem__(self, index), fields = self.fields)
    elif isinstance(index, tuple):
        i, j = index
        # Datasheet[i,j] =&gt; item from column j in row i.
        # Datasheet[i,j1:j2] =&gt; columns j1-j2 from row i.
        if not isinstance(i, slice):
            return list.__getitem__(self, i)[j]
        # Datasheet[i1:i2,j] =&gt; column j from rows i1-i2.
        if not isinstance(j, slice):
            return [row[j] for row in list.__getitem__(self, i)]
        # Datasheet[i1:i2,j1:j2] =&gt; Datasheet with columns j1-j2 from rows i1-i2.
        return Datasheet(
              rows = (row[j] for row in list.__getitem__(self, i)),
            fields = self.fields and self.fields[j] or self.fields)
    raise TypeError("Datasheet indices must be int, tuple or slice")

# Python 2 (backward compatibility)
__getslice__ = lambda self, i, j: self.__getitem__(slice(i, j))

</t>
<t tx="karstenw.20230303123103.214">def __delitem__(self, index):
    self.pop(index)

# datasheet1 = datasheet2 + datasheet3
# datasheet1 = [[...],[...]] + datasheet2
# datasheet1 += datasheet2
</t>
<t tx="karstenw.20230303123103.215">def __add__(self, datasheet):
    m = self.copy()
    m.extend(datasheet)
    return m

</t>
<t tx="karstenw.20230303123103.216">def __radd__(self, datasheet):
    m = Datasheet(datasheet)
    m.extend(self)
    return m

</t>
<t tx="karstenw.20230303123103.217">def __iadd__(self, datasheet):
    self.extend(datasheet)
    return self

</t>
<t tx="karstenw.20230303123103.218">def insert(self, i, row, default=None, **kwargs):
    """ Inserts the given row into the matrix.
        Missing columns at the end (right) will be filled with the default value.
    """
    try:
        # Copy the row (fast + safe for generators and DatasheetColumns).
        row = [v for v in row]
    except:
        raise TypeError("Datasheet.insert(x): x must be list")
    list.insert(self, i, row)
    m = max((len(self) &gt; 1 and self._m or 0, len(row)))
    if len(row) &lt; m:
        row.extend([default] * (m - len(row)))
    if self._m &lt; m:
        # The given row might have more columns than the rows in the matrix.
        # Performance takes a hit when these rows have to be expanded:
        for row in self:
            if len(row) &lt; m:
                row.extend([default] * (m - len(row)))
    self.__dict__["_m"] = m

</t>
<t tx="karstenw.20230303123103.219">def append(self, row, default=None, _m=None, **kwargs):
    self.insert(len(self), row, default)

</t>
<t tx="karstenw.20230303123103.22">def date(*args, **kwargs):
    """ Returns a Date from the given parameters:
        - date(format=Date.format) =&gt; now
        - date(int)
        - date(string)
        - date(string, format=Date.format)
        - date(string, inputformat, format=Date.format)
        - date(year, month, day, format=Date.format)
        - date(year, month, day, hours, minutes, seconds, format=Date.format)
        If a string is given without an explicit input format, all known formats will be tried.
    """
    d = None
    f = None
    if len(args) == 0 \
    and kwargs.get("year") is not None \
    and kwargs.get("month") \
    and kwargs.get("day"):
        # Year, month, day.
        d = Date(**kwargs)
    elif kwargs.get("week"):
        # Year, week, weekday.
        f = kwargs.pop("format", None)
        d = Date(*_yyyywwd2yyyymmdd(
            kwargs.pop("year", args and args[0] or Date.now().year),
            kwargs.pop("week"),
            kwargs.pop("weekday", kwargs.pop("day", 1))), **kwargs)
    elif len(args) == 0 or args[0] == NOW:
        # No parameters or one parameter NOW.
        d = Date.now()
    elif len(args) == 1 \
     and isinstance(args[0], (Date, datetime)):
        # One parameter, a Date or datetime object.
        d = Date.fromtimestamp(int(mktime(args[0].timetuple())))
        d += time(microseconds=args[0].microsecond)
    elif len(args) == 1 \
     and (isinstance(args[0], int) \
      or isinstance(args[0], (str, bytes)) and args[0].isdigit()):
        # One parameter, an int or string timestamp.
        if isinstance(args[0], bytes):
            args = (args[0].decode("utf-8"),)
        d = Date.fromtimestamp(int(args[0]))
    elif len(args) == 1 \
     and isinstance(args[0], (str, bytes)):
        # One parameter, a date string for which we guess the input format (RFC2822 or known formats).
        if isinstance(args[0], bytes):
            args = (args[0].decode("utf-8"),)
        try:
            d = Date.fromtimestamp(mktime_tz(parsedate_tz(args[0])))
        except:
            for format in ("format" in kwargs and [kwargs["format"]] or []) + date_formats:
                try:
                    d = Date.strptime(args[0], format)
                    break
                except:
                    pass
        if d is None:
            raise DateError("unknown date format for %s" % repr(args[0]))
    elif len(args) == 2 \
     and isinstance(args[0], (str, bytes)):
        # Two parameters, a date string and an explicit input format.
        if isinstance(args[0], bytes):
            args = (args[0].decode("utf-8"), args[1].decode("utf-8"))
        d = Date.strptime(args[0], args[1])
    elif len(args) &gt;= 3:
        # 3-6 parameters: year, month, day, hours, minutes, seconds.
        f = kwargs.pop("format", None)
        d = Date(*args[:7], **kwargs)
    else:
        raise DateError("unknown date format")
    d.format = kwargs.get("format") or len(args) &gt; 7 and args[7] or f or Date.format
    return d


</t>
<t tx="karstenw.20230303123103.220">def extend(self, rows, default=None, **kwargs):
    for row in rows:
        self.insert(len(self), row, default)

</t>
<t tx="karstenw.20230303123103.221">def group(self, j, function=FIRST, key=lambda v: v):
    """ Returns a datasheet with unique values in column j by grouping rows with the given function.
        The function takes a list of column values as input and returns a single value,
        e.g. FIRST, LAST, COUNT, MAX, MIN, SUM, AVG, STDEV, CONCATENATE.
        The function can also be a list of functions (one for each column).
        TypeError will be raised when the function cannot handle the data in a column.
        The key argument can be used to map the values in column j, for example:
        key=lambda date: date.year to group Date objects by year.
    """
    if isinstance(function, tuple):
        function = list(function)
    if not isinstance(function, list):
        function = [function] * self._m
    if len(function) &lt; self._m:
        function += [FIRST] * (self._m - len(function))
    for i, f in enumerate(function):
        if i == j: # Group column j is always FIRST.
            f = FIRST
        if f == FIRST:
            function[i] = lambda a: a[+0]
        if f == LAST:
            function[i] = lambda a: a[-1]
        if f == COUNT:
            function[i] = lambda a: len(a)
        if f == MAX:
            function[i] = lambda a: max(a)
        if f == MIN:
            function[i] = lambda a: min(a)
        if f == SUM:
            function[i] = lambda a: _sum([x for x in a if x is not None])
        if f == AVG:
            function[i] = lambda a: avg([x for x in a if x is not None])
        if f == STDEV:
            function[i] = lambda a: stdev([x for x in a if x is not None])
        if f == CONCATENATE:
            function[i] = lambda a: ",".join(decode_utf8(x) for x in a if x is not None)
    J = j
    # Map unique values in column j to a list of rows that contain this value.
    g = {}
    [g.setdefault(key(v), []).append(i) for i, v in enumerate(self.columns[j])]
    # Map unique values in column j to a sort index in the new, grouped list.
    o = [(g[v][0], v) for v in g]
    o = dict([(v, i) for i, (ii, v) in enumerate(sorted(o))])
    # Create a list of rows with unique values in column j,
    # applying the group function to the other columns.
    u = [None] * len(o)
    for v in g:
        # List the column values for each group row.
        u[o[v]] = [[list.__getitem__(self, i)[j] for i in g[v]] for j in range(self._m)]
        # Apply the group function to each row, except the unique value in column j.
        u[o[v]] = [function[j](column) for j, column in enumerate(u[o[v]])]
        u[o[v]][J] = v # list.__getitem__(self, i)[J]
    return Datasheet(rows=u)

</t>
<t tx="karstenw.20230303123103.222">def record(self, row):
    """ Returns the given row as a dictionary of (field or alias, value)-items.
    """
    return dict(list(zip((f for f, type in self.fields), row)))

</t>
<t tx="karstenw.20230303123103.223">def map(self, function=lambda item: item):
    """ Applies the given function to each item in the matrix.
    """
    for i, row in enumerate(self):
        for j, item in enumerate(row):
            row[j] = function(item)

</t>
<t tx="karstenw.20230303123103.224">def slice(self, i, j, n, m):
    """ Returns a new Datasheet starting at row i and column j and spanning n rows and m columns.
    """
    return Datasheet(rows=[list.__getitem__(self, i)[j:j + m] for i in range(i, i + n)])

</t>
<t tx="karstenw.20230303123103.225">def copy(self, rows=ALL, columns=ALL):
    """ Returns a new Datasheet from a selective list of row and/or column indices.
    """
    if rows == ALL and columns == ALL:
        return Datasheet(rows=self)
    if rows == ALL:
        return Datasheet(rows=list(zip(*(self.columns[j] for j in columns))))
    if columns == ALL:
        return Datasheet(rows=(self.rows[i] for i in rows))
    z = list(zip(*(self.columns[j] for j in columns)))
    return Datasheet(rows=(z[i] for i in rows))

</t>
<t tx="karstenw.20230303123103.226">@property
def array(self):
    """ Returns a NumPy array.
        Arrays must have elements of the same type, and rows of equal size.
    """
    import numpy
    return numpy.array(self)

</t>
<t tx="karstenw.20230303123103.227">@property
def json(self, **kwargs):
    """ Returns a JSON-string, as a list of dictionaries (if fields are defined) or as a list of lists.
        This is useful for sending a Datasheet to JavaScript, for example.
    """
    kwargs.setdefault("ensure_ascii", False) # Disable simplejson's Unicode encoder.
    if self.fields is not None:
        s = json.dumps([dict((f[0], row[i]) for i, f in enumerate(self.fields)) for row in self], **kwargs)
    else:
        s = json.dumps(self, **kwargs)
    return decode_utf8(s)

</t>
<t tx="karstenw.20230303123103.228">@property
def html(self):
    """ Returns a HTML-string with a &lt;table&gt;.
        This is useful for viewing the data, e.g., open("data.html", "wb").write(datasheet.html).
    """
    def encode(s):
        s = "%s" % s
        s = s.replace("&amp;", "&amp;amp;")
        s = s.replace("&lt;", "&amp;lt;")
        s = s.replace("&gt;", "&amp;gt;")
        s = s.replace("-", "&amp;#8209;")
        s = s.replace("\n", "&lt;br&gt;\n")
        return s
    a = []
    a.append("&lt;meta charset=\"utf8\"&gt;\n")
    a.append("&lt;style&gt;")
    a.append("table.datasheet { border-collapse: collapse; font: 11px sans-serif; } ")
    a.append("table.datasheet * { border: 1px solid #ddd; padding: 4px; } ")
    a.append("&lt;/style&gt;\n")
    a.append("&lt;table class=\"datasheet\"&gt;\n")
    if self.fields is not None:
        a.append("&lt;tr&gt;\n")
        a.append("\t&lt;th&gt;%s&lt;/th&gt;\n" % "#")
        a.extend("\t&lt;th&gt;%s&lt;/th&gt;\n" % encode(f[0]) for f in self.fields)
        a.append("&lt;/tr&gt;\n")
    for i, row in enumerate(self):
        a.append("&lt;tr&gt;\n")
        a.append("\t&lt;td&gt;%s&lt;/td&gt;\n" % (i + 1))
        a.extend("\t&lt;td&gt;%s&lt;/td&gt;\n" % encode(v) for v in row)
        a.append("&lt;/tr&gt;\n")
    a.append("&lt;/table&gt;")
    return encode_utf8("".join(a))


</t>
<t tx="karstenw.20230303123103.229">def flip(datasheet):
    """ Returns a new datasheet with rows for columns and columns for rows.
    """
    return Datasheet(rows=datasheet.columns)


</t>
<t tx="karstenw.20230303123103.23">class Time(timedelta):

    @others
</t>
<t tx="karstenw.20230303123103.230">def csv(*args, **kwargs):
    """ Returns a Datasheet from the given CSV file path.
    """
    if len(args) == 0:
        return Datasheet(**kwargs)
    return Datasheet.load(*args, **kwargs)

#--- DATASHEET ROWS --------------------------------------------------------------------------------
# Datasheet.rows mimics the operations on Datasheet:


</t>
<t tx="karstenw.20230303123103.231">class DatasheetRows(list):

    @others
#--- DATASHEET COLUMNS -----------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123103.232">def __init__(self, datasheet):
    self._datasheet = datasheet

</t>
<t tx="karstenw.20230303123103.233">def __setitem__(self, i, row):
    self._datasheet.pop(i)
    self._datasheet.insert(i, row)

</t>
<t tx="karstenw.20230303123103.234">def __getitem__(self, i):
    return list.__getitem__(self._datasheet, i)

</t>
<t tx="karstenw.20230303123103.235">def __getslice__(self, i, j):
    return self._datasheet[i:j]

</t>
<t tx="karstenw.20230303123103.236">def __delitem__(self, i):
    self.pop(i)

</t>
<t tx="karstenw.20230303123103.237">def __len__(self):
    return len(self._datasheet)

</t>
<t tx="karstenw.20230303123103.238">def __iter__(self):
    for i in range(len(self)):
        yield list.__getitem__(self._datasheet, i)

</t>
<t tx="karstenw.20230303123103.239">def __repr__(self):
    return repr(self._datasheet)

</t>
<t tx="karstenw.20230303123103.24">def __new__(cls, *args, **kwargs):
    """ A convenience wrapper for datetime.timedelta that handles months and years.
    """
    # Time.years
    # Time.months
    # Time.days
    # Time.seconds
    # Time.microseconds
    y = kwargs.pop("years", 0)
    m = kwargs.pop("months", 0)
    t = timedelta.__new__(cls, *args, **kwargs)
    setattr(t, "years", y)
    setattr(t, "months", m)
    return t


</t>
<t tx="karstenw.20230303123103.240">def __add__(self, row):
    raise TypeError("unsupported operand type(s) for +: 'Datasheet.rows' and '%s'" % row.__class__.__name__)

</t>
<t tx="karstenw.20230303123103.241">def __iadd__(self, row):
    self.append(row)
    return self

</t>
<t tx="karstenw.20230303123103.242">def __eq__(self, rows):
    return self._datasheet.__eq__(rows)

</t>
<t tx="karstenw.20230303123103.243">def __ne__(self, rows):
    return self._datasheet.__ne__(rows)

</t>
<t tx="karstenw.20230303123103.244">def insert(self, i, row, default=None):
    self._datasheet.insert(i, row, default)

</t>
<t tx="karstenw.20230303123103.245">def append(self, row, default=None):
    self._datasheet.append(row, default)

</t>
<t tx="karstenw.20230303123103.246">def extend(self, rows, default=None):
    self._datasheet.extend(rows, default)

</t>
<t tx="karstenw.20230303123103.247">def remove(self, row):
    self._datasheet.remove(row)

</t>
<t tx="karstenw.20230303123103.248">def pop(self, i):
    return self._datasheet.pop(i)

</t>
<t tx="karstenw.20230303123103.249">def count(self, row):
    return self._datasheet.count(row)

</t>
<t tx="karstenw.20230303123103.25">def time(days=0, seconds=0, minutes=0, hours=0, **kwargs):
    """ Returns a Time that can be added to a Date object.
        Other parameters: microseconds, milliseconds, weeks, months, years.
    """
    return Time(days=days, seconds=seconds, minutes=minutes, hours=hours, **kwargs)


</t>
<t tx="karstenw.20230303123103.250">def index(self, row):
    return self._datasheet.index(row)

</t>
<t tx="karstenw.20230303123103.251">def sort(self, cmp=None, key=None, reverse=False):
    self._datasheet.sort(cmp, key, reverse)

</t>
<t tx="karstenw.20230303123103.252">def reverse(self):
    self._datasheet.reverse()

</t>
<t tx="karstenw.20230303123103.253">def swap(self, i1, i2):
    self[i1], self[i2] = self[i2], self[i1]

</t>
<t tx="karstenw.20230303123103.254">class DatasheetColumns(list):

    @others
#--- DATASHEET COLUMN ------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123103.255">def __init__(self, datasheet):
    self._datasheet = datasheet
    self._cache = {} # Keep a reference to DatasheetColumn objects generated with Datasheet.columns[j].
                      # This way we can unlink them when they are deleted.

</t>
<t tx="karstenw.20230303123103.256">def __setitem__(self, j, column):
    if self._datasheet.fields is not None and j &lt; len(self._datasheet.fields):
        # Preserve the column header if it exists.
        f = self._datasheet.fields[j]
    else:
        f = None
    self.pop(j)
    self.insert(j, column, field=f)

</t>
<t tx="karstenw.20230303123103.257">def __getitem__(self, j):
    if j &lt; 0:
        j = j % len(self) # DatasheetColumns[-1]
    if j &gt;= len(self):
        raise IndexError("list index out of range")
    return self._cache.setdefault(j, DatasheetColumn(self._datasheet, j))

</t>
<t tx="karstenw.20230303123103.258">def __getslice__(self, i, j):
    return self._datasheet[:, i:j]

</t>
<t tx="karstenw.20230303123103.259">def __delitem__(self, j):
    self.pop(j)

</t>
<t tx="karstenw.20230303123103.26">def string(value, default=""):
    """ Returns the value cast to unicode, or default if it is None/empty.
    """
    # Useful for HTML interfaces.
    if value is None or value == "": # Don't do value != None because this includes 0.
        return default
    return decode_utf8(value)


</t>
<t tx="karstenw.20230303123103.260">def __len__(self):
    return len(self._datasheet) &gt; 0 and len(self._datasheet[0]) or 0

</t>
<t tx="karstenw.20230303123103.261">def __iter__(self):
    for i in range(len(self)):
        yield self.__getitem__(i)

</t>
<t tx="karstenw.20230303123103.262">def __repr__(self):
    return repr(list(iter(self)))

</t>
<t tx="karstenw.20230303123103.263">def __add__(self, column):
    raise TypeError("unsupported operand type(s) for +: 'Datasheet.columns' and '%s'" % column.__class__.__name__)

</t>
<t tx="karstenw.20230303123103.264">def __iadd__(self, column):
    self.append(column)
    return self

</t>
<t tx="karstenw.20230303123103.265">def __eq__(self, columns):
    return list(self) == columns

</t>
<t tx="karstenw.20230303123103.266">def __ne__(self, columns):
    return not self.__eq__(self, columns)

</t>
<t tx="karstenw.20230303123103.267">def insert(self, j, column, default=None, field=None):
    """ Inserts the given column into the matrix.
        Missing rows at the end (bottom) will be filled with the default value.
    """
    try:
        column = [v for v in column]
    except:
        raise TypeError("Datasheet.columns.insert(x): x must be list")
    column = column + [default] * (len(self._datasheet) - len(column))
    if len(column) &gt; len(self._datasheet):
        self._datasheet.extend([[None]] * (len(column) - len(self._datasheet)))
    for i, row in enumerate(self._datasheet):
        row.insert(j, column[i])
    self._datasheet.__dict__["_m"] += 1 # Increase column count.
    # Add a new header.
    if self._datasheet.fields is not None:
        self._datasheet.fields += [(None, None)] * (len(self) - len(self._datasheet.fields) - 1)
        self._datasheet.fields.insert(j, field or (None, None))

</t>
<t tx="karstenw.20230303123103.268">def append(self, column, default=None, field=None):
    self.insert(len(self), column, default, field)

</t>
<t tx="karstenw.20230303123103.269">def extend(self, columns, default=None, fields=[]):
    for j, column in enumerate(columns):
        self.insert(len(self), column, default, j &lt; len(fields) and fields[j] or None)

</t>
<t tx="karstenw.20230303123103.27">class EncryptionError(Exception):
    pass


</t>
<t tx="karstenw.20230303123103.270">def remove(self, column):
    if isinstance(column, DatasheetColumn) and column._datasheet == self._datasheet:
        self.pop(column._j)
        return
    raise ValueError("list.remove(x): x not in list")

</t>
<t tx="karstenw.20230303123103.271">def pop(self, j):
    column = list(self[j]) # Return a list copy.
    for row in self._datasheet:
        row.pop(j)
    # At one point a DatasheetColumn object was created with Datasheet.columns[j].
    # It might still be in use somewhere, so we unlink it from the datasheet:
    self._cache[j]._datasheet = Datasheet(rows=[[v] for v in column])
    self._cache[j]._j = 0
    self._cache.pop(j)
    for k in range(j + 1, len(self) + 1):
        if k in self._cache:
            # Shift the DatasheetColumn objects on the right to the left.
            self._cache[k - 1] = self._cache.pop(k)
            self._cache[k - 1]._j = k - 1
    self._datasheet.__dict__["_m"] -= 1 # Decrease column count.
    # Remove the header.
    if self._datasheet.fields is not None:
        self._datasheet.fields.pop(j)
    return column

</t>
<t tx="karstenw.20230303123103.272">def count(self, column):
    return len([True for c in self if c == column])

</t>
<t tx="karstenw.20230303123103.273">def index(self, column):
    if isinstance(column, DatasheetColumn) and column._datasheet == self._datasheet:
        return column._j
    return list(self).index(column)

</t>
<t tx="karstenw.20230303123103.274">def sort(self, cmp=None, key=None, reverse=False, order=None):
    # This makes most sense if the order in which columns should appear is supplied.
    if order and reverse is True:
        o = list(reversed(order))
    if order and reverse is False:
        o = list(order)
    if not order:
        o = _order(self, cmp, key, reverse)
    for i, row in enumerate(self._datasheet):
        # The main difficulty is modifying each row in-place,
        # since other variables might be referring to it.
        r = list(row)
        [row.__setitem__(i2, r[i1]) for i2, i1 in enumerate(o)]
    # Reorder the datasheet headers.
    if self._datasheet.fields is not None:
        self._datasheet.fields = [self._datasheet.fields[i] for i in o]

</t>
<t tx="karstenw.20230303123103.275">def swap(self, j1, j2):
    self[j1], self[j2] = self[j2], self[j1]
    # Reorder the datasheet headers.
    if self._datasheet.fields is not None:
        self._datasheet.fields[j1], self._datasheet.fields[j2] = (
            self._datasheet.fields[j2],
            self._datasheet.fields[j1])

</t>
<t tx="karstenw.20230303123103.276">class DatasheetColumn(list):

    @others
#---------------------------------------------------------------------------------------------------

_UID = 0


</t>
<t tx="karstenw.20230303123103.277">def __init__(self, datasheet, j):
    """ A dynamic column in a Datasheet.
        If the actual column is deleted with Datasheet.columns.remove() or Datasheet.columms.pop(),
        the DatasheetColumn object will be orphaned (i.e., it is no longer part of the table).
    """
    self._datasheet = datasheet
    self._j = j

</t>
<t tx="karstenw.20230303123103.278">def __getslice__(self, i, j):
    return list(list.__getitem__(self._datasheet, i)[self._j] for i in range(i, min(j, len(self._datasheet))))

</t>
<t tx="karstenw.20230303123103.279">def __getitem__(self, i):
    return list.__getitem__(self._datasheet, i)[self._j]

</t>
<t tx="karstenw.20230303123103.28">class DecryptionError(Exception):
    pass


</t>
<t tx="karstenw.20230303123103.280">def __setitem__(self, i, value):
    list.__getitem__(self._datasheet, i)[self._j] = value

</t>
<t tx="karstenw.20230303123103.281">def __len__(self):
    return len(self._datasheet)

</t>
<t tx="karstenw.20230303123103.282">def __iter__(self): # Can be put more simply but optimized for performance:
    for i in range(len(self)):
        yield list.__getitem__(self._datasheet, i)[self._j]

</t>
<t tx="karstenw.20230303123103.283">def __reversed__(self):
    return reversed(list(iter(self)))

</t>
<t tx="karstenw.20230303123103.284">def __repr__(self):
    return repr(list(iter(self)))

</t>
<t tx="karstenw.20230303123103.285">def __gt__(self, column):
    return list(self) &gt; list(column)

</t>
<t tx="karstenw.20230303123103.286">def __lt__(self, column):
    return list(self) &lt; list(column)

</t>
<t tx="karstenw.20230303123103.287">def __ge__(self, column):
    return list(self) &gt;= list(column)

</t>
<t tx="karstenw.20230303123103.288">def __le__(self, column):
    return list(self) &lt;= list(column)

</t>
<t tx="karstenw.20230303123103.289">def __eq__(self, column):
    return list(self) == column

</t>
<t tx="karstenw.20230303123103.29">def encrypt_string(s, key=""):
    """ Returns the given string as an encrypted bytestring.
    """
    key += " "
    a = []
    for i in range(len(s)):
        try:
            a.append(chr(ord(s[i]) + ord(key[i % len(key)]) % 256).encode("latin-1"))
        except:
            raise EncryptionError()
    s = b"".join(a)
    s = base64.urlsafe_b64encode(s)
    return s


</t>
<t tx="karstenw.20230303123103.290">def __ne__(self, column):
    return not self.__eq__(column)

</t>
<t tx="karstenw.20230303123103.291">def __add__(self, column):
    return list(self) + list(column)

</t>
<t tx="karstenw.20230303123103.292">def __iadd__(self, column):
    self.extend(column)

</t>
<t tx="karstenw.20230303123103.293">def __contains__(self, value):
    for v in self:
        if v == value:
            return True
    return False

</t>
<t tx="karstenw.20230303123103.294">def count(self, value):
    return len([True for v in self if v == value])

</t>
<t tx="karstenw.20230303123103.295">def index(self, value):
    for i, v in enumerate(self):
        if v == value:
            return i
    raise ValueError("list.index(x): x not in list")

</t>
<t tx="karstenw.20230303123103.296">def remove(self, value):
    """ Removes the matrix row that has the given value in this column.
    """
    for i, v in enumerate(self):
        if v == value:
            self._datasheet.pop(i)
            return
    raise ValueError("list.remove(x): x not in list")

</t>
<t tx="karstenw.20230303123103.297">def pop(self, i):
    """ Removes the entire row from the matrix and returns the value at the given index.
    """
    row = self._datasheet.pop(i)
    return row[self._j]

</t>
<t tx="karstenw.20230303123103.298">def sort(self, cmp=None, key=None, reverse=False):
    """ Sorts the rows in the matrix according to the values in this column,
        e.g. clicking ascending / descending on a column header in a datasheet viewer.
    """
    o = order(list(self), cmp, key, reverse)
    # Modify the table in place, more than one variable may be referencing it:
    r = list(self._datasheet)
    [self._datasheet.__setitem__(i2, r[i1]) for i2, i1 in enumerate(o)]

</t>
<t tx="karstenw.20230303123103.299">def insert(self, i, value, default=None):
    """ Inserts the given value in the column.
        This will create a new row in the matrix, where other columns are set to the default.
    """
    self._datasheet.insert(i, [default] * self._j + [value] + [default] * (len(self._datasheet) - self._j - 1))

</t>
<t tx="karstenw.20230303123103.3">def pd(*args):
    """ Returns the path to the parent directory of the script that calls pd() + given relative path.
        For example, in this script: pd("..") =&gt; /usr/local/lib/python2.x/site-packages/pattern/db/..
    """
    f = inspect.currentframe()
    f = inspect.getouterframes(f)[1][1]
    f = f != "&lt;stdin&gt;" and f or os.getcwd()
    return os.path.join(os.path.dirname(os.path.realpath(f)), *args)

_sum = sum # pattern.db.sum() is also a column aggregate function.

#### DATE FUNCTIONS ################################################################################

NOW, YEAR = "now", datetime.now().year

# Date formats can be found in the Python documentation:
# http://docs.python.org/library/time.html#time.strftime
DEFAULT_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"
date_formats = [
    DEFAULT_DATE_FORMAT,           # 2010-09-21 09:27:01  =&gt; SQLite + MySQL
    "%Y-%m-%dT%H:%M:%SZ",          # 2010-09-20T09:27:01Z =&gt; Bing
    "%a, %d %b %Y %H:%M:%S +0000", # Fri, 21 Sep 2010 09:27:01 +000 =&gt; Twitter
    "%a %b %d %H:%M:%S +0000 %Y",  # Fri Sep 21 09:21:01 +0000 2010 =&gt; Twitter
    "%Y-%m-%dT%H:%M:%S+0000",      # 2010-09-20T09:27:01+0000 =&gt; Facebook
    "%Y-%m-%d %H:%M",              # 2010-09-21 09:27
    "%Y-%m-%d",                    # 2010-09-21
    "%d/%m/%Y",                    # 21/09/2010
    "%d %B %Y",                    # 21 September 2010
    "%d %b %Y",                    # 21 Sep 2010
    "%B %d %Y",                    # September 21 2010
    "%B %d, %Y",                   # September 21, 2010
]


</t>
<t tx="karstenw.20230303123103.30">def decrypt_string(s, key=""):
    """ Returns the given string as a decrypted Unicode string.
    """
    key += " "
    s = base64.urlsafe_b64decode(s)
    s = s.decode("latin-1")
    a = []
    for i in range(len(s)):
        try:
            a.append(chr(ord(s[i]) - ord(key[i % len(key)]) % 256))
        except:
            raise DecryptionError()
    s = "".join(a)
    s = decode_utf8(s)
    return s

RE_AMPERSAND = re.compile("\&amp;(?!\#)")           # &amp; not followed by #
RE_UNICODE = re.compile(r'&amp;(#?)(x|X?)(\w+);') # &amp;#201;


</t>
<t tx="karstenw.20230303123103.300">def append(self, value, default=None):
    self.insert(len(self), value, default)

</t>
<t tx="karstenw.20230303123103.301">def extend(self, values, default=None):
    for value in values:
        self.insert(len(self), value, default)

</t>
<t tx="karstenw.20230303123103.302">def map(self, function=lambda value: value):
    """ Applies the given function to each value in the column.
    """
    for j, value in enumerate(self):
        self[j] = function(value)

</t>
<t tx="karstenw.20230303123103.303">def filter(self, function=lambda value: True):
    """ Removes the matrix rows for which function(value) in the column is not True.
    """
    i = len(self)
    for v in reversed(self):
        i -= 1
        if not function(v):
            self._datasheet.pop(i)

</t>
<t tx="karstenw.20230303123103.304">def swap(self, i1, i2):
    self._datasheet.swap(i1, i2)

</t>
<t tx="karstenw.20230303123103.305">def uid():
    global _UID
    _UID += 1
    return _UID


</t>
<t tx="karstenw.20230303123103.306">def truncate(string, length=100):
    """ Returns a (head, tail)-tuple, where the head string length is less than the given length.
        Preferably the string is split at a space, otherwise a hyphen ("-") is injected.
    """
    if len(string) &lt;= length:
        return string, ""
    n, words = 0, string.split(" ")
    for i, w in enumerate(words):
        if n + len(w) &gt; length:
            break
        n += len(w) + 1
    if i == 0 and len(w) &gt; length:
        return (w[:length - 1] + "-",
                (w[length - 1:] + " " + " ".join(words[1:])).strip())
    return (" ".join(words[:i]),
            " ".join(words[i:]))

_truncate = truncate


</t>
<t tx="karstenw.20230303123103.307">def pprint(datasheet, truncate=40, padding=" ", fill="."):
    """ Prints a string where the rows in the datasheet are organized in outlined columns.
    """
    # Calculate the width of each column, based on the longest field in each column.
    # Long fields can be split across different lines, so we need to check each line.
    w = [0 for column in datasheet.columns]
    R = []
    for i, row in enumerate(datasheet.rows):
        fields = []
        for j, v in enumerate(row):
            # Cast each field in the row to a string.
            # Strings that span beyond the maximum column width are wrapped.
            # Thus, each "field" in the row is a list of lines.
            lines = []
            if not isinstance(v, str):
                v = str(v)
            for v in v.splitlines():
                v = decode_utf8(v.strip())
                while v:
                    head, v = _truncate(v, truncate)
                    lines.append(head)
                    w[j] = max(w[j], len(head))
            fields.append(lines)
        R.append(fields)
    for i, fields in enumerate(R):
        # Add empty lines to each field so they are of equal height.
        n = max([len(lines) for lines in fields])
        fields = [lines + [""] * (n - len(lines)) for lines in fields]
        # Print the row line per line, justifying the fields with spaces.
        columns = []
        for k in range(n):
            for j, lines in enumerate(fields):
                s = lines[k]
                s += ((k == 0 or len(lines[k]) &gt; 0) and fill or " ") * (w[j] - len(lines[k]))
                s += padding
                columns.append(s)
            print(" ".join(columns))
</t>
<t tx="karstenw.20230303123103.31">def encode_entities(string):
    """ Encodes HTML entities in the given string ("&lt;" =&gt; "&amp;lt;").
        For example, to display "&lt;em&gt;hello&lt;/em&gt;" in a browser,
        we need to pass "&amp;lt;em&amp;gt;hello&amp;lt;/em&amp;gt;" (otherwise "hello" in italic is displayed).
    """
    if isinstance(string, str):
        string = RE_AMPERSAND.sub("&amp;amp;", string)
        string = string.replace("&lt;", "&amp;lt;")
        string = string.replace("&gt;", "&amp;gt;")
        string = string.replace('"', "&amp;quot;")
        string = string.replace("'", "&amp;#39;")
    return string


</t>
<t tx="karstenw.20230303123103.32">def decode_entities(string):
    """ Decodes HTML entities in the given string ("&amp;lt;" =&gt; "&lt;").
    """
    # http://snippets.dzone.com/posts/show/4569
    def replace_entity(match):
        hash, hex, name = match.group(1), match.group(2), match.group(3)
        if hash == "#" or name.isdigit():
            if hex == '':
                return chr(int(name))              # "&amp;#38;" =&gt; "&amp;"
            if hex in ("x", "X"):
                return chr(int('0x' + name, 16))   # "&amp;#x0026;" = &gt; "&amp;"
        else:
            cp = name2codepoint.get(name)          # "&amp;amp;" =&gt; "&amp;"
            return cp and chr(cp) or match.group() # "&amp;foo;" =&gt; "&amp;foo;"
    if isinstance(string, str):
        return RE_UNICODE.subn(replace_entity, string)[0]
    return string


</t>
<t tx="karstenw.20230303123103.33">class _Binary(object):
    """ A wrapper for BLOB data with engine-specific encoding.
        See also: Database.binary().
    """

    @others
</t>
<t tx="karstenw.20230303123103.34">def __init__(self, data, type=SQLITE):
    self.data, self.type = str(hasattr(data, "read") and data.read() or data), type

</t>
<t tx="karstenw.20230303123103.35">def escape(self):
    if self.type == SQLITE:
        return str(self.data.encode("string-escape")).replace("'", "''")
    if self.type == MYSQL:
        return MySQLdb.escape_string(self.data)


</t>
<t tx="karstenw.20230303123103.36">def _escape(value, quote=lambda string: "'%s'" % string.replace("'", "\\'")):
    """ Returns the quoted, escaped string (e.g., "'a bird\'s feathers'") for database entry.
        Anything that is not a string (e.g., an integer) is converted to string.
        Booleans are converted to "0" and "1", None is converted to "null".
        See also: Database.escape()
    """
    # Note: use Database.escape() for MySQL/SQLITE-specific escape.
    if value in ("current_timestamp",):
        # Don't quote constants such as current_timestamp.
        return value
    if isinstance(value, str):
        # Strings are quoted, single quotes are escaped according to the database engine.
        return quote(value)
    if isinstance(value, bool):
        # Booleans are converted to "0" or "1".
        return str(int(value))
    if isinstance(value, (int, float)):
        # Numbers are converted to string.
        return str(value)
    if isinstance(value, datetime):
        # Dates are formatted as string.
        return quote(value.strftime(DEFAULT_DATE_FORMAT))
    if isinstance(value, type(None)):
        # None is converted to NULL.
        return "null"
    if isinstance(value, Query):
        # A Query is converted to "("+Query.SQL()+")" (=subquery).
        return "(%s)" % value.SQL().rstrip(";")
    if isinstance(value, _Binary):
        # Binary data is escaped with attention to null bytes.
        return "'%s'" % value.escape()
    return value


</t>
<t tx="karstenw.20230303123103.37">def cast(x, f, default=None):
    """ Returns f(x) or default.
    """
    if f is str and isinstance(x, str):
        return decode_utf8(x)
    if f is bool and x in ("1", "True", "true"):
        return True
    if f is bool and x in ("0", "False", "false"):
        return False
    if f is int:
        f = lambda x: int(round(float(x)))
    try:
        return f(x)
    except:
        return default

#### LIST FUNCTIONS ################################################################################


</t>
<t tx="karstenw.20230303123103.38">def find(match=lambda item: False, list=[]):
    """ Returns the first item in the list for which match(item) is True.
    """
    for item in list:
        if match(item) is True:
            return item


</t>
<t tx="karstenw.20230303123103.39">def order(list, cmp=None, key=None, reverse=False):
    """ Returns a list of indices in the order as when the given list is sorted.
        For example: ["c","a","b"] =&gt; [1, 2, 0]
        This means that in the sorted list, "a" (index 1) comes first and "c" (index 0) last.
    """
    if cmp and key:
        f = lambda i, j: cmp(key(list[i]), key(list[j]))
    elif cmp:
        f = lambda i, j: cmp(list[i], list[j])
    elif key:
        f = lambda i, j: int(key(list[i]) &gt;= key(list[j])) * 2 - 1
    else:
        f = lambda i, j: int(list[i] &gt;= list[j]) * 2 - 1
    return sorted(range(len(list)), key=cmp_to_key(f), reverse=reverse)

_order = order


</t>
<t tx="karstenw.20230303123103.4">def _yyyywwd2yyyymmdd(year, week, weekday):
    """ Returns (year, month, day) for given (year, week, weekday).
    """
    d = datetime(year, month=1, day=4) # 1st week contains January 4th.
    d = d - timedelta(d.isoweekday() - 1) + timedelta(days=weekday - 1, weeks=week - 1)
    return (d.year, d.month, d.day)


</t>
<t tx="karstenw.20230303123103.40">def avg(list):
    """ Returns the arithmetic mean of the given list of values.
        For example: mean([1,2,3,4]) = 10/4 = 2.5.
    """
    return float(_sum(list)) / (len(list) or 1)


</t>
<t tx="karstenw.20230303123103.41">def variance(list):
    """ Returns the variance of the given list of values.
        The variance is the average of squared deviations from the mean.
    """
    a = avg(list)
    return _sum([(x - a)**2 for x in list]) / (len(list) - 1 or 1)


</t>
<t tx="karstenw.20230303123103.42">def stdev(list):
    """ Returns the standard deviation of the given list of values.
        Low standard deviation =&gt; values are close to the mean.
        High standard deviation =&gt; values are spread out over a large range.
    """
    return sqrt(variance(list))

#### SQLITE FUNCTIONS ##############################################################################
# Convenient MySQL functions not in in pysqlite2. These are created at each Database.connect().


</t>
<t tx="karstenw.20230303123103.43">class sqlite_first(list):
    @others
</t>
<t tx="karstenw.20230303123103.44">def step(self, value): self.append(value)

</t>
<t tx="karstenw.20230303123103.45">def finalize(self):
    return self[0]


</t>
<t tx="karstenw.20230303123103.46">class sqlite_last(list):
    @others
</t>
<t tx="karstenw.20230303123103.47">def step(self, value): self.append(value)

</t>
<t tx="karstenw.20230303123103.48">def finalize(self):
    return self[-1]


</t>
<t tx="karstenw.20230303123103.49">class sqlite_group_concat(list):
    @others
# SQLite (and MySQL) date string format:
# yyyy-mm-dd hh:mm:ss


</t>
<t tx="karstenw.20230303123103.5">def _strftime1900(d, format):
    """ Returns the given date formatted as a string.
    """
    if d.year &lt; 1900: # Python's strftime() doesn't handle year &lt; 1900.
        return strftime(format, (1900,) + d.timetuple()[1:]).replace("1900", str(d.year), 1)
    return datetime.strftime(d, format)


</t>
<t tx="karstenw.20230303123103.50">def step(self, value): self.append(value)

</t>
<t tx="karstenw.20230303123103.51">def finalize(self):
    return ",".join(string(v) for v in self if v is not None)

</t>
<t tx="karstenw.20230303123103.52">def sqlite_year(datestring):
    return int(datestring.split(" ")[0].split("-")[0])


</t>
<t tx="karstenw.20230303123103.53">def sqlite_month(datestring):
    return int(datestring.split(" ")[0].split("-")[1])


</t>
<t tx="karstenw.20230303123103.54">def sqlite_day(datestring):
    return int(datestring.split(" ")[0].split("-")[2])


</t>
<t tx="karstenw.20230303123103.55">def sqlite_hour(datestring):
    return int(datestring.split(" ")[1].split(":")[0])


</t>
<t tx="karstenw.20230303123103.56">def sqlite_minute(datestring):
    return int(datestring.split(" ")[1].split(":")[1])


</t>
<t tx="karstenw.20230303123103.57">def sqlite_second(datestring):
    return int(datestring.split(" ")[1].split(":")[2])

#### DATABASE ######################################################################################


</t>
<t tx="karstenw.20230303123103.58">class DatabaseConnectionError(Exception):
    pass


</t>
<t tx="karstenw.20230303123103.59">class Database(object):

    @others
#### FIELD #########################################################################################


</t>
<t tx="karstenw.20230303123103.6">class DateError(Exception):
    pass


</t>
<t tx="karstenw.20230303123103.60">class Tables(dict):
    # Table objects are lazily constructed when retrieved.
    # This saves time because each table executes a metadata query when constructed.
    @others
</t>
<t tx="karstenw.20230303123103.61">def __init__(self, db, *args, **kwargs):
    dict.__init__(self, *args, **kwargs)
    self.db = db

</t>
<t tx="karstenw.20230303123103.62">def __getitem__(self, k):
    if dict.__getitem__(self, k) is None:
        dict.__setitem__(self, k, Table(name=k, database=self.db))
    return dict.__getitem__(self, k)

</t>
<t tx="karstenw.20230303123103.63">def __init__(self, name, host="localhost", port=3306, username="root", password="", type=SQLITE, unicode=True, **kwargs):
    """ A collection of tables stored in an SQLite or MySQL database.
        If the database does not exist, creates it.
        If the host, user or password is wrong, raises DatabaseConnectionError.
    """
    _import_db(type)
    self.type = type
    self.name = name
    self.host = host
    self.port = port
    self.username = kwargs.get("user", username)
    self.password = password
    self._connection = None
    self.connect(unicode)
    # Table names are available in the Database.tables dictionary,
    # table objects as attributes (e.g. Database.table_name).
    q = self.type == SQLITE and "select name from sqlite_master where type='table';" or "show tables;"
    self.tables = Database.Tables(self)
    for name, in self.execute(q):
        if not name.startswith(("sqlite_",)):
            self.tables[name] = None
    # The SQL syntax of the last query is kept in cache.
    self._query = None
    # Persistent relations between tables, stored as (table1, table2, key1, key2, join) tuples.
    self.relations = []

</t>
<t tx="karstenw.20230303123103.64">def connect(self, unicode=True):
    # Connections for threaded applications work differently,
    # see http://tools.cherrypy.org/wiki/Databases
    # (have one Database object for each thread).
    if self._connection is not None:
        return
    # MySQL
    if self.type == MYSQL:
        try:
            self._connection = MySQLdb.connect(self.host, self.username, self.password, self.name, port=self.port, use_unicode=unicode)
            self._connection.autocommit(False)
        except Exception as e:
            # Create the database if it doesn't exist yet.
            if "unknown database" not in str(e).lower():
                raise DatabaseConnectionError(e[1]) # Wrong host, username and/or password.
            connection = MySQLdb.connect(self.host, self.username, self.password)
            cursor = connection.cursor()
            cursor.execute("create database if not exists `%s`;" % self.name)
            cursor.close()
            connection.close()
            self._connection = MySQLdb.connect(self.host, self.username, self.password, self.name, port=self.port, use_unicode=unicode)
            self._connection.autocommit(False)
        if unicode:
            self._connection.set_character_set("utf8")
    # SQLite
    if self.type == SQLITE:
        self._connection = sqlite.connect(self.name, detect_types=sqlite.PARSE_DECLTYPES)
        # Create functions that are not natively supported by the engine.
        # Aggregate functions (for grouping rows) + date functions.
        self._connection.create_aggregate("first", 1, sqlite_first)
        self._connection.create_aggregate("last", 1, sqlite_last)
        self._connection.create_aggregate("group_concat", 1, sqlite_group_concat)
        self._connection.create_function("year", 1, sqlite_year)
        self._connection.create_function("month", 1, sqlite_month)
        self._connection.create_function("day", 1, sqlite_day)
        self._connection.create_function("hour", 1, sqlite_hour)
        self._connection.create_function("minute", 1, sqlite_minute)
        self._connection.create_function("second", 1, sqlite_second)
    # Map field type INTEGER to int (not long(), e.g., 1L).
    # Map field type BOOLEAN to bool.
    # Map field type DATE to str, yyyy-mm-dd hh:mm:ss.
    if self.type == MYSQL:
        type = MySQLdb.constants.FIELD_TYPE
        self._connection.converter[type.LONG]       = int
        self._connection.converter[type.LONGLONG]   = int
        self._connection.converter[type.DECIMAL]    = float
        self._connection.converter[type.NEWDECIMAL] = float
        self._connection.converter[type.TINY]       = bool
        self._connection.converter[type.TIMESTAMP]  = date
    if self.type == SQLITE:
        sqlite.converters["TINYINT(1)"] = lambda v: bool(int(v))
        sqlite.converters["BLOB"]       = lambda v: str(v).decode("string-escape")
        sqlite.converters["TIMESTAMP"]  = date

</t>
<t tx="karstenw.20230303123103.65">def disconnect(self):
    if self._connection is not None:
        self._connection.commit()
        self._connection.close()
        self._connection = None

</t>
<t tx="karstenw.20230303123103.66">@property
def connection(self):
    return self._connection

</t>
<t tx="karstenw.20230303123103.67">@property
def connected(self):
    return self._connection is not None

</t>
<t tx="karstenw.20230303123103.68">def __getattr__(self, k):
    """ Tables are available as attributes by name, e.g., Database.persons.
    """
    if k in self.__dict__["tables"]:
        return self.__dict__["tables"][k]
    if k in self.__dict__:
        return self.__dict__[k]
    raise AttributeError("'Database' object has no attribute '%s'" % k)

</t>
<t tx="karstenw.20230303123103.69">def __len__(self):
    return len(self.tables)

</t>
<t tx="karstenw.20230303123103.7">class Date(datetime):
    """ A convenience wrapper for datetime.datetime with a default string format.
    """
    format = DEFAULT_DATE_FORMAT
    # Date.year
    # Date.month
    # Date.day
    # Date.minute
    # Date.second

    @others
</t>
<t tx="karstenw.20230303123103.70">def __iter__(self):
    return iter(self.tables.keys())

</t>
<t tx="karstenw.20230303123103.71">def __getitem__(self, table):
    return self.tables[table]

</t>
<t tx="karstenw.20230303123103.72">def __setitem__(self, table, fields):
    self.create(table, fields)

</t>
<t tx="karstenw.20230303123103.73">def __delitem__(self, table):
    self.drop(table)

</t>
<t tx="karstenw.20230303123103.74">def __nonzero__(self):
    return True

# Backwards compatibility.
</t>
<t tx="karstenw.20230303123103.75">def _get_user(self):
    return self.username

</t>
<t tx="karstenw.20230303123103.76">def _set_user(self, v):
    self.username = v
user = property(_get_user, _set_user)

</t>
<t tx="karstenw.20230303123103.77">@property
def query(self):
    """ Yields the last executed SQL query as a string.
    """
    return self._query

</t>
<t tx="karstenw.20230303123103.78">def execute(self, SQL, commit=False):
    """ Executes the given SQL query and return an iterator over the rows.
        With commit=True, automatically commits insert/update/delete changes.
    """
    self._query = SQL
    if not SQL:
        return # MySQL doesn't like empty queries.
    #print(SQL)
    cursor = self._connection.cursor()
    cursor.execute(SQL)
    if commit is not False:
        self._connection.commit()
    return self.RowsIterator(cursor)

</t>
<t tx="karstenw.20230303123103.79">class RowsIterator(object):
    """ Iterator over the rows returned from Database.execute().
    """

    @others
</t>
<t tx="karstenw.20230303123103.8">@property
def minutes(self):
    return self.minute

</t>
<t tx="karstenw.20230303123103.80">def __init__(self, cursor):
    self._cursor = cursor
    self._iter = iter(self._cursor.fetchall())

</t>
<t tx="karstenw.20230303123103.81">def __next__(self):
    return next(self._iter)

</t>
<t tx="karstenw.20230303123103.82">def __iter__(self):
    return self

</t>
<t tx="karstenw.20230303123103.83">def __del__(self):
    self._cursor.close()

</t>
<t tx="karstenw.20230303123103.84">def commit(self):
    """ Commit all pending insert/update/delete changes.
    """
    self._connection.commit()

</t>
<t tx="karstenw.20230303123103.85">def rollback(self):
    """ Discard changes since the last commit.
    """
    self._connection.rollback()

</t>
<t tx="karstenw.20230303123103.86">def escape(self, value):
    """ Returns the quoted, escaped string (e.g., "'a bird\'s feathers'") for database entry.
        Anything that is not a string (e.g., an integer) is converted to string.
        Booleans are converted to "0" and "1", None is converted to "null".
    """
    def quote(string):
        # How to escape strings differs between database engines.
        if self.type == MYSQL:
            #return "'%s'" % self._connection.escape_string(string) # Doesn't like Unicode.
            return "'%s'" % string.replace("'", "\\'")
        if self.type == SQLITE:
            return "'%s'" % string.replace("'", "''")
    return _escape(value, quote)

</t>
<t tx="karstenw.20230303123103.87">def binary(self, data):
    """ Returns the string of binary data as a value that can be inserted in a BLOB field.
    """
    return _Binary(data, self.type)

blob = binary

</t>
<t tx="karstenw.20230303123103.88">def _field_SQL(self, table, field):
    # Returns a (field, index)-tuple with SQL strings for the given field().
    # The field string can be used in a CREATE TABLE or ALTER TABLE statement.
    # The index string is an optional CREATE INDEX statement (or None).
    auto = " auto%sincrement" % (self.type == MYSQL and "_" or "")
    field = isinstance(field, str) and [field, STRING(255)] or field
    field = list(field) + [STRING, None, False, True][len(field) - 1:]
    field = list(_field(field[0], field[1], default=field[2], index=field[3], optional=field[4]))
    if field[1] == "timestamp" and field[2] == "now":
        field[2] = "current_timestamp"
    a = b = None
    a = "`%s` %s%s%s%s" % (
        # '`id` integer not null primary key auto_increment'
        field[0],
        field[1] == STRING and field[1]() or field[1],
        field[4] is False and " not null" or " null",
        field[2] is not None and " default %s" % self.escape(field[2]) or "",
        field[3] == PRIMARY and " primary key%s" % ("", auto)[field[1] == INTEGER] or "")
    if field[3] in (UNIQUE, True):
        b = "create %sindex `%s_%s` on `%s` (`%s`);" % (
            field[3] == UNIQUE and "unique " or "", table, field[0], table, field[0])
    return a, b

</t>
<t tx="karstenw.20230303123103.89">def create(self, table, fields=[], encoding="utf-8", **kwargs):
    """ Creates a new table with the given fields.
        The given list of fields must contain values returned from the field() function.
    """
    if table in self.tables:
        raise TableError("table '%s' already exists" % (self.name + "." + table))
    if table.startswith(XML_HEADER):
        # From an XML-string generated with Table.xml.
        return parse_xml(self, table,
                table = kwargs.get("name"),
                field = kwargs.get("field", lambda s: s.replace(".", "_")))
    encoding = self.type == MYSQL and " default charset=" + encoding.replace("utf-8", "utf8") or ""
    fields, indices = list(zip(*[self._field_SQL(table, f) for f in fields]))
    self.execute("create table `%s` (%s)%s;" % (table, ", ".join(fields), encoding))
    for index in indices:
        if index is not None:
            self.execute(index, commit=True)
    self.tables[table] = None # lazy loading
    return self.tables[table]

</t>
<t tx="karstenw.20230303123103.9">@property
def seconds(self):
    return self.second

</t>
<t tx="karstenw.20230303123103.90">def drop(self, table):
    """ Removes the table with the given name.
    """
    if isinstance(table, Table) and table.db == self:
        table = table.name
    if table in self.tables:
        self.tables[table].database = None
        self.tables.pop(table)
        self.execute("drop table `%s`;" % table, commit=True)

remove = drop

</t>
<t tx="karstenw.20230303123103.91">def link(self, table1, field1, table2, field2, join="left"):
    """ Defines a relation between two tables in the database.
        When executing a table query, fields from the linked table will also be available
        (to disambiguate between field names, use table.field_name).
    """
    if isinstance(table1, Table):
        table1 = table1.name
    if isinstance(table2, Table):
        table2 = table2.name
    self.relations.append((table1, field1, table2, field2, join))

</t>
<t tx="karstenw.20230303123103.92">def __repr__(self):
    return "Database(name=%s, host=%s, tables=%s)" % (
        repr(self.name),
        repr(self.host),
        repr(self.tables.keys()))

</t>
<t tx="karstenw.20230303123103.93">def _delete(self):
    # No warning is issued, seems a bad idea to document the method.
    # Anyone wanting to delete an entire database should use an editor.
    if self.type == MYSQL:
        self.execute("drop database `%s`" % self.name, commit=True)
        self.disconnect()
    if self.type == SQLITE:
        self.disconnect()
        os.unlink(self.name)

</t>
<t tx="karstenw.20230303123103.94">def __delete__(self):
    try:
        self.disconnect()
    except:
        pass

</t>
<t tx="karstenw.20230303123103.95">class _String(str):
    # The STRING constant can be called with a length when passed to field(),
    # for example field("language", type=STRING(2), default="en", index=True).
    @others
# Field type.
# Note: SQLite string fields do not impose a string limit.
# Unicode strings have more characters than actually displayed (e.g. "&amp;#9829;").
# Boolean fields are stored as tinyint(1), int 0 or 1.
STRING, INTEGER, FLOAT, TEXT, BLOB, BOOLEAN, DATE = \
    _String(), "integer", "float", "text", "blob", "boolean", "date"

STR, INT, BOOL = STRING, INTEGER, BOOLEAN

# Field index.
PRIMARY = "primary"
UNIQUE = "unique"

# DATE default.
NOW = "now"

#--- FIELD- ----------------------------------------------------------------------------------------

#def field(name, type=STRING, default=None, index=False, optional=True)


</t>
<t tx="karstenw.20230303123103.96">def __new__(self):
    return str.__new__(self, "string")

</t>
<t tx="karstenw.20230303123103.97">def __call__(self, length=100):
    return "varchar(%s)" % (length &gt; 255 and 255 or (length &lt; 1 and 1 or length))

</t>
<t tx="karstenw.20230303123103.98">def field(name, type=STRING, **kwargs):
    """ Returns a table field definition that can be passed to Database.create().
        The column can be indexed by setting index to True, PRIMARY or UNIQUE.
        Primary key number columns are always auto-incremented.
    """
    default, index, optional = (
        kwargs.get("default", type == DATE and NOW or None),
        kwargs.get("index", False),
        kwargs.get("optional", True)
    )
    if type == STRING:
        type = STRING()
    if type == FLOAT:
        type = "real"
    if type == BOOLEAN:
        type = "tinyint(1)"
    if type == DATE:
        type = "timestamp"
    if str(index) in "01":
        index = bool(int(index))
    if str(optional) in "01":
        optional = bool(int(optional))
    return (name, type, default, index, optional)

_field = field


</t>
<t tx="karstenw.20230303123103.99">def primary_key(name="id"):
    """ Returns an auto-incremented integer primary key field named "id".
    """
    return field(name, INTEGER, index=PRIMARY, optional=False)

pk = primary_key

#--- FIELD SCHEMA ----------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123122.1"></t>
<t tx="karstenw.20230303123127.1">#### PATTERN | GRAPH ###############################################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303123148.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range, next

import os
import sys

from math import sqrt, pow
from math import sin, cos, atan2, degrees, radians, pi
from random import random
from heapq import heappush, heappop
from warnings import warn
from shutil import rmtree

from io import open

from functools import cmp_to_key

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

# float("inf") doesn't work on windows.
INFINITE = 1e20

#--- LIST FUNCTIONS --------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123148.10">def __init__(self, x=0, y=0):
    self.x = x
    self.y = y


</t>
<t tx="karstenw.20230303123148.100">def predecessor_path(tree, u, v):
    """ Returns the path between node u and node v as a list of node id's.
        The given tree is the return value of floyd_warshall_all_pairs_distance().predecessors.
    """
    def _traverse(u, v):
        w = tree[u][v]
        if w == u:
            return []
        return _traverse(u, w) + [w] + _traverse(w, v)
    return [u] + _traverse(u, v) + [v]

#--- GRAPH CENTRALITY ------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123148.101">def brandes_betweenness_centrality(graph, normalized=True, directed=False):
    """ Betweenness centrality for nodes in the graph.
        Betweenness centrality is a measure of the number of shortests paths that pass through a node.
        Nodes in high-density areas will get a good score.
    """
    # Ulrik Brandes, A Faster Algorithm for Betweenness Centrality,
    # Journal of Mathematical Sociology 25(2):163-177, 2001,
    # http://www.inf.uni-konstanz.de/algo/publications/b-fabc-01.pdf
    # Based on: Dijkstra's algorithm for shortest paths modified from Eppstein.
    # Based on: NetworkX 1.0.1: Aric Hagberg, Dan Schult and Pieter Swart.
    # http://python-networkx.sourcearchive.com/documentation/1.0.1/centrality_8py-source.html
    W = adjacency(graph, directed=directed)
    b = dict.fromkeys(graph, 0.0)
    for id in graph:
        Q = [] # Use Q as a heap with (distance, node id)-tuples.
        D = {} # Dictionary of final distances.
        P = {} # Dictionary of paths.
        for n in graph:
            P[n] = []
        seen = {id: 0}
        heappush(Q, (0, id, id))
        S = []
        E = dict.fromkeys(graph, 0) # sigma
        E[id] = 1.0
        while Q:
            (dist, pred, v) = heappop(Q)
            if v in D:
                continue
            D[v] = dist
            S.append(v)
            E[v] += E[pred]
            for w in W[v]:
                vw_dist = D[v] + W[v][w]
                if w not in D and (w not in seen or vw_dist &lt; seen[w]):
                    seen[w] = vw_dist
                    heappush(Q, (vw_dist, v, w))
                    P[w] = [v]
                    E[w] = 0.0
                elif vw_dist == seen[w]: # Handle equal paths.
                    P[w].append(v)
                    E[w] += E[v]
        d = dict.fromkeys(graph, 0.0)
        for w in reversed(S):
            for v in P[w]:
                d[v] += (1.0 + d[w]) * E[v] / E[w]
            if w != id:
                b[w] += d[w]
    # Normalize between 0.0 and 1.0.
    m = normalized and max(b.values()) or 1
    b = dict((id, w / m) for id, w in b.items())
    return b


</t>
<t tx="karstenw.20230303123148.102">def eigenvector_centrality(graph, normalized=True, reversed=True, rating={}, iterations=100, tolerance=0.0001):
    """ Eigenvector centrality for nodes in the graph (cfr. Google's PageRank).
        Eigenvector centrality is a measure of the importance of a node in a directed network. 
        It rewards nodes with a high potential of (indirectly) connecting to high-scoring nodes.
        Nodes with no incoming connections have a score of zero.
        If you want to measure outgoing connections, reversed should be False.        
    """
    # Based on: NetworkX, Aric Hagberg (hagberg@lanl.gov)
    # http://python-networkx.sourcearchive.com/documentation/1.0.1/centrality_8py-source.html
    # Note: much faster than betweenness centrality (which grows exponentially).
    def normalize(vector):
        w = 1.0 / (sum(vector.values()) or 1)
        for node in vector:
            vector[node] *= w
        return vector
    G = adjacency(graph, directed=True, reversed=reversed)
    v = normalize(dict([(n, random()) for n in graph])) # Node ID =&gt; weight vector.
    # Eigenvector calculation using the power iteration method: y = Ax.
    # It has no guarantee of convergence.
    for i in range(iterations):
        v0 = v
        v = dict.fromkeys(v0.keys(), 0)
        for n1 in v:
            for n2 in G[n1]:
                v[n1] += 0.01 + v0[n2] * G[n1][n2] * rating.get(n1, 1)
        normalize(v)
        e = sum([abs(v[n] - v0[n]) for n in v]) # Check for convergence.
        if e &lt; len(G) * tolerance:
            # Normalize between 0.0 and 1.0.
            m = normalized and max(v.values()) or 1
            v = dict((id, w / m) for id, w in v.items())
            return v
    warn("node weight is 0 because eigenvector_centrality() did not converge.", Warning)
    return dict((n, 0) for n in G)

#--- GRAPH PARTITIONING ----------------------------------------------------------------------------

# a | b =&gt; all elements from a and all the elements from b.
# a &amp; b =&gt; elements that appear in a as well as in b.
# a - b =&gt; elements that appear in a but not in b.


</t>
<t tx="karstenw.20230303123148.103">def union(a, b):
    return list(set(a) | set(b))


</t>
<t tx="karstenw.20230303123148.104">def intersection(a, b):
    return list(set(a) &amp; set(b))


</t>
<t tx="karstenw.20230303123148.105">def difference(a, b):
    return list(set(a) - set(b))


</t>
<t tx="karstenw.20230303123148.106">def partition(graph):
    """ Returns a list of unconnected subgraphs.
    """
    # Creates clusters of nodes and directly connected nodes.
    # Iteratively merges two clusters if they overlap.
    g = []
    for n in graph.nodes:
        g.append(dict.fromkeys((n.id for n in n.flatten()), True))
    for i in reversed(list(range(len(g)))):
        for j in reversed(list(range(i + 1, len(g)))):
            if g[i] and g[j] and len(intersection(g[i], g[j])) &gt; 0:
                g[i] = union(g[i], g[j])
                g[j] = []
    g = [graph.copy(nodes=[graph[id] for id in n]) for n in g if n]
    g.sort(key = cmp_to_key(lambda a, b: len(b) - len(a)))
    return g


</t>
<t tx="karstenw.20230303123148.107">def is_clique(graph):
    """ A clique is a set of nodes in which each node is connected to all other nodes.
    """
    #for n1 in graph.nodes:
    #    for n2 in graph.nodes:
    #        if n1 != n2 and graph.edge(n1.id, n2.id) is None:
    #            return False
    return graph.density == 1.0


</t>
<t tx="karstenw.20230303123148.108">def clique(graph, id):
    """ Returns the largest possible clique for the node with given id.
    """
    if isinstance(id, Node):
        id = id.id
    a = [id]
    for n in graph.nodes:
        try:
            # Raises StopIteration if all nodes in the clique are connected to n:
            next(id for id in a if n.id == id or graph.edge(n.id, id) is None)
        except StopIteration:
            a.append(n.id)
    return a


</t>
<t tx="karstenw.20230303123148.109">def cliques(graph, threshold=3):
    """ Returns all cliques in the graph with at least the given number of nodes.
    """
    a = []
    for n in graph.nodes:
        c = clique(graph, n.id)
        if len(c) &gt;= threshold:
            c.sort()
            if c not in a:
                a.append(c)
    return a

#### GRAPH UTILITY FUNCTIONS #######################################################################
# Utility functions for safely linking and unlinking of nodes,
# with respect for the surrounding nodes.


</t>
<t tx="karstenw.20230303123148.11">def coordinates(x, y, distance, angle):
    return (
        (x + distance * cos(radians(angle))),
        (y + distance * sin(radians(angle)))
    )

</t>
<t tx="karstenw.20230303123148.110">def unlink(graph, node1, node2=None):
    """ Removes the edges between node1 and node2.
        If only node1 is given, removes all edges to and from it.
        This does not remove node1 from the graph.
    """
    if not isinstance(node1, Node):
        node1 = graph[node1]
    if not isinstance(node2, Node) and node2 is not None:
        node2 = graph[node2]
    for e in list(graph.edges):
        if node1 in (e.node1, e.node2) and node2 in (e.node1, e.node2, None):
            graph.edges.remove(e)
            try:
                node1.links.remove(node2)
                node2.links.remove(node1)
            except: # 'NoneType' object has no attribute 'links'
                pass


</t>
<t tx="karstenw.20230303123148.111">def redirect(graph, node1, node2):
    """ Connects all of node1's edges to node2 and unlinks node1.
    """
    if not isinstance(node1, Node):
        node1 = graph[node1]
    if not isinstance(node2, Node):
        node2 = graph[node2]
    for e in graph.edges:
        if node1 in (e.node1, e.node2):
            if e.node1 == node1 and e.node2 != node2:
                graph._add_edge_copy(e, node1=node2, node2=e.node2)
            if e.node2 == node1 and e.node1 != node2:
                graph._add_edge_copy(e, node1=e.node1, node2=node2)
    unlink(graph, node1)


</t>
<t tx="karstenw.20230303123148.112">def cut(graph, node):
    """ Unlinks the given node, but keeps edges intact by connecting the surrounding nodes.
        If A, B, C, D are nodes and A-&gt;B, B-&gt;C, B-&gt;D, if we then cut B: A-&gt;C, A-&gt;D.
    """
    if not isinstance(node, Node):
        node = graph[node]
    for e in graph.edges:
        if node in (e.node1, e.node2):
            for n in node.links:
                if e.node1 == node and e.node2 != n:
                    graph._add_edge_copy(e, node1=n, node2=e.node2)
                if e.node2 == node and e.node1 != n:
                    graph._add_edge_copy(e, node1=e.node1, node2=n)
    unlink(graph, node)


</t>
<t tx="karstenw.20230303123148.113">def insert(graph, node, a, b):
    """ Inserts the given node between node a and node b.
        If A, B, C are nodes and A-&gt;B, if we then insert C: A-&gt;C, C-&gt;B.
    """
    if not isinstance(node, Node):
        node = graph[node]
    if not isinstance(a, Node):
        a = graph[a]
    if not isinstance(b, Node):
        b = graph[b]
    for e in graph.edges:
        if e.node1 == a and e.node2 == b:
            graph._add_edge_copy(e, node1=a, node2=node)
            graph._add_edge_copy(e, node1=node, node2=b)
        if e.node1 == b and e.node2 == a:
            graph._add_edge_copy(e, node1=b, node2=node)
            graph._add_edge_copy(e, node1=node, node2=a)
    unlink(graph, a, b)

#### GRAPH EXPORT ##################################################################################


</t>
<t tx="karstenw.20230303123148.114">class GraphRenderer(object):

    @others
#--- GRAPH EXPORT: HTML5 &lt;CANVAS&gt; ELEMENT ---------------------------------------------------------
# Exports graphs to interactive web pages using graph.js.


</t>
<t tx="karstenw.20230303123148.115">def __init__(self, graph):
    self.graph = graph

</t>
<t tx="karstenw.20230303123148.116">def serialize(self, *args, **kwargs):
    pass

</t>
<t tx="karstenw.20230303123148.117">def export(self, path, *args, **kwargs):
    pass

</t>
<t tx="karstenw.20230303123148.118">def minify(js):
    """ Returns a compressed Javascript string with comments and whitespace removed.
    """
    import re
    W = (
        "\(\[\{\,\;\=\-\+\*\/",
        "\)\]\}\,\;\=\-\+\*\/"
    )
    for a, b in (
      (re.compile(r"\/\*.*?\*\/", re.S), ""),    # multi-line comments /**/
      (re.compile(r"\/\/.*"), ""),               # singe line comments //
      (re.compile(r";\n"), "; "),                # statements (correctly) terminated with ;
      (re.compile(r"[ \t]+"), " "),              # spacing and indentation
      (re.compile(r"[ \t]([\(\[\{\,\;\=\-\+\*\/])"), "\\1"),
      (re.compile(r"([\)\]\}\,\;\=\-\+\*\/])[ \t]"), "\\1"),
      (re.compile(r"\s+\n"), "\n"),
      (re.compile(r"\n+"), "\n")):
        js = a.sub(b, js)
    return js.strip()

DEFAULT, INLINE = "default", "inline"
HTML, CANVAS, STYLE, CSS, SCRIPT, DATA = \
    "html", "canvas", "style", "css", "script", "data"


</t>
<t tx="karstenw.20230303123148.119">class HTMLCanvasRenderer(GraphRenderer):

    @others
#--- GRAPH EXPORT: GRAPHML ------------------------------------------------------------------------
# Exports graphs as GraphML XML, which can be read by Gephi (https://gephi.org).
# Author: Frederik Elwert &lt;frederik.elwert@web.de&gt;, 2014.

GRAPHML = "graphml"


</t>
<t tx="karstenw.20230303123148.12">def deepcopy(o):
    """ Returns a deep (recursive) copy of the given object.
    """
    if o is None:
        return o
    if hasattr(o, "copy"):
        return o.copy()
    if isinstance(o, (str, bool, int, float, complex)):
        return o
    if isinstance(o, (list, tuple, set)):
        return o.__class__(deepcopy(v) for v in o)
    if isinstance(o, dict):
        return dict((deepcopy(k), deepcopy(v)) for k, v in o.items())
    raise Exception("don't know how to copy %s" % o.__class__.__name__)

</t>
<t tx="karstenw.20230303123148.120">def __init__(self, graph, **kwargs):
    self.graph = graph
    self._source = \
        "&lt;!doctype html&gt;\n" \
        "&lt;html&gt;\n" \
        "&lt;head&gt;\n" \
            "\t&lt;title&gt;%s&lt;/title&gt;\n" \
            "\t&lt;meta charset=\"utf-8\"&gt;\n" \
            "\t%s\n" \
            "\t&lt;script type=\"text/javascript\" src=\"%scanvas.js\"&gt;&lt;/script&gt;\n" \
            "\t&lt;script type=\"text/javascript\" src=\"%sgraph.js\"&gt;&lt;/script&gt;\n" \
        "&lt;/head&gt;\n" \
        "&lt;body&gt;\n" \
            "\t&lt;div id=\"%s\" style=\"width:%spx; height:%spx;\"&gt;\n" \
                "\t\t&lt;script type=\"text/canvas\"&gt;\n" \
                    "\t\t%s\n" \
                "\t\t&lt;/script&gt;\n" \
            "\t&lt;/div&gt;\n" \
        "&lt;/body&gt;\n" \
        "&lt;/html&gt;"
    # HTML
    self.title      = "Graph" # &lt;title&gt;Graph&lt;/title&gt;
    self.javascript = None    # Path to canvas.js + graph.js.
    self.stylesheet = INLINE  # Either None, INLINE, DEFAULT (style.css) or a custom path.
    self.id         = "graph" # &lt;div id="graph"&gt;
    self.ctx        = "canvas.element"
    self.width      = 700     # Canvas width in pixels.
    self.height     = 500     # Canvas height in pixels.
    # JS Graph
    self.frames     = 500     # Number of frames of animation.
    self.fps        = 30      # Frames per second.
    self.ipf        = 2       # Iterations per frame.
    self.weighted   = False   # Indicate betweenness centrality as a shadow?
    self.directed   = False   # Indicate edge direction with an arrow?
    self.prune      = None    # None or int, calls Graph.prune() in Javascript.
    self.pack       = True    # Shortens leaf edges, adds eigenvector weight to node radius.
    # JS GraphLayout
    self.distance   = graph.distance         # Node spacing.
    self.k          = graph.layout.k         # Force constant.
    self.force      = graph.layout.force     # Force dampener.
    self.repulsion  = graph.layout.repulsion # Repulsive force radius.
    # Data
    self.weight     = [DEGREE, WEIGHT, CENTRALITY]
    self.href       = {}      # Dictionary of Node.id =&gt; URL.
    self.css        = {}      # Dictionary of Node.id =&gt; CSS classname.
    # Default options.
    # If a Node or Edge has one of these settings,
    # it is not passed to Javascript to save bandwidth.
    self.default = {
            "radius": 5,
             "fixed": False,
              "fill": None,
            "stroke": (0, 0, 0, 1),
       "strokewidth": 1,
              "text": (0, 0, 0, 1),
          "fontsize": 11,
    }
    # Override settings from keyword arguments.
    self.default.update(kwargs.pop("default", {}))
    for k, v in kwargs.items():
        setattr(self, k, v)

</t>
<t tx="karstenw.20230303123148.121">def _escape(self, s):
    if isinstance(s, str):
        return "\"%s\"" % s.replace("\"", "\\\"")
    return s

</t>
<t tx="karstenw.20230303123148.122">def _rgba(self, clr):
    # Color or tuple to a CSS "rgba(255,255,255,1.0)" string.
    return "\"rgba(%s,%s,%s,%.2f)\"" % (int(clr[0] * 255), int(clr[1] * 255), int(clr[2] * 255), clr[3])

</t>
<t tx="karstenw.20230303123148.123">@property
def data(self):
    """ Yields a string of Javascript code that loads the nodes and edges into variable g,
        which is a Javascript Graph object (see graph.js).
        This can be the response of an XMLHttpRequest, after wich you move g into your own variable.
    """
    return "".join(self._data())

</t>
<t tx="karstenw.20230303123148.124">def _data(self):
    s = []
    s.append("g = new Graph(%s, %s);\n" % (self.ctx, self.distance))
    s.append("var n = {")
    if len(self.graph.nodes) &gt; 0:
        s.append("\n")
    # Translate node properties to Javascript dictionary (var n).
    for n in self.graph.nodes:
        p = []
        if n._x != 0:
            p.append("x:%i" % n._x)                           # 0
        if n._y != 0:
            p.append("y:%i" % n._y)                           # 0
        if n.radius != self.default["radius"]:
            p.append("radius:%.1f" % n.radius)                # 5.0
        if n.fixed != self.default["fixed"]:
            p.append("fixed:%s" % repr(n.fixed).lower())      # false
        if n.fill != self.default["fill"]:
            p.append("fill:%s" % self._rgba(n.fill))          # [0,0,0,1.0]
        if n.stroke != self.default["stroke"]:
            p.append("stroke:%s" % self._rgba(n.stroke))      # [0,0,0,1.0]
        if n.strokewidth != self.default["strokewidth"]:
            p.append("strokewidth:%.1f" % n.strokewidth)      # 0.5
        if n.text is None:
            p.append("text:false")
        if n.text and n.text.fill != self.default["text"]:
            p.append("text:%s" % self._rgba(n.text.fill))     # [0,0,0,1.0]
        if n.text and "font" in n.text.__dict__:
            p.append("font:\"%s\"" % n.text.__dict__["font"]) # "sans-serif"
        if n.text and n.text.__dict__.get("fontsize", self.default["fontsize"]) != self.default["fontsize"]:
            p.append("fontsize:%i" % int(max(1, n.text.fontsize)))
        if n.text and "fontweight" in n.text.__dict__:        # "bold"
            p.append("fontweight:\"%s\"" % n.text.__dict__["fontweight"])
        if n.text and n.text.string != n.id:
            p.append("label:\"%s\"" % n.text.string)
        if n.id in self.href:
            p.append("href:\"%s\"" % self.href[n.id])
        if n.id in self.css:
            p.append("css:\"%s\"" % self.css[n.id])
        s.append("\t%s: {%s},\n" % (self._escape(n.id), ", ".join(p)))
    s[-1] = s[-1].rstrip(",\n") # Trailing comma breaks in IE.
    s.append("\n};\n")
    s.append("var e = [")
    if len(self.graph.edges) &gt; 0:
        s.append("\n")
    # Translate edge properties to Javascript dictionary (var e).
    for e in self.graph.edges:
        id1, id2 = self._escape(e.node1.id), self._escape(e.node2.id)
        p = []
        if e.weight != 0:
            p.append("weight:%.2f" % e.weight)                # 0.00
        if e.length != 1:
            p.append("length:%.2f" % e.length)                # 1.00
        if e.type is not None:
            p.append("type:\"%s\"" % e.type)                  # "is-part-of"
        if e.stroke != self.default["stroke"]:
            p.append("stroke:%s" % self._rgba(e.stroke))      # [0,0,0,1.0]
        if e.strokewidth != self.default["strokewidth"]:
            p.append("strokewidth:%.2f" % e.strokewidth)      # 0.5
        s.append("\t[%s, %s, {%s}],\n" % (id1, id2, ", ".join(p)))
    s[-1] = s[-1].rstrip(",\n") # Trailing comma breaks in IE.
    s.append("\n];\n")
    # Append the nodes to graph g.
    s.append("for (var id in n) {\n"
                "\tg.addNode(id, n[id]);\n"
             "}\n")
    # Append the edges to graph g.
    s.append("for (var i=0; i &lt; e.length; i++) {\n"
                "\tvar n1 = g.nodeset[e[i][0]];\n"
                "\tvar n2 = g.nodeset[e[i][1]];\n"
                "\tg.addEdge(n1, n2, e[i][2]);\n"
             "}")
    return s

</t>
<t tx="karstenw.20230303123148.125">@property
def script(self):
    """ Yields a string of canvas.js code.
        A setup() function loads the nodes and edges into variable g (Graph),
        A draw() function starts the animation and updates the layout of g.
    """
    return "".join(self._script())

</t>
<t tx="karstenw.20230303123148.126">def _script(self):
    s = []
    s.append("function setup(canvas) {\n")
    s.append("\tcanvas.size(%s, %s);\n" % (self.width, self.height))
    s.append("\tcanvas.fps = %s;\n" % (self.fps))
    s.append("\t" + "".join(self._data()).replace("\n", "\n\t"))
    s.append("\n")
    # Apply the layout settings.
    s.append("\tg.layout.k = %s; // Force constant (= edge length).\n"
                "\tg.layout.force = %s; // Repulsive strength.\n"
                "\tg.layout.repulsion = %s; // Repulsive radius.\n" % (
                    self.k,
                    self.force,
                    self.repulsion))
    # Apply eigenvector, betweenness and degree centrality.
    if self.weight is True:
        s.append(
                "\tg.eigenvectorCentrality();\n"
                "\tg.betweennessCentrality();\n"
                "\tg.degreeCentrality();\n")
    if isinstance(self.weight, (list, tuple)):
        if WEIGHT in self.weight:
            s.append(
                "\tg.eigenvectorCentrality();\n")
        if CENTRALITY in self.weight:
            s.append(
                "\tg.betweennessCentrality();\n")
        if DEGREE in self.weight:
            s.append(
                "\tg.degreeCentrality();\n")
    # Apply node weight to node radius.
    if self.pack:
        s.append(
                "\t// Apply Node.weight to Node.radius.\n"
                "\tfor (var i=0; i &lt; g.nodes.length; i++) {\n"
                    "\t\tvar n = g.nodes[i];\n"
                    "\t\tn.radius = n.radius + n.radius * n.weight;\n"
                "\t}\n")
    # Apply edge length (leaves get shorter edges).
    if self.pack:
        s.append(
                "\t// Apply Edge.length (leaves get shorter edges).\n"
                "\tfor (var i=0; i &lt; g.nodes.length; i++) {\n"
                    "\t\tvar e = g.nodes[i].edges();\n"
                    "\t\tif (e.length == 1) {\n"
                    "\t\t\te[0].length *= 0.2;\n"
                    "\t\t}\n"
                "\t}\n")
    # Apply pruning.
    if self.prune is not None:
        s.append(
                "\tg.prune(%s);\n" % self.prune)
    # Implement &lt;canvas&gt; draw().
    s.append("}\n")
    s.append("function draw(canvas) {\n"
                "\tif (g.layout.iterations &lt;= %s) {\n"
                    "\t\tcanvas.clear();\n"
                    "\t\t//shadow();\n"
                    "\t\tstroke(0);\n"
                    "\t\tfill(0,0);\n"
                    "\t\tg.update(%s);\n"
                    "\t\tg.draw(%s, %s);\n"
                "\t}\n"
                "\tg.drag(canvas.mouse);\n"
             "}" % (
        int(self.frames),
        int(self.ipf),
        str(self.weighted).lower(),
        str(self.directed).lower()))
    return s

</t>
<t tx="karstenw.20230303123148.127">@property
def canvas(self):
    """ Yields a string of HTML with a &lt;div id="graph"&gt; containing a &lt;script type="text/canvas"&gt;.
        The &lt;div id="graph"&gt; wrapper is required as a container for the node labels.
    """
    s = [
        "&lt;div id=\"%s\" style=\"width:%spx; height:%spx;\"&gt;\n" % (self.id, self.width, self.height),
            "\t&lt;script type=\"text/canvas\"&gt;\n",
            "\t\t%s\n" % self.script.replace("\n", "\n\t\t"),
            "\t&lt;/script&gt;\n",
        "&lt;/div&gt;"
    ]
    return "".join(s)

</t>
<t tx="karstenw.20230303123148.128">@property
def style(self):
    """ Yields a string of CSS for &lt;div id="graph"&gt;.
    """
    return \
        "body { font: 11px sans-serif; }\n" \
        "a { color: dodgerblue; }\n" \
        "#%s canvas { }\n" \
        "#%s .node-label { font-size: 11px; }\n" \
        "#%s {\n" \
            "\tdisplay: inline-block;\n" \
            "\tposition: relative;\n" \
            "\toverflow: hidden;\n" \
            "\tborder: 1px solid #ccc;\n" \
        "}" % (self.id, self.id, self.id)

</t>
<t tx="karstenw.20230303123148.129">@property
def html(self):
    """ Yields a string of HTML to visualize the graph using a force-based spring layout.
        The js parameter sets the path to graph.js and canvas.js.
    """
    js = self.javascript or ""
    if self.stylesheet == INLINE:
        css = self.style.replace("\n", "\n\t\t").rstrip("\t")
        css = "&lt;style type=\"text/css\"&gt;\n\t\t%s\n\t&lt;/style&gt;" % css
    elif self.stylesheet == DEFAULT:
        css = "&lt;link rel=\"stylesheet\" href=\"style.css\" type=\"text/css\" media=\"screen\" /&gt;"
    elif self.stylesheet is not None:
        css = "&lt;link rel=\"stylesheet\" href=\"%s\" type=\"text/css\" media=\"screen\" /&gt;" % self.stylesheet
    else:
        css = ""
    s = self._script()
    s = "".join(s)
    s = "\t" + s.replace("\n", "\n\t\t\t")
    s = s.rstrip()
    s = self._source % (
        self.title,
        css,
        js,
        js,
        self.id,
        self.width,
        self.height,
        s)
    return s

</t>
<t tx="karstenw.20230303123148.13">class Node(object):

    @others
#--- NODE LINKS ------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123148.130">def serialize(self, type=HTML):
    if type == HTML:
        return self.html
    if type == CANVAS:
        return self.canvas
    if type in (STYLE, CSS):
        return self.style
    if type == SCRIPT:
        return self.script
    if type == DATA:
        return self.data

# Backwards compatibility.
render = serialize

</t>
<t tx="karstenw.20230303123148.131">def export(self, path, encoding="utf-8"):
    """ Generates a folder at the given path containing an index.html
        that visualizes the graph using the HTML5 &lt;canvas&gt; tag.
    """
    if os.path.exists(path):
        rmtree(path)
    os.mkdir(path)
    # Copy compressed graph.js + canvas.js (unless a custom path is given.)
    if self.javascript is None:
        for p, f in (("..", "canvas.js"), (".", "graph.js")):
            a = open(os.path.join(MODULE, p, f), "r", encoding="utf-8")
            b = open(os.path.join(path, f), "w", encoding="utf-8")
            b.write(minify(a.read()))
            b.close()
    # Create style.css.
    if self.stylesheet == DEFAULT:
        f = open(os.path.join(path, "style.css"), "w")
        f.write(self.style)
        f.close()
    # Create index.html.
    f = open(os.path.join(path, "index.html"), "w", encoding=encoding)
    f.write(self.html)
    f.close()

</t>
<t tx="karstenw.20230303123148.132">class GraphMLRenderer(GraphRenderer):

    @others
#--------------------------------------------------------------------------------------------------
# The export() and serialize() function are called from Graph.export() and Graph.serialize(),
# and are expected to handle any GraphRenderer by specifying an optional type=HTML|GRAPHML.


</t>
<t tx="karstenw.20230303123148.133">def serialize(self, directed=False):
    p = "tmp.graphml"
    self.export(p, directed, encoding="utf-8")
    s = open(p, encoding="utf-8").read()
    os.unlink(p)
    return s

</t>
<t tx="karstenw.20230303123148.134">def export(self, path, directed=False, encoding="utf-8"):
    """ Generates a GraphML XML file at the given path.
    """
    import xml.etree.ElementTree as etree
    ns = "{http://graphml.graphdrawing.org/xmlns}"
    etree.register_namespace("", ns.strip("{}"))
    # Define type for node labels (string).
    # Define type for node edges (float).
    root = etree.Element(ns + "graphml")
    root.insert(0, etree.Element(ns + "key", **{
        "id": "node_label", "for": "node", "attr.name": "label", "attr.type": "string"
    }))
    root.insert(0, etree.Element(ns + "key", **{
        "id": "edge_weight", "for": "edge", "attr.name": "weight", "attr.type": "double"
    }))
    # Map Node.id =&gt; GraphML node id.
    m = {}
    g = etree.SubElement(root, ns + "graph", id="g", edgedefault=directed and "directed" or "undirected")
    # Export nodes.
    for i, n in enumerate(self.graph.nodes):
        m[n.id] = "node%s" % i
        x = etree.SubElement(g, ns + "node", id=m[n.id])
        x = etree.SubElement(x, ns + "data", key="node_label")
        if n.text and n.text.string != n.id:
            x.text = n.text.string
    # Export edges.
    for i, e in enumerate(self.graph.edges):
        x = etree.SubElement(g, ns + "edge", id="edge%s" % i, source=m[e.node1.id], target=m[e.node2.id])
        x = etree.SubElement(x, ns + "data", key="edge_weight")
        x.text = "%.3f" % e.weight
    # Export graph with pretty indented XML.
    # http://effbot.org/zone/element-lib.htm#prettyprint

    def indent(e, level=0):
        w = "\n" + level * "  "
        if len(e):
            if not e.text or not e.text.strip():
                e.text = w + "  "
            if not e.tail or not e.tail.strip():
                e.tail = w
            for e in e:
                indent(e, level + 1)
            if not e.tail or not e.tail.strip():
                e.tail = w
        else:
            if level and (not e.tail or not e.tail.strip()):
                e.tail = w
    indent(root)
    tree = etree.ElementTree(root)
    tree.write(path, encoding=encoding)

</t>
<t tx="karstenw.20230303123148.135">def export(graph, path, encoding="utf-8", **kwargs):
    type = kwargs.pop("type", HTML)
    # Export to GraphML.
    if type == GRAPHML or path.endswith(".graphml"):
        r = GraphMLRenderer(graph)
        return r.export(path, directed=kwargs.get("directed", False), encoding=encoding)
    # Export to HTML with &lt;canvas&gt;.
    if type == HTML:
        kwargs.setdefault("stylesheet", DEFAULT)
        r = HTMLCanvasRenderer(graph, **kwargs)
        return r.export(path, encoding)


</t>
<t tx="karstenw.20230303123148.136">def serialize(graph, type=HTML, **kwargs):
    # Return GraphML string.
    if type == GRAPHML:
        r = GraphMLRenderer(graph)
        return r.serialize(directed=kwargs.get("directed", False))
    # Return HTML string.
    if type in (HTML, CANVAS, STYLE, CSS, SCRIPT, DATA):
        kwargs.setdefault("stylesheet", INLINE)
        r = HTMLCanvasRenderer(graph, **kwargs)
        return r.serialize(type)

# Backwards compatibility.
write, render = export, serialize
</t>
<t tx="karstenw.20230303123148.14">def __init__(self, id="", radius=5, **kwargs):
    """ A node with a unique id in the graph.
        Node.id is drawn as a text label, unless optional parameter text=False.
        Optional parameters include: fill, stroke, strokewidth, text, font, fontsize, fontweight.
    """
    self.graph       = None
    self.links       = Links()
    self.id          = id
    self._x          = 0.0 # Calculated by Graph.layout.update().
    self._y          = 0.0 # Calculated by Graph.layout.update().
    self.force       = Vector(0.0, 0.0)
    self.radius      = radius
    self.fixed       = kwargs.pop("fixed", False)
    self.fill        = kwargs.pop("fill", None)
    self.stroke      = kwargs.pop("stroke", (0, 0, 0, 1))
    self.strokewidth = kwargs.pop("strokewidth", 1)
    self.text        = kwargs.get("text", True) and \
        Text(isinstance(id, str) and id or str(id),
               width = 85,
                fill = kwargs.pop("text", (0, 0, 0, 1)),
            fontsize = kwargs.pop("fontsize", 11), **kwargs) or None
    self._weight     = None # Calculated by Graph.eigenvector_centrality().
    self._centrality = None # Calculated by Graph.betweenness_centrality().

</t>
<t tx="karstenw.20230303123148.15">@property
def _distance(self):
    # Graph.distance controls the (x,y) spacing between nodes.
    return self.graph and float(self.graph.distance) or 1.0

</t>
<t tx="karstenw.20230303123148.16">def _get_x(self):
    return self._x * self._distance

</t>
<t tx="karstenw.20230303123148.17">def _get_y(self):
    return self._y * self._distance

</t>
<t tx="karstenw.20230303123148.18">def _set_x(self, v):
    self._x = v / self._distance

</t>
<t tx="karstenw.20230303123148.19">def _set_y(self, v):
    self._y = v / self._distance

x = property(_get_x, _set_x)
y = property(_get_y, _set_y)

</t>
<t tx="karstenw.20230303123148.2">def unique(iterable):
    """ Returns a list copy in which each item occurs only once (in-order).
    """
    seen = set()
    return [x for x in iterable if x not in seen and not seen.add(x)]

</t>
<t tx="karstenw.20230303123148.20">@property
def edges(self):
    """ Yields a list of edges from/to the node.
    """
    return self.graph is not None \
       and [e for e in self.graph.edges if self.id in (e.node1.id, e.node2.id)] \
        or []

</t>
<t tx="karstenw.20230303123148.21">@property
def edge(self, node, reverse=False):
    """ Yields the Edge from this node to the given node, or None.
    """
    if not isinstance(node, Node):
        node = self.graph and self.graph.get(node) or node
    if reverse:
        return node.links.edge(self)
    return self.links.edge(node)

</t>
<t tx="karstenw.20230303123148.22">@property
def weight(self):
    """ Yields eigenvector centrality as a number between 0.0-1.0.
    """
    if self.graph and self._weight is None:
        self.graph.eigenvector_centrality()
    return self._weight

</t>
<t tx="karstenw.20230303123148.23">@property
def centrality(self):
    """ Yields betweenness centrality as a number between 0.0-1.0.
    """
    if self.graph and self._centrality is None:
        self.graph.betweenness_centrality()
    return self._centrality

eigenvector = eigenvector_centrality = weight
betweenness = betweenness_centrality = centrality

</t>
<t tx="karstenw.20230303123148.24">@property
def degree(self):
    """ Yields degree centrality as a number between 0.0-1.0.
    """
    return self.graph and (1.0 * len(self.links) / len(self.graph)) or 0.0

</t>
<t tx="karstenw.20230303123148.25">def flatten(self, depth=1, traversable=lambda node, edge: True, _visited=None):
    """ Recursively lists the node and nodes linked to it.
        Depth 0 returns a list with the node.
        Depth 1 returns a list with the node and all the directly linked nodes.
        Depth 2 includes the linked nodes' links, and so on.
    """
    _visited = _visited or {}
    _visited[self.id] = (self, depth)
    if depth &gt;= 1:
        for n in self.links:
            if n.id not in _visited or _visited[n.id][1] &lt; depth - 1:
                if traversable(self, self.links.edges[n.id]):
                    n.flatten(depth - 1, traversable, _visited)
    return [n for n, d in _visited.values()] # Fast, but not order-preserving.

</t>
<t tx="karstenw.20230303123148.26">def draw(self, weighted=False):
    """ Draws the node as a circle with the given radius, fill, stroke and strokewidth.
        Draws the node centrality as a shadow effect when weighted=True.
        Draws the node text label.
        Override this method in a subclass for custom drawing.
    """
    # Draw the node weight as a shadow (based on node betweenness centrality).
    if weighted is not False and self.centrality &gt; (weighted and -1 or weighted):
        w = self.centrality * 35
        ellipse(
            self.x,
            self.y,
            self.radius * 2 + w,
            self.radius * 2 + w, fill=(0, 0, 0, 0.2), stroke=None)
    # Draw the node.
    ellipse(
        self.x,
        self.y,
        self.radius * 2,
        self.radius * 2, fill=self.fill, stroke=self.stroke, strokewidth=self.strokewidth)
    # Draw the node text label.
    if self.text:
        self.text.draw(
            self.x + self.radius,
            self.y + self.radius)

</t>
<t tx="karstenw.20230303123148.27">def contains(self, x, y):
    """ Returns True if the given coordinates (x, y) are inside the node radius.
    """
    return abs(self.x - x) &lt; self.radius * 2 and \
           abs(self.y - y) &lt; self.radius * 2

</t>
<t tx="karstenw.20230303123148.28">def __repr__(self):
    return "%s(id=%s)" % (self.__class__.__name__, repr(self.id))

</t>
<t tx="karstenw.20230303123148.29">def __eq__(self, node):
    return isinstance(node, Node) and self.id == node.id

</t>
<t tx="karstenw.20230303123148.3">def line(x1, y1, x2, y2, stroke=(0, 0, 0, 1), strokewidth=1):
    """ Draws a line from (x1, y1) to (x2, y2) using the given stroke color and stroke width.
    """
    pass


</t>
<t tx="karstenw.20230303123148.30">def __ne__(self, node):
    return not self.__eq__(node)

# This is required because we overwrite the parent's __eq__() method.
# Otherwise objects will be unhashable in Python 3.
# More information: http://docs.python.org/3.6/reference/datamodel.html#object.__hash__
__hash__ = object.__hash__

</t>
<t tx="karstenw.20230303123148.31">class Links(list):

    @others
#### EDGE ##########################################################################################


</t>
<t tx="karstenw.20230303123148.32">def __init__(self):
    """ A list in which each node has an associated edge.
        The Links.edge() method returns the edge for a given node id.
    """
    self.edges = dict()

</t>
<t tx="karstenw.20230303123148.33">def append(self, node, edge=None):
    if node.id not in self.edges:
        list.append(self, node)
    self.edges[node.id] = edge

</t>
<t tx="karstenw.20230303123148.34">def remove(self, node):
    list.remove(self, node)
    self.edges.pop(node.id, None)

</t>
<t tx="karstenw.20230303123148.35">def edge(self, node):
    return self.edges.get(isinstance(node, Node) and node.id or node)

</t>
<t tx="karstenw.20230303123148.36">class Edge(object):

    @others
#### GRAPH #########################################################################################

#--- GRAPH NODE DICTIONARY -------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123148.37">def __init__(self, node1, node2, weight=0.0, length=1.0, type=None, stroke=(0, 0, 0, 1), strokewidth=1):
    """ A connection between two nodes.
        Its weight indicates the importance (not the cost) of the connection.
        Its type is useful in a semantic network (e.g. "is-a", "is-part-of", ...)
    """
    self.node1 = node1
    self.node2 = node2
    self._weight = weight
    self.length = length
    self.type = type
    self.stroke = stroke
    self.strokewidth = strokewidth

</t>
<t tx="karstenw.20230303123148.38">def _get_weight(self):
    return self._weight

</t>
<t tx="karstenw.20230303123148.39">def _set_weight(self, v):
    self._weight = v
    # Clear cached adjacency map in the graph, since edge weights have changed.
    if self.node1.graph is not None:
        self.node1.graph._adjacency = None
    if self.node2.graph is not None:
        self.node2.graph._adjacency = None

weight = property(_get_weight, _set_weight)

</t>
<t tx="karstenw.20230303123148.4">def ellipse(x, y, width, height, fill=(0, 0, 0, 1), stroke=None, strokewidth=1):
    """ Draws an ellipse at (x, y) with given fill and stroke color and stroke width.
    """
    pass


</t>
<t tx="karstenw.20230303123148.40">def draw(self, weighted=False, directed=False):
    """ Draws the edge as a line with the given stroke and strokewidth (increased with Edge.weight).
        Override this method in a subclass for custom drawing.
    """
    w = weighted and self.weight or 0
    line(
        self.node1.x,
        self.node1.y,
        self.node2.x,
        self.node2.y, stroke=self.stroke, strokewidth=self.strokewidth + w)
    if directed:
        self.draw_arrow(stroke=self.stroke, strokewidth=self.strokewidth + w)

</t>
<t tx="karstenw.20230303123148.41">def draw_arrow(self, **kwargs):
    """ Draws the direction of the edge as an arrow on the rim of the receiving node.
    """
    x0, y0 = self.node1.x, self.node1.y
    x1, y1 = self.node2.x, self.node2.y
    # Find the edge's angle based on node1 and node2 position.
    a = degrees(atan2(y1 - y0, x1 - x0))
    # The arrow points to node2's rim instead of it's center.
    r = self.node2.radius
    d = sqrt(pow(x1 - x0, 2) + pow(y1 - y0, 2))
    x01, y01 = coordinates(x0, y0, d - r - 1, a)
    # Find the two other arrow corners under the given angle.
    r = max(kwargs.get("strokewidth", 1) * 3, 6)
    dx1, dy1 = coordinates(x01, y01, -r, a - 20)
    dx2, dy2 = coordinates(x01, y01, -r, a + 20)
    line(x01, y01, dx1, dy1, **kwargs)
    line(x01, y01, dx2, dy2, **kwargs)
    line(dx1, dy1, dx2, dy2, **kwargs)

</t>
<t tx="karstenw.20230303123148.42">def __repr__(self):
    return "%s(id1=%s, id2=%s)" % (self.__class__.__name__, repr(self.node1.id), repr(self.node2.id))

</t>
<t tx="karstenw.20230303123148.43">class nodedict(dict):

    @others
#--- GRAPH -----------------------------------------------------------------------------------------

# Graph layouts:
SPRING = "spring"

# Graph node centrality:
EIGENVECTOR = "eigenvector"
BETWEENNESS = "betweenness"
DEGREE = "degree"

# Graph node sort order:
WEIGHT, CENTRALITY = "weight", "centrality"

ALL = "all"


</t>
<t tx="karstenw.20230303123148.44">def __init__(self, graph, *args, **kwargs):
    """ Graph.shortest_paths() and Graph.eigenvector_centrality() return a nodedict,
        where dictionary values can be accessed by Node as well as by node id.
    """
    dict.__init__(self, *args, **kwargs)
    self.graph = graph

</t>
<t tx="karstenw.20230303123148.45">def __contains__(self, node):
    return dict.__contains__(self, self.graph.get(node, node))

</t>
<t tx="karstenw.20230303123148.46">def __getitem__(self, node):
    return dict.__getitem__(self, isinstance(node, Node) and node or self.graph[node])

</t>
<t tx="karstenw.20230303123148.47">def get(self, node, default=None):
    return dict.get(self, self.graph.get(node, node), default)

</t>
<t tx="karstenw.20230303123148.48">class Graph(dict):

    @others
#--- GRAPH LAYOUT ----------------------------------------------------------------------------------
# Graph drawing or graph layout, as a branch of graph theory,
# applies topology and geometry to derive two-dimensional representations of graphs.


</t>
<t tx="karstenw.20230303123148.49">def __init__(self, layout=SPRING, distance=10.0):
    """ A network of nodes connected by edges that can be drawn with a given layout.
    """
    self.nodes = []   # List of Node objects.
    self.edges = []   # List of Edge objects.
    self.root = None
    self._adjacency = None # Cached adjacency() dict.
    self.layout = layout == SPRING and GraphSpringLayout(self) or GraphLayout(self)
    self.distance = distance

</t>
<t tx="karstenw.20230303123148.5">class Text(object):

    @others
</t>
<t tx="karstenw.20230303123148.50">def __getitem__(self, id):
    try:
        return dict.__getitem__(self, id)
    except KeyError:
        raise KeyError("no node with id '%s' in graph" % id)

</t>
<t tx="karstenw.20230303123148.51">def append(self, base, *args, **kwargs):
    """ Appends a Node or Edge to the graph: Graph.append(Node, id="rabbit").
    """
    kwargs["base"] = base
    if issubclass(base, Node):
        return self.add_node(*args, **kwargs)
    if issubclass(base, Edge):
        return self.add_edge(*args, **kwargs)

</t>
<t tx="karstenw.20230303123148.52">def add_node(self, id, *args, **kwargs):
    """ Appends a new Node to the graph.
        An optional base parameter can be used to pass a subclass of Node.
    """
    n = kwargs.pop("base", Node)
    n = isinstance(id, Node) and id or self.get(id) or n(id, *args, **kwargs)
    if n.id not in self:
        self.nodes.append(n)
        self[n.id] = n
        n.graph = self
        self.root = kwargs.get("root", False) and n or self.root
        # Clear adjacency cache.
        self._adjacency = None
    return n

</t>
<t tx="karstenw.20230303123148.53">def add_edge(self, id1, id2, *args, **kwargs):
    """ Appends a new Edge to the graph.
        An optional base parameter can be used to pass a subclass of Edge:
        Graph.add_edge("cold", "winter", base=IsPropertyOf)
    """
    # Create nodes that are not yet part of the graph.
    n1 = self.add_node(id1)
    n2 = self.add_node(id2)
    # Creates an Edge instance.
    # If an edge (in the same direction) already exists, yields that edge instead.
    e1 = n1.links.edge(n2)
    if e1 and e1.node1 == n1 and e1.node2 == n2:
        return e1
    e2 = kwargs.pop("base", Edge)
    e2 = e2(n1, n2, *args, **kwargs)
    self.edges.append(e2)
    # Synchronizes Node.links:
    # A.links.edge(B) yields edge A-&gt;B
    # B.links.edge(A) yields edge B-&gt;A
    n1.links.append(n2, edge=e2)
    n2.links.append(n1, edge=e1 or e2)
    # Clear adjacency cache.
    self._adjacency = None
    return e2

</t>
<t tx="karstenw.20230303123148.54">def remove(self, x):
    """ Removes the given Node (and all its edges) or Edge from the graph.
        Note: removing Edge a-&gt;b does not remove Edge b-&gt;a.
    """
    if isinstance(x, Node) and x.id in self:
        self.pop(x.id)
        self.nodes.remove(x)
        x.graph = None
        # Remove all edges involving the given node.
        for e in list(self.edges):
            if x in (e.node1, e.node2):
                if x in e.node1.links:
                    e.node1.links.remove(x)
                if x in e.node2.links:
                    e.node2.links.remove(x)
                self.edges.remove(e)
    if isinstance(x, Edge):
        self.edges.remove(x)
    # Clear adjacency cache.
    self._adjacency = None

</t>
<t tx="karstenw.20230303123148.55">def node(self, id):
    """ Returns the node in the graph with the given id.
    """
    if isinstance(id, Node) and id.graph == self:
        return id
    return self.get(id, None)

</t>
<t tx="karstenw.20230303123148.56">def edge(self, id1, id2):
    """ Returns the edge between the nodes with given id1 and id2.
    """
    if isinstance(id1, Node) and id1.graph == self:
        id1 = id1.id
    if isinstance(id2, Node) and id2.graph == self:
        id2 = id2.id
    return id1 in self and id2 in self and self[id1].links.edge(id2) or None

</t>
<t tx="karstenw.20230303123148.57">def paths(self, node1, node2, length=4, path=[]):
    """ Returns a list of paths (shorter than or equal to given length) connecting the two nodes.
    """
    if not isinstance(node1, Node):
        node1 = self[node1]
    if not isinstance(node2, Node):
        node2 = self[node2]
    return [[self[id] for id in p] for p in paths(self, node1.id, node2.id, length, path)]

</t>
<t tx="karstenw.20230303123148.58">def shortest_path(self, node1, node2, heuristic=None, directed=False):
    """ Returns a list of nodes connecting the two nodes.
    """
    if not isinstance(node1, Node):
        node1 = self[node1]
    if not isinstance(node2, Node):
        node2 = self[node2]
    try:
        p = dijkstra_shortest_path(self, node1.id, node2.id, heuristic, directed)
        p = [self[id] for id in p]
        return p
    except IndexError:
        return None

</t>
<t tx="karstenw.20230303123148.59">def shortest_paths(self, node, heuristic=None, directed=False):
    """ Returns a dictionary of nodes, each linked to a list of nodes (shortest path).
    """
    if not isinstance(node, Node):
        node = self[node]
    p = nodedict(self)
    for id, path in dijkstra_shortest_paths(self, node.id, heuristic, directed).items():
        p[self[id]] = path and [self[id] for id in path] or None
    return p

</t>
<t tx="karstenw.20230303123148.6">def __init__(self, string, **kwargs):
    """ Draws the node label.
        Optional properties include width, fill, font, fontsize, fontweight.
    """
    self.string = string
    self.__dict__.update(kwargs)

</t>
<t tx="karstenw.20230303123148.60">def eigenvector_centrality(self, normalized=True, reversed=True, rating={}, iterations=100, tolerance=0.0001):
    """ Calculates eigenvector centrality and returns a node =&gt; weight dictionary.
        Node.weight is updated in the process.
        Node.weight is higher for nodes with a lot of (indirect) incoming traffic.
    """
    ec = eigenvector_centrality(self, normalized, reversed, rating, iterations, tolerance)
    ec = nodedict(self, ((self[id], w) for id, w in ec.items()))
    for n, w in ec.items():
        n._weight = w
    return ec

</t>
<t tx="karstenw.20230303123148.61">def betweenness_centrality(self, normalized=True, directed=False):
    """ Calculates betweenness centrality and returns a node =&gt; weight dictionary.
        Node.centrality is updated in the process.
        Node.centrality is higher for nodes with a lot of passing traffic.
    """
    bc = brandes_betweenness_centrality(self, normalized, directed)
    bc = nodedict(self, ((self[id], w) for id, w in bc.items()))
    for n, w in bc.items():
        n._centrality = w
    return bc

</t>
<t tx="karstenw.20230303123148.62">def sorted(self, order=WEIGHT, threshold=0.0):
    """ Returns a list of nodes sorted by WEIGHT or CENTRALITY.
        Nodes with a lot of traffic will be at the start of the list.
    """
    o = lambda node: getattr(node, order)
    nodes = sorted(self.nodes, key = o, reverse = True)
    return list([node for node in nodes if o(node) &gt;= threshold])

</t>
<t tx="karstenw.20230303123148.63">def prune(self, depth=0):
    """ Removes all nodes with less or equal links than depth.
    """
    for n in (n for n in self.nodes if len(n.links) &lt;= depth):
        self.remove(n)

</t>
<t tx="karstenw.20230303123148.64">def fringe(self, depth=0, traversable=lambda node, edge: True):
    """ For depth=0, returns the list of leaf nodes (nodes with only one connection).
        For depth=1, returns the list of leaf nodes and their connected nodes, and so on.
    """
    u = []
    [u.extend(n.flatten(depth, traversable)) for n in self.nodes if len(n.links) == 1]
    return unique(u)

</t>
<t tx="karstenw.20230303123148.65">@property
def density(self):
    """ Yields the number of edges vs. the maximum number of possible edges.
        For example, &lt;0.35 =&gt; sparse, &gt;0.65 =&gt; dense, 1.0 =&gt; complete.
    """
    return 2.0 * len(self.edges) / (len(self.nodes) * (len(self.nodes) - 1))

</t>
<t tx="karstenw.20230303123148.66">@property
def is_complete(self):
    return self.density == 1.0

</t>
<t tx="karstenw.20230303123148.67">@property
def is_dense(self):
    return self.density &gt; 0.65

</t>
<t tx="karstenw.20230303123148.68">@property
def is_sparse(self):
    return self.density &lt; 0.35

</t>
<t tx="karstenw.20230303123148.69">def split(self):
    """ Returns the list of unconnected subgraphs.
    """
    return partition(self)

</t>
<t tx="karstenw.20230303123148.7">def copy(self):
    k = self.__dict__.copy()
    k.pop("string")
    return Text(self.string, **k)

</t>
<t tx="karstenw.20230303123148.70">def update(self, iterations=10, **kwargs):
    """ Graph.layout.update() is called the given number of iterations.
    """
    for i in range(iterations):
        self.layout.update(**kwargs)

</t>
<t tx="karstenw.20230303123148.71">def draw(self, weighted=False, directed=False):
    """ Draws all nodes and edges.
    """
    for e in self.edges:
        e.draw(weighted, directed)
    for n in reversed(self.nodes): # New nodes (with Node._weight=None) first.
        n.draw(weighted)

</t>
<t tx="karstenw.20230303123148.72">def node_at(self, x, y):
    """ Returns the node at (x,y) or None.
    """
    for n in self.nodes:
        if n.contains(x, y):
            return n

</t>
<t tx="karstenw.20230303123148.73">def _add_node_copy(self, n, **kwargs):
    # Magical fairy dust to copy subclasses of Node.
    # We assume that the subclass constructor takes an optional "text" parameter
    # (Text objects in NodeBox for OpenGL's implementation are expensive).
    try:
        new = self.add_node(n.id, root=kwargs.get("root", False), text=False)
    except TypeError:
        new = self.add_node(n.id, root=kwargs.get("root", False))
    new.__class__ = n.__class__
    new.__dict__.update((k, deepcopy(v)) for k, v in n.__dict__.items()
        if k not in ("graph", "links", "_x", "_y", "force", "_weight", "_centrality"))

</t>
<t tx="karstenw.20230303123148.74">def _add_edge_copy(self, e, **kwargs):
    if kwargs.get("node1", e.node1).id not in self \
    or kwargs.get("node2", e.node2).id not in self:
        return
    new = self.add_edge(
        kwargs.get("node1", self[e.node1.id]),
        kwargs.get("node2", self[e.node2.id]))
    new.__class__ = e.__class__
    new.__dict__.update((k, deepcopy(v)) for k, v in e.__dict__.items()
        if k not in ("node1", "node2"))

</t>
<t tx="karstenw.20230303123148.75">def copy(self, nodes=ALL):
    """ Returns a copy of the graph with the given list of nodes (and connecting edges).
        The layout will be reset.
    """
    g = Graph(layout=None, distance=self.distance)
    g.layout = self.layout.copy(graph=g)
    for n in (nodes == ALL and self.nodes or (isinstance(n, Node) and n or self[n] for n in nodes)):
        g._add_node_copy(n, root=self.root == n)
    for e in self.edges:
        g._add_edge_copy(e)
    return g

</t>
<t tx="karstenw.20230303123148.76">def export(self, *args, **kwargs):
    export(self, *args, **kwargs)

</t>
<t tx="karstenw.20230303123148.77">def write(self, *args, **kwargs):
    write(self, *args, **kwargs)

</t>
<t tx="karstenw.20230303123148.78">def serialize(self, *args, **kwargs):
    return render(self, *args, **kwargs)

</t>
<t tx="karstenw.20230303123148.79">class GraphLayout(object):

    @others
#--- GRAPH LAYOUT: FORCE-BASED ---------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123148.8">def draw(self):
    pass


</t>
<t tx="karstenw.20230303123148.80">def __init__(self, graph):
    """ Calculates node positions iteratively when GraphLayout.update() is called.
    """
    self.graph = graph
    self.iterations = 0

</t>
<t tx="karstenw.20230303123148.81">def update(self):
    self.iterations += 1

</t>
<t tx="karstenw.20230303123148.82">def reset(self):
    self.iterations = 0
    for n in self.graph.nodes:
        n._x = 0.0
        n._y = 0.0
        n.force = Vector(0.0, 0.0)

</t>
<t tx="karstenw.20230303123148.83">@property
def bounds(self):
    """ Returns a (x, y, width, height)-tuple of the approximate layout dimensions.
    """
    x0, y0 = +INFINITE, +INFINITE
    x1, y1 = -INFINITE, -INFINITE
    for n in self.graph.nodes:
        if (n.x &lt; x0):
            x0 = n.x
        if (n.y &lt; y0):
            y0 = n.y
        if (n.x &gt; x1):
            x1 = n.x
        if (n.y &gt; y1):
            y1 = n.y
    return (x0, y0, x1 - x0, y1 - y0)

</t>
<t tx="karstenw.20230303123148.84">def copy(self, graph):
    return GraphLayout(self, graph)

</t>
<t tx="karstenw.20230303123148.85">class GraphSpringLayout(GraphLayout):

    @others
#### GRAPH ANALYSIS ################################################################################

#--- GRAPH SEARCH ----------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123148.86">def __init__(self, graph):
    """ A force-based layout in which edges are regarded as springs.
        The forces are applied to the nodes, pulling them closer or pushing them apart.
    """
    # Based on: http://snipplr.com/view/1950/graph-javascript-framework-version-001/
    GraphLayout.__init__(self, graph)
    self.k = 4.0  # Force constant.
    self.force = 0.01 # Force multiplier.
    self.repulsion = 50   # Maximum repulsive force radius.

</t>
<t tx="karstenw.20230303123148.87">def _distance(self, node1, node2):
    # Yields a tuple with distances (dx, dy, d, d**2).
    # Ensures that the distance is never zero (which deadlocks the animation).
    dx = node2._x - node1._x
    dy = node2._y - node1._y
    d2 = dx * dx + dy * dy
    if d2 &lt; 0.01:
        dx = random() * 0.1 + 0.1
        dy = random() * 0.1 + 0.1
        d2 = dx * dx + dy * dy
    return dx, dy, sqrt(d2), d2

</t>
<t tx="karstenw.20230303123148.88">def _repulse(self, node1, node2):
    # Updates Node.force with the repulsive force.
    dx, dy, d, d2 = self._distance(node1, node2)
    if d &lt; self.repulsion:
        f = self.k ** 2 / d2
        node2.force.x += f * dx
        node2.force.y += f * dy
        node1.force.x -= f * dx
        node1.force.y -= f * dy

</t>
<t tx="karstenw.20230303123148.89">def _attract(self, node1, node2, weight=0, length=1.0):
    # Updates Node.force with the attractive edge force.
    dx, dy, d, d2 = self._distance(node1, node2)
    d = min(d, self.repulsion)
    f = (d2 - self.k ** 2) / self.k * length
    f *= weight * 0.5 + 1
    f /= d
    node2.force.x -= f * dx
    node2.force.y -= f * dy
    node1.force.x += f * dx
    node1.force.y += f * dy

</t>
<t tx="karstenw.20230303123148.9">class Vector(object):

    @others
</t>
<t tx="karstenw.20230303123148.90">def update(self, weight=10.0, limit=0.5):
    """ Updates the position of nodes in the graph.
        The weight parameter determines the impact of edge weight.
        The limit parameter determines the maximum movement each update().
    """
    GraphLayout.update(self)
    # Forces on all nodes due to node-node repulsions.
    for i, n1 in enumerate(self.graph.nodes):
        for j, n2 in enumerate(self.graph.nodes[i + 1:]):
            self._repulse(n1, n2)
    # Forces on nodes due to edge attractions.
    for e in self.graph.edges:
        self._attract(e.node1, e.node2, weight * e.weight, 1.0 / (e.length or 0.01))
    # Move nodes by given force.
    for n in self.graph.nodes:
        if not n.fixed:
            n._x += max(-limit, min(self.force * n.force.x, limit))
            n._y += max(-limit, min(self.force * n.force.y, limit))
        n.force.x = 0
        n.force.y = 0

</t>
<t tx="karstenw.20230303123148.91">def copy(self, graph):
    g = GraphSpringLayout(graph)
    g.k, g.force, g.repulsion = self.k, self.force, self.repulsion
    return g

</t>
<t tx="karstenw.20230303123148.92">def depth_first_search(node, visit=lambda node: False, traversable=lambda node, edge: True, _visited=None):
    """ Visits all the nodes connected to the given root node, depth-first.
        The visit function is called on each node.
        Recursion will stop if it returns True, and subsequently dfs() will return True.
        The traversable function takes the current node and edge,
        and returns True if we are allowed to follow this connection to the next node.
        For example, the traversable for directed edges is follows:
         lambda node, edge: node == edge.node1
    """
    stop = visit(node)
    _visited = _visited or {}
    _visited[node.id] = True
    for n in node.links:
        if stop:
            return True
        if traversable(node, node.links.edge(n)) is False:
            continue
        if n.id not in _visited:
            stop = depth_first_search(n, visit, traversable, _visited)
    return stop

dfs = depth_first_search


</t>
<t tx="karstenw.20230303123148.93">def breadth_first_search(node, visit=lambda node: False, traversable=lambda node, edge: True):
    """ Visits all the nodes connected to the given root node, breadth-first.
    """
    q = [node]
    _visited = {}
    while q:
        node = q.pop(0)
        if node.id not in _visited:
            if visit(node):
                return True
            q.extend((n for n in node.links if traversable(node, node.links.edge(n)) is not False))
            _visited[node.id] = True
    return False

bfs = breadth_first_search


</t>
<t tx="karstenw.20230303123148.94">def paths(graph, id1, id2, length=4, path=[], _root=True):
    """ Returns a list of paths from node with id1 to node with id2.
        Only paths shorter than or equal to the given length are included.
        Uses a brute-force DFS approach (performance drops exponentially for longer paths).
    """
    if len(path) &gt;= length:
        return []
    if id1 not in graph:
        return []
    if id1 == id2:
        return [path + [id1]]
    path = path + [id1]
    p = []
    s = set(path) # 5% speedup.
    for node in graph[id1].links:
        if node.id not in s:
            p.extend(paths(graph, node.id, id2, length, path, False))
    return _root and sorted(p, key=len) or p


</t>
<t tx="karstenw.20230303123148.95">def edges(path):
    """ Returns an iterator of Edge objects for the given list of nodes.
        It yields None where two successive nodes are not connected.
    """
    # For example, the distance (i.e., edge weight sum) of a path:
    # sum(e.weight for e in edges(path))
    return len(path) &gt; 1 and (n.links.edge(path[i + 1]) for i, n in enumerate(path[:-1])) or iter(())

#--- GRAPH ADJACENCY -------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123148.96">def adjacency(graph, directed=False, reversed=False, stochastic=False, heuristic=None):
    """ Returns a dictionary indexed by node id1's,
        in which each value is a dictionary of connected node id2's linking to the edge weight.
        If directed=True, edges go from id1 to id2, but not the other way.
        If stochastic=True, all the weights for the neighbors of a given node sum to 1.
        A heuristic function can be given that takes two node id's and returns
        an additional cost for movement between the two nodes.
    """
    # Caching a heuristic from a method won't work.
    # Bound method objects are transient,
    # i.e., id(object.method) returns a new value each time.
    if graph._adjacency is not None and \
       graph._adjacency[1:] == (directed, reversed, stochastic, heuristic and heuristic.__code__):
        return graph._adjacency[0]
    map = {}
    for n in graph.nodes:
        map[n.id] = {}
    for e in graph.edges:
        id1, id2 = not reversed and (e.node1.id, e.node2.id) or (e.node2.id, e.node1.id)
        map[id1][id2] = 1.0 - 0.5 * e.weight
        if heuristic:
            map[id1][id2] += heuristic(id1, id2)
        if not directed:
            map[id2][id1] = map[id1][id2]
    if stochastic:
        for id1 in map:
            n = sum(map[id1].values())
            for id2 in map[id1]:
                map[id1][id2] /= n
    # Cache the adjacency map: this makes dijkstra_shortest_path() 2x faster in repeated use.
    graph._adjacency = (map, directed, reversed, stochastic, heuristic and heuristic.__code__)
    return map


</t>
<t tx="karstenw.20230303123148.97">def dijkstra_shortest_path(graph, id1, id2, heuristic=None, directed=False):
    """ Dijkstra algorithm for finding the shortest path between two nodes.
        Returns a list of node id's, starting with id1 and ending with id2.
        Raises an IndexError between nodes on unconnected graphs.
    """
    # Based on: Connelly Barnes, http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/119466
    def flatten(list):
        # Flattens a linked list of the form [0,[1,[2,[]]]]
        while len(list) &gt; 0:
            yield list[0]
            list = list[1]
    G = adjacency(graph, directed=directed, heuristic=heuristic)
    q = [(0, id1, ())] # Heap of (cost, path_head, path_rest).
    visited = set()    # Visited nodes.
    while True:
        (cost1, n1, path) = heappop(q)
        if n1 not in visited:
            visited.add(n1)
        if n1 == id2:
            return list(flatten(path))[::-1] + [n1]
        path = (n1, path)
        for (n2, cost2) in G[n1].items():
            if n2 not in visited:
                heappush(q, (cost1 + cost2, n2, path))


</t>
<t tx="karstenw.20230303123148.98">def dijkstra_shortest_paths(graph, id, heuristic=None, directed=False):
    """ Dijkstra algorithm for finding the shortest paths from the given node to all other nodes.
        Returns a dictionary of node id's, each linking to a list of node id's (i.e., the path).
    """
    # Based on: Dijkstra's algorithm for shortest paths modified from Eppstein.
    # Based on: NetworkX 1.4.1: Aric Hagberg, Dan Schult and Pieter Swart.
    # This is 5x faster than:
    # for n in g: dijkstra_shortest_path(g, id, n.id)
    W = adjacency(graph, directed=directed, heuristic=heuristic)
    Q = [] # Use Q as a heap with (distance, node id)-tuples.
    D = {} # Dictionary of final distances.
    P = {} # Dictionary of paths.
    P[id] = [id]
    seen = {id: 0}
    heappush(Q, (0, id))
    while Q:
        (dist, v) = heappop(Q)
        if v in D:
            continue
        D[v] = dist
        for w in W[v].keys():
            vw_dist = D[v] + W[v][w]
            if w not in D and (w not in seen or vw_dist &lt; seen[w]):
                seen[w] = vw_dist
                heappush(Q, (vw_dist, w))
                P[w] = P[v] + [w]
    for n in graph:
        if n not in P:
            P[n] = None
    return P


</t>
<t tx="karstenw.20230303123148.99">def floyd_warshall_all_pairs_distance(graph, heuristic=None, directed=False):
    """ Floyd-Warshall's algorithm for finding the path length for all pairs for nodes.
        Returns a dictionary of node id's, 
        each linking to a dictionary of node id's linking to path length.
    """
    from collections import defaultdict
    g = graph.keys()
    d = defaultdict(lambda: defaultdict(lambda: 1e30)) # float('inf')
    p = defaultdict(dict) # Predecessors.
    for e in graph.edges:
        u = e.node1.id
        v = e.node2.id
        w = 1.0 - 0.5 * e.weight
        w = heuristic and heuristic(u, v) + w or w
        d[u][v] = min(w, d[u][v])
        d[u][u] = 0
        p[u][v] = u
        if not directed:
            d[v][u] = min(w, d[v][u])
            p[v][u] = v
    for w in g:
        dw = d[w]
        for u in g:
            du, duw = d[u], d[u][w]
            for v in g:
                # Performance optimization, assumes d[w][v] &gt; 0.
                #if du[v] &gt; duw + dw[v]:
                if du[v] &gt; duw and du[v] &gt; duw + dw[v]:
                    d[u][v] = duw + dw[v]
                    p[u][v] = p[w][v]

    class pdict(dict):
        def __init__(self, predecessors, *args, **kwargs):
            dict.__init__(self, *args, **kwargs)
            self.predecessors = predecessors
    return pdict(p, ((u, dict((v, w) for v, w in d[u].items() if w &lt; 1e30)) for u in d))


</t>
<t tx="karstenw.20230303123158.1">#### PATTERN | COMMONSENSE #########################################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303123206.1">from __future__ import print_function
from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

from codecs import BOM_UTF8
from itertools import chain
from functools import cmp_to_key

from io import open

try:
    # Python 2
    from urllib import urlopen
except ImportError:
    # Python 3
    from urllib.request import urlopen

from .__init__ import Graph, Node, Edge, bfs
from .__init__ import WEIGHT, CENTRALITY, EIGENVECTOR, BETWEENNESS

import os
import sys

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

if sys.version &gt; "3":
    BOM_UTF8 = str(BOM_UTF8.decode("utf-8"))
else:
    BOM_UTF8 = BOM_UTF8.decode("utf-8")

</t>
<t tx="karstenw.20230303123206.10">#--- COMMONSENSE -----------------------------------------------------------------------------------


class Commonsense(Graph):

    @others
#g = Commonsense()
#print(g.nn("party", g.field("animal")))
#print(g.nn("creepy", g.field("animal")))

</t>
<t tx="karstenw.20230303123206.11">def __init__(self, data=os.path.join(MODULE, "commonsense.csv"), **kwargs):
    """ A semantic network of commonsense, using different relation types:
        - is-a,
        - is-part-of,
        - is-opposite-of,
        - is-property-of,
        - is-related-to,
        - is-same-as,
        - is-effect-of.
    """
    Graph.__init__(self, **kwargs)
    self._properties = None
    # Load data from the given path,
    # a CSV-file of (concept1, relation, concept2, context, weight)-items.
    if data is not None:
        s = open(data, encoding = 'utf-8').read()
        s = s.strip(BOM_UTF8)
        s = ((v.strip("\"") for v in r.split(",")) for r in s.splitlines())
        for concept1, relation, concept2, context, weight in s:
            self.add_edge(concept1, concept2,
                type = relation,
             context = context,
              weight = min(int(weight) * 0.1, 1.0))

</t>
<t tx="karstenw.20230303123206.12">@property
def concepts(self):
    return self.nodes

</t>
<t tx="karstenw.20230303123206.13">@property
def relations(self):
    return self.edges

</t>
<t tx="karstenw.20230303123206.14">@property
def properties(self):
    """ Yields all concepts that are properties (i.e., adjectives).
        For example: "cold is-property-of winter" =&gt; "cold".
    """
    if self._properties is None:
        #self._properties = set(e.node1.id for e in self.edges if e.type == "is-property-of")
        self._properties = (e for e in self.edges if e.context == "properties")
        self._properties = set(chain(*((e.node1.id, e.node2.id) for e in self._properties)))
    return self._properties

</t>
<t tx="karstenw.20230303123206.15">def add_node(self, id, *args, **kwargs):
    """ Returns a Concept (Node subclass).
    """
    self._properties = None
    kwargs.setdefault("base", Concept)
    return Graph.add_node(self, id, *args, **kwargs)

</t>
<t tx="karstenw.20230303123206.16">def add_edge(self, id1, id2, *args, **kwargs):
    """ Returns a Relation between two concepts (Edge subclass).
    """
    self._properties = None
    kwargs.setdefault("base", Relation)
    return Graph.add_edge(self, id1, id2, *args, **kwargs)

</t>
<t tx="karstenw.20230303123206.17">def remove(self, x):
    self._properties = None
    Graph.remove(self, x)

</t>
<t tx="karstenw.20230303123206.18">def similarity(self, concept1, concept2, k=3, heuristic=COMMONALITY):
    """ Returns the similarity of the given concepts,
        by cross-comparing shortest path distance between k concept properties.
        A given concept can also be a flat list of properties, e.g. ["creepy"].
        The given heuristic is a tuple of two functions:
        1) function(concept) returns a list of salient properties,
        2) function(edge) returns the cost for traversing this edge (0.0-1.0).
    """
    if isinstance(concept1, str):
        concept1 = self[concept1]
    if isinstance(concept2, str):
        concept2 = self[concept2]
    if isinstance(concept1, Node):
        concept1 = heuristic[0](concept1)
    if isinstance(concept2, Node):
        concept2 = heuristic[0](concept2)
    if isinstance(concept1, list):
        concept1 = [isinstance(n, Node) and n or self[n] for n in concept1]
    if isinstance(concept2, list):
        concept2 = [isinstance(n, Node) and n or self[n] for n in concept2]
    h = lambda id1, id2: heuristic[1](self.edge(id1, id2))
    w = 0.0
    for p1 in concept1[:k]:
        for p2 in concept2[:k]:
            p = self.shortest_path(p1, p2, heuristic=h)
            w += 1.0 / (p is None and 1e10 or len(p))
    return w / k

</t>
<t tx="karstenw.20230303123206.19">def nearest_neighbors(self, concept, concepts=[], k=3):
    """ Returns the k most similar concepts from the given list.
    """
    return sorted(concepts, key=lambda candidate: self.similarity(concept, candidate, k), reverse=True)

similar = neighbors = nn = nearest_neighbors

</t>
<t tx="karstenw.20230303123206.2">#--- CONCEPT ---------------------------------------------------------------------------------------


class Concept(Node):

    @others
</t>
<t tx="karstenw.20230303123206.20">def taxonomy(self, concept, depth=3, fringe=2):
    """ Returns a list of concepts that are descendants of the given concept, using "is-a" relations.
        Creates a subgraph of "is-a" related concepts up to the given depth,
        then takes the fringe (i.e., leaves) of the subgraph.
    """
    def traversable(node, edge):
        # Follow parent-child edges.
        return edge.node2 == node and edge.type == "is-a"
    if not isinstance(concept, Node):
        concept = self[concept]
    g = self.copy(nodes=concept.flatten(depth, traversable))
    g = g.fringe(depth=fringe)
    g = [self[n.id] for n in g if n != concept]
    return g

field = semantic_field = taxonomy

</t>
<t tx="karstenw.20230303123206.21">def download(path=os.path.join(MODULE, "commonsense.csv"), threshold=50):
    """ Downloads commonsense data from http://nodebox.net/perception.
        Saves the data as commonsense.csv which can be the input for Commonsense.load().
    """
    s = "http://nodebox.net/perception?format=txt&amp;robots=1"
    s = urlopen(s).read()
    s = s.decode("utf-8")
    s = s.replace("\\'", "'")
    # Group relations by author.
    a = {}
    for r in ([v.strip("'") for v in r.split(", ")] for r in s.split("\n")):
        if len(r) == 7:
            a.setdefault(r[-2], []).append(r)
    # Iterate authors sorted by number of contributions.
    # 1) Authors with 50+ contributions can define new relations and context.
    # 2) Authors with 50- contributions (or robots) can only reinforce existing relations.
    a = sorted(a.items(), key=cmp_to_key(lambda v1, v2: len(v2[1]) - len(v1[1])))
    r = {}
    for author, relations in a:
        if author == "" or author.startswith("robots@"):
            continue
        if len(relations) &lt; threshold:
            break
        # Sort latest-first (we prefer more recent relation types).
        relations = sorted(relations, key=cmp_to_key(lambda r1, r2: r1[-1] &gt; r2[-1]))
        # 1) Define new relations.
        for concept1, relation, concept2, context, weight, author, date in relations:
            id = (concept1, relation, concept2)
            if id not in r:
                r[id] = [None, 0]
            if r[id][0] is None and context is not None:
                r[id][0] = context
    for author, relations in a:
        # 2) Reinforce existing relations.
        for concept1, relation, concept2, context, weight, author, date in relations:
            id = (concept1, relation, concept2)
            if id in r:
                r[id][1] += int(weight)
    # Export CSV-file.
    s = []
    for (concept1, relation, concept2), (context, weight) in r.items():
        s.append("\"%s\",\"%s\",\"%s\",\"%s\",%s" % (
            concept1, relation, concept2, context, weight))
    f = open(path, "w", encoding = 'utf-8')
    f.write(BOM_UTF8)
    f.write("\n".join(s))
    f.close()


</t>
<t tx="karstenw.20230303123206.22">def json():
    """ Returns a JSON-string with the data from commonsense.csv.
        Each relation is encoded as a [concept1, relation, concept2, context, weight] list.
    """
    f = lambda s: s.replace("'", "\\'").encode("utf-8")
    s = []
    g = Commonsense()
    for e in g.edges:
        s.append("\n\t['%s', '%s', '%s', '%s', %.2f]" % (
            f(e.node1.id),
            f(e.type),
            f(e.node2.id),
            f(e.context),
              e.weight
        ))
    return "commonsense = [%s];" % ", ".join(s)

#download("commonsense.csv", threshold=50)
#open("commonsense.js", "w", encoding = 'utf-8').write(json())
</t>
<t tx="karstenw.20230303123206.3">def __init__(self, *args, **kwargs):
    """ A concept in the sematic network.
    """
    Node.__init__(self, *args, **kwargs)
    self._properties = None

</t>
<t tx="karstenw.20230303123206.4">@property
def halo(self, depth=2):
    """ Returns the concept halo: a list with this concept + surrounding concepts.
        This is useful to reason more fluidly about the concept,
        since the halo will include latent properties linked to nearby concepts.
    """
    return self.flatten(depth=depth)

</t>
<t tx="karstenw.20230303123206.5">@property
def properties(self):
    """ Returns the top properties in the concept halo, sorted by betweenness centrality.
        The return value is a list of concept id's instead of Concepts (for performance).
    """
    if self._properties is None:
        g = self.graph.copy(nodes=self.halo)
        p = (n for n in g.nodes if n.id in self.graph.properties)
        p = [n.id for n in reversed(sorted(p, key=lambda n: n.centrality))]
        self._properties = p
    return self._properties


</t>
<t tx="karstenw.20230303123206.6">def halo(concept, depth=2):
    return concept.flatten(depth=depth)


</t>
<t tx="karstenw.20230303123206.7">def properties(concept, depth=2, centrality=BETWEENNESS):
    g = concept.graph.copy(nodes=halo(concept, depth))
    p = (n for n in g.nodes if n.id in concept.graph.properties)
    p = [n.id for n in reversed(sorted(p, key=lambda n: getattr(n, centrality)))]
    return p

</t>
<t tx="karstenw.20230303123206.8">#--- RELATION --------------------------------------------------------------------------------------


class Relation(Edge):

    @others
</t>
<t tx="karstenw.20230303123206.9">def __init__(self, *args, **kwargs):
    """ A relation between two concepts, with an optional context.
        For example, "Felix is-a cat" is in the "media" context, "tiger is-a cat" in "nature".
    """
    self.context = kwargs.pop("context", None)
    Edge.__init__(self, *args, **kwargs)

</t>
<t tx="karstenw.20230303123256.1">@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303123309.1">from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

#--- STRING FUNCTIONS ------------------------------------------------------------------------------
# Latin-1 (ISO-8859-1) encoding is identical to Windows-1252 except for the code points 128-159:
# Latin-1 assigns control codes in this range, Windows-1252 has characters, punctuation, symbols
# assigned to these code points.


</t>
<t tx="karstenw.20230303123309.2">def decode_string(v, encoding="utf-8"):
    """ Returns the given value as a Unicode string (if possible).
    """
    if isinstance(encoding, str):
        encoding = ((encoding,),) + (("windows-1252",), ("utf-8", "ignore"))
    if isinstance(v, bytes):
        for e in encoding:
            try:
                return v.decode(*e)
            except:
                pass
        return v
    return str(v)


</t>
<t tx="karstenw.20230303123309.3">def encode_string(v, encoding="utf-8"):
    """ Returns the given value as a Python byte string (if possible).
    """
    if isinstance(encoding, str):
        encoding = ((encoding,),) + (("windows-1252",), ("utf-8", "ignore"))
    if isinstance(v, str):
        for e in encoding:
            try:
                return v.encode(*e)
            except:
                pass
        return v
    return bytes(v)

decode_utf8 = decode_string
encode_utf8 = encode_string
</t>
<t tx="karstenw.20230303123315.1">#### PATTERN | METRICS #############################################################################
# coding: utf-8
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303123322.1">from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

from io import open

import sys

from time import time
from math import sqrt, floor, ceil, modf, exp, pi, log

from collections import Counter, defaultdict, deque
from itertools import chain
from operator import itemgetter, lt, le
from heapq import nlargest
from bisect import bisect_right
from random import gauss

#--- FREQUENCY DICT --------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123322.10">def cumsum(iterable):
    """ Returns an iterator over the cumulative sum of values in the given list.
    """
    n = 0
    for x in iterable:
        n += x
        yield n

#### PROFILER ######################################################################################


</t>
<t tx="karstenw.20230303123322.11">def duration(function, *args, **kwargs):
    """ Returns the running time of the given function, in seconds.
    """
    t = time()
    function(*args, **kwargs)
    return time() - t


</t>
<t tx="karstenw.20230303123322.12">def profile(function, *args, **kwargs):
    """ Returns the performance analysis (as a string) of the given Python function.
    """
    def run():
        function(*args, **kwargs)
    if not hasattr(function, "__call__"):
        raise TypeError("%s is not a function" % type(function))
    try:
        import cProfile as profile
    except:
        import profile
    import pstats
    import os
    import sys
    sys.modules["__main__"].__profile_run__ = run
    id = function.__name__ + "()"
    profile.run("__profile_run__()", id)
    p = pstats.Stats(id)
    p.stream = open(id, "w")
    p.sort_stats("cumulative").print_stats(30)
    p.stream.close()
    s = open(id).read()
    os.remove(id)
    return s


</t>
<t tx="karstenw.20230303123322.13">def sizeof(object):
    """ Returns the memory size of the given object (in bytes).
    """
    return sys.getsizeof(object)


</t>
<t tx="karstenw.20230303123322.14">def kb(object):
    """ Returns the memory size of the given object (in kilobytes).
    """
    return sys.getsizeof(object) * 0.01

#### PRECISION &amp; RECALL ############################################################################

ACCURACY, PRECISION, RECALL, F1_SCORE = "accuracy", "precision", "recall", "F1-score"

MACRO = "macro"


</t>
<t tx="karstenw.20230303123322.15">def confusion_matrix(classify=lambda document: False, documents=[(None, False)]):
    """ Returns the performance of a binary classification task (i.e., predicts True or False)
        as a tuple of (TP, TN, FP, FN):
        - TP: true positives  = correct hits, 
        - TN: true negatives  = correct rejections,
        - FP: false positives = false alarm (= type I error), 
        - FN: false negatives = misses (= type II error).
        The given classify() function returns True or False for a document.
        The list of documents contains (document, bool)-tuples for testing,
        where True means a document that should be identified as True by classify().
    """
    TN = TP = FN = FP = 0
    for document, b1 in documents:
        b2 = classify(document)
        if b1 and b2:
            TP += 1 # true positive
        elif not b1 and not b2:
            TN += 1 # true negative
        elif not b1 and b2:
            FP += 1 # false positive (type I error)
        elif b1 and not b2:
            FN += 1 # false negative (type II error)
    return TP, TN, FP, FN


</t>
<t tx="karstenw.20230303123322.16">def test(classify=lambda document: False, documents=[], average=None):
    """ Returns an (accuracy, precision, recall, F1-score)-tuple.
        With average=None, precision &amp; recall are computed for the positive class (True).
        With average=MACRO, precision &amp; recall for positive and negative class are macro-averaged.
    """
    TP, TN, FP, FN = confusion_matrix(classify, documents)
    A = float(TP + TN) / ((TP + TN + FP + FN) or 1)
    P1 = float(TP) / ((TP + FP) or 1) # positive class precision
    R1 = float(TP) / ((TP + FN) or 1) # positive class recall
    P0 = float(TN) / ((TN + FN) or 1) # negative class precision
    R0 = float(TN) / ((TN + FP) or 1) # negative class recall
    if average is None:
        P, R = (P1, R1)
    if average == MACRO:
        P, R = ((P1 + P0) / 2,
                (R1 + R0) / 2)
    F1 = 2 * P * R / ((P + R) or 1)
    return (A, P, R, F1)


</t>
<t tx="karstenw.20230303123322.17">def accuracy(classify=lambda document: False, documents=[], average=None):
    """ Returns the percentage of correct classifications (true positives + true negatives).
    """
    return test(classify, documents, average)[0]


</t>
<t tx="karstenw.20230303123322.18">def precision(classify=lambda document: False, documents=[], average=None):
    """ Returns the percentage of correct positive classifications.
    """
    return test(classify, documents, average)[1]


</t>
<t tx="karstenw.20230303123322.19">def recall(classify=lambda document: False, documents=[], average=None):
    """ Returns the percentage of positive cases correctly classified as positive.
    """
    return test(classify, documents, average)[2]


</t>
<t tx="karstenw.20230303123322.2">class freq(Counter):

    @others
#--- CUMULATIVE SUM --------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123322.20">def F1(classify=lambda document: False, documents=[], average=None):
    """ Returns the harmonic mean of precision and recall.
    """
    return test(classify, documents, average)[3]


</t>
<t tx="karstenw.20230303123322.21">def F(classify=lambda document: False, documents=[], beta=1, average=None):
    """ Returns the weighted harmonic mean of precision and recall,
        where recall is beta times more important than precision.
    """
    A, P, R, F1 = test(classify, documents, average)
    return (beta ** 2 + 1) * P * R / ((beta ** 2 * P + R) or 1)

#### SENSITIVITY &amp; SPECIFICITY #####################################################################


</t>
<t tx="karstenw.20230303123322.22">def sensitivity(classify=lambda document: False, documents=[]):
    """ Returns the percentage of positive cases correctly classified as positive (= recall).
    """
    return recall(classify, document, average=None)


</t>
<t tx="karstenw.20230303123322.23">def specificity(classify=lambda document: False, documents=[]):
    """ Returns the percentage of negative cases correctly classified as negative.
    """
    TP, TN, FP, FN = confusion_matrix(classify, documents)
    return float(TN) / ((TN + FP) or 1)

TPR = sensitivity # true positive rate
TNR = specificity # true negative rate

#### ROC &amp; AUC #####################################################################################
# See: Tom Fawcett (2005), An Introduction to ROC analysis.


</t>
<t tx="karstenw.20230303123322.24">def roc(tests=[]):
    """ Returns the ROC curve as an iterator of (x, y)-points,
        for the given list of (TP, TN, FP, FN)-tuples.
        The x-axis represents FPR = the false positive rate (1 - specificity).
        The y-axis represents TPR = the true positive rate.
    """
    x = FPR = lambda TP, TN, FP, FN: float(FP) / ((FP + TN) or 1)
    y = TPR = lambda TP, TN, FP, FN: float(TP) / ((TP + FN) or 1)
    return sorted([(0.0, 0.0), (1.0, 1.0)] + [(x(*m), y(*m)) for m in tests])


</t>
<t tx="karstenw.20230303123322.25">def auc(curve=[]):
    """ Returns the area under the curve for the given list of (x, y)-points.
        The area is calculated using the trapezoidal rule.
        For the area under the ROC-curve, 
        the return value is the probability (0.0-1.0) that a classifier will rank 
        a random positive document (True) higher than a random negative one (False).
    """
    curve = sorted(curve)
    # Trapzoidal rule: area = (a + b) * h / 2, where a=y0, b=y1 and h=x1-x0.
    return sum(0.5 * (x1 - x0) * (y1 + y0) for (x0, y0), (x1, y1) in sorted(list(zip(curve, curve[1:]))))

#### AGREEMENT #####################################################################################
# +1.0 = total agreement between voters
# +0.0 = votes based on random chance
# -1.0 = total disagreement


</t>
<t tx="karstenw.20230303123322.26">def fleiss_kappa(m):
    """ Returns the reliability of agreement as a number between -1.0 and +1.0,
        for a number of votes per category per task.
        The given m is a list in which each row represents a task.
        Each task is a list with the number of votes per category.
        Each column represents a category.
        For example, say 5 people are asked to vote "cat" and "dog" as "good" or "bad":
         m = [# + -        
               [3,2], # cat
               [5,0]] # dog
    """
    N = len(m)    # Total number of tasks.
    n = sum(m[0]) # The number of votes per task.
    k = len(m[0]) # The number of categories.
    if n == 1:
        return 1.0
    assert all(sum(row) == n for row in m[1:]), "numer of votes for each task differs"
    # p[j] = the proportion of all assignments which were to the j-th category.
    p = [sum(m[i][j] for i in range(N)) / float(N * n) for j in range(k)]
    # P[i] = the extent to which voters agree for the i-th subject.
    P = [(sum(m[i][j]**2 for j in range(k)) - n) / float(n * (n - 1)) for i in range(N)]
    # Pm = the mean of P[i] and Pe.
    Pe = sum(pj**2 for pj in p)
    Pm = sum(P) / N
    K = (Pm - Pe) / ((1 - Pe) or 1) # kappa
    return K

agreement = fleiss_kappa

#### TEXT METRICS ##################################################################################

#--- SIMILARITY ------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123322.27">def levenshtein(string1, string2):
    """ Measures the amount of difference between two strings.
        The return value is the number of operations (insert, delete, replace)
        required to transform string a into string b.
    """
    # http://hetland.org/coding/python/levenshtein.py
    n, m = len(string1), len(string2)
    if n &gt; m:
        # Make sure n &lt;= m to use O(min(n,m)) space.
        string1, string2, n, m = string2, string1, m, n
    current = list(range(n + 1))
    for i in range(1, m + 1):
        previous, current = current, [i] + [0] * n
        for j in range(1, n + 1):
            insert, delete, replace = previous[j] + 1, current[j - 1] + 1, previous[j - 1]
            if string1[j - 1] != string2[i - 1]:
                replace += 1
            current[j] = min(insert, delete, replace)
    return current[n]

edit_distance = levenshtein


</t>
<t tx="karstenw.20230303123322.28">def levenshtein_similarity(string1, string2):
    """ Returns the similarity of string1 and string2 as a number between 0.0 and 1.0.
    """
    return 1 - levenshtein(string1, string2) / float(max(len(string1), len(string2), 1.0))


</t>
<t tx="karstenw.20230303123322.29">def dice_coefficient(string1, string2):
    """ Returns the similarity between string1 and string1 as a number between 0.0 and 1.0,
        based on the number of shared bigrams, e.g., "night" and "nacht" have one common bigram "ht".
    """
    def bigrams(s):
        return set(s[i:i + 2] for i in range(len(s) - 1))
    nx = bigrams(string1)
    ny = bigrams(string2)
    nt = nx.intersection(ny)
    return 2.0 * len(nt) / ((len(nx) + len(ny)) or 1)

LEVENSHTEIN, DICE = "levenshtein", "dice"


</t>
<t tx="karstenw.20230303123322.3">def __init__(self, *args, **kwargs):
    """ A dictionary with sorted float values (by default, 0.0).
    """
    Counter.__init__(self, dict(*args, **kwargs))

</t>
<t tx="karstenw.20230303123322.30">def similarity(string1, string2, metric=LEVENSHTEIN):
    """ Returns the similarity of string1 and string2 as a number between 0.0 and 1.0,
        using LEVENSHTEIN edit distance or DICE coefficient.
    """
    if metric == LEVENSHTEIN:
        return levenshtein_similarity(string1, string2)
    if metric == DICE:
        return dice_coefficient(string1, string2)

#--- READABILITY -----------------------------------------------------------------------------------
# 0.9-1.0 = easily understandable by 11-year old.
# 0.6-0.7 = easily understandable by 13- to 15-year old.
# 0.0-0.3 = best understood by university graduates.


</t>
<t tx="karstenw.20230303123322.31">def flesch_reading_ease(string):
    """ Returns the readability of the string as a value between 0.0-1.0:
        0.30-0.50 (difficult) =&gt; 0.60-0.70 (standard) =&gt; 0.90-1.00 (very easy).
    """
    def count_syllables(word, vowels="aeiouy"):
        n = 0
        p = False # True if the previous character was a vowel.
        for ch in word.endswith("e") and word[:-1] or word:
            v = ch in vowels
            n += int(v and not p)
            p = v
        return n
    if not isinstance(string, str):
        raise TypeError("%s is not a string" % repr(string))
    if len(string) &lt; 3:
        return 1.0
    if len(string.split(" ")) &lt; 2:
        return 1.0
    string = string.strip()
    string = string.strip("\"'().")
    string = string.lower()
    string = string.replace("!", ".")
    string = string.replace("?", ".")
    string = string.replace(",", " ")
    string = " ".join(string.split())
    y = [count_syllables(w) for w in string.split() if w != ""]
    w = max(1, len([w for w in string.split(" ") if w != ""]))
    s = max(1, len([s for s in string.split(".") if len(s) &gt; 2]))
    #R = 206.835 - 1.015 * w/s - 84.6 * sum(y)/w
    # Use the Farr, Jenkins &amp; Patterson algorithm,
    # which uses simpler syllable counting (count_syllables() is the weak point here).
    R = 1.599 * sum(1 for v in y if v == 1) * 100 / w - 1.015 * w / s - 31.517
    R = max(0.0, min(R * 0.01, 1.0))
    return R

readability = flesch_reading_ease

#--- INTERTEXTUALITY -------------------------------------------------------------------------------
# Intertextuality may be useful for plagiarism detection.
# For example, on the Corpus of Plagiarised Short Answers (Clough &amp; Stevenson, 2009),
# accuracy (F1) is 94.5% with n=3 and intertextuality threshold &gt; 0.1.

PUNCTUATION = ".,;:!?()[]{}`'\"@#$^&amp;*+-|=~_"


</t>
<t tx="karstenw.20230303123322.32">def ngrams(string, n=3, punctuation=PUNCTUATION, **kwargs):
    """ Returns a list of n-grams (tuples of n successive words) from the given string.
        Punctuation marks are stripped from words.
    """
    s = string
    s = s.replace(".", " .")
    s = s.replace("?", " ?")
    s = s.replace("!", " !")
    s = [w.strip(punctuation) for w in s.split()]
    s = [w.strip() for w in s if w.strip()]
    return [tuple(s[i:i + n]) for i in range(len(s) - n + 1)]


</t>
<t tx="karstenw.20230303123322.33">class Weight(float):
    """ A float with a magic "assessments" property,
        which is the set of all n-grams contributing to the weight.
    """

    @others
</t>
<t tx="karstenw.20230303123322.34">def __new__(self, value=0.0, assessments=[]):
    return float.__new__(self, value)

</t>
<t tx="karstenw.20230303123322.35">def __init__(self, value=0.0, assessments=[]):
    self.assessments = set(assessments)

</t>
<t tx="karstenw.20230303123322.36">def __iadd__(self, value):
    return Weight(self + value, self.assessments)

</t>
<t tx="karstenw.20230303123322.37">def __isub__(self, value):
    return Weight(self - value, self.assessments)

</t>
<t tx="karstenw.20230303123322.38">def __imul__(self, value):
    return Weight(self * value, self.assessments)

</t>
<t tx="karstenw.20230303123322.39">def __idiv__(self, value):
    return Weight(self / value, self.assessments)


</t>
<t tx="karstenw.20230303123322.4">def __missing__(self):
    return 0.0

</t>
<t tx="karstenw.20230303123322.40">def intertextuality(texts=[], n=5, weight=lambda ngram: 1.0, **kwargs):
    """ Returns a dictionary of (i, j) =&gt; float.
        For indices i and j in the given list of texts,
        the corresponding float is the percentage of text i that is also in text j.
        Overlap is measured by matching n-grams (by default, 5 successive words).
        An optional weight function can be used to supply the weight of each n-gram.
    """
    map = {} # n-gram =&gt; text id's
    sum = {} # text id =&gt; sum of weight(n-gram)
    for i, txt in enumerate(texts):
        for j, ngram in enumerate(ngrams(txt, n, **kwargs)):
            if ngram not in map:
                map[ngram] = set()
            map[ngram].add(i)
            sum[i] = sum.get(i, 0) + weight(ngram)
    w = defaultdict(Weight) # (id1, id2) =&gt; percentage of id1 that overlaps with id2
    for ngram in map:
        for i in map[ngram]:
            for j in map[ngram]:
                if i != j:
                    if (i, j) not in w:
                        w[i, j] = Weight(0.0)
                    w[i, j] += weight(ngram)
                    w[i, j].assessments.add(ngram)
    for i, j in w:
        w[i, j] /= float(sum[i])
        w[i, j] = min(w[i, j], Weight(1.0))
    return w

#--- WORD TYPE-TOKEN RATIO -------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123322.41">def type_token_ratio(string, n=100, punctuation=PUNCTUATION):
    """ Returns the percentage of unique words in the given string as a number between 0.0-1.0,
        as opposed to the total number of words (= lexical diversity, vocabulary richness).
    """
    def window(a, n=100):
        if n &gt; 0:
            for i in range(max(len(a) - n + 1, 1)):
                yield a[i:i + n]
    s = string.lower().split()
    s = [w.strip(punctuation) for w in s]
    # Covington &amp; McFall moving average TTR algorithm.
    return mean(1.0 * len(set(x)) / max(len(x), 1) for x in window(s, n))

ttr = type_token_ratio

#--- WORD INFLECTION -------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123322.42">def suffixes(inflections=[], n=3, top=10, reverse=True):
    """ For a given list of (base, inflection)-tuples,
        returns a list of (count, inflected suffix, [(base suffix, frequency), ... ]):
        suffixes([("beau", "beaux"), ("jeune", "jeunes"), ("hautain", "hautaines")], n=3) =&gt;
        [(2, "nes", [("ne", 0.5), ("n", 0.5)]), (1, "aux", [("au", 1.0)])]
    """
    # This is utility function we use to train singularize() and lemma()
    # in pattern.de, pattern.es, pattern.fr, etc.
    d = {}
    for x, y in (reverse and (y, x) or (x, y) for x, y in inflections):
        x0 = x[:-n]      # be-   jeu-  hautai-
        x1 = x[-n:]      # -aux  -nes  -nes
        y1 = y[len(x0):] # -au   -ne   -n
        if x0 + y1 != y:
            continue
        if x1 not in d:
            d[x1] = {}
        if y1 not in d[x1]:
            d[x1][y1] = 0.0
        d[x1][y1] += 1.0
    # Sort by frequency of inflected suffix: 2x -nes, 1x -aux.
    # Sort by frequency of base suffixes for each inflection:
    # [(2, "nes", [("ne", 0.5), ("n", 0.5)]), (1, "aux", [("au", 1.0)])]
    d = [(int(sum(y.values())), x, y.items()) for x, y in d.items()]
    d = sorted(d, reverse=True)
    d = ((n, x, (sorted(y, key=itemgetter(1)))) for n, x, y in d)
    d = ((n, x, [(y, m / n) for y, m in y]) for n, x, y in d)
    return list(d)[:top]

#--- WORD CO-OCCURRENCE ----------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123322.43">class Sentinel(object):
    pass


</t>
<t tx="karstenw.20230303123322.44">def isplit(string, sep="\t\n\x0b\x0c\r "):
    """ Returns an iterator over string.split().
        This is efficient in combination with cooccurrence(), 
        since the string may be very long (e.g., Brown corpus).
    """
    a = []
    for ch in string:
        if ch not in sep:
            a.append(ch)
            continue
        if a:
            yield "".join(a)
            a = []
    if a:
        yield "".join(a)


</t>
<t tx="karstenw.20230303123322.45">def cooccurrence(iterable, window=(-1, -1), term1=lambda x: True, term2=lambda x: True, normalize=lambda x: x, matrix=None, update=None):
    """ Returns the co-occurence matrix of terms in the given iterable, string, file or file list,
        as a dictionary: {term1: {term2: count, term3: count, ...}}.
        The window specifies the size of the co-occurence window.
        The term1() function defines anchors.
        The term2() function defines co-occurring terms to count.
        The normalize() function can be used to remove punctuation, lowercase words, etc.
        Optionally, a user-defined matrix to update can be given.
        Optionally, a user-defined update(matrix, term1, term2, index2) function can be given.
    """
    if not isinstance(matrix, dict):
        matrix = {}
    # Memory-efficient iteration:
    if isinstance(iterable, str):
        iterable = isplit(iterable)
    if isinstance(iterable, (list, tuple)) and all(hasattr(f, "read") for f in iterable):
        iterable = chain(*(isplit(chain(*x)) for x in iterable))
    if hasattr(iterable, "read"):
        iterable = isplit(chain(*iterable))
    # Window of terms before and after the search term.
    # Deque is more efficient than list.pop(0).
    q = deque()
    # Window size of terms alongside the search term.
    # Note that window=(0,0) will return a dictionary of search term frequency
    # (since it counts co-occurence with itself).
    n = -min(0, window[0]) + max(window[1], 0)
    m = matrix
    # Search terms may fall outside the co-occurrence window, e.g., window=(-3,-2).
    # We add sentinel markers at the start and end of the given iterable.
    for x in chain([Sentinel()] * n, iterable, [Sentinel()] * n):
        q.append(x)
        if len(q) &gt; n:
            # Given window q size and offset,
            # find the index of the candidate term:
            if window[1] &gt;= 0:
                i = -1 - window[1]
            if window[1] &lt; 0:
                i = len(q) - 1
            if i &lt; 0:
                i = len(q) + i
            x1 = q[i]
            if not isinstance(x1, Sentinel):
                x1 = normalize(x1)
                if term1(x1):
                    # Iterate the window and filter co-occurent terms.
                    for j, x2 in enumerate(list(q)[i + window[0]:i + window[1] + 1]):
                        if not isinstance(x2, Sentinel):
                            x2 = normalize(x2)
                            if term2(x2):
                                if update:
                                    update(matrix, x1, x2, j)
                                    continue
                                if x1 not in m:
                                    m[x1] = {}
                                if x2 not in m[x1]:
                                    m[x1][x2] = 0
                                m[x1][x2] += 1
            # Slide window.
            q.popleft()
    return m

co_occurrence = cooccurrence

## Words occuring before and after the word "cat":
## {"cat": {"sat": 1, "black": 1, "cat": 1}}
#s = "The black cat sat on the mat."
#print(cooccurrence(s, window=(-1,1),
#       search = lambda w: w in ("cat",),
#    normalize = lambda w: w.lower().strip(".:;,!?()[]'\"")))

## Adjectives preceding nouns:
## {("cat", "NN"): {("black", "JJ"): 1}}
#s = [("The","DT"), ("black","JJ"), ("cat","NN"), ("sat","VB"), ("on","IN"), ("the","DT"), ("mat","NN")]
#print(cooccurrence(s, window=(-2,-1),
#       search = lambda token: token[1].startswith("NN"),
#       filter = lambda token: token[1].startswith("JJ")))

# Adjectives preceding nouns:
# {("cat", "NN"): {("black", "JJ"): 1}}

#### INTERPOLATION #################################################################################


</t>
<t tx="karstenw.20230303123322.5">def __iter__(self):
    return iter(self.keys())

</t>
<t tx="karstenw.20230303123322.6">def items(self, relative=False):
    """ Returns a list of (key, value)-tuples sorted by value, highest-first.
        With relative=True, the sum of values is 1.0.
    """
    a = Counter.most_common(self)
    if relative:
        n = sum(v for k, v in a) or 1.
        a = [(k, v / n) for v, k in a]
    return a

</t>
<t tx="karstenw.20230303123322.7">def keys(self):
    return [k for k, v in self.items()]

</t>
<t tx="karstenw.20230303123322.8">def values(self, relative=False):
    return [v for k, v in self.items(relative)]

</t>
<t tx="karstenw.20230303123322.9">def copy(self):
    return freq(self)

</t>
<t tx="karstenw.20230303123323.1">def lerp(a, b, t):
    """ Returns the linear interpolation between a and b at time t between 0.0-1.0.
        For example: lerp(100, 200, 0.5) =&gt; 150.
    """
    if t &lt; 0.0:
        return a
    if t &gt; 1.0:
        return b
    return a + (b - a) * t


</t>
<t tx="karstenw.20230303123323.10">def histogram(iterable, k=10, interval=None, *args, **kwargs):
    """ Returns a dictionary with k items: {(start, stop): [values], ...},
        with equal (start, stop) intervals between min(list) =&gt; max(list).
    """
    # To loop through the intervals in sorted order, use:
    # for (i, j), values in sorted(histogram(iterable).items()):
    #     m = i + (j - i) / 2 # midpoint
    #     print(i, j, m, values)

    # Map the range argument to interval (for backward compatibility)
    if 'range' in kwargs:
        interval = kwargs['range']

    a = iterable if isinstance(iterable, list) else list(iterable)
    r = interval or (min(a), max(a))
    k = max(int(k), 1)
    w = float(r[1] - r[0] + 0.000001) / k # interval (bin width)
    h = [[] for i in range(k)]
    for x in a:
        i = int(floor((x - r[0]) / w))
        if 0 &lt;= i &lt; len(h):
            #print(x, i, "(%.2f, %.2f)" % (r[0] + w * i, r[0] + w + w * i))
            h[i].append(x)
    return dict(((r[0] + w * i, r[0] + w + w * i), v) for i, v in enumerate(h))

#--- MOMENT ----------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123323.11">def moment(iterable, n=2, sample=False):
    """ Returns the n-th central moment of the given list of values
        (2nd central moment = variance, 3rd and 4th are used to define skewness and kurtosis).
    """
    if n == 1:
        return 0.0
    a = iterable if isinstance(iterable, list) else list(iterable)
    m = mean(a)
    return sum((x - m) ** n for x in a) / (len(a) - int(sample) or 1)


</t>
<t tx="karstenw.20230303123323.12">def skewness(iterable, sample=False):
    """ Returns the degree of asymmetry of the given list of values:
        &gt; 0.0 =&gt; relatively few values are higher than mean(list),
        &lt; 0.0 =&gt; relatively few values are lower than mean(list),
        = 0.0 =&gt; evenly distributed on both sides of the mean (= normal distribution).
    """
    # Distributions with skew and kurtosis between -1 and +1
    # can be considered normal by approximation.
    a = iterable if isinstance(iterable, list) else list(iterable)
    return moment(a, 3, sample) / (moment(a, 2, sample) ** 1.5 or 1)

skew = skewness


</t>
<t tx="karstenw.20230303123323.13">def kurtosis(iterable, sample=False):
    """ Returns the degree of peakedness of the given list of values:
        &gt; 0.0 =&gt; sharper peak around mean(list) = more infrequent, extreme values,
        &lt; 0.0 =&gt; wider peak around mean(list),
        = 0.0 =&gt; normal distribution,
        =  -3 =&gt; flat
    """
    a = iterable if isinstance(iterable, list) else list(iterable)
    return moment(a, 4, sample) / (moment(a, 2, sample) ** 2.0 or 1) - 3

#a = 1
#b = 1000
#U = [float(i-a)/(b-a) for i in range(a,b)] # uniform distribution
#print(abs(-1.2 - kurtosis(U)) &lt; 0.0001)

#--- QUANTILE --------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123323.14">def quantile(iterable, p=0.5, sort=True, a=1, b=-1, c=0, d=1):
    """ Returns the value from the sorted list at point p (0.0-1.0).
        If p falls between two items in the list, the return value is interpolated.
        For example, quantile(list, p=0.5) = median(list)
    """
    # Based on: Ernesto P. Adorio, http://adorio-research.org/wordpress/?p=125
    # Parameters a, b, c, d refer to the algorithm by Hyndman and Fan (1996):
    # http://stat.ethz.ch/R-manual/R-patched/library/stats/html/quantile.html
    s = sorted(iterable) if sort is True else list(iterable)
    n = len(s)
    f, i = modf(a + (b + n) * p - 1)
    if n == 0:
        raise ValueError("quantile() arg is an empty sequence")
    if f == 0:
        return float(s[int(i)])
    if i &lt; 0:
        return float(s[int(i)])
    if i &gt;= n:
        return float(s[-1])
    i = int(floor(i))
    return s[i] + (s[i + 1] - s[i]) * (c + d * f)

#print(quantile(range(10), p=0.5) == median(range(10)))


</t>
<t tx="karstenw.20230303123323.15">def boxplot(iterable, **kwargs):
    """ Returns a tuple (min(list), Q1, Q2, Q3, max(list)) for the given list of values.
        Q1, Q2, Q3 are the quantiles at 0.25, 0.5, 0.75 respectively.
    """
    # http://en.wikipedia.org/wiki/Box_plot
    kwargs.pop("p", None)
    kwargs.pop("sort", None)
    s = sorted(iterable)
    Q1 = quantile(s, p=0.25, sort=False, **kwargs)
    Q2 = quantile(s, p=0.50, sort=False, **kwargs)
    Q3 = quantile(s, p=0.75, sort=False, **kwargs)
    return float(min(s)), Q1, Q2, Q3, float(max(s))

#### STATISTICAL TESTS #############################################################################

#--- FISHER'S EXACT TEST ---------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123323.16">def fisher_exact_test(a, b, c, d, **kwargs):
    """ Fast implementation of Fisher's exact test (two-tailed).
        Returns the significance p for the given 2 x 2 contingency table:
        p &lt; 0.05: significant
        p &lt; 0.01: very significant
        The following test shows a very significant correlation between gender &amp; dieting:
        -----------------------------
        |             | men | women |
        |     dieting |  1  |   9   |
        | non-dieting | 11  |   3   |
        -----------------------------
        fisher_exact_test(a=1, b=9, c=11, d=3) =&gt; 0.0028
    """
    _cache = {}
    # Hypergeometric distribution.
    # (a+b)!(c+d)!(a+c)!(b+d)! / a!b!c!d!n! for n=a+b+c+d

    def p(a, b, c, d):
        return C(a + b, a) * C(c + d, c) / C(a + b + c + d, a + c)
    # Binomial coefficient.
    # n! / k!(n-k)! for 0 &lt;= k &lt;= n

    def C(n, k):
        if len(_cache) &gt; 10000:
            _cache.clear()
        if k &gt; n - k: # 2x speedup.
            k = n - k
        if 0 &lt;= k &lt;= n and (n, k) not in _cache:
            c = 1.0
            for i in range(1, int(k + 1)):
                c *= n - k + i
                c /= i
            _cache[(n, k)] = c # 3x speedup.
        return _cache.get((n, k), 0.0)
    # Probability of the given data.
    cutoff = p(a, b, c, d)
    # Probabilities of "more extreme" data, in both directions (two-tailed).
    # Based on: http://www.koders.com/java/fid868948AD5196B75C4C39FEA15A0D6EAF34920B55.aspx?s=252
    s = [cutoff] + \
        [p(a + i, b - i, c - i, d + i) for i in range(1, min(int(b), int(c)) + 1)] + \
        [p(a - i, b + i, c + i, d - i) for i in range(1, min(int(a), int(d)) + 1)]
    return sum(v for v in s if v &lt;= cutoff) or 0.0

fisher = fisher_test = fisher_exact_test

#--- PEARSON'S CHI-SQUARED TEST --------------------------------------------------------------------

LOWER = "lower"
UPPER = "upper"


</t>
<t tx="karstenw.20230303123323.17">def _expected(observed):
    """ Returns the table of (absolute) expected frequencies
        from the given table of observed frequencies.
    """
    o = observed
    if len(o) == 0:
        return []
    if len(o) == 1:
        return [[sum(o[0]) / float(len(o[0]))] * len(o[0])]
    n = [sum(o[i]) for i in range(len(o))]
    m = [sum(o[i][j] for i in range(len(o))) for j in range(len(o[0]))]
    s = float(sum(n))
    # Each cell = row sum * column sum / total.
    return [[n[i] * m[j] / s for j in range(len(o[i]))] for i in range(len(o))]


</t>
<t tx="karstenw.20230303123323.18">def pearson_chi_squared_test(observed=[], expected=[], df=None, tail=UPPER):
    """ Returns (x2, p) for the n x m observed and expected data (containing absolute frequencies).
        If expected is None, an equal distribution over all classes is assumed.
        If df is None, it is (n-1) * (m-1).
        p &lt; 0.05: significant
        p &lt; 0.01: very significant
        This means that if p &lt; 5%, the data is unevenly distributed (e.g., biased).
        The following test shows that the die is fair:
        ---------------------------------------
        |       | 1  | 2  | 3  | 4  | 5  | 6  | 
        | rolls | 22 | 21 | 22 | 27 | 22 | 36 |
        ---------------------------------------
        chi2([[22, 21, 22, 27, 22, 36]]) =&gt; (6.72, 0.24)
    """
    # The p-value (upper tail area) is obtained from the incomplete gamma integral:
    # p(x2 | v) = gammai(v/2, x/2) with v degrees of freedom.
    # See: Cephes, https://github.com/scipy/scipy/blob/master/scipy/special/cephes/chdtr.c
    o = list(observed)
    e = list(expected) or _expected(o)
    n = len(o)
    m = len(o[0]) if o else 0
    df = df or (n - 1) * (m - 1)
    df = df or (m == 1 and n - 1 or m - 1)
    x2 = 0.0
    for i in range(n):
        for j in range(m):
            if o[i][j] != 0 and e[i][j] != 0:
                x2 += (o[i][j] - e[i][j]) ** 2.0 / e[i][j]
    p = gammai(df * 0.5, x2 * 0.5, tail)
    return (x2, p)

X2 = x2 = chi2 = chi_square = chi_squared = pearson_chi_squared_test


</t>
<t tx="karstenw.20230303123323.19">def chi2p(x2, df=1, tail=UPPER):
    """ Returns p-value for given x2 and degrees of freedom.
    """
    return gammai(df * 0.5, x2 * 0.5, tail)

#o, e = [[44, 56]], [[50, 50]]
#assert round(chi_squared(o, e)[0], 4)  == 1.4400
#assert round(chi_squared(o, e)[1], 4)  == 0.2301

#--- PEARSON'S LOG LIKELIHOOD RATIO APPROXIMATION --------------------------------------------------


</t>
<t tx="karstenw.20230303123323.2">def smoothstep(a, b, x):
    """ Returns the Hermite interpolation (cubic spline) for x between a and b.
        The return value between 0.0-1.0 eases (slows down) as x nears a or b.
    """
    if x &lt; a:
        return 0.0
    if x &gt;= b:
        return 1.0
    x = float(x - a) / (b - a)
    return x * x * (3 - 2 * x)


</t>
<t tx="karstenw.20230303123323.20">def pearson_log_likelihood_ratio(observed=[], expected=[], df=None, tail=UPPER):
    """ Returns (g, p) for the n x m observed and expected data (containing absolute frequencies).
        If expected is None, an equal distribution over all classes is assumed.
        If df is None, it is (n-1) * (m-1).
        p &lt; 0.05: significant
        p &lt; 0.01: very significant
    """
    o = list(observed)
    e = list(expected) or _expected(o)
    n = len(o)
    m = len(o[0]) if o else 0
    df = df or (n - 1) * (m - 1)
    df = df or (m == 1 and n - 1 or m - 1)
    g = 0.0
    for i in range(n):
        for j in range(m):
            if o[i][j] != 0 and e[i][j] != 0:
                g += o[i][j] * log(o[i][j] / e[i][j])
    g = g * 2
    p = gammai(df * 0.5, g * 0.5, tail)
    return (g, p)

llr = likelihood = pearson_log_likelihood_ratio

#--- KOLMOGOROV-SMIRNOV TWO SAMPLE TEST ------------------------------------------------------------
# Based on: https://github.com/scipy/scipy/blob/master/scipy/stats/stats.py
# Thanks to prof. F. De Smedt for additional information.

NORMAL = "normal"


</t>
<t tx="karstenw.20230303123323.21">def kolmogorov_smirnov_two_sample_test(a1, a2=NORMAL, n=1000):
    """ Returns the likelihood that two independent samples are drawn from the same distribution.
        Returns a (d, p)-tuple with maximum distance d and two-tailed p-value.
        By default, the second sample is the normal distribution.
    """
    if a2 == NORMAL:
        a2 = norm(max(n, len(a1)), mean(a1), stdev(a1))
    n1 = float(len(a1))
    n2 = float(len(a2))
    a1 = sorted(a1) # [1, 2, 5]
    a2 = sorted(a2) # [3, 4, 6]
    a3 = a1 + a2    # [1, 2, 5, 3, 4, 6]
    # Find the indices in a1 so that,
    # if the values in a3 were inserted before these indices,
    # the order of a1 would be preserved:
    cdf1 = [bisect_right(a1, v) for v in a3] # [1, 2, 3, 2, 2, 3]
    cdf2 = [bisect_right(a2, v) for v in a3]
    # Cumulative distributions.
    cdf1 = [v / n1 for v in cdf1]
    cdf2 = [v / n2 for v in cdf2]
    # Compute maximum deviation between cumulative distributions.
    d = max(abs(v1 - v2) for v1, v2 in zip(cdf1, cdf2))
    # Compute p-value.
    e = sqrt(n1 * n2 / (n1 + n2))
    p = kolmogorov((e + 0.12 + 0.11 / e) * d)
    return d, p

ks2 = kolmogorov_smirnov_two_sample_test

#### SPECIAL FUNCTIONS #############################################################################

#--- GAMMA FUNCTION --------------------------------------------------------------------------------
# Based on: http://www.johnkerl.org/python/sp_funcs_m.py.txt, Tom Loredo
# See also: http://www.mhtl.uwaterloo.ca/courses/me755/web_chap1.pdf


</t>
<t tx="karstenw.20230303123323.22">def gamma(x):
    """ Returns the gamma function at x.
    """
    return exp(gammaln(x))


</t>
<t tx="karstenw.20230303123323.23">def gammaln(x):
    """ Returns the natural logarithm of the gamma function at x.
    """
    x = x - 1.0
    y = x + 5.5
    y = (x + 0.5) * log(y) - y
    n = 1.0
    for i in range(6):
        x += 1
        n += (
          76.18009173,
         -86.50532033,
          24.01409822,
          -1.231739516e0,
           0.120858003e-2,
          -0.536382e-5)[i] / x
    return y + log(2.50662827465 * n)

lgamma = gammaln


</t>
<t tx="karstenw.20230303123323.24">def gammai(a, x, tail=UPPER):
    """ Returns the incomplete gamma function for LOWER or UPPER tail.
    """

    # Series approximation.
    def _gs(a, x, epsilon=3.e-7, iterations=700):
        ln = gammaln(a)
        s = 1.0 / a
        d = 1.0 / a
        for i in range(1, iterations):
            d = d * x / (a + i)
            s = s + d
            if abs(d) &lt; abs(s) * epsilon:
                return (s * exp(-x + a * log(x) - ln), ln)
        raise StopIteration(abs(d), abs(s) * epsilon)

    # Continued fraction approximation.
    def _gf(a, x, epsilon=3.e-7, iterations=200):
        ln = gammaln(a)
        g0 = 0.0
        a0 = 1.0
        b0 = 0.0
        a1 = x
        b1 = 1.0
        f = 1.0
        for i in range(1, iterations):
            a0 = (a1 + a0 * (i - a)) * f
            b0 = (b1 + b0 * (i - a)) * f
            a1 = x * a0 + a1 * i * f
            b1 = x * b0 + b1 * i * f
            if a1 != 0.0:
                f = 1.0 / a1
                g = b1 * f
                if abs((g - g0) / g) &lt; epsilon:
                    return (g * exp(-x + a * log(x) - ln), ln)
                g0 = g
        raise StopIteration(abs((g - g0) / g))

    if a &lt;= 0.0:
        return 1.0
    if x &lt;= 0.0:
        return 1.0
    if x &lt; a + 1:
        if tail == LOWER:
            return _gs(a, x)[0]
        return 1 - _gs(a, x)[0]
    else:
        if tail == UPPER:
            return _gf(a, x)[0]
        return 1 - _gf(a, x)[0]

#--- ERROR FUNCTION --------------------------------------------------------------------------------
# Based on: http://www.johnkerl.org/python/sp_funcs_m.py.txt, Tom Loredo


</t>
<t tx="karstenw.20230303123323.25">def erf(x):
    """ Returns the error function at x.
    """
    return 1.0 - erfc(x)


</t>
<t tx="karstenw.20230303123323.26">def erfc(x):
    """ Returns the complementary error function at x.
    """
    z = abs(x)
    t = 1.0 / (1 + 0.5 * z)
    r = 0.0
    for y in (
      0.17087277,
     -0.82215223,
      1.48851587,
     -1.13520398,
      0.27886807,
     -0.18628806,
      0.09678418,
      0.37409196,
      1.00002368,
     -1.26551223):
        r = y + t * r
    r = t * exp(-z ** 2 + r)
    if x &gt;= 0:
        return r
    return 2.0 - r

#--- NORMAL DISTRIBUTION ---------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123323.27">def cdf(x, mean=0.0, stdev=1.0):
    """ Returns the cumulative distribution function at x.
    """
    return min(1.0, 0.5 * erfc((-x + mean) / (stdev * 2**0.5)))


</t>
<t tx="karstenw.20230303123323.28">def pdf(x, mean=0.0, stdev=1.0):
    """ Returns the probability density function at x:
        the likelihood of x in a distribution with given mean and standard deviation.
    """
    u = float(x - mean) / abs(stdev)
    return (1 / (sqrt(2 * pi) * abs(stdev))) * exp(-u * u / 2)

normpdf = pdf


</t>
<t tx="karstenw.20230303123323.29">def norm(n, mean=0.0, stdev=1.0):
    """ Returns a list of n random samples from the normal distribution.
    """
    return [gauss(mean, stdev) for i in range(n)]

#--- KOLMOGOROV DISTRIBUTION -----------------------------------------------------------------------
# Based on: http://www.math.ucla.edu/~tom/distributions/Kolmogorov.html, Thomas Ferguson


</t>
<t tx="karstenw.20230303123323.3">def smoothrange(a=None, b=None, n=10):
    """ Returns an iterator of approximately n values v1, v2, ... vn,
        so that v1 &lt;= a, and vn &gt;= b, and all values are multiples of 1, 2, 5 and 10.
        For example: list(smoothrange(1, 123)) =&gt; [0, 20, 40, 60, 80, 100, 120, 140],
    """
    def _multiple(v, round=False):
        e = floor(log(v, 10)) # exponent
        m = pow(10, e)        # magnitude
        f = v / m             # fraction
        if round is True:
            op, x, y, z = lt, 1.5, 3.0, 7.0
        if round is False:
            op, x, y, z = le, 1.0, 2.0, 5.0
        if op(f, x):
            return m * 1
        if op(f, y):
            return m * 2
        if op(f, z):
            return m * 5
        else:
            return m * 10
    if a is None and b is None:
        a, b = 0, 1
    if a is None:
        a, b = 0, b
    if b is None:
        a, b = 0, a
    if a == b:
        yield float(a)
        raise StopIteration
    r = _multiple(b - a)
    t = _multiple(r / (n - 1), round=True)
    a = floor(a / t) * t
    b = ceil(b / t) * t
    for i in range(int((b - a) / t) + 1):
        yield a + i * t

#### STATISTICS ####################################################################################

#--- MEAN ------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123323.30">def kolmogorov(x):
    """ Returns the approximation of Kolmogorov's distribution of the two-sample test.
        For a sample of size m and a sample of size n,
        it is the probability that the maximum deviation &gt; x / sqrt(m+n).
    """
    if x &lt; 0.27:
        return 1.0
    if x &gt; 3.2:
        return 0.0
    x = -2.0 * x * x
    k = 0
    for i in reversed(range(1, 27 + 1, 2)): # 27 25 23 ... 1
        k = (1 - k) * exp(x * i)
    return 2.0 * k
</t>
<t tx="karstenw.20230303123323.4">def mean(iterable):
    """ Returns the arithmetic mean of the given list of values.
        For example: mean([1,2,3,4]) = 10/4 = 2.5.
    """
    a = iterable if isinstance(iterable, list) else list(iterable)
    return float(sum(a)) / (len(a) or 1)

avg = mean


</t>
<t tx="karstenw.20230303123323.5">def hmean(iterable):
    """ Returns the harmonic mean of the given list of values.
    """
    a = iterable if isinstance(iterable, list) else list(iterable)
    return float(len(a)) / sum(1.0 / x for x in a)


</t>
<t tx="karstenw.20230303123323.6">def median(iterable, sort=True):
    """ Returns the value that separates the lower half from the higher half of values in the list.
    """
    s = sorted(iterable) if sort is True else list(iterable)
    n = len(s)
    if n == 0:
        raise ValueError("median() arg is an empty sequence")
    if n % 2 == 0:
        return float(s[(n // 2) - 1] + s[n // 2]) / 2
    return s[n // 2]


</t>
<t tx="karstenw.20230303123323.7">def variance(iterable, sample=False):
    """ Returns the variance of the given list of values.
        The variance is the average of squared deviations from the mean.
    """
    # Sample variance = E((xi-m)^2) / (n-1)
    # Population variance = E((xi-m)^2) / n
    a = iterable if isinstance(iterable, list) else list(iterable)
    m = mean(a)
    return sum((x - m) ** 2 for x in a) / (len(a) - int(sample) or 1)


</t>
<t tx="karstenw.20230303123323.8">def standard_deviation(iterable, *args, **kwargs):
    """ Returns the standard deviation of the given list of values.
        Low standard deviation =&gt; values are close to the mean.
        High standard deviation =&gt; values are spread out over a large range.
    """
    return sqrt(variance(iterable, *args, **kwargs))

stdev = standard_deviation


</t>
<t tx="karstenw.20230303123323.9">def simple_moving_average(iterable, k=10):
    """ Returns an iterator over the simple moving average of the given list of values.
    """
    a = iterable if isinstance(iterable, list) else list(iterable)
    for m in range(len(a)):
        i = m - k
        j = m + k + 1
        w = a[max(0, i):j]
        yield float(sum(w)) / (len(w) or 1)

sma = simple_moving_average


</t>
<t tx="karstenw.20230303123433.1">@language python
@tabwidth -4
@others

</t>
<t tx="karstenw.20230303123446.1">#### PATTERN | SERVER ##############################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2014 University of Antwerp, Belgium
# Copyright (c) 2014 St. Lucas University College of Art &amp; Design, Antwerp.
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).

####################################################################################################

from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division
from __future__ import with_statement

from builtins import str, bytes, dict, int, chr
from builtins import map, zip, filter
from builtins import object, range

from io import open

import __main__
import re
import os
import sys
import pwd
import grp
import time
_time = time
import atexit
import urllib
import hashlib
import hmac
import base64
import struct
import random
import string
import textwrap
import types
import inspect
import threading
import subprocess
import tempfile
import itertools
import collections
import sqlite3 as sqlite
import cherrypy as cp

import pdb

try:
    import json
    json.encoder.FLOAT_REPR = lambda f: ("%.2f" % f)
except AttributeError:
    pass

try: # Python 2.x vs 3.x
    import htmlentitydefs
except ImportError:
    from html import entities as htmlentitydefs

try: # Python 2.x vs 3.x
    from cStringIO import StringIO
except ImportError:
    from io import BytesIO as StringIO

try: # Python 2.x vs 3.x
    import cPickle as pickle
except ImportError:
    import pickle

try:
    # Folder that contains pattern.server.
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

try:
    # Folder that contains the script that (indirectly) imports pattern.server.
    # This is used as the default App.path.
    f = inspect.currentframe()
    f = inspect.getouterframes(f)[-1][0]
    f = f.f_globals["__file__"]
    SCRIPT = os.path.dirname(os.path.abspath(f))
except:
    SCRIPT = os.getcwd()


</t>
<t tx="karstenw.20230303123446.10">def __init__(self, cursor, row):
    """ Row as dictionary.
    """
    d = cursor.description
    dict.__init__(self, ((d[i][0], v) for i, v in enumerate(row)))

</t>
<t tx="karstenw.20230303123446.100">class Application(object):

    @others
App = Application

</t>
<t tx="karstenw.20230303123446.101">def __init__(self, name=None, path=SCRIPT, static="./static", rate="rate.db", owner=None):
    """ A web app served by a WSGI-server that starts with App.run().
        By default, the app is served from the folder of the script that imports pattern.server.
        By default, static content is served from the given subfolder.
        @App.route(path) defines a URL path handler.
        @App.error(code) defines a HTTP error handler.
    """
    # RateLimit db resides in app folder:
    rate = os.path.join(path, rate)
    self._owner  = owner        # App owner (e.g., "www-data" with mod_wsgi).
    self._name   = name         # App name.
    self._path   = path         # App path.
    self._host   = None         # Server host, see App.run().
    self._port   = None         # Server port, see App.run().
    self._app    = None         # CherryPy Application object.
    self._up     = False        # True if server is up &amp; running.
    self._cache  = {}           # Memoize cache.
    self._cached = 1000         # Memoize cache size.
    self._static = static       # Static content folder.
    self._rate   = rate         # RateLimit db name, see also App.route(limit=True).
    self.router  = Router()     # Router object, maps URL paths to handlers.
    self.thread  = App.Thread() # Thread-safe dictionary.
    # Change path:
    os.chdir(path)
    # Change owner:
    # (= grant SQLite write permission)
    chown(path, owner)

</t>
<t tx="karstenw.20230303123446.102">@property
def owner(self):
    return self._owner

</t>
<t tx="karstenw.20230303123446.103">@property
def name(self):
    return self._name

</t>
<t tx="karstenw.20230303123446.104">@property
def host(self):
    return self._host

</t>
<t tx="karstenw.20230303123446.105">@property
def port(self):
    return self._port

</t>
<t tx="karstenw.20230303123446.106">@property
def up(self):
    return self._up

running = up

</t>
<t tx="karstenw.20230303123446.107">@property
def path(self):
    """ Yields the absolute path to the folder containing the app.
    """
    return self._path

</t>
<t tx="karstenw.20230303123446.108">@property
def static(self):
    """ Yields the absolute path to the folder with static content.
    """
    return os.path.join(self._path, self._static)

</t>
<t tx="karstenw.20230303123446.109">@property
def session(self):
    """ Yields the dictionary of session data.
    """
    return cp.session

</t>
<t tx="karstenw.20230303123446.11">def __getattr__(self, k):
    return self[k] # Row.[field]


</t>
<t tx="karstenw.20230303123446.110">@property
def request(self):
    """ Yields a request object with metadata
        (IP address, request path, query data and headers).
    """
    r = cp.request # Deep copy (ensures garbage colletion).
    return HTTPRequest(
            app = self,
             ip = r.remote.ip,
           path = r.path_info,
         method = r.method,
           data = r.params,
        headers = r.headers)

</t>
<t tx="karstenw.20230303123446.111">@property
def response(self):
    """ Yields a response object with metadata
        (status, headers).
    """
    return cp.response

</t>
<t tx="karstenw.20230303123446.112">@property
def elapsed(self):
    """ Yields the elapsed time since the start of the request.
    """
    return time.time() - cp.request.time # See also _request_time().

</t>
<t tx="karstenw.20230303123446.113">def _cast(self, v):
    """ Returns the given value as a string (used to cast handler functions).
        If the value is a dictionary, returns a JSON-string.
        If the value is a generator, starts a stream.
        If the value is an iterable, joins the values with a space.
    """
    if isinstance(v, str):
        return v
    if isinstance(v, cp.lib.file_generator): # serve_file()
        return v
    if isinstance(v, dict):
        cp.response.headers["Content-Type"] = "application/json; charset=utf-8"
        return json.dumps(v)
    if isinstance(v, types.GeneratorType):
        cp.response.stream = True
        return iter(self._cast(v) for v in v)
    if isinstance(v, (list, tuple, set)):
        return " ".join(self._cast(v) for v in v)
    if isinstance(v, HTTPError):
        raise cp.HTTPError(v.status, message=v.message)
    if v is None:
        return ""
    try: # (bool, int, float, object.__unicode__)
        return str(v)
    except:
        return encode_entities(repr(v))

</t>
<t tx="karstenw.20230303123446.114">@cp.expose
def default(self, *path, **data):
    """ Resolves URL paths to handler functions and casts the return value.
    """
    # Enable cross-origin resource sharing (CORS, by default: "*")
    cp.response.headers["Access-Control-Allow-Origin"] = self._xhr
    # If there is an app.thread.db connection,
    # pass it as a keyword argument named "db".
    # If there is a query parameter named "db",
    # it is overwritten (the reverse is not safe).
    for k, v in g.items():
        data[k] = v
    # Call the handler function for the given path.
    # Call @app.error(404) if no handler is found.
    # Call @app.error(403) if rate limit forbidden (= no API key).
    # Call @app.error(429) if rate limit exceeded.
    # Call @app.error(503) if a database error occurs.
    try:
        v = self.router(path, **data)
    except RouteError:
        raise cp.HTTPError("404 Not Found")
    except RateLimitForbidden:
        raise cp.HTTPError("403 Forbidden")
    except RateLimitExceeded:
        raise cp.HTTPError("429 Too Many Requests")
    except DatabaseError as e:
        raise cp.HTTPError("503 Service Unavailable", message=str(e))
    except HTTPRedirect as e:
        raise cp.HTTPRedirect(e.url)
    except HTTPError as e:
        raise cp.HTTPError(e.status, message=e.message)
    v = self._cast(v)
    #print(self.elapsed)
    return v

</t>
<t tx="karstenw.20230303123446.115">def route(self, path, limit=False, time=None, key=lambda data: data.get("key"), reset=100000):
    """ The @app.route(path) decorator defines the handler function for the given path.
        The function can take arguments (path) and keyword arguments (query data), e.g.,
        if no handler exists for URL "/api/1/en", but a handler exists for URL "/api/1",
        this handler will be called with 1 argument: "en".
        It returns a string, a generator or a dictionary (which is parsed to a JSON-string).
    """
    _a = (key, limit, time, reset) # Avoid ambiguity with key=lambda inside define().

    def decorator(handler):
        def ratelimited(handler):
            # With @app.route(path, limit=True), rate limiting is applied.
            # The handler function is wrapped in a function that first calls
            # RateLimit()(key, path, limit, time) before calling the handler.
            # By default, a query parameter "key" is expected.
            # If the key is known, apply rate limiting (429 Too Many Requests).
            # If the key is unknown or None, deny access (403 Forbidden).
            # If the key is unknown and a default limit and time are given,
            # grant this IP address these default credentials, e.g.:
            # @app.route(path, limit=100, time=HOUR).
            # This grants each IP-address a 100 requests per hour.
            @self.thread(START)
            def connect():
                g.rate = RateLimit(name=self._rate)

            def wrapper(*args, **kwargs):
                self = cp.request.app.root
                self.rate(
                       ip = cp.request.remote.ip,
                      key = _a[0](cp.request.params),
                     path = "/" + path.strip("/"),
                    limit = _a[1], # Default limit for each IP.
                     time = _a[2], # Default time for each IP.
                    reset = _a[3]  # Threshold for clearing cache.
                )
                return handler(*args, **kwargs)
            return wrapper
        if limit is True or (limit is not False and limit is not None and time is not None):
            handler = ratelimited(handler)
        self.router[path] = handler # Register the handler.
        return handler
    return decorator

</t>
<t tx="karstenw.20230303123446.116">def error(self, code="*"):
    """ The @app.error(code) decorator defines the handler function for the given HTTP error.
        The function takes a HTTPError object and returns a string.
    """
    def decorator(handler):
        # CherryPy error handlers take keyword arguments.
        # Wrap as a HTTPError and pass it to the handler.
        def wrapper(status="", message="", traceback="", version=""):
            # Avoid CherryPy bug "ValueError: status message was not supplied":
            v = handler(HTTPError(status, message, traceback))
            v = self._cast(v) if not isinstance(v, HTTPError) else repr(v)
            return v
        # app.error("*") catches all error codes.
        if code in ("*", None):
            cp.config.update({"error_page.default": wrapper})
        # app.error(404) catches 404 error codes.
        elif isinstance(code, (int, str)):
            cp.config.update({"error_page.%s" % code: wrapper})
        # app.error((404, 500)) catches 404 + 500 error codes.
        elif isinstance(code, (tuple, list)):
            for x in code:
                cp.config.update({"error_page.%s" % x: wrapper})
        return handler
    return decorator

</t>
<t tx="karstenw.20230303123446.117">def view(self, template, cached=True):
    """ The @app.view(template) decorator defines a template to format the handler function.
        The function returns a dict of keyword arguments for Template.render().
    """
    def decorator(handler):
        def wrapper(*args, **kwargs):
            if not hasattr(template, "render"): # bottle.py templates have render() too.
                t = Template(template, root=self.static, cached=cached)
            else:
                t = template
            v = handler(*args, **kwargs)
            if isinstance(v, dict):
                return t.render(**v) # {kwargs}
            return t.render(*v) # (globals(), locals(), {kwargs})
        return wrapper
    return decorator

</t>
<t tx="karstenw.20230303123446.118">class Thread(localdict):
    """ The @app.thread(event) decorator can be used to initialize thread-safe data.
        Get data (e.g., a database connection) with app.thread.[name] or g.[name].
    """

    @others
</t>
<t tx="karstenw.20230303123446.119">def __init__(self):
    localdict.__init__(self, data=cp.thread_data, handlers=set())

</t>
<t tx="karstenw.20230303123446.12">class DatabaseError(Exception):
    pass


</t>
<t tx="karstenw.20230303123446.120">def __call__(self, event=START): # START / STOP
    def decorator(handler):
        def wrapper(id):
            return handler()
        # If @app.thread() is called twice for
        # the same handler, register it only once.
        if not (event, handler) in self.handlers:
            self.handlers.add((event, handler))
            cp.engine.subscribe(event + "_thread", wrapper)
        return handler
    return decorator

</t>
<t tx="karstenw.20230303123446.121">@property
def rate(self, name="rate"):
    """ Yields a thread-safe connection to the app's RateLimit db.
    """
    if not hasattr(g, name):
        setattr(g, name, RateLimit(name=self._rate, owner=self._owner))
    return getattr(g, name)

</t>
<t tx="karstenw.20230303123446.122">def bind(self, name="db"):
    """ The @app.bind(name) decorator binds the given function to a keyword argument
        that can be used with @app.route() handlers.
        The return value is stored thread-safe in app.thread.[name] &amp; g.[name].
        The return value is available in handlers as a keyword argument [name].
    """
    # This is useful for multi-threaded database connections:
    # &gt;&gt;&gt;
    # &gt;&gt;&gt; @app.bind("db")
    # &gt;&gt;&gt; def db():
    # &gt;&gt;&gt;     return Database("products.db")
    # &gt;&gt;&gt;
    # &gt;&gt;&gt; @app.route("/products")
    # &gt;&gt;&gt; def products(id, db=None):
    # &gt;&gt;&gt;     return db.execute("select * from products where id=?", (id,))
    def decorator(handler):
        f = lambda: setattr(g, name, handler())
        f()
        self.thread(START)(f)
        return handler
    return decorator

</t>
<t tx="karstenw.20230303123446.123">def __getattr__(self, k):
    """ Yields the value of the bound function with the given name (e.g., app.db).
    """
    if k in self.__dict__:
        return self.__dict__[k]
    if k in g:
        return g[k]
    raise AttributeError("'%s' object has no attribute '%s'" % (self.__class__.__name__, k))

</t>
<t tx="karstenw.20230303123446.124">@property
def cached(self):
    """ The @app.cached decorator caches the return value of the given handler.
        This is useful if the handler is computationally expensive,
        and often called with the same arguments (e.g., recursion).
    """
    # Note: to cache path handlers, first do @app.cached, then @app.route(),
    # i.e., route the cached handler, don't cache the router (= no effect):
    # @app.route("/search")
    # @app.cached
    # def heavy_search(q):
    #     ...
    def decorator(handler):
        def wrapper(*args, **kwargs):
            # Cache return value for given arguments
            # (excepting Database objects).
            k = sorted((k, v) for k, v in kwargs.items() if not isinstance(v, Database))
            k = (handler, pickle.dumps(args), pickle.dumps(k))
            if len(self._cache) &gt;= self._cached:
                self._cache.clear()
            if k not in self._cache:
                self._cache[k] = handler(*args, **kwargs)
            return self._cache[k]
        return wrapper
    return decorator

memoize = cached

</t>
<t tx="karstenw.20230303123446.125">def task(self, interval=MINUTE):
    """ The @app.task(interval) decorator will call the given function repeatedly (in a thread).
        For example, this can be used to commit a Database.batch periodically,
        instead of executing and committing to a Database during each request.
    """
    def decorator(handler):
        _, _, args, kwargs = define(handler)

        def wrapper():
            # Bind data from @app.thread(START) or @app.set().
            m = cp.process.plugins.ThreadManager(cp.engine)
            m.acquire_thread()
            # If there is an app.thread.db connection,
            # pass it as a keyword argument named "db".
            return handler(**dict((k, v) for k, v in g.items() if k in kwargs))
        p = cp.process.plugins.BackgroundTask(interval, wrapper)
        p.start()
        return handler
    return decorator

</t>
<t tx="karstenw.20230303123446.126">def redirect(path, code=303):
    """ Redirects the server to another route handler path 
        (or to another server for absolute URL's).
    """
    raise HTTPRedirect(path, int(code))

</t>
<t tx="karstenw.20230303123446.127">def run(self, host=LOCALHOST, port=8080, threads=30, queue=20, timeout=10, sessions=False, embedded=False, xhr="*", ssl=None, debug=True):
    """ Starts the server.
        Static content (e.g., "g/img.jpg") is served from the App.static subfolder (e.g., "static/g").
        With threads=10, the server can handle up to 10 concurrent requests.
        With queue=10, the server will queue up to 10 waiting requests.
        With sessions=True, stores session id in cookie.
        With embedded=True, runs under Apache mod_wsgi.
        With xhr="*", the server will respond to cross-origin XMLHttpRequests.
        With ssl=(key, certificate), runs under https:// (see certificate() function).
        With debug=False, starts a production server.
    """
    # Do nothing if the app is running.
    if self._up:
        return
    self._host = str(host)
    self._port = int(port)
    self._up = True
    self._xhr = xhr
    # Production environment disables errors.
    if debug is False:
        cp.config.update({"environment": "production"})
    # Embedded environment (mod_wsgi) disables errors &amp; signal handlers.
    if embedded is True:
        cp.config.update({"environment": "embedded"})
    # Global configuration.
    # If more concurrent requests are made than can be queued / handled,
    # the server will time out and a "connection reset by peer" occurs.
    # Note: SQLite cannot handle many concurrent writes (e.g., UPDATE).
    else:
        cp.config.update({
            "server.socket_host": self._host,
            "server.socket_port": self._port,
            "server.socket_timeout": max(1, timeout),
            "server.socket_queue_size": max(1, queue),
            "server.thread_pool": max(1, threads),
            "server.thread_pool_max": -1
        })
    # Secure SSL (https://).
    if ssl:
        cp.config.update({
            "server.ssl_module": "builtin",
            "server.ssl_private_key": ssl[0] if os.path.exists(ssl[0]) else openable(ssl[0]),
            "server.ssl_certificate": ssl[1] if os.path.exists(ssl[1]) else openable(ssl[1])
        })
    # Static content is served from the /static subfolder,
    # e.g., &lt;img src="g/cat.jpg" /&gt; refers to "/static/g/cat.jpg".
    self._app = cp.tree.mount(self, "/",
        config={"/": {
            "tools.staticdir.on": self.static is not None,
            "tools.staticdir.dir": self.static,
            "tools.sessions.on": bool(sessions),
            "tools.sessions.timeout": 60 if sessions is True else int(sessions)
    }})
    # Static content can include favicon.ico
    self.favicon_ico = cp.tools.staticfile.handler(
        os.path.join(self.static, "favicon.ico")
    )
    # Relative root = project path.
    os.chdir(self._path)
    # With mod_wsgi, stdout is restricted.
    if embedded:
        sys.stdout = sys.stderr
    else:
        atexit.register(self.stop)
        cp.engine.start()
        cp.engine.block()

</t>
<t tx="karstenw.20230303123446.128">def stop(self):
    """ Stops the server (registered with atexit).
    """
    try:
        atexit._exithandlers.remove((self.stop, (), {}))
    except:
        pass
    cp.engine.exit()
    sys.stdout = sys.__stdout__
    self._host = None
    self._port = None
    self._app = None
    self._up = False

</t>
<t tx="karstenw.20230303123446.129">def __call__(self, *args, **kwargs):
    # Called when deployed with mod_wsgi.
    if self._app is not None:
        return self._app(*args, **kwargs)
    raise ApplicationError("application not running")

</t>
<t tx="karstenw.20230303123446.13">class Database(object):

    @others
#--- DATABASE TRANSACTION BUFFER -------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123446.130">def certificate(domain=LOCALHOST, country=None, state=None, city=None, company=None, contact=None, signed=True, **kwargs):
    """ Returns a (private key, certificate)-tuple for a secure SSL-encrypted https server.
        With signed=False, returns a (private key, certificate request)-tuple.
        Only works on Unix with OpenSSL.
    """
    # Generate private key.
    # &gt; openssl genrsa 2048 -out ssl.key
    s = subprocess.PIPE
    p = ("openssl", "genrsa", "%s" % kwargs.get("encryption", 2048))
    p = subprocess.Popen(p, stdin=s, stdout=s, stderr=s)
    k = kwargs.get("key") or p.communicate()[0]
    f = tempfile.NamedTemporaryFile(delete=False)
    f.write(k)
    f.close()
    # Generate certificate.
    # &gt; openssl req -new -x509 -days 365 -key ssl.key -out ssl.crt
    p = ("openssl", "req", "-new", "-key", f.name)
    p = p + ("-x509", "-days", "365") if signed else p
    p = subprocess.Popen(p, stdin=s, stdout=s, stderr=s)
    x = p.communicate("%s\n%s\n%s\n%s\n.\n%s\n%s\n\n\n" % (
          country or ".",       # BE
            state or ".",       # Antwerp
             city or ".",       # Antwerp
          company or ".",       # CLiPS
           domain or LOCALHOST, # Tom De Smedt
          contact or "."        # tom@organisms.be
    ))[0]
    os.unlink(f.name)
    return (k, x)

#k, x = certificate(country="BE", state="Antwerp", company="CLiPS", contact="tom@organisms.be")
#open("ssl.key", "w").write(k)
#open("ssl.crt", "w").write(x)
#app.run(ssl=("ssl.key", "ssl.crt"))

#---------------------------------------------------------------------------------------------------
# Apache + mod_wsgi installation notes (thanks to Frederik De Bleser).
# The APP placeholder is the URL of your app, e.g., pattern.emrg.be.
#
# 1) Create a DNS-record for APP, which maps the url to your server's IP-address.
#
# 2) sudo apt-get install apache2
#    sudo apt-get install libapache2-mod-wsgi
#
# 3) sudo mkdir -p /www/APP/static
#    sudo mkdir -p /www/APP/log
#
# 4) sudo nano /etc/apache2/sites-available/APP
#    &gt; &lt;VirtualHost *:80&gt;
#    &gt;     ServerName APP
#    &gt;     DocumentRoot /www/APP/static
#    &gt;     CustomLog /www/APP/logs/access.log combined
#    &gt;     ErrorLog /www/APP/logs/error.log
#    &gt;     WSGIScriptAlias / /www/APP/app.py
#    &gt;     WSGIDaemonProcess APP processes=1 threads=x
#    &gt;     WSGIProcessGroup APP
#    &gt; &lt;/VirtualHost&gt;
#
# 5) sudo nano /www/APP/app.py
#    &gt; from pattern.server import App
#    &gt; from pattern.text import sentiment
#    &gt;
#    &gt; app = application = App() # mod_wsgi app must be available as "application"!
#    &gt;
#    &gt; @app.route("/api/1/sentiment", limit=100, time=HOUR)
#    &gt; def api_sentiment(q=None, lang="en"):
#    &gt;     return {"polarity": sentiment(q, language=lang)[0]}
#    &gt;
#    &gt; app.run(embedded=True)
#
# 6) sudo a2ensite APP
#    sudo apache2ctl configtest
#    sudo service apache2 restart
#
# 7) Try: http://APP/api/1/sentiment?q=marvelously+extravagant&amp;lang=en

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123446.131">def redirect(path, code=303):
    """ Redirects the server to another route handler path 
        (or to another server for absolute URL's).
    """
    raise HTTPRedirect(path, int(code))

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303123446.132">def static(path, root=None, mimetype=None):
    """ Returns the contents of the file at the given absolute path.
        To serve relative paths from the app folder, use root=app.path.
    """
    p = os.path.join(root or "", path)
    p = os.path.realpath(p)
    return cp.lib.static.serve_file(p, content_type=mimetype)

#---------------------------------------------------------------------------------------------------
# http://cherrypy.readthedocs.org/en/latest/progguide/extending/customtools.html


</t>
<t tx="karstenw.20230303123446.133">def _register(event, handler):
    """ Registers the given event handler (e.g., "on_end_request").
    """
    k = handler.__name__
    setattr(cp.tools, k, cp.Tool(event, handler))
    cp.config.update({"tools.%s.on" % k: True})


</t>
<t tx="karstenw.20230303123446.134">def _request_start():
    # Register request start time.
    cp.request.time = time.time()


</t>
<t tx="karstenw.20230303123446.135">def _request_end():
    #print(time.time() - cp.request.time)
    pass

_register("on_start_resource", _request_start)
_register("on_end_request", _request_end)

#---------------------------------------------------------------------------------------------------
# The error template used when the error handler itself raises an error.

cp._cperror._HTTPErrorTemplate = \
    "&lt;h1&gt;%(status)s&lt;/h1\n&gt;&lt;p&gt;%(message)s&lt;/p&gt;\n&lt;pre&gt;%(traceback)s&lt;/pre&gt;"

#### TEMPLATE ######################################################################################
# A template is a HTML-file with placeholders, which can be variable names or Python source code.
# Based on: http://davidbau.com/archives/2011/09/09/python_templating_with_stringfunction.html

_MARKUP = [
    r"\$[_a-z][\w]*",     # $var
    r"\$\{[_a-z][\w]*\}", # ${var}iable
    r"\&lt;\%=.*?\%\&gt;",      # &lt;%= var + 1 %&gt;
    r"\&lt;\%.*?\%\&gt;",       # &lt;% print(var) %&gt;
    r"\&lt;\%[^\n]*?"        # SyntaxError (no closing tag)
]

# &lt;% if x in y: %&gt; ... &lt;% end if %&gt;
# &lt;% for x in y: %&gt; ... &lt;% end for %&gt;
_MARKUP.insert(0, r"\&lt;\% if (.*?) : \%\&gt;(.*)\&lt;\% end if \%\&gt;") # No "elif", "else" yet.
_MARKUP.insert(1, r"\&lt;\% for (.*?) in (.*?) : \%\&gt;(.*)\&lt;\% end for \%\&gt;")

_MARKUP = (p.replace(" ", r"\s*") for p in _MARKUP)
_MARKUP = "(%s)" % "|".join(_MARKUP)
_MARKUP = re.compile(_MARKUP, re.I | re.S | re.M)


</t>
<t tx="karstenw.20230303123446.136">class Template(object):

    _cache = {}

    @others
</t>
<t tx="karstenw.20230303123446.137">def __init__(self, path, root=None, cached=True):
    """ A template with placeholders and/or source code loaded from the given string or path.
        Placeholders that start with $ are replaced with keyword arguments in Template.render().
        Source code enclosed in &lt;?= var + 100 ?&gt; is executed with eval().
        Source code enclosed in &lt;? write(var) ?&gt; is executed with exec().
    """
    p = os.path.join(root or "", path)
    k = hash(p)
    b = k in Template._cache
    # Caching enabled + template already cached.
    if cached is True and b is True:
        a = Template._cache[k]
    # Caching disabled / template not yet cached.
    if cached is False or b is False:
        a = "".join(static(p, mimetype="text/html")) if os.path.exists(p) else path
        a = self._compile(a)
    # Caching enabled + template not yet cached.
    if cached is True and b is False:
        a = Template._cache.setdefault(k, a)
    self._compiled = a

</t>
<t tx="karstenw.20230303123446.138">def _escape(self, s):
    """ Returns a string with no leading indentation and escaped newlines.
    """
    # Used in Template._compile() with eval() and exec().
    s = s.replace("\n", "\\n")
    s = textwrap.dedent(s)
    return s

</t>
<t tx="karstenw.20230303123446.139">def _encode(self, v, indent=""):
    """ Returns the given value as a string (empty string for None).
    """
    # Used in Template._render().
    v = "%s" % (v if v is not None else "")
    v = v.replace("\n", "\n" + indent) if indent else v
    return v

</t>
<t tx="karstenw.20230303123446.14">def __init__(self, name, **kwargs):
    """ Creates and opens the SQLite database with the given name.
    """
    k = kwargs.get
    self._name    = name
    self._type    = k("type", SQLITE)
    self._host    = k("host", LOCALHOST)
    self._port    = k("port", 3306)
    self._user    = k("user", (k("username", "root"), k("password", "")))
    self._factory = k("factory", Row)
    self._timeout = k("timeout", 10)
    self._connection = None
    if k("connect", True):
        self.connect()
    if k("schema"):
        # Database(schema="create table if not exists" `...`)
        # initializes the database table and index structure.
        for q in kwargs["schema"].split(";"):
            self.execute(q + ";", commit=False)
        self.commit()
    if k("owner"):
        # Database(owner="www-data")
        # grants write permission to user.
        chown(name, k("owner"))

</t>
<t tx="karstenw.20230303123446.140">def _dict(self, k="", v=[]):
    """ Returns a dictionary of keys k and values v, where k is a string.
        Used in Template._render() with &lt;for&gt; blocks.
    """
    # For example: "&lt;% for $i, $x in enumerate([1, 2, 3]): %&gt;",
    # "$i, $x" is mapped to {"i": 0, "x": 1}, {"i": 1, "x": 2}, ...
    # Nested tuples are not supported (e.g., "($i, ($k, $v))").
    k = [k.strip("$ ") for k in k.strip("()").split(",")]
    return dict(zip(k, v if len(k) &gt; 1 else [v]))

</t>
<t tx="karstenw.20230303123446.141">def _compile(self, string):
    """ Returns the template string as a (type, value, indent) list,
        where type is either &lt;str&gt;, &lt;arg&gt;, &lt;if&gt;, &lt;for&gt;, &lt;eval&gt; or &lt;exec&gt;.
        With &lt;eval&gt; and &lt;exec&gt;, value is a compiled code object
        that can be executed with eval() or exec() respectively.
    """
    a = []
    i = 0
    for m in _MARKUP.finditer(string):
        s = m.group(1)
        j = m.start(1)
        n = string[:j].count("\n")      # line number
        w = re.compile(r"(^|\n)(.*?)$") # line indent
        w = re.search(w, string[:j])
        w = re.sub(r"[^\t]", " ", string[w.start(2):j])
        if i != j:
            a.append(("&lt;str&gt;", string[i:j], ""))
        # $$escaped
        if s.startswith("$") and j &gt; 0 and string[j - 1] == "$":
            a.append(("&lt;str&gt;", s, ""))
        # ${var}iable
        elif s.startswith("${") and s.endswith("}"):
            a.append(("&lt;arg&gt;", s[2:-1], w))
        # $var
        elif s.startswith("$"):
            a.append(("&lt;arg&gt;", s[1:], w))
        # &lt;% if x in y: %&gt; ... &lt;% end if %&gt;
        elif s.startswith("&lt;%") and m.group(2):
            a.append(("&lt;if&gt;", (m.group(2), self._compile(m.group(3).lstrip("\n"))), w))
        # &lt;% for x in y: %&gt; ... &lt;% end for %&gt;
        elif s.startswith("&lt;%") and m.group(4):
            a.append(("&lt;for&gt;", (m.group(4), m.group(5), self._compile(m.group(6).lstrip("\n"))), w))
        # &lt;%= var + 1 %&gt;
        elif s.startswith("&lt;%=") and s.endswith("%&gt;"):
            a.append(("&lt;eval&gt;", compile("\n" * n + self._escape(s[3:-2]), "&lt;string&gt;", "eval"), w))
        # &lt;% print(var) %&gt;
        elif s.startswith("&lt;%") and s.endswith("%&gt;"):
            a.append(("&lt;exec&gt;", compile("\n" * n + self._escape(s[2:-2]), "&lt;string&gt;", "exec"), w))
        else:
            raise SyntaxError("template has no end tag for '%s' (line %s)" % (s, n + 1))
        i = m.end(1)
    a.append(("&lt;str&gt;", string[i:], ""))
    return a

</t>
<t tx="karstenw.20230303123446.142">def _render(self, compiled, *args, **kwargs):
    """ Returns the rendered string as an iterator.
        Replaces template placeholders with keyword arguments (if any).
        Replaces source code with the return value of eval() or exec().
    """
    k = {}
    for d in args:
        k.update(d)
    k.update(kwargs)
    k["template"] = template
    indent = kwargs.pop("indent", False)
    for cmd, v, w in compiled:
        if indent is False:
            w = ""
        if cmd is None:
            continue
        elif cmd == "&lt;str&gt;":
            yield self._encode(v, w)
        elif cmd == "&lt;arg&gt;":
            yield self._encode(k.get(v, "$" + v), w)
        elif cmd == "&lt;if&gt;":
            yield "".join(self._render(v[1], k)) if eval(v[0]) else ""
        elif cmd == "&lt;for&gt;":
            yield "".join(["".join(self._render(v[2], k, self._dict(v[0], i))) for i in eval(v[1], k)])
        elif cmd == "&lt;eval&gt;":
            yield self._encode(eval(v, k), w)
        elif cmd == "&lt;exec&gt;":
            o = StringIO()
            k["write"] = o.write # Code blocks use write() for output.
            exec(v, k)
            yield self._encode(o.getvalue(), w)
            del k["write"]
            o.close()

</t>
<t tx="karstenw.20230303123446.143">def render(self, *args, **kwargs):
    """ Returns the rendered template as a string.
        Replaces template placeholders with keyword arguments (if any).
        Replaces source code with the return value of eval() or exec().
        The keyword arguments are used as namespace for eval() and exec().
        For example, source code in Template.render(re=re) has access to the regex library.
        Multiple dictionaries can be given, e.g.,
        Template.render(globals(), locals(), foo="bar").
        Code blocks in &lt;? ?&gt; can use write() and template().
    """
    return "".join(self._render(self._compiled, *args, **kwargs))


</t>
<t tx="karstenw.20230303123446.144">def template(string, *args, **kwargs):
    """ Returns the rendered template as a string.
    """
    if hasattr(string, "render"):
        return string.render(*args, **kwargs)
    root, cached = (
        kwargs.pop("root", None),
        kwargs.pop("cached", None))
    if root is None and len(args) &gt; 0 and isinstance(args[0], str):
        root = args[0]
        args = args[1:]
    return Template(string, root, cached).render(*args, **kwargs)

#s = """
#&lt;html&gt;
#&lt;head&gt;
#    &lt;title&gt;$title&lt;/title&gt;
#&lt;/head&gt;
#&lt;body&gt;
#&lt;% for $i, $name in enumerate(names): %&gt;
#    &lt;b&gt;&lt;%= i+1 %&gt;) Hello $name!&lt;/b&gt;
#&lt;% end for %&gt;
#&lt;/body&gt;
#&lt;/html&gt;
#"""
#
#print(template(s.strip(), title="test", names=["Tom", "Walter"]))

#### HTML ##########################################################################################
# Useful HTML generators.


</t>
<t tx="karstenw.20230303123446.145">class HTML:

    @others
html = HTML()

</t>
<t tx="karstenw.20230303123446.146">def _attrs(self, **kwargs):
    """ Returns a string of HTML element attributes.
        Use "css" for the CSS classname (since "class" is a reserved word).
    """
    a = []
    if "id" in kwargs:
        a.append("id=\"%s\"" % kwargs.pop("id"))
    if "name" in kwargs:
        a.append("name=\"%s\"" % kwargs.pop("name"))
    if "css" in kwargs:
        a.append("class=\"%s\"" % kwargs.pop("css"))
    for k, v in kwargs.items():
        a.append("%s=\"%s\"" % (k, v))
    return (" " + " ".join(a)).rstrip()

</t>
<t tx="karstenw.20230303123446.147">def div(self, content, **attributes):
    """ Returns a string with a HTML &lt;div&gt; with the given content.
    """
    return "&lt;div%s&gt;\n\t%s\n&lt;/div&gt;\n" % (self._attrs(**attributes), content)

</t>
<t tx="karstenw.20230303123446.148">def span(self, content, **attributes):
    """ Returns a string with a HTML &lt;span&gt; with the given content.
    """
    return "&lt;span%s&gt;\n\t%s\n&lt;/span&gt;\n" % (self._attrs(**attributes), content)

</t>
<t tx="karstenw.20230303123446.149">def table(self, rows=[], headers=[], striped=True, **attributes):
    """ Returns a string with a HTML &lt;table&gt; for the given list,
        where each item is a list of values.
        With striped=True, generates &lt;tr class="even|odd"&gt;.
        With striped=True and headers, generates &lt;td class="header[i]"&gt;.
    """
    h = list(headers)
    r = list(rows) if not h else [h] + list(rows)
    a = ["&lt;table%s&gt;\n" % self._attrs(**attributes)]
    if h:
        a.append("\t&lt;colgroup&gt;\n")
        a.extend("\t\t&lt;col class=\"%s\"&gt;\n" % v for v in h)
        a.append("\t&lt;/colgroup&gt;\n")
    for i, row in enumerate(r):
        a.append("\t&lt;tr%s&gt;\n" % (" class=\"%s\"" % ("odd", "even")[i % 2] if striped else ""))
        for j, v in enumerate(row):
            if i == 0 and h:
                a.append("\t\t&lt;th&gt;%s&lt;/th&gt;\n" % v)
            else:
                a.append("\t\t&lt;td&gt;%s&lt;/td&gt;\n" % v)
        a.append("\t&lt;/tr&gt;\n")
    a.append("&lt;/table&gt;\n")
    return "".join(a)

</t>
<t tx="karstenw.20230303123446.15">@property
def name(self):
    """ Yields the database name (for SQLITE, file path).
    """
    return self._name

</t>
<t tx="karstenw.20230303123446.150">def select(self, options={}, selected=None, **attributes):
    """ Returns a string with a HTML &lt;select&gt; for the given dictionary,
        where each dict item is an &lt;option value="key"&gt;value&lt;/option&gt;.
    """
    a = ["&lt;select%s&gt;\n" % self._attrs(**attributes)]
    for k, v in sorted(options.items()):
        if k == selected:
            a.append("\t&lt;option value=\"%s\" selected&gt;%s&lt;/option&gt;\n" % (k, v))
        else:
            a.append("\t&lt;option value=\"%s\"&gt;%s&lt;/option&gt;\n" % (k, v))
    a.append("&lt;/select&gt;\n")
    return "".join(a)

dropdown = select

</t>
<t tx="karstenw.20230303123446.16">@property
def type(self):
    """ Yields the database type (SQLITE or MYSQL).
    """
    return self._type

</t>
<t tx="karstenw.20230303123446.17">@property
def host(self):
    """ Yields the database server host (MYSQL).
    """
    return self._host

</t>
<t tx="karstenw.20230303123446.18">@property
def port(self):
    """ Yields the database server port (MYSQL).
    """
    return self._port

</t>
<t tx="karstenw.20230303123446.19">@property
def connection(self):
    """ Yields the sqlite3.Connection object.
    """
    return self._connection

</t>
<t tx="karstenw.20230303123446.2">def chown(path, owner=None):
    """ Changes the ownership of the given file to the given (user, group).
        Returns True if successful.
    """
    if owner:
        try:
            x, y = owner, -1 # x = user, y = group
            x, y = x if isinstance(x, tuple) else (x, y)
            x, y = (pwd.getpwnam(x).pw_uid if not isinstance(x, int) else x,
                    grp.getgrnam(y).gr_gid if not isinstance(y, int) else y)
            os.chown(path, x, y)
            return True
        except:
            return False

</t>
<t tx="karstenw.20230303123446.20">def connect(self):
    if self._type == SQLITE:
        self._connection = sqlite.connect(self._name, timeout=self._timeout)
        self._connection.row_factory = self._factory
    if self._type == MYSQL:
        import MySQLdb
        self._connection = MySQLdb.connect(
              host = self._host,
              port = self._port,
              user = self._user[0],
            passwd = self._user[1],
   connect_timeout = self._timeout,
       use_unicode = True,
           charset = "utf8"
        )
        self._connection.row_factory = self._factory
        self._connection.cursor().execute("create database if not exists `%s`" % self._name)
        self._connection.cursor().execute("use `%s`" % self._name)

</t>
<t tx="karstenw.20230303123446.21">def disconnect(self):
    if self._connection is not None:
        self._connection.commit()
        self._connection.close()
        self._connection = None

</t>
<t tx="karstenw.20230303123446.22">def execute(self, sql, values=(), first=False, commit=True):
    """ Executes the given SQL query string and returns an iterator of rows.
        With first=True, returns the first row.
    """
    try:
        r = self._connection.cursor().execute(sql, values)
        if commit:
            self._connection.commit()
    except Exception as e:
        # "OperationalError: database is locked" means that
        # SQLite is receiving too many concurrent write ops.
        # A write operation locks the entire database;
        # other threaded connections may time out waiting.
        # In this case you can raise Database(timeout=10),
        # lower Application.run(threads=10) or switch to MySQL or Redis.
        self._connection.rollback()
        raise DatabaseError(str(e))
    return r.fetchone() if first else r

</t>
<t tx="karstenw.20230303123446.23">def commit(self):
    """ Commits changes (pending insert/update/delete queries).
    """
    self._connection.commit()

</t>
<t tx="karstenw.20230303123446.24">def rollback(self):
    """ Discard changes since the last commit.
    """
    self._connection.rollback()

</t>
<t tx="karstenw.20230303123446.25">def __call__(self, *args, **kwargs):
    return self.execute(*args, **kwargs)

</t>
<t tx="karstenw.20230303123446.26">def __repr__(self):
    return "Database(name=%s)" % repr(self._name)

</t>
<t tx="karstenw.20230303123446.27">def __del__(self):
    try:
        self.disconnect()
    except:
        pass

</t>
<t tx="karstenw.20230303123446.28">@property
def batch(self):
    return Database._batch.setdefault(self._name, DatabaseTransaction(self._name, **self.__dict__))

_batch = {} # Shared across all instances.

</t>
<t tx="karstenw.20230303123446.29">class DatabaseTransaction(Database):

    @others

</t>
<t tx="karstenw.20230303123446.3">def encode_entities(string):
    """ Encodes HTML entities in the given string ("&lt;" =&gt; "&amp;lt;").
        For example, to display "&lt;em&gt;hello&lt;/em&gt;" in a browser,
        we need to pass "&amp;lt;em&amp;gt;hello&amp;lt;/em&amp;gt;" (otherwise "hello" in italic is displayed).
    """
    if isinstance(string, str):
        string = RE_AMPERSAND.sub("&amp;amp;", string)
        string = string.replace("&lt;", "&amp;lt;")
        string = string.replace("&gt;", "&amp;gt;")
        string = string.replace('"', "&amp;quot;")
        string = string.replace("'", "&amp;#39;")
    return string


</t>
<t tx="karstenw.20230303123446.30">def __init__(self, name, **kwargs):
    """ Database.batch.execute() stores given the SQL query in RAM memory, across threads.
        Database.batch.commit() commits all buffered queries.
        This can be combined with @app.task() to periodically write batches to the database
        (instead of writing on each request).
    """
    Database.__init__(self, name, **dict(kwargs, connect=False))
    self._queue = []

</t>
<t tx="karstenw.20230303123446.31">def execute(self, sql, values=()):
    self._queue.append((sql, values))

</t>
<t tx="karstenw.20230303123446.32">def commit(self):
    q, self._queue = self._queue, []
    if q:
        try:
            Database.connect(self)  # Connect in this thread.
            for sql, v in q:
                Database.execute(self, sql, v, commit=False)
            Database.commit(self)
        except DatabaseError as e:
            Database.rollback(self) # Data in q will be lost.
            raise e

</t>
<t tx="karstenw.20230303123446.33">def rollback(self):
    self._queue = []

</t>
<t tx="karstenw.20230303123446.34">def __len__(self):
    return len(self._queue)

</t>
<t tx="karstenw.20230303123446.35">def __call__(self, *args, **kwargs):
    return self.execute(*args, **kwargs)

</t>
<t tx="karstenw.20230303123446.36">def __repr__(self):
    return "DatabaseTransaction(name=%s)" % repr(self._name)

</t>
<t tx="karstenw.20230303123446.37">@property
def batch(self):
    raise AttributeError

</t>
<t tx="karstenw.20230303123446.38">def pbkdf2(s, salt, iterations=10000, n=32, f="sha256"):
    """ Returns a hashed string of length n using the PBKDF2 algorithm.
        Password-Based Key Derivation Function 2 uses a cryptographic salt
        and multiple iterations of a pseudorandom function ("key stretching").
    """
    h = hmac.new(s, digestmod=getattr(hashlib, f))

    def prf(h, s):
        h = h.copy()
        h.update(s)
        return bytearray(h.digest())
    k = bytearray()
    i = 1
    while len(k) &lt; n:
        a = b = prf(h, salt + struct.pack('&gt;i', i))
        for _ in range(iterations - 1):
            a = prf(h, a)
            b = bytearray(x ^ y for x, y in zip(b, a))
        k += b
        i += 1
    return str(k)[:n].encode("hex")


</t>
<t tx="karstenw.20230303123446.39">def streql(s1, s2):
    """ Returns True if the given strings are identical.
    """
    if len(s1) != len(s2):
        return False
    b = True
    for ch1, ch2 in zip(s1, s2):
        if ch1 != ch2:
            b = False # contstant-time comparison
    return b


</t>
<t tx="karstenw.20230303123446.4">def decode_entities(string):
    """ Decodes HTML entities in the given string ("&amp;lt;" =&gt; "&lt;").
    """
    # http://snippets.dzone.com/posts/show/4569
    def replace_entity(match):
        hash, hex, name = match.group(1), match.group(2), match.group(3)
        if hash == "#" or name.isdigit():
            if hex == "":
                return chr(int(name))                 # "&amp;#38;" =&gt; "&amp;"
            if hex.lower() == "x":
                return chr(int("0x" + name, 16))      # "&amp;#x0026;" = &gt; "&amp;"
        else:
            cp = htmlentitydefs.name2codepoint.get(name) # "&amp;amp;" =&gt; "&amp;"
            return chr(cp) if cp else match.group()   # "&amp;foo;" =&gt; "&amp;foo;"
    if isinstance(string, str):
        return RE_UNICODE.subn(replace_entity, string)[0]
    return string


</t>
<t tx="karstenw.20230303123446.40">def encode_password(s):
    """ Returns a PBKDF2-hashed string.
    """
    if isinstance(s, str):
        s = s.encode("utf-8")
    x = base64.b64encode(os.urandom(32))
    return "pbkdf2:sha256:10000:%s:%s" % (x, pbkdf2(s[:1024], x))


</t>
<t tx="karstenw.20230303123446.41">def verify_password(s1, s2):
    """ Returns True if the given strings are identical, after hashing the first.
    """
    if isinstance(s1, str):
        s1 = s1.encode("utf-8")
    if isinstance(s2, str):
        s2 = s2.encode("utf-8")
    m, f, n, x, s2 = s2.split(":")
    return streql(pbkdf2(s1[:1024], x, int(n), len(s2) / 2, f), s2)

# k1 = "1234"
# k2 = encode_password(k1)
# print(k2)
# print(verify_password(k1, k2))

#---------------------------------------------------------------------------------------------------
# MySQL on Mac OS X installation notes:

# 1) Download Sequel Pro: http://www.sequelpro.com (GUI).
# 2) Download MySQL .dmg: http://dev.mysql.com/downloads/mysql/ (for 64-bit Python, 64-bit MySQL).
# 3) Install the .pkg, startup item and preferences pane.
# 4) Start server in preferences pane (user: "root", password: "").
# 5) Command line: open -a "TextEdit" .bash_profile =&gt;
# 6) export PATH=~/bin:/usr/local/bin:/usr/local/mysql/bin:$PATH
# 7) Command line: sudo pip install MySQL-python
# 8) Command line: sudo ln -s /usr/local/mysql/lib/libmysqlclient.xx.dylib
#                             /usr/lib/libmysqlclient.xx.dylib
# 9) import MySQLdb

#### RATE LIMITING #################################################################################
# With @app.route(path, limit=True), the decorated URL path handler function calls RateLimit().
# For performance, rate limiting uses a RAM cache of api keys + the time of the last request.
# This will not work with multi-processing, since each process gets its own RAM.

_RATELIMIT_CACHE = {} # RAM cache of request counts.
_RATELIMIT_LOCK = threading.RLock()

SECOND, MINUTE, HOUR, DAY = 1., 60., 60 * 60., 60 * 60 * 24.


</t>
<t tx="karstenw.20230303123446.42">class RateLimitError(Exception):
    pass


</t>
<t tx="karstenw.20230303123446.43">class RateLimitExceeded(RateLimitError):
    pass


</t>
<t tx="karstenw.20230303123446.44">class RateLimitForbidden(RateLimitError):
    pass


</t>
<t tx="karstenw.20230303123446.45">class RateLimit(Database):

    @others


</t>
<t tx="karstenw.20230303123446.46">def __init__(self, name="rate.db", **kwargs):
    """ A database for rate limiting API requests.
        It manages a table with (key, path, limit, time) entries.
        It grants each key a rate (number of requests / time) for a URL path.
        It keeps track of the number of requests in local memory (i.e., RAM).
        If RateLimit()() is called with the optional limit and time arguments,
        each IP address is granted this rate (even without a key).
    """
    Database.__init__(self, name, **dict(kwargs, factory=None, schema=(
        "create table if not exists `rate` ("
              "`key` text,"    # API key (e.g., ?key="1234").
             "`path` text,"    # API URL path (e.g., "/api/1/").
            "`limit` integer," # Maximum number of requests.
             "`time` float"    # Time frame.
        ");"
        "create index if not exists `rate1` on rate(key);"
        "create index if not exists `rate2` on rate(path);")
    ))
    self.load()

</t>
<t tx="karstenw.20230303123446.47">@property
def cache(self):
    return _RATELIMIT_CACHE

</t>
<t tx="karstenw.20230303123446.48">@property
def lock(self):
    return _RATELIMIT_LOCK

</t>
<t tx="karstenw.20230303123446.49">@property
def key(self, pairs=("rA", "aZ", "gQ", "hH", "hG", "aR", "DD"), n=32):
    """ Yields a new random key ("ZjNmYTc4ZDk0MTkyYk...").
    """
    k = str(random.getrandbits(256))
    k = hashlib.sha256(k).hexdigest()
    k = base64.b64encode(k, random.choice(pairs)).rstrip('==')
    return k[:n]

</t>
<t tx="karstenw.20230303123446.5">def encode_url(string):
    return urllib.quote_plus(string.encode("utf-8")) # "black/white" =&gt; "black%2Fwhite".


</t>
<t tx="karstenw.20230303123446.50">def reset(self):
    self.cache.clear()
    self.load()

</t>
<t tx="karstenw.20230303123446.51">def load(self):
    """ For performance, rate limiting is handled in memory (i.e., RAM).
        Loads the stored rate limits in memory (5,000 records ~= 1MB RAM).
    """
    with self.lock:
        if not self.cache:
            # Lock concurrent threads when modifying cache.
            for r in self.execute("select * from `rate`;"):
                self.cache[(r[0], r[1])] = (0, r[2], r[3], _time.time())
        self._n = len(self.cache)

</t>
<t tx="karstenw.20230303123446.52">def set(self, key, path="/", limit=100, time=HOUR):
    """ Sets the rate for the given key and path,
        where limit is the maximum number of requests in the given time (e.g., 100/hour).
    """
    # Map time as str to float.
    time = {"second": SECOND, "minute": MINUTE, "hour": HOUR, "day": DAY}.get(time, time)
    # Update database.
    p = "/" + path.strip("/")
    q1 = "delete from `rate` where key=? and path=?;"
    q2 = "insert into `rate` values (?, ?, ?, ?);"
    self.execute(q1, (key, p), commit=False)
    self.execute(q2, (key, p, limit, time))
    # Update cache.
    with self.lock:
        self.cache[(key, p)] = (0, limit, time, _time.time())
        self._n += 1
    return (key, path, limit, time)

</t>
<t tx="karstenw.20230303123446.53">def get(self, key, path="/"):
    """ Returns the rate for the given key and path (or None).
    """
    p = "/" + path.strip("/")
    q = "select * from `rate` where key=? and path=?;"
    return self.execute(q, (key, p), first=True, commit=False)

</t>
<t tx="karstenw.20230303123446.54">def delete(self, key, path="/"):
    """ Revokes the rate for the given key and path.
    """
    p = "/" + path.strip("/")
    q = "delete from `rate` where key=? and path=?;"
    self.execute(q, (key, p))
    with self.lock:
        if self.cache.pop((key, p), False):
            self._n -= 1

</t>
<t tx="karstenw.20230303123446.55">def __setitem__(self, k, v): # (key, path), (limit, time)
    return self.set(*(k + v))

</t>
<t tx="karstenw.20230303123446.56">def __getitem__(self, k):    # (key, path)
    return self.get(*k)

</t>
<t tx="karstenw.20230303123446.57">def __delitem__(self, k):    # (key, path)
    return self.delete(*k)

</t>
<t tx="karstenw.20230303123446.58">def __contains__(self, key, path="%"):
    """ Returns True if the given key exists (for the given path).
    """
    q = "select * from `rate` where key=? and path like ?;"
    return self.execute(q, (key, path), first=True, commit=False) is not None

</t>
<t tx="karstenw.20230303123446.59">def __call__(self, key, path="/", limit=None, time=None, ip=None, reset=100000):
    """ Increases the (cached) request count by 1 for the given key and path.
        If the request count exceeds its limit, raises RateLimitExceeded.
        If the given key does not exist, each IP address (if given) gets 
        limit / time requests. Otherwise, a RateLimitForbidden is raised.
    """
    with self.lock:
        t = _time.time()
        p = "/" + path.strip("/")
        r = self.cache.get((key, p))
        # Reset the cache if too large (e.g., 1M+ IP addresses).
        if reset and reset &lt; len(self.cache) and reset &gt; self._n:
            self.reset()
        # Unknown key: apply root key (if any).
        if r is None and p != "/":
            r = self.cache.get((key, "/"))
        # Unknown key: apply default limit (if IP).
        if r is None and ip is not None and limit is not None and time is not None:
            r = self.cache.setdefault((ip, p), (0, limit, time, t))
            key = ip
        # Unknown key.
        if r is None:
            raise RateLimitForbidden
        # Limit reached within time frame (raise error).
        elif r[0] &gt;= r[1] and r[2] &gt; t - r[3]:
            raise RateLimitExceeded
        # Limit reached out of time frame (reset count).
        elif r[0] &gt;= r[1]:
            self.cache[(key, p)] = (1, r[1], r[2], t)
        # Limit not reached (increment count).
        elif r[0] &lt; r[1]:
            self.cache[(key, p)] = (r[0] + 1, r[1], r[2], r[3])
    #print(self.cache.get((key, path)))

</t>
<t tx="karstenw.20230303123446.6">def decode_url(string):
    return urllib.unquote_plus(string)


</t>
<t tx="karstenw.20230303123446.60">def count(self, key, path="/"):
    """ Returns the current count for the given key and path.
    """
    with self.lock:
        p = "/" + path.strip("/")
        r = self.cache.get((key, p), [0])[0]
        return r

</t>
<t tx="karstenw.20230303123446.61">class RouteError(Exception):
    pass


</t>
<t tx="karstenw.20230303123446.62">class Router(dict):

    @others
</t>
<t tx="karstenw.20230303123446.63">def __init__(self):
    """ A router resolves URL paths to handler functions.
    """
    pass

</t>
<t tx="karstenw.20230303123446.64">def __setitem__(self, path, handler):
    """ Defines the handler function for the given URL path.
        The path is a slash-formatted string (e.g., "/api/1/en/parser").
        The handler is a function that takes 
        arguments (path) and keyword arguments (query data).
    """
    p = "/" + path.strip("/")
    p = p.lower()
    p = p.encode("utf8") if isinstance(p, str) else p
    # Store the handler + its argument names (tuple(args), dict(kwargs)),
    # so that we can call this function without (all) keyword arguments,
    # if it does not take (all) query data.
    if callable(handler):
        dict.__setitem__(self, p, (handler, define(handler)[2:]))
    else:
        dict.__setitem__(self, p, (handler, ((), {})))

</t>
<t tx="karstenw.20230303123446.65">def __call__(self, path, **data):
    """ Calls the handler function for the given URL path.
        If no handler is found, raises a RouteError.
        If a base handler is found (e.g., "/api" for "/api/1/en"),
        calls the handler with arguments (e.g., handler("1", "en")).
    """
    if not isinstance(path, tuple):
        path = path.strip("/").split("/") # ["api", "1", "en"]
    n = len(path)
    for i in range(n + 1):
        p0 = "/" + "/".join(path[:n - i])
        p0 = p0.lower()                   # "/api/1/en", "/api/1", "/api", ...
        p1 = path[n - i:]                   # [], ["en"], ["1", "en"], ...
        if p0 in self:
            (handler, (args, kwargs)) = self[p0]
            i = len(p1)
            j = len(args) if args is not True else i
            # Handler takes 1 argument, 0 given (pass None for convenience).
            if i == 0 and j == 1:
                p1 = (None,)
                i = j
            # Handler does not take path.
            if i != j:
                continue
            # Handler is a string / dict.
            if not callable(handler):
                return handler
            # Handler takes path, but no query data.
            if not kwargs:
                return handler(*p1)
            # Handler takes path and all query data.
            if kwargs is True:
                return handler(*p1, **data)
            # Handler takes path and some query data.
            return handler(*p1, **dict((k, v) for k, v in data.items() if k in kwargs))
    # No handler.
    raise RouteError

</t>
<t tx="karstenw.20230303123446.66">class HTTPRequest(object):

    @others
</t>
<t tx="karstenw.20230303123446.67">def __init__(self, app, ip, path="/", method="get", data={}, headers={}):
    """ A HTTP request object with metadata returned from app.request.
    """
    self.app = app
    self.ip = ip
    self.path = "/" + path.strip("/")
    self.method = method.lower()
    self.data = dict(data)
    self.headers = dict(headers)

</t>
<t tx="karstenw.20230303123446.68">def __repr__(self):
    return "HTTPRequest(ip=%s, path=%s)" % (repr(self.ip), repr(self.path))


</t>
<t tx="karstenw.20230303123446.69">class HTTPRedirect(Exception):

    @others
</t>
<t tx="karstenw.20230303123446.7">def openable(string, **kwargs):
    """ Returns the path to a temporary file that contains the given string.
    """
    f = tempfile.NamedTemporaryFile(**kwargs)
    f.write(string)
    f.seek(0)
    _TEMPORARY_FILES.append(f) # Delete when program terminates.
    return f.name


</t>
<t tx="karstenw.20230303123446.70">def __init__(self, url, code=303):
    """ A HTTP redirect raised in an @app.route() handler.
    """
    self.url = url
    self.code = code

</t>
<t tx="karstenw.20230303123446.71">def __repr__(self):
    return "HTTPRedirect(url=%s)" % repr(self.url)


</t>
<t tx="karstenw.20230303123446.72">class HTTPError(Exception):

    @others
</t>
<t tx="karstenw.20230303123446.73">def __init__(self, status="", message="", traceback=""):
    """ A HTTP error raised in an @app.route() handler + passed to @app.error().
    """
    self.code = int(status.split(" ")[0])
    self.status = status
    self.message = message
    self.traceback = traceback or ""

</t>
<t tx="karstenw.20230303123446.74">def __repr__(self):
    return "HTTPError(status=%s)" % repr(self.status)


</t>
<t tx="karstenw.20230303123446.75">def _HTTPErrorSubclass(status):
    if sys.version &gt; "3":
        return type("HTTP%sError" % status.split(" ")[0], (HTTPError,), {'__init__': \
            lambda self, message="", traceback="": HTTPError.__init__(self, status, message, traceback)})
    else:
        return type(b"HTTP%sError" % status.split(" ")[0].encode("utf-8"), (HTTPError,), {'__init__': \
            lambda self, message="", traceback="": HTTPError.__init__(self, status, message, traceback)})

</t>
<t tx="karstenw.20230303123446.76">class localdict(dict):

    @others
# Global alias for app.thread (Flask-style):
g = localdict(data=cp.thread_data)


</t>
<t tx="karstenw.20230303123446.77">def __init__(self, data=None, **kwargs):
    """ Thread-safe dictionary.
    """
    self.__dict__["_data"] = data if data is not None else threading.local()
    self.__dict__.update(kwargs) # Attributes are global in every thread.

</t>
<t tx="karstenw.20230303123446.78">def items(self):
    return self._data.__dict__.items()

</t>
<t tx="karstenw.20230303123446.79">def keys(self):
    return self._data.__dict__.keys()

</t>
<t tx="karstenw.20230303123446.8">def define(f):
    """ Returns (name, type, tuple, dict) for the given function,
        with a tuple of argument names and a dict of keyword arguments.
        If the given function has *args, returns True instead of tuple.
        If the given function has **kwargs, returns True instead of dict.
    """
    def undecorate(f): # "__closure__" in Py3.
        while getattr(f, "func_closure", None):
            f = [v.cell_contents for v in getattr(f, "func_closure")]
            f = [v for v in f if callable(v)]
            f = f[0]   # We need guess (arg could also be a function).
        return f
    f = undecorate(f)
    a = inspect.getargspec(f) # (names, *args, **kwargs, values)
    # a[0] = names
    # a[1] = args
    # a[2] = kwargs
    # a[3] = values
    
    # pdb.set_trace()

    # 
    i = len(a[0]) - len(a[3] or [])
    x = tuple(a[0][:i])
    y = dict(zip(a[0][i:], a[3] or []))
    x = x if not a[1] else True
    y = y if not a[2] else True
    return (f.__name__, type(f), x, y)

</t>
<t tx="karstenw.20230303123446.80">def values(self):
    return self._data.__dict__.values()

</t>
<t tx="karstenw.20230303123446.81">def update(self, d):
    return self._data.__dict__.update(d)

</t>
<t tx="karstenw.20230303123446.82">def clear(self):
    return self._data.__dict__.clear()

</t>
<t tx="karstenw.20230303123446.83">def pop(self, *kv):
    return self._data.__dict__.pop(*kv)

</t>
<t tx="karstenw.20230303123446.84">def setdefault(self, k, v=None):
    return self._data.__dict__.setdefault(k, v)

</t>
<t tx="karstenw.20230303123446.85">def set(self, k, v):
    return setattr(self._data, k, v)

</t>
<t tx="karstenw.20230303123446.86">def get(self, k, default=None):
    return getattr(self._data, k, default)

</t>
<t tx="karstenw.20230303123446.87">def __delitem__(self, k):
    return delattr(self._data, k)

</t>
<t tx="karstenw.20230303123446.88">def __getitem__(self, k):
    return getattr(self._data, k)

</t>
<t tx="karstenw.20230303123446.89">def __setitem__(self, k, v):
    return setattr(self._data, k, v)

</t>
<t tx="karstenw.20230303123446.9">class Row(dict):

    @others
</t>
<t tx="karstenw.20230303123446.90">def __delattr__(self, k):
    return delattr(self._data, k)

</t>
<t tx="karstenw.20230303123446.91">def __getattr__(self, k):
    return getattr(self._data, k)

</t>
<t tx="karstenw.20230303123446.92">def __setattr__(self, k, v):
    return setattr(self._data, k, v)

</t>
<t tx="karstenw.20230303123446.93">def __len__(self):
    return len(self._data.__dict__)

</t>
<t tx="karstenw.20230303123446.94">def __iter__(self):
    return iter(self._data.__dict__)

</t>
<t tx="karstenw.20230303123446.95">def __contains__(self, k):
    return k in self._data.__dict__

</t>
<t tx="karstenw.20230303123446.96">def __str__(self):
    return repr(self)

</t>
<t tx="karstenw.20230303123446.97">def __repr__(self):
    return "localdict({%s})" % ", ".join(
        ("%s: %s" % (repr(k), repr(v)) for k, v in self.items()))

</t>
<t tx="karstenw.20230303123446.98">def threadsafe(function):
    """ The @threadsafe decorator ensures that no two threads execute the function simultaneously.
    """
    # In some cases, global data must be available across all threads (e.g., rate limits).
    # Atomic operations like dict.get() or list.append() (= single execution step) are thread-safe,
    # but some operations like dict[k] += 1 are not, and require a lock.
    # http://effbot.org/zone/thread-synchronization.htm
    #
    # &gt;&gt;&gt; count = defaultdict(int)
    # &gt;&gt;&gt; @threadsafe
    # &gt;&gt;&gt; def inc(k):
    # &gt;&gt;&gt;     count[k] += 1
    #
    lock = threading.RLock()

    def decorator(*args, **kwargs):
        with lock:
            v = function(*args, **kwargs)
        return v
    return decorator


</t>
<t tx="karstenw.20230303123446.99">class ApplicationError(Exception):
    pass


</t>
<t tx="karstenw.20230303123854.1"># On Linux + Apache mod_wsgi, the user that executes the Python script is "www-data".
# If the app folder was created by "root", "www-data" will not have write permission,
# and consequently cannot write to an SQLite database (e.g., App.rate) in the folder,
# or create SQLite -journal files.

# The solution is for "www-data" to chown() the folder, and any database files in it.
# This can also be done from Python with App(owner=("www-data", "www-data"))
# and Database(owner=("www-data", "www-data")), which will call chown().

#### STRING FUNCTIONS ##############################################################################

RE_AMPERSAND = re.compile("\&amp;(?!\#)")           # &amp; not followed by #
RE_UNICODE = re.compile(r'&amp;(#?)(x|X?)(\w+);') # &amp;#201;

_TEMPORARY_FILES = []

</t>
<t tx="karstenw.20230303123934.1">#### INTROSPECTION #################################################################################
# URL paths are routed to handler functions, whose arguments represent URL path &amp; query parameters.
# So we need to know what the arguments and keywords arguments are at runtime.


</t>
<t tx="karstenw.20230303124013.1">#### DATABASE ######################################################################################

#--- DATABASE --------------------------------------------------------------------------------------
# A simple wrapper for SQLite and MySQL databases.

# Database type:
SQLITE, MYSQL = "sqlite", "mysql"

# Database host:
LOCALHOST = "127.0.0.1"


</t>
<t tx="karstenw.20230303124122.1">#--- DATABASE SECURITY -----------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303124220.1">#### ROUTER ########################################################################################
# The @app.route(path) decorator registers each URL path handler in Application.router.

</t>
<t tx="karstenw.20230303124242.1">#### APPLICATION ###################################################################################

#--- APPLICATION ERRORS &amp; REQUESTS -----------------------------------------------------------------


</t>
<t tx="karstenw.20230303124355.1">HTTP200OK                  = _HTTPErrorSubclass("200 OK")
HTTP400BadRequest          = _HTTPErrorSubclass("400 Bad Request")
HTTP401Authentication      = _HTTPErrorSubclass("401 Authentication")
HTTP403Forbidden           = _HTTPErrorSubclass("403 Forbidden")
HTTP404NotFound            = _HTTPErrorSubclass("404 Not Found")
HTTP429TooManyRequests     = _HTTPErrorSubclass("429 Too Many Requests")
HTTP500InternalServerError = _HTTPErrorSubclass("500 Internal Server Error")
HTTP503ServiceUnavailable  = _HTTPErrorSubclass("503 Service Unavailable")

</t>
<t tx="karstenw.20230303124543.1">#--- APPLICATION THREAD-SAFE DATA ------------------------------------------------------------------
# With a multi-threaded server, each thread requires its own local data (i.e., database connection).
# Local data can be initialized with @app.thread(START):
#
# &gt;&gt;&gt; @app.thread(START)
# &gt;&gt;&gt; def db():
# &gt;&gt;&gt;    g.db = Database()
# &gt;&gt;&gt;
# &gt;&gt;&gt; @app.route("/")
# &gt;&gt;&gt; def index(*path, db=None):
# &gt;&gt;&gt;    print(db) # = Database object.
#
# The thread-safe database connection can then be retrieved from
# app.thread.db, g.db, or as a keyword argument of a URL handler.


</t>
<t tx="karstenw.20230303124612.1">#--- APPLICATION -----------------------------------------------------------------------------------
# With Apache + mod_wsgi, the Application instance must be named "application".

# Server host.
LOCALHOST = "127.0.0.1"
INTRANET = "0.0.0.0"

# Server thread handlers.
START = "start"
STOP = "stop"

</t>
<t tx="karstenw.20230303124836.1">#### CERTIFICATE ###################################################################################
# A certificate can be used to secure a web app (i.e., a https:// connection).
# A certificate confirms the owner's identity, as verified by a signer.
# This signer can be trusted third-party (e.g., Comodo) or self-signed.
# The certificate() function yields a free, self-signed certificate (valid for 365 days).
# Visitors will get a browser warning that the certificate is not signed by a trusted third party.


</t>
<t tx="karstenw.20230303125114.1">####################################################################################################

#from pattern.en import sentiment
#
#app = App()
#app.rate[("1234", "/api/en/sentiment")] = (100, MINUTE)
#
#@app.bind("db")
#def db():
#    return Database("log.db", schema="create table if not exists `log` (q text);")
#
#
## http://localhost:8080/whatever
#@app.route("/")
#def index(*path, **data):
#    return "%s&lt;br&gt;%s" % (path, data.get("db"))
#
## http://localhost:8080/api/en/sentiment?q=awesome
##@app.route("/api/en/sentiment", limit=True)
#@app.route("/api/en/sentiment", limit=10, time=MINUTE)
#def nl_sentiment(q="", db=None):
#    polarity, subjectivity = sentiment(q)
#    db.batch.execute("insert into `log` (q) values (?);", (q,))
#    return {"polarity": polarity}
#
#@app.task(interval=MINUTE)
#def log(db=None):
#    print("committing log...")
#    db.batch.commit()
#
#@app.error((403, 404, 429, 500, 503))
#def error(e):
#    return "&lt;h2&gt;%s&lt;/h2&gt;&lt;pre&gt;%s&lt;/pre&gt;" % (e.status, e.traceback)
#
#app.run(debug=True, threads=100, queue=50)
</t>
<t tx="karstenw.20230303131659.1">#### PATTERN | TEXT | PARSER #######################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303131711.1">from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import sys
import os
import re
import string
import types
import json
import codecs
import operator

import pdb

from io import open

from codecs import BOM_UTF8
BOM_UTF8 = BOM_UTF8.decode('utf-8')

from xml.etree import cElementTree
from itertools import chain
from collections import defaultdict
from math import log, sqrt

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

from pattern.text.tree import Tree, Text, Sentence, Slice, Chunk, PNPChunk, Chink, Word, table
from pattern.text.tree import SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA, AND, OR

DEFAULT = "default"

from pattern.helpers import encode_string, decode_string

decode_utf8 = decode_string
encode_utf8 = encode_string

PUNCTUATION = ".,;:!?()\[]{}`'\"@#$^&amp;*+-|=~_”—“"

</t>
<t tx="karstenw.20230303131711.10">@property
def delimiters_regex(self):
    return self._delimiters_regex

</t>
<t tx="karstenw.20230303131711.11">@delimiters_regex.setter
def delimiters_regex(self, value):
    self._delimiters_regex = [re.compile(p) for p in value]

</t>
<t tx="karstenw.20230303131711.12">@property
def lengthInWords(self):
    return self._lengthInWords

</t>
<t tx="karstenw.20230303131711.13">@lengthInWords.setter
def lengthInWords(self, value):
    self._lengthInWords = value

</t>
<t tx="karstenw.20230303131711.14">def frequentPhraseMining(self, document_list, threshhold, max_ngramm_len=10):
    """ Function for collecting phrases and its frequencies"""
    n = 1
    A = {}
    for doc_id, doc in enumerate(document_list):
        A[doc_id] = {n: range(len(doc) - 1)}
        for w in doc:
            self._phrase2freq.setdefault(w, 0)
            self._phrase2freq[w] += 1
    D = set(range(len(document_list)))

    for n in range(2, max_ngramm_len + 1):
        print("extracting {}-grams".format(n))
        if not D:
            break
        to_remove = []
        for doc_id in D:
            doc = document_list[doc_id]
            A[doc_id][n] = []
            for i in A[doc_id][n - 1]:
                if n == 2:
                    flag = False
                    flag2 = False
                    if doc[i] in self._delimiters:
                        flag = True
                    for p in self._delimiters_regex:
                        if re.match(p, doc[i]):
                            flag2 = True
                            break
                    if not flag2:
                        self._lengthInWords += 1
                    if flag or flag2:
                        continue
                ngram = u'_'.join([doc[i + j] for j in range(n - 1)])
                if self._phrase2freq.get(ngram, threshhold - 1) &gt;= threshhold:
                    A[doc_id][n] += [i]
            if A[doc_id][n]:
                A[doc_id][n].remove(A[doc_id][n][-1])
            if not A[doc_id][n]:
                to_remove += [doc_id]
            else:
                for i in A[doc_id][n]:
                    if i + 1 in A[doc_id][n]:
                        ngram = u'_'.join([doc[i + j] for j in range(n)])
                        self._phrase2freq.setdefault(ngram, 0)
                        self._phrase2freq[ngram] += 1
        for r in to_remove:
            D.remove(r)

</t>
<t tx="karstenw.20230303131711.2">def ngrams(string, n=3, punctuation=PUNCTUATION, continuous=False):
    """ Returns a list of n-grams (tuples of n successive words) from the given string.
        Alternatively, you can supply a Text or Sentence object.
        With continuous=False, n-grams will not run over sentence markers (i.e., .!?).
        Punctuation marks are stripped from words.
    """
    def strip_punctuation(s, punctuation=set(punctuation)):
        return [w for w in s if (isinstance(w, Word) and w.string or w) not in punctuation]
    if n &lt;= 0:
        return []
    if isinstance(string, list):
        s = [strip_punctuation(string)]
    if isinstance(string, str):
        s = [strip_punctuation(s.split(" ")) for s in tokenize(string)]
    if isinstance(string, Sentence):
        s = [strip_punctuation(string)]
    if isinstance(string, Text):
        s = [strip_punctuation(s) for s in string]
    if continuous:
        s = [sum(s, [])]
    g = []

    for st in s:
        #s = [None] + s + [None]
        g.extend([tuple(st[i:i + n]) for i in range(len(st) - n + 1)])
    return g


</t>
<t tx="karstenw.20230303131711.3">def split_document_by_delimeters(string, regexp="[.,!?;: ]", min_word_len=1, stopwords=None):
    """
    :param string: input string (text document)
    :return: list of words
    """
    string = re.sub(r"(-\n)", "", string.lower())
    string = re.sub(r"(\n)", " ", string)
    words = re.split(regexp, string)

    if not stopwords:
        stopwords = []

    return [word for word in words if len(word) &gt; min_word_len and word not in stopwords]


</t>
<t tx="karstenw.20230303131711.4">def train_topmine_ngrammer(documents, threshhold=1, max_ngramm_len=3, min_word_len=2, regexp="[.,!?;: ]", stopwords=None):
    """
    :param documents: list of documents, where each document is represented by a string or by a list of prepared words (ex. stemmed)
    :return: trained ngrammer for text corpus
    """

    splitted_docs = []
    for doc in documents:
        if isinstance(doc, str):
            splitted_docs.append(split_document_by_delimeters(doc, regexp, min_word_len=min_word_len, stopwords=stopwords))
        elif isinstance(doc, list):
            splitted_docs.append(doc)
        else:
            print("Wrong document format")


    ng = None
    try:
        ng = NGrammer(regexp=regexp)
        ng.frequentPhraseMining(splitted_docs, threshhold=threshhold, max_ngramm_len=max_ngramm_len)

    except Exception:
        print('Exception occurred while training ngrammer for abstracts')

    return ng


</t>
<t tx="karstenw.20230303131711.5">def topmine_ngramms(doc, ng, threshhold=1):
    """
    :param doc: document from text corpus, represented by a list of words without delimeters
    :param ng: trained ngramer for text corpus
    :param threshhold: the hyperparameter
    :return: dictionary of ngramms
    """
    splitted_doc = split_document_by_delimeters(doc, ng.regexp)
    extracted_terms = ng.ngramm(splitted_doc, threshhold=threshhold)[0]
    terms_dict = defaultdict(int)
    for term in extracted_terms:
        terms_dict[term] += 1
    return terms_dict


</t>
<t tx="karstenw.20230303131711.6">class NGrammer(object):
    _phrase2freq = {}
    _delimiters = None
    _delimiters_regex = None
    _lengthInWords = 0

    @others
FLOODING = re.compile(r"((.)\2{2,})", re.I) # ooo, xxx, !!!, ...


</t>
<t tx="karstenw.20230303131711.7">def __init__(self, regexp):
    self._phrase2freq = {}
    self._delimiters = {}
    self._delimiters_regex = []
    self._lengthInWords = 0
    self.regexp = regexp

</t>
<t tx="karstenw.20230303131711.8">@property
def delimiters(self):
    return self._delimiters

</t>
<t tx="karstenw.20230303131711.9">@delimiters.setter
def delimiters(self, value):
    self._delimiters = value

</t>
<t tx="karstenw.20230303131712.1">def _significanceScore(self, ngramm1, ngramm2):
    mu0 = float(self._phrase2freq.get(ngramm1, 0) *
                self._phrase2freq.get(ngramm2, 0))
    mu0 /= self._lengthInWords
    f12 = float(self._phrase2freq.get(ngramm1 + u'_' + ngramm2, 0))
    return (f12 - mu0) / sqrt(f12 + 1)

</t>
<t tx="karstenw.20230303131712.10">def load(self):
    # Must be overridden in a subclass.
    # Must load data with dict.__setitem__(self, k, v) instead of lazydict[k] = v.
    pass

</t>
<t tx="karstenw.20230303131712.100">def extend(self, rules=[]):
    for r in rules:
        self.append(*r)

</t>
<t tx="karstenw.20230303131712.101">#--- CONTEXT RULES ---------------------------------------------------------------------------------
# Brill's algorithm generates contextual rules in the following format:
# VBD VB PREVTAG TO =&gt; unknown word tagged VBD changes to VB if preceded by a word tagged TO.

class Context(lazylist):

    @others
</t>
<t tx="karstenw.20230303131712.102">def __init__(self, path=""):
    """ A list of rules based on context (preceding and following words).
    """
    self._path = path
    self._cmd = set((
           "prevtag", # Preceding word is tagged x.
           "nexttag", # Following word is tagged x.
          "prev2tag", # Word 2 before is tagged x.
          "next2tag", # Word 2 after is tagged x.
       "prev1or2tag", # One of 2 preceding words is tagged x.
       "next1or2tag", # One of 2 following words is tagged x.
    "prev1or2or3tag", # One of 3 preceding words is tagged x.
    "next1or2or3tag", # One of 3 following words is tagged x.
       "surroundtag", # Preceding word is tagged x and following word is tagged y.
             "curwd", # Current word is x.
            "prevwd", # Preceding word is x.
            "nextwd", # Following word is x.
        "prev1or2wd", # One of 2 preceding words is x.
        "next1or2wd", # One of 2 following words is x.
     "next1or2or3wd", # One of 3 preceding words is x.
     "prev1or2or3wd", # One of 3 following words is x.
         "prevwdtag", # Preceding word is x and tagged y.
         "nextwdtag", # Following word is x and tagged y.
         "wdprevtag", # Current word is y and preceding word is tagged x.
         "wdnexttag", # Current word is x and following word is tagged y.
         "wdand2aft", # Current word is x and word 2 after is y.
      "wdand2tagbfr", # Current word is y and word 2 before is tagged x.
      "wdand2tagaft", # Current word is x and word 2 after is tagged y.
           "lbigram", # Current word is y and word before is x.
           "rbigram", # Current word is x and word after is y.
        "prevbigram", # Preceding word is tagged x and word before is tagged y.
        "nextbigram", # Following word is tagged x and word after is tagged y.
    ))

</t>
<t tx="karstenw.20230303131712.103">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230303131712.104">def load(self):
    # ["VBD", "VB", "PREVTAG", "TO"]
    list.extend(self, (x.split() for x in _read(self._path)))

</t>
<t tx="karstenw.20230303131712.105">def apply(self, tokens):
    """ Applies contextual rules to the given list of tokens,
        where each token is a [word, tag] list.
    """
    o = [("STAART", "STAART")] * 3 # Empty delimiters for look ahead/back.
    t = o + tokens + o
    for i, token in enumerate(t):
        for r in self:
            if token[1] == "STAART":
                continue
            if token[1] != r[0] and r[0] != "*":
                continue
            cmd, x, y = r[2], r[3], r[4] if len(r) &gt; 4 else ""
            cmd = cmd.lower()
            if (cmd == "prevtag"        and x ==  t[i - 1][1]) \
            or (cmd == "nexttag"        and x ==  t[i + 1][1]) \
            or (cmd == "prev2tag"       and x ==  t[i - 2][1]) \
            or (cmd == "next2tag"       and x ==  t[i + 2][1]) \
            or (cmd == "prev1or2tag"    and x in (t[i - 1][1], t[i - 2][1])) \
            or (cmd == "next1or2tag"    and x in (t[i + 1][1], t[i + 2][1])) \
            or (cmd == "prev1or2or3tag" and x in (t[i - 1][1], t[i - 2][1], t[i - 3][1])) \
            or (cmd == "next1or2or3tag" and x in (t[i + 1][1], t[i + 2][1], t[i + 3][1])) \
            or (cmd == "surroundtag"    and x ==  t[i - 1][1] and y == t[i + 1][1]) \
            or (cmd == "curwd"          and x ==  t[i + 0][0]) \
            or (cmd == "prevwd"         and x ==  t[i - 1][0]) \
            or (cmd == "nextwd"         and x ==  t[i + 1][0]) \
            or (cmd == "prev1or2wd"     and x in (t[i - 1][0], t[i - 2][0])) \
            or (cmd == "next1or2wd"     and x in (t[i + 1][0], t[i + 2][0])) \
            or (cmd == "prevwdtag"      and x ==  t[i - 1][0] and y == t[i - 1][1]) \
            or (cmd == "nextwdtag"      and x ==  t[i + 1][0] and y == t[i + 1][1]) \
            or (cmd == "wdprevtag"      and x ==  t[i - 1][1] and y == t[i + 0][0]) \
            or (cmd == "wdnexttag"      and x ==  t[i + 0][0] and y == t[i + 1][1]) \
            or (cmd == "wdand2aft"      and x ==  t[i + 0][0] and y == t[i + 2][0]) \
            or (cmd == "wdand2tagbfr"   and x ==  t[i - 2][1] and y == t[i + 0][0]) \
            or (cmd == "wdand2tagaft"   and x ==  t[i + 0][0] and y == t[i + 2][1]) \
            or (cmd == "lbigram"        and x ==  t[i - 1][0] and y == t[i + 0][0]) \
            or (cmd == "rbigram"        and x ==  t[i + 0][0] and y == t[i + 1][0]) \
            or (cmd == "prevbigram"     and x ==  t[i - 2][1] and y == t[i - 1][1]) \
            or (cmd == "nextbigram"     and x ==  t[i + 1][1] and y == t[i + 2][1]):
                t[i] = [t[i][0], r[1]]
    return t[len(o):-len(o)]

</t>
<t tx="karstenw.20230303131712.106">def insert(self, i, tag1, tag2, cmd="prevtag", x=None, y=None):
    """ Inserts a new rule that updates words with tag1 to tag2,
        given constraints x and y, e.g., Context.append("TO &lt; NN", "VB")
    """
    if " &lt; " in tag1 and not x and not y:
        tag1, x = tag1.split(" &lt; ")
        cmd = "prevtag"
    if " &gt; " in tag1 and not x and not y:
        x, tag1 = tag1.split(" &gt; ")
        cmd = "nexttag"
    lazylist.insert(self, i, [tag1, tag2, cmd, x or "", y or ""])

</t>
<t tx="karstenw.20230303131712.107">def append(self, *args, **kwargs):
    self.insert(len(self) - 1, *args, **kwargs)

</t>
<t tx="karstenw.20230303131712.108">def extend(self, rules=[]):
    for r in rules:
        self.append(*r)

</t>
<t tx="karstenw.20230303131712.109">#--- NAMED ENTITY RECOGNIZER -----------------------------------------------------------------------

RE_ENTITY1 = re.compile(r"^http://")                            # http://www.domain.com/path
RE_ENTITY2 = re.compile(r"^www\..*?\.(com|org|net|edu|de|uk)$") # www.domain.com
RE_ENTITY3 = re.compile(r"^[\w\-\.\+]+@(\w[\w\-]+\.)+[\w\-]+$") # name@domain.com


class Entities(lazydict):

    @others
</t>
<t tx="karstenw.20230303131712.11">def _lazy(self, method, *args):
    """ If the dictionary is empty, calls lazydict.load().
        Replaces lazydict.method() with dict.method() and calls it.
    """
    if dict.__len__(self) == 0:
        self.load()
        setattr(self, method, types.MethodType(getattr(dict, method), self))
    return getattr(dict, method)(self, *args)

</t>
<t tx="karstenw.20230303131712.110">def __init__(self, path="", tag="NNP"):
    """ A dictionary of named entities and their labels.
        For domain names and e-mail adresses, regular expressions are used.
    """
    self.tag = tag
    self._path = path
    self._cmd = ((
        "pers", # Persons: George/NNP-PERS
         "loc", # Locations: Washington/NNP-LOC
         "org", # Organizations: Google/NNP-ORG
    ))

</t>
<t tx="karstenw.20230303131712.111">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230303131712.112">def load(self):
    # ["Alexander", "the", "Great", "PERS"]
    # {"alexander": [["alexander", "the", "great", "pers"], ...]}
    for x in _read(self.path):
        x = [x.lower() for x in x.split()]
        dict.setdefault(self, x[0], []).append(x)

</t>
<t tx="karstenw.20230303131712.113">def apply(self, tokens):
    """ Applies the named entity recognizer to the given list of tokens,
        where each token is a [word, tag] list.
    """
    # Note: we could also scan for patterns, e.g.,
    # "my|his|her name is|was *" =&gt; NNP-PERS.
    i = 0
    while i &lt; len(tokens):
        w = tokens[i][0].lower()
        if RE_ENTITY1.match(w) \
        or RE_ENTITY2.match(w) \
        or RE_ENTITY3.match(w):
            tokens[i][1] = self.tag
        if w in self:
            for e in self[w]:
                # Look ahead to see if successive words match the named entity.
                e, tag = (e[:-1], "-" + e[-1].upper()) if e[-1] in self._cmd else (e, "")
                b = True
                for j, e in enumerate(e):
                    if i + j &gt;= len(tokens) or tokens[i + j][0].lower() != e:
                        b = False
                        break
                if b:
                    for token in tokens[i:i + j + 1]:
                        token[1] = token[1] if token[1].startswith(self.tag) else self.tag
                        token[1] += tag
                    i += j
                    break
        i += 1
    return tokens

</t>
<t tx="karstenw.20230303131712.114">def append(self, entity, name="pers"):
    """ Appends a named entity to the lexicon,
        e.g., Entities.append("Hooloovoo", "PERS")
    """
    e = list(map(lambda s: s.lower(), entity.split(" ") + [name]))
    self.setdefault(e[0], []).append(e)

</t>
<t tx="karstenw.20230303131712.115">def extend(self, entities):
    for entity, name in entities:
        self.append(entity, name)

</t>
<t tx="karstenw.20230303131712.116">#### PARSER ########################################################################################

#--- PARSER ----------------------------------------------------------------------------------------
# A shallow parser can be used to retrieve syntactic-semantic information from text
# in an efficient way (usually at the expense of deeper configurational syntactic information).
# The shallow parser in Pattern is meant to handle the following tasks:
# 1)  Tokenization: split punctuation marks from words and find sentence periods.
# 2)       Tagging: find the part-of-speech tag of each word (noun, verb, ...) in a sentence.
# 3)      Chunking: find words that belong together in a phrase.
# 4) Role labeling: find the subject and object of the sentence.
# 5) Lemmatization: find the base form of each word ("was" =&gt; "is").

#    WORD     TAG     CHUNK      PNP        ROLE        LEMMA
#------------------------------------------------------------------
#     The      DT      B-NP        O        NP-SBJ-1      the
#   black      JJ      I-NP        O        NP-SBJ-1      black
#     cat      NN      I-NP        O        NP-SBJ-1      cat
#     sat      VB      B-VP        O        VP-1          sit
#      on      IN      B-PP      B-PNP      PP-LOC        on
#     the      DT      B-NP      I-PNP      NP-OBJ-1      the
#     mat      NN      I-NP      I-PNP      NP-OBJ-1      mat
#       .      .        O          O          O           .

# The example demonstrates what information can be retrieved:
#
# - the period is split from "mat." = the end of the sentence,
# - the words are annotated: NN (noun), VB (verb), JJ (adjective), DT (determiner), ...
# - the phrases are annotated: NP (noun phrase), VP (verb phrase), PNP (preposition), ...
# - the phrases are labeled: SBJ (subject), OBJ (object), LOC (location), ...
# - the phrase start is marked: B (begin), I (inside), O (outside),
# - the past tense "sat" is lemmatized =&gt; "sit".

# By default, the English parser uses the Penn Treebank II tagset:
# http://www.clips.ua.ac.be/pages/penn-treebank-tagset
PTB = PENN = "penn"


class Parser(object):

    @others
</t>
<t tx="karstenw.20230303131712.117">def __init__(self, lexicon={}, frequency={}, model=None, morphology=None, context=None, entities=None, default=("NN", "NNP", "CD"), language=None):
    """ A simple shallow parser using a Brill-based part-of-speech tagger.
        The given lexicon is a dictionary of known words and their part-of-speech tag.
        The given default tags are used for unknown words.
        Unknown words that start with a capital letter are tagged NNP (except for German).
        Unknown words that contain only digits and punctuation are tagged CD.
        Optionally, morphological and contextual rules (or a language model) can be used 
        to improve the tags of unknown words.
        The given language can be used to discern between
        Germanic and Romance languages for phrase chunking.
    """
    self.lexicon = lexicon or {}
    self.frequency = frequency or {}
    self.model = model
    self.morphology = morphology
    self.context = context
    self.entities = entities
    self.default = default
    self.language = language
    # Load data.
    f = lambda s: isinstance(s, str) or hasattr(s, "read")
    if f(lexicon):
        # Known words.
        self.lexicon = Lexicon(path=lexicon)
    if f(frequency):
        # Word frequency.
        self.frequency = Frequency(path=frequency)
    if f(morphology):
        # Unknown word rules based on word suffix.
        self.morphology = Morphology(path=morphology, known=self.lexicon)
    if f(context):
        # Unknown word rules based on word context.
        self.context = Context(path=context)
    if f(entities):
        # Named entities.
        self.entities = Entities(path=entities, tag=default[1])
    if f(model):
        # Word part-of-speech classifier.
        try:
            self.model = Model(path=model)
        except ImportError: # pattern.vector
            pass

</t>
<t tx="karstenw.20230303131712.118">def find_keywords(self, string, **kwargs):
    """ Returns a sorted list of keywords in the given string.
    """
    return find_keywords(string,
                 parser = self,
                    top = kwargs.pop("top", 10),
              frequency = kwargs.pop("frequency", {}), **kwargs
    )

</t>
<t tx="karstenw.20230303131712.119">def find_tokens(self, string, **kwargs):
    """ Returns a list of sentences from the given string.
        Punctuation marks are separated from each word by a space.
    """
    # "The cat purs." =&gt; ["The cat purs ."]
    return find_tokens(string,
            punctuation = kwargs.get("punctuation", PUNCTUATION),
          abbreviations = kwargs.get("abbreviations", ABBREVIATIONS),
                replace = kwargs.get("replace", replacements),
              linebreak = r"\n{2,}")

</t>
<t tx="karstenw.20230303131712.12">def __repr__(self):
    return self._lazy("__repr__")

</t>
<t tx="karstenw.20230303131712.120">def find_tags(self, tokens, **kwargs):
    """ Annotates the given list of tokens with part-of-speech tags.
        Returns a list of tokens, where each token is now a [word, tag]-list.
    """
    # ["The", "cat", "purs"] =&gt; [["The", "DT"], ["cat", "NN"], ["purs", "VB"]]
    return find_tags(tokens,
                lexicon = kwargs.get("lexicon", self.lexicon or {}),
                  model = kwargs.get("model", self.model),
             morphology = kwargs.get("morphology", self.morphology),
                context = kwargs.get("context", self.context),
               entities = kwargs.get("entities", self.entities),
               language = kwargs.get("language", self.language),
                default = kwargs.get("default", self.default),
                    map = kwargs.get("map", None))

</t>
<t tx="karstenw.20230303131712.121">def find_chunks(self, tokens, **kwargs):
    """ Annotates the given list of tokens with chunk tags.
        Several tags can be added, for example chunk + preposition tags.
    """
    # [["The", "DT"], ["cat", "NN"], ["purs", "VB"]] =&gt;
    # [["The", "DT", "B-NP"], ["cat", "NN", "I-NP"], ["purs", "VB", "B-VP"]]
    return find_prepositions(
           find_chunks(tokens,
               language = kwargs.get("language", self.language)))

</t>
<t tx="karstenw.20230303131712.122">def find_prepositions(self, tokens, **kwargs):
    """ Annotates the given list of tokens with prepositional noun phrase tags.
    """
    return find_prepositions(tokens) # See also Parser.find_chunks().

</t>
<t tx="karstenw.20230303131712.123">def find_labels(self, tokens, **kwargs):
    """ Annotates the given list of tokens with verb/predicate tags.
    """
    return find_relations(tokens)

</t>
<t tx="karstenw.20230303131712.124">def find_lemmata(self, tokens, **kwargs):
    """ Annotates the given list of tokens with word lemmata.
    """
    return [token + [token[0].lower()] for token in tokens]

</t>
<t tx="karstenw.20230303131712.125">def parse(self, s, tokenize=True, tags=True, chunks=True, relations=False, lemmata=False, encoding="utf-8", **kwargs):
    """ Takes a string (sentences) and returns a tagged Unicode string (TaggedString).
        Sentences in the output are separated by newlines.
        With tokenize=True, punctuation is split from words and sentences are separated by \n.
        With tags=True, part-of-speech tags are parsed (NN, VB, IN, ...).
        With chunks=True, phrase chunk tags are parsed (NP, VP, PP, PNP, ...).
        With relations=True, semantic role labels are parsed (SBJ, OBJ).
        With lemmata=True, word lemmata are parsed.
        Optional parameters are passed to
        the tokenizer, tagger, chunker, labeler and lemmatizer.
    """
    # Tokenizer.
    if tokenize is True:
        s = self.find_tokens(s, **kwargs)
    if isinstance(s, (list, tuple)):
        s = [isinstance(s, str) and s.split(" ") or s for s in s]
    if isinstance(s, str):
        s = [s.split(" ") for s in s.split("\n")]
    # Unicode.
    for i in range(len(s)):
        for j in range(len(s[i])):
            if isinstance(s[i][j], str):
                s[i][j] = decode_string(s[i][j], encoding)
        # Tagger (required by chunker, labeler &amp; lemmatizer).
        if tags or chunks or relations or lemmata:
            s[i] = self.find_tags(s[i], **kwargs)
        else:
            s[i] = [[w] for w in s[i]]
        # Chunker.
        if chunks or relations:
            s[i] = self.find_chunks(s[i], **kwargs)
        # Labeler.
        if relations:
            s[i] = self.find_labels(s[i], **kwargs)
        # Lemmatizer.
        if lemmata:
            s[i] = self.find_lemmata(s[i], **kwargs)
    # Slash-formatted tagged string.
    # With collapse=False (or split=True), returns raw list
    # (this output is not usable by tree.Text).
    if not kwargs.get("collapse", True) \
        or kwargs.get("split", False):
        return s
    # Construct TaggedString.format.
    # (this output is usable by tree.Text).
    format = ["word"]
    if tags:
        format.append("part-of-speech")
    if chunks:
        format.extend(("chunk", "preposition"))
    if relations:
        format.append("relation")
    if lemmata:
        format.append("lemma")
    # Collapse raw list.
    # Sentences are separated by newlines, tokens by spaces, tags by slashes.
    # Slashes in words are encoded with &amp;slash;
    for i in range(len(s)):
        for j in range(len(s[i])):
            s[i][j][0] = s[i][j][0].replace("/", "&amp;slash;")
            s[i][j] = "/".join(s[i][j])
        s[i] = " ".join(s[i])
    s = "\n".join(s)
    s = TaggedString(s, format, language=kwargs.get("language", self.language))
    return s

</t>
<t tx="karstenw.20230303131712.126">#--- TAGGED STRING ---------------------------------------------------------------------------------
# Pattern.parse() returns a TaggedString: a Unicode string with "tags" and "language" attributes.
# The pattern.text.tree.Text class uses this attribute to determine the token format and
# transform the tagged string to a parse tree of nested Sentence, Chunk and Word objects.

TOKENS = "tokens"


class TaggedString(str):

    @others
</t>
<t tx="karstenw.20230303131712.127">def __new__(self, string, tags=["word"], language=None):
    """ Unicode string with tags and language attributes.
        For example: TaggedString("cat/NN/NP", tags=["word", "pos", "chunk"]).
    """
    # From a TaggedString:
    if isinstance(string, str) and hasattr(string, "tags"):
        tags, language = string.tags, string.language
    # From a TaggedString.split(TOKENS) list:
    if isinstance(string, list):
        string = [[[x.replace("/", "&amp;slash;") for x in token] for token in s] for s in string]
        string = "\n".join(" ".join("/".join(token) for token in s) for s in string)
    s = str.__new__(self, string)
    s.tags = list(tags)
    s.language = language
    return s

</t>
<t tx="karstenw.20230303131712.128">def split(self, sep=TOKENS):
    """ Returns a list of sentences, where each sentence is a list of tokens,
        where each token is a list of word + tags.
    """
    if sep != TOKENS:
        return str.split(self, sep)
    if len(self) == 0:
        return []
    return [[[x.replace("&amp;slash;", "/") for x in token.split("/")]
        for token in sentence.split(" ")]
            for sentence in str.split(self, "\n")]

</t>
<t tx="karstenw.20230303131712.129">#--- UNIVERSAL TAGSET ------------------------------------------------------------------------------
# The default part-of-speech tagset used in Pattern is Penn Treebank II.
# However, not all languages are well-suited to Penn Treebank (which was developed for English).
# As more languages are implemented, this is becoming more problematic.
#
# A universal tagset is proposed by Slav Petrov (2012):
# http://www.petrovi.de/data/lrec.pdf
#
# Subclasses of Parser should start implementing
# Parser.parse(tagset=UNIVERSAL) with a simplified tagset.
# The names of the constants correspond to Petrov's naming scheme, while
# the value of the constants correspond to Penn Treebank.

UNIVERSAL = "universal"

NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X = \
    "NN", "VB", "JJ", "RB", "PR", "DT", "PP", "PP", "NO", "CJ", "UH", "PT", ".", "X"


def penntreebank2universal(token, tag):
    """ Returns a (token, tag)-tuple with a simplified universal part-of-speech tag.
    """
    if tag.startswith(("NNP-", "NNPS-")):
        return (token, "%s-%s" % (NOUN, tag.split("-")[-1]))
    if tag in ("NN", "NNS", "NNP", "NNPS", "NP"):
        return (token, NOUN)
    if tag in ("MD", "VB", "VBD", "VBG", "VBN", "VBP", "VBZ"):
        return (token, VERB)
    if tag in ("JJ", "JJR", "JJS"):
        return (token, ADJ)
    if tag in ("RB", "RBR", "RBS", "WRB"):
        return (token, ADV)
    if tag in ("PR", "PRP", "PRP$", "WP", "WP$"):
        return (token, PRON)
    if tag in ("DT", "PDT", "WDT", "EX"):
        return (token, DET)
    if tag in ("IN", "PP"):
        return (token, PREP)
    if tag in ("CD", "NO"):
        return (token, NUM)
    if tag in ("CC", "CJ"):
        return (token, CONJ)
    if tag in ("UH",):
        return (token, INTJ)
    if tag in ("POS", "PT", "RP", "TO"):
        return (token, PRT)
    if tag in ("SYM", "LS", ".", "!", "?", ",", ":", "(", ")", "\"", "#", "$"):
        return (token, PUNC)
    return (token, X)

#--- TOKENIZER -------------------------------------------------------------------------------------

TOKEN = re.compile(r"(\S+)\s")

# Common accent letters.
DIACRITICS = \
diacritics = "àáâãäåąāæçćčςďèéêëēěęģìíîïīłįķļľņñňńйðòóôõöøþřšťùúûüůųýÿўžż"

# Common punctuation marks.
PUNCTUATION = \
punctuation = ".,;:!?()[]{}`''\"@#$^&amp;*+-|=~_"

# Common abbreviations.
ABBREVIATIONS = \
abbreviations = set((
    "a.", "adj.", "adv.", "al.", "a.m.", "art.", "c.", "capt.", "cert.", "cf.", "col.", "Col.",
    "comp.", "conf.", "def.", "Dep.", "Dept.", "Dr.", "dr.", "ed.", "e.g.", "esp.", "etc.", "ex.",
    "f.", "fig.", "gen.", "id.", "i.e.", "int.", "l.", "m.", "Med.", "Mil.", "Mr.", "n.", "n.q.",
    "orig.", "pl.", "pred.", "pres.", "p.m.", "ref.", "v.", "vs.", "w/"
))

RE_ABBR1 = re.compile(r"^[A-Za-z]\.$")     # single letter, "T. De Smedt"
RE_ABBR2 = re.compile(r"^([A-Za-z]\.)+$")  # alternating letters, "U.S."
RE_ABBR3 = re.compile(r"^[A-Z][%s]+.$" % ( # capital followed by consonants, "Mr."
        "|".join("bcdfghjklmnpqrstvwxz")))

# Common contractions.
replacements = {
     "'d": " 'd",
     "'m": " 'm",
     "'s": " 's",
    "'ll": " 'll",
    "'re": " 're",
    "'ve": " 've",
    "n't": " n't"
}

# Common emoticons.
EMOTICONS = \
emoticons = { # (facial expression, sentiment)-keys
    ("love" , +1.00): set(("&lt;3", "♥", "❤")),
    ("grin" , +1.00): set(("&gt;:D", ":-D", ":D", "=-D", "=D", "X-D", "x-D", "XD", "xD", "8-D")),
    ("taunt", +0.75): set(("&gt;:P", ":-P", ":P", ":-p", ":p", ":-b", ":b", ":c)", ":o)", ":^)")),
    ("smile", +0.50): set(("&gt;:)", ":-)", ":)", "=)", "=]", ":]", ":}", ":&gt;", ":3", "8)", "8-)")),
    ("wink" , +0.25): set(("&gt;;]", ";-)", ";)", ";-]", ";]", ";D", ";^)", "*-)", "*)")),
    ("blank", +0.00): set((":-|", ":|")),
    ("gasp" , -0.05): set(("&gt;:o", ":-O", ":O", ":o", ":-o", "o_O", "o.O", "°O°", "°o°")),
    ("worry", -0.25): set(("&gt;:/", ":-/", ":/", ":\\", "&gt;:\\", ":-.", ":-s", ":s", ":S", ":-S", "&gt;.&gt;")),
    ("frown", -0.75): set(("&gt;:[", ":-(", ":(", "=(", ":-[", ":[", ":{", ":-&lt;", ":c", ":-c", "=/")),
    ("cry"  , -1.00): set((":'(", ":'''(", ";'("))
}

RE_EMOTICONS = [r" ?".join(map(re.escape, e)) for v in EMOTICONS.values() for e in v]
RE_EMOTICONS = re.compile(r"(%s)($|\s)" % "|".join(RE_EMOTICONS))

# Common emoji.
EMOJI = \
emoji = { # (facial expression, sentiment)-keys
    ("love" , +1.00): set(("❤️", "💜", "💚", "💙", "💛", "💕")),
    ("grin" , +1.00): set(("😀", "😄", "😃", "😆", "😅", "😂", "😁", "😻", "😍", "😈", "👌")),
    ("taunt", +0.75): set(("😛", "😝", "😜", "😋", "😇")),
    ("smile", +0.50): set(("😊", "😌", "😏", "😎", "☺", "👍")),
    ("wink" , +0.25): set(("😉")),
    ("blank", +0.00): set(("😐", "😶")),
    ("gasp" , -0.05): set(("😳", "😮", "😯", "😧", "😦", "🙀")),
    ("worry", -0.25): set(("😕", "😬")),
    ("frown", -0.75): set(("😟", "😒", "😔", "😞", "😠", "😩", "😫", "😡", "👿")),
    ("cry"  , -1.00): set(("😢", "😥", "😓", "😪", "😭", "😿")),
}

RE_EMOJI = [e for v in EMOJI.values() for e in v]
RE_EMOJI = re.compile(r"(\s?)(%s)(\s?)" % "|".join(RE_EMOJI))

# Mention marker: "@tomdesmedt".
RE_MENTION = re.compile(r"\@([0-9a-zA-z_]+)(\s|\,|\:|\.|\!|\?|$)")

# Sarcasm marker: "(!)".
RE_SARCASM = re.compile(r"\( ?\! ?\)")

# Paragraph line breaks
# (\n\n marks end of sentence).
EOS = "END-OF-SENTENCE"


</t>
<t tx="karstenw.20230303131712.13">def __len__(self):
    return self._lazy("__len__")

</t>
<t tx="karstenw.20230303131712.130">def find_tokens(string, punctuation=PUNCTUATION, abbreviations=ABBREVIATIONS, replace=replacements, linebreak=r"\n{2,}"):
    """ Returns a list of sentences. Each sentence is a space-separated string of tokens (words).
        Handles common cases of abbreviations (e.g., etc., ...).
        Punctuation marks are split from other words. Periods (or ?!) mark the end of a sentence.
        Headings without an ending period are inferred by line breaks.
    """
    # Handle punctuation.
    punctuation = tuple(punctuation)
    # Handle replacements (contractions).
    for a, b in replace.items():
        string = re.sub(a, b, string)
    # Handle Unicode quotes.
    if isinstance(string, str):
        string = string.replace("“", " “ ")
        string = string.replace("”", " ” ")
        string = string.replace("‘", " ‘ ")
        string = string.replace("’", " ’ ")
    # Collapse whitespace.
    string = re.sub("\r\n", "\n", string)
    string = re.sub(linebreak, " %s " % EOS, string)
    string = re.sub(r"\s+", " ", string)
    tokens = []
    # Handle punctuation marks.
    for t in TOKEN.findall(string + " "):
        if len(t) &gt; 0:
            tail = []
            if not RE_MENTION.match(t):
                while t.startswith(punctuation) and \
                  t not in replace:
                    # Split leading punctuation.
                    if t.startswith(punctuation):
                        tokens.append(t[0]); t = t[1:]
            if not False:
                while t.endswith(punctuation) and \
                  t not in replace:
                    # Split trailing punctuation.
                    if t.endswith(punctuation) and not t.endswith("."):
                        tail.append(t[-1]); t = t[:-1]
                    # Split ellipsis (...) before splitting period.
                    if t.endswith("..."):
                        tail.append("..."); t = t[:-3].rstrip(".")
                    # Split period (if not an abbreviation).
                    if t.endswith("."):
                        if t in abbreviations or \
                          RE_ABBR1.match(t) is not None or \
                          RE_ABBR2.match(t) is not None or \
                          RE_ABBR3.match(t) is not None:
                            break
                        else:
                            tail.append(t[-1]); t = t[:-1]
            if t != "":
                tokens.append(t)
            tokens.extend(reversed(tail))
    # Handle citations (periods + quotes).
    if isinstance(string, str):
        quotes = ("'", "\"", "”", "’")
    else:
        quotes = ("'", "\"")
    # Handle sentence breaks (periods, quotes, parenthesis).
    sentences, i, j = [[]], 0, 0
    while j &lt; len(tokens):
        if tokens[j] in ("...", ".", "!", "?", EOS):
            while j &lt; len(tokens) \
              and (tokens[j] in ("...", ".", "!", "?", EOS) or tokens[j] in quotes):
                if tokens[j] in quotes and sentences[-1].count(tokens[j]) % 2 == 0:
                    break # Balanced quotes.
                j += 1
            sentences[-1].extend(t for t in tokens[i:j] if t != EOS)
            sentences.append([])
            i = j
        j += 1
    # Handle emoticons.
    sentences[-1].extend(tokens[i:j])
    sentences = (" ".join(s) for s in sentences if len(s) &gt; 0)
    sentences = (RE_SARCASM.sub("(!)", s) for s in sentences)
    sentences = [RE_EMOTICONS.sub(
        lambda m: m.group(1).replace(" ", "") + m.group(2), s) for s in sentences]
    sentences = [RE_EMOJI.sub(
        lambda m: (m.group(1) or " ") + m.group(2) + (m.group(3) or " "), s) for s in sentences]
    sentences = [s.replace("  ", " ").strip() for s in sentences]
    return sentences

</t>
<t tx="karstenw.20230303131712.131">#--- PART-OF-SPEECH TAGGER -------------------------------------------------------------------------

# Unknown words are recognized as numbers if they contain only digits and -,.:/%$
CD = re.compile(r"^[0-9\-\,\.\:\/\%\$]+$")


def _suffix_rules(token, tag="NN"):
    """ Default morphological tagging rules for English, based on word suffixes.
    """
    if isinstance(token, (list, tuple)):
        token, tag = token
    if token.endswith("ing"):
        tag = "VBG"
    if token.endswith("ly"):
        tag = "RB"
    if token.endswith("s") and not token.endswith(("is", "ous", "ss")):
        tag = "NNS"
    if token.endswith(("able", "al", "ful", "ible", "ient", "ish", "ive", "less", "tic", "ous")) or "-" in token:
        tag = "JJ"
    if token.endswith("ed"):
        tag = "VBN"
    if token.endswith(("ate", "ify", "ise", "ize")):
        tag = "VBP"
    return [token, tag]


</t>
<t tx="karstenw.20230303131712.132">def find_tags(tokens, lexicon={}, model=None, morphology=None, context=None, entities=None, default=("NN", "NNP", "CD"), language="en", map=None, **kwargs):
    """ Returns a list of [token, tag]-items for the given list of tokens:
        ["The", "cat", "purs"] =&gt; [["The", "DT"], ["cat", "NN"], ["purs", "VB"]]
        Words are tagged using the given lexicon of (word, tag)-items.
        Unknown words are tagged NN by default.
        Unknown words that start with a capital letter are tagged NNP (unless language="de").
        Unknown words that consist only of digits and punctuation marks are tagged CD.
        Unknown words are then improved with morphological rules.
        All words are improved with contextual rules.
        If a model is given, uses model for unknown words instead of morphology and context.
        If map is a function, it is applied to each (token, tag) after applying all rules.
    """
    tagged = []
    # Tag known words.
    for i, token in enumerate(tokens):
        tagged.append([token, lexicon.get(token, i == 0 and lexicon.get(token.lower()) or None)])
    # Tag unknown words.
    for i, (token, tag) in enumerate(tagged):
        prev, next = (None, None), (None, None)
        if i &gt; 0:
            prev = tagged[i - 1]
        if i &lt; len(tagged) - 1:
            next = tagged[i + 1]
        if tag is None or token in (model is not None and model.unknown or ()):
            # Use language model (i.e., SLP).
            if model is not None:
                tagged[i] = model.apply([token, None], prev, next)
            # Use NNP for capitalized words (except in German).
            elif token.istitle() and language != "de":
                tagged[i] = [token, default[1]]
            # Use CD for digits and numbers.
            elif CD.match(token) is not None:
                tagged[i] = [token, default[2]]
            # Use suffix rules (e.g., -ly = RB).
            elif morphology is not None:
                tagged[i] = morphology.apply([token, default[0]], prev, next)
            # Use suffix rules (English default).
            elif language == "en":
                tagged[i] = _suffix_rules([token, default[0]])
            # Use most frequent tag (NN).
            else:
                tagged[i] = [token, default[0]]
    # Tag words by context.
    if context is not None and model is None:
        tagged = context.apply(tagged)
    # Tag named entities.
    if entities is not None:
        tagged = entities.apply(tagged)
    # Map tags with a custom function.
    if map is not None:
        tagged = [list(map(token, tag)) or [token, default[0]] for token, tag in tagged]
    return tagged

</t>
<t tx="karstenw.20230303131712.133">def find_chunks(tagged, language="en"):
    """ The input is a list of [token, tag]-items.
        The output is a list of [token, tag, chunk]-items:
        The/DT nice/JJ fish/NN is/VBZ dead/JJ ./. =&gt;
        The/DT/B-NP nice/JJ/I-NP fish/NN/I-NP is/VBZ/B-VP dead/JJ/B-ADJP ././O
    """
    chunked = [x for x in tagged]
    tags = "".join("%s%s" % (tag, SEPARATOR) for token, tag in tagged)
    # Use Germanic or Romance chunking rules according to given language.
    for tag, rule in CHUNKS[int(language in ("ca", "es", "fr", "it", "pt", "ro"))]:
        for m in rule.finditer(tags):
            # Find the start of chunks inside the tags-string.
            # Number of preceding separators = number of preceding tokens.
            i = m.start()
            j = tags[:i].count(SEPARATOR)
            n = m.group(0).count(SEPARATOR)
            for k in range(j, j + n):
                if len(chunked[k]) == 3:
                    continue
                if len(chunked[k]) &lt; 3:
                    # A conjunction or comma cannot be start of a chunk.
                    if k == j and chunked[k][1] in ("CC", "CJ", ","):
                        j += 1
                    # Mark first token in chunk with B-.
                    elif k == j:
                        chunked[k].append("B-" + tag)
                    # Mark other tokens in chunk with I-.
                    else:
                        chunked[k].append("I-" + tag)
    # Mark chinks (tokens outside of a chunk) with O-.
    for chink in filter(lambda x: len(x) &lt; 3, chunked):
        chink.append("O")
    # Post-processing corrections.
    for i, (word, tag, chunk) in enumerate(chunked):
        if tag.startswith("RB") and chunk == "B-NP":
            # "Perhaps you" =&gt; ADVP + NP
            # "Really nice work" =&gt; NP
            # "Really, nice work" =&gt; ADVP + O + NP
            if i &lt; len(chunked) - 1 and not chunked[i + 1][1].startswith("JJ"):
                chunked[i + 0][2] = "B-ADVP"
                chunked[i + 1][2] = "B-NP"
            if i &lt; len(chunked) - 1 and chunked[i + 1][1] in ("CC", "CJ", ","):
                chunked[i + 1][2] = "O"
            if i &lt; len(chunked) - 2 and chunked[i + 1][2] == "O":
                chunked[i + 2][2] = "B-NP"
    return chunked


</t>
<t tx="karstenw.20230303131712.134">def find_prepositions(chunked):
    """ The input is a list of [token, tag, chunk]-items.
        The output is a list of [token, tag, chunk, preposition]-items.
        PP-chunks followed by NP-chunks make up a PNP-chunk.
    """
    # Tokens that are not part of a preposition just get the O-tag.
    for ch in chunked:
        ch.append("O")
    for i, chunk in enumerate(chunked):
        if chunk[2].endswith("PP") and chunk[-1] == "O":
            # Find PP followed by other PP, NP with nouns and pronouns, VP with a gerund.
            if i &lt; len(chunked) - 1 and \
             (chunked[i + 1][2].endswith(("NP", "PP")) or \
              chunked[i + 1][1] in ("VBG", "VBN")):
                chunk[-1] = "B-PNP"
                pp = True
                for ch in chunked[i + 1:]:
                    if not (ch[2].endswith(("NP", "PP")) or ch[1] in ("VBG", "VBN")):
                        break
                    if ch[2].endswith("PP") and pp:
                        ch[-1] = "I-PNP"
                    if not ch[2].endswith("PP"):
                        ch[-1] = "I-PNP"
                        pp = False
    return chunked

</t>
<t tx="karstenw.20230303131712.135">#--- SEMANTIC ROLE LABELER -------------------------------------------------------------------------
# Naive approach.

BE = dict.fromkeys(("be", "am", "are", "is", "being", "was", "were", "been"), True)
GO = dict.fromkeys(("go", "goes", "going", "went"), True)


def find_relations(chunked):
    """ The input is a list of [token, tag, chunk]-items.
        The output is a list of [token, tag, chunk, relation]-items.
        A noun phrase preceding a verb phrase is perceived as sentence subject.
        A noun phrase following a verb phrase is perceived as sentence object.
    """
    tag = lambda token: token[2].split("-")[-1] # B-NP =&gt; NP
    # Group successive tokens with the same chunk-tag.
    chunks = []
    for token in chunked:
        if len(chunks) == 0 \
        or token[2].startswith("B-") \
        or tag(token) != tag(chunks[-1][-1]):
            chunks.append([])
        chunks[-1].append(token + ["O"])
    # If a VP is preceded by a NP, the NP is tagged as NP-SBJ-(id).
    # If a VP is followed by a NP, the NP is tagged as NP-OBJ-(id).
    # Chunks that are not part of a relation get an O-tag.
    id = 0
    for i, chunk in enumerate(chunks):
        if tag(chunk[-1]) == "VP" and i &gt; 0 and tag(chunks[i - 1][-1]) == "NP":
            if chunk[-1][-1] == "O":
                id += 1
            for token in chunk:
                token[-1] = "VP-" + str(id)
            for token in chunks[i - 1]:
                token[-1] += "*NP-SBJ-" + str(id)
                token[-1] = token[-1].lstrip("O-*")
        if tag(chunk[-1]) == "VP" and i &lt; len(chunks) - 1 and tag(chunks[i + 1][-1]) == "NP":
            if chunk[-1][-1] == "O":
                id += 1
            for token in chunk:
                token[-1] = "VP-" + str(id)
            for token in chunks[i + 1]:
                token[-1] = "*NP-OBJ-" + str(id)
                token[-1] = token[-1].lstrip("O-*")
    # This is more a proof-of-concept than useful in practice:
    # PP-LOC = be + in|at + the|my
    # PP-DIR = go + to|towards + the|my
    for i, chunk in enumerate(chunks):
        if 0 &lt; i &lt; len(chunks) - 1 and len(chunk) == 1 and chunk[-1][-1] == "O":
            t0, t1, t2 = chunks[i - 1][-1], chunks[i][0], chunks[i + 1][0] # previous / current / next
            if tag(t1) == "PP" and t2[1] in ("DT", "PR", "PRP$"):
                if t0[0] in BE and t1[0] in ("in", "at"):
                    t1[-1] = "PP-LOC"
                if t0[0] in GO and t1[0] in ("to", "towards"):
                    t1[-1] = "PP-DIR"
    related = []
    [related.extend(chunk) for chunk in chunks]
    return related

</t>
<t tx="karstenw.20230303131712.136">#--- KEYWORDS EXTRACTION ---------------------------------------------------------------------------

def find_keywords(string, parser, top=10, frequency={}, ignore=("rt",), pos=("NN",), **kwargs):
    """ Returns a sorted list of keywords in the given string.
        The given parser (e.g., pattern.en.parser) is used to identify noun phrases.
        The given frequency dictionary can be a reference corpus,
        with relative document frequency (df, 0.0-1.0) for each lemma, 
        e.g., {"the": 0.8, "cat": 0.1, ...}
    """
    lemmata = kwargs.pop("lemmata", kwargs.pop("stem", True))
    t = []
    p = None
    n = 0
    # Remove hashtags.
    s = string.replace("#", ". ")
    # Parse + chunk string.
    for sentence in parser.parse(s, chunks=True, lemmata=lemmata).split():
        for w in sentence: # [token, tag, chunk, preposition, lemma]
            if w[2].startswith(("B", "O")):
                t.append([])
                p = None
            if w[1].startswith(("NNP", "DT")) and p and \
               p[1].startswith("NNP") and \
               p[0][0] != "@" and \
               w[0][0] != "A":
                p[+0] += " " + w[+0] # Merge NNP's: "Ms Kitty".
                p[-1] += " " + w[-1]
            else:
                t[-1].append(w)
            p = t[-1][-1] # word before
            n = n + 1     # word count
    # Parse context: {word: chunks}.
    ctx = {}
    for i, chunk in enumerate(t):
        ch = " ".join(w[0] for w in chunk)
        ch = ch.lower()
        for w in chunk:
            ctx.setdefault(w[0], set()).add(ch)
    # Parse keywords.
    m = {}
    for i, chunk in enumerate(t):
        # Head of "cat hair" =&gt; "hair".
        # Head of "poils de chat" =&gt; "poils".
        head = chunk[-int(parser.language not in ("ca", "es", "pt", "fr", "it", "pt", "ro"))]
        for w in chunk:
            # Lemmatize known words.
            k = lemmata and w[-1] in parser.lexicon and w[-1] or w[0]
            k = re.sub(r"\"\(\)", "", k)
            k = k.strip(":.?!")
            k = k.lower()
            if not w[1].startswith(pos):
                continue
            if len(k) == 1:
                continue
            if k.startswith(("http", "www.")):
                continue
            if k in ignore or lemmata and w[0] in ignore:
                continue
            if k not in m:
                m[k] = [0, 0, 0, 0, 0, 0]
            # Scoring:
            # 0) words that appear more frequently.
            # 1) words that appear in more contexts (semantic centrality).
            # 2) words that appear at the start (25%) of the text.
            # 3) words that are nouns.
            # 4) words that are not in a prepositional phrase.
            # 5) words that are the head of a chunk.
            noun = w[1].startswith("NN")
            m[k][0] += 1 / float(n)
            m[k][1] |= 1 if len(ctx[w[0]]) &gt; 1 else 0
            m[k][2] |= 1 if i / float(len(t)) &lt;= 0.25 else 0
            m[k][3] |= 1 if noun else 0
            m[k][4] |= 1 if noun and w[3].startswith("O") else 0
            m[k][5] |= 1 if noun and w == head else 0
    # Rate tf-idf.
    if frequency:
        for k in m:
            if not k.isalpha(): # @username, odd!ti's
                df = 1.0
            else:
                df = 1.0 / max(frequency.get(w[0].lower(), frequency.get(k, 0)), 0.0001)
                df = log(df)
            m[k][0] *= df
            #print k, m[k]
    # Sort candidates alphabetically by total score.
    # The harmonic mean will emphasize tf-idf score.
    hmean = lambda a: len(a) / sum(1.0 / (x or 0.0001) for x in a)
    m = [(hmean(m[k]), k) for k in m]
    m = sorted(m, key=lambda x: x[1])
    m = sorted(m, key=lambda x: x[0], reverse=True)
    m = [k for score, k in m]
    m = m[:top]
    return m

</t>
<t tx="karstenw.20230303131712.137">#### COMMAND LINE ##################################################################################
# The commandline() function enables command line support for a Parser.
# The following code can be added to pattern.en, for example:
#
# from pattern.text import Parser, commandline
# parse = Parser(lexicon=LEXICON).parse
# if __name__ == "main":
#     commandline(parse)
#
# The parser is then accessible from the command line:
# python -m pattern.en.parser xml -s "Hello, my name is Dr. Sbaitso. Nice to meet you." -OTCLI


def commandline(parse=Parser().parse):
    import optparse
    import codecs
    p = optparse.OptionParser()
    p.add_option("-f", "--file",      dest="file",      action="store",      help="text file to parse",   metavar="FILE")
    p.add_option("-s", "--string",    dest="string",    action="store",      help="text string to parse", metavar="STRING")
    p.add_option("-O", "--tokenize",  dest="tokenize",  action="store_true", help="tokenize the input")
    p.add_option("-T", "--tags",      dest="tags",      action="store_true", help="parse part-of-speech tags")
    p.add_option("-C", "--chunks",    dest="chunks",    action="store_true", help="parse chunk tags")
    p.add_option("-R", "--relations", dest="relations", action="store_true", help="find verb/predicate relations")
    p.add_option("-L", "--lemmata",   dest="lemmata",   action="store_true", help="find word lemmata")
    p.add_option("-e", "--encoding",  dest="encoding",  action="store_true", help="character encoding", default="utf-8")
    p.add_option("-v", "--version",   dest="version",   action="store_true", help="version info")
    o, arguments = p.parse_args()
    # Version info.
    if o.version:
        sys.path.insert(0, os.path.join(MODULE, "..", ".."))
        from pattern import __version__
        print(__version__)
        sys.path.pop(0)
    # Either a text file (-f) or a text string (-s) must be supplied.
    s = o.file and codecs.open(o.file, "r", o.encoding).read() or o.string
    # The given text can be parsed in two modes:
    # - implicit: parse everything (tokenize, tag/chunk, find relations, lemmatize).
    # - explicit: define what to parse manually.
    if s:
        explicit = False
        for option in [o.tokenize, o.tags, o.chunks, o.relations, o.lemmata]:
            if option is not None:
                explicit = True
                break
        if not explicit:
            a = {"encoding": o.encoding}
        else:
            a = {"tokenize": o.tokenize  or False,
                     "tags": o.tags      or False,
                   "chunks": o.chunks    or False,
                "relations": o.relations or False,
                  "lemmata": o.lemmata   or False,
                 "encoding": o.encoding}
        s = parse(s, **a)
        # The output can be either slash-formatted string or XML.
        if "xml" in arguments:
            s = Tree(s, s.tags).xml
        print(s)

#### VERBS #########################################################################################

#--- VERB TENSES -----------------------------------------------------------------------------------
# Conjugation is the inflection of verbs by tense, person, number, mood and aspect.

# VERB TENSE:
INFINITIVE, PRESENT, PAST, FUTURE = \
    INF, PRES, PST, FUT = \
        "infinitive", "present", "past", "future"

# VERB PERSON:
# 1st person = I or we (plural).
# 2nd person = you.
# 3rd person = he, she, it or they (plural).
FIRST, SECOND, THIRD = \
    1, 2, 3

# VERB NUMBER:
# singular number = I, you, he, she, it.
#   plural number = we, you, they.
SINGULAR, PLURAL = \
    SG, PL = \
        "singular", "plural"

# VERB MOOD:
#  indicative mood = a fact: "the cat meowed".
#  imperative mood = a command: "meow!".
# conditional mood = a hypothesis: "a cat *will* meow *if* it is hungry".
# subjunctive mood = a wish, possibility or necessity: "I *wish* the cat *would* stop meowing".
INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE = \
    IND, IMP, COND, SJV = \
        "indicative", "imperative", "conditional", "subjunctive"

# VERB ASPECT:
# imperfective aspect = a habitual or ongoing action: "it was midnight; the cat meowed".
#   perfective aspect = a momentary or completed action: "I flung my pillow at the cat".
#  progressive aspect = a incomplete action in progress: "the cat was meowing".
# Note: the progressive aspect is a subtype of the imperfective aspect.
IMPERFECTIVE, PERFECTIVE, PROGRESSIVE = \
    IPFV, PFV, PROG = \
        "imperfective", "perfective", "progressive"

# Imperfect = past tense + imperfective aspect.
# Preterite = past tense + perfective aspect.
IMPERFECT = "imperfect"
PRETERITE = "preterite"

# Participle = present tense  + progressive aspect.
PARTICIPLE, GERUND = "participle", "gerund"

# Continuous aspect ≈ progressive aspect.
CONTINUOUS = CONT = "continuous"

_ = None # prettify the table =&gt;

# Unique index per tense (= tense + person + number + mood + aspect + negated? + aliases).
# The index is used to describe the format of the verb lexicon file.
# The aliases can be passed to Verbs.conjugate() and Tenses.__contains__().
TENSES = {
   None: (None, _,  _,    _,    _, False, (None   ,)), #       ENGLISH   SPANISH   GERMAN    DUTCH     FRENCH
     0 : ( INF, _,  _,    _,    _, False, ("inf"  ,)), #       to be     ser       sein      zijn      être
     1 : (PRES, 1, SG,  IND, IPFV, False, ("1sg"  ,)), #     I am        soy       bin       ben       suis
     2 : (PRES, 2, SG,  IND, IPFV, False, ("2sg"  ,)), #   you are       eres      bist      bent      es
     3 : (PRES, 3, SG,  IND, IPFV, False, ("3sg"  ,)), # (s)he is        es        ist       is        est
     4 : (PRES, 1, PL,  IND, IPFV, False, ("1pl"  ,)), #    we are       somos     sind      zijn      sommes
     5 : (PRES, 2, PL,  IND, IPFV, False, ("2pl"  ,)), #   you are       sois      seid      zijn      êtes
     6 : (PRES, 3, PL,  IND, IPFV, False, ("3pl"  ,)), #  they are       son       sind      zijn      sont
     7 : (PRES, _, PL,  IND, IPFV, False, ( "pl"  ,)), #       are
     8 : (PRES, _,  _,  IND, PROG, False, ("part" ,)), #       being     siendo              zijnd     étant
     9 : (PRES, 1, SG,  IND, IPFV, True,  ("1sg-" ,)), #     I am not
    10 : (PRES, 2, SG,  IND, IPFV, True,  ("2sg-" ,)), #   you aren't
    11 : (PRES, 3, SG,  IND, IPFV, True,  ("3sg-" ,)), # (s)he isn't
    12 : (PRES, 1, PL,  IND, IPFV, True,  ("1pl-" ,)), #    we aren't
    13 : (PRES, 2, PL,  IND, IPFV, True,  ("2pl-" ,)), #   you aren't
    14 : (PRES, 3, PL,  IND, IPFV, True,  ("3pl-" ,)), #  they aren't
    15 : (PRES, _, PL,  IND, IPFV, True,  ( "pl-" ,)), #       aren't
    16 : (PRES, _,  _,  IND, IPFV, True,  (   "-" ,)), #       isn't
    17 : ( PST, 1, SG,  IND, IPFV, False, ("1sgp" ,)), #     I was       era       war       was       étais
    18 : ( PST, 2, SG,  IND, IPFV, False, ("2sgp" ,)), #   you were      eras      warst     was       étais
    19 : ( PST, 3, SG,  IND, IPFV, False, ("3sgp" ,)), # (s)he was       era       war       was       était
    20 : ( PST, 1, PL,  IND, IPFV, False, ("1ppl" ,)), #    we were      éramos    waren     waren     étions
    21 : ( PST, 2, PL,  IND, IPFV, False, ("2ppl" ,)), #   you were      erais     wart      waren     étiez
    22 : ( PST, 3, PL,  IND, IPFV, False, ("3ppl" ,)), #  they were      eran      waren     waren     étaient
    23 : ( PST, _, PL,  IND, IPFV, False, ( "ppl" ,)), #       were
    24 : ( PST, _,  _,  IND, PROG, False, ("ppart",)), #       been      sido      gewesen   geweest   été
    25 : ( PST, _,  _,  IND, IPFV, False, (   "p" ,)), #       was
    26 : ( PST, 1, SG,  IND, IPFV, True,  ("1sgp-",)), #     I wasn't
    27 : ( PST, 2, SG,  IND, IPFV, True,  ("2sgp-",)), #   you weren't
    28 : ( PST, 3, SG,  IND, IPFV, True,  ("3sgp-",)), # (s)he wasn't
    29 : ( PST, 1, PL,  IND, IPFV, True,  ("1ppl-",)), #    we weren't
    30 : ( PST, 2, PL,  IND, IPFV, True,  ("2ppl-",)), #   you weren't
    31 : ( PST, 3, PL,  IND, IPFV, True,  ("3ppl-",)), #  they weren't
    32 : ( PST, _, PL,  IND, IPFV, True,  ( "ppl-",)), #       weren't
    33 : ( PST, _,  _,  IND, IPFV, True,  ( "p-"  ,)), #       wasn't
    34 : ( PST, 1, SG,  IND,  PFV, False, ("1sg+" ,)), #     I           fui                           fus
    35 : ( PST, 2, SG,  IND,  PFV, False, ("2sg+" ,)), #   you           fuiste                        fus
    36 : ( PST, 3, SG,  IND,  PFV, False, ("3sg+" ,)), # (s)he           fue                           fut
    37 : ( PST, 1, PL,  IND,  PFV, False, ("1pl+" ,)), #    we           fuimos                        fûmes
    38 : ( PST, 2, PL,  IND,  PFV, False, ("2pl+" ,)), #   you           fuisteis                      fûtes
    39 : ( PST, 3, PL,  IND,  PFV, False, ("3pl+" ,)), #  they           fueron                        furent
    40 : ( FUT, 1, SG,  IND, IPFV, False, ("1sgf" ,)), #     I           seré                          serai
    41 : ( FUT, 2, SG,  IND, IPFV, False, ("2sgf" ,)), #   you           serás                         seras
    42 : ( FUT, 3, SG,  IND, IPFV, False, ("3sgf" ,)), # (s)he           será                          sera
    43 : ( FUT, 1, PL,  IND, IPFV, False, ("1plf" ,)), #    we           seremos                       serons
    44 : ( FUT, 2, PL,  IND, IPFV, False, ("2plf" ,)), #   you           seréis                        serez
    45 : ( FUT, 3, PL,  IND, IPFV, False, ("3plf" ,)), #  they           serán                         seron
    46 : (PRES, 1, SG, COND, IPFV, False, ("1sg-&gt;",)), #     I           sería                         serais
    47 : (PRES, 2, SG, COND, IPFV, False, ("2sg-&gt;",)), #   you           serías                        serais
    48 : (PRES, 3, SG, COND, IPFV, False, ("3sg-&gt;",)), # (s)he           sería                         serait
    49 : (PRES, 1, PL, COND, IPFV, False, ("1pl-&gt;",)), #    we           seríamos                      serions
    50 : (PRES, 2, PL, COND, IPFV, False, ("2pl-&gt;",)), #   you           seríais                       seriez
    51 : (PRES, 3, PL, COND, IPFV, False, ("3pl-&gt;",)), #  they           serían                        seraient
    52 : (PRES, 2, SG,  IMP, IPFV, False, ("2sg!" ,)), #   you           sé        sei                 sois
    521: (PRES, 3, SG,  IMP, IPFV, False, ("3sg!" ,)), # (s)he
    53 : (PRES, 1, PL,  IMP, IPFV, False, ("1pl!" ,)), #    we                     seien               soyons
    54 : (PRES, 2, PL,  IMP, IPFV, False, ("2pl!" ,)), #   you           sed       seid                soyez
    541: (PRES, 3, PL,  IMP, IPFV, False, ("3pl!" ,)), #   you
    55 : (PRES, 1, SG,  SJV, IPFV, False, ("1sg?" ,)), #     I           sea       sei                 sois
    56 : (PRES, 2, SG,  SJV, IPFV, False, ("2sg?" ,)), #   you           seas      seist               sois
    57 : (PRES, 3, SG,  SJV, IPFV, False, ("3sg?" ,)), # (s)he           sea       sei                 soit
    58 : (PRES, 1, PL,  SJV, IPFV, False, ("1pl?" ,)), #    we           seamos    seien               soyons
    59 : (PRES, 2, PL,  SJV, IPFV, False, ("2pl?" ,)), #   you           seáis     seiet               soyez
    60 : (PRES, 3, PL,  SJV, IPFV, False, ("3pl?" ,)), #  they           sean      seien               soient
    61 : (PRES, 1, SG,  SJV,  PFV, False, ("1sg?+",)), #     I
    62 : (PRES, 2, SG,  SJV,  PFV, False, ("2sg?+",)), #   you
    63 : (PRES, 3, SG,  SJV,  PFV, False, ("3sg?+",)), # (s)he
    64 : (PRES, 1, PL,  SJV,  PFV, False, ("1pl?+",)), #    we
    65 : (PRES, 2, PL,  SJV,  PFV, False, ("2pl?+",)), #   you
    66 : (PRES, 3, PL,  SJV,  PFV, False, ("3pl?+",)), #  they
    67 : ( PST, 1, SG,  SJV, IPFV, False, ("1sgp?",)), #     I           fuera     wäre                fusse
    68 : ( PST, 2, SG,  SJV, IPFV, False, ("2sgp?",)), #   you           fueras    wärest              fusses
    69 : ( PST, 3, SG,  SJV, IPFV, False, ("3sgp?",)), # (s)he           fuera     wäre                fût
    70 : ( PST, 1, PL,  SJV, IPFV, False, ("1ppl?",)), #    we           fuéramos  wären               fussions
    71 : ( PST, 2, PL,  SJV, IPFV, False, ("2ppl?",)), #   you           fuerais   wäret               fussiez
    72 : ( PST, 3, PL,  SJV, IPFV, False, ("3ppl?",)), #  they           fueran    wären               fussent
}

# Map tenses and aliases to unique index.
# Aliases include:
# - a short number: "s", "sg", "singular" =&gt; SINGULAR,
# - a short string: "1sg" =&gt; 1st person singular present,
# - a unique index:  1    =&gt; 1st person singular present,
# -  Penn treebank: "VBP" =&gt; 1st person singular present.
TENSES_ID = {}
TENSES_ID[INFINITIVE] = 0
for i, (tense, person, number, mood, aspect, negated, aliases) in TENSES.items():
    for a in aliases + (i,):
        TENSES_ID[i] = \
        TENSES_ID[a] = \
        TENSES_ID[(tense, person, number, mood, aspect, negated)] = i
    if number == SG:
        for sg in ("s", "sg", "singular"):
            TENSES_ID[(tense, person, sg, mood, aspect, negated)] = i
    if number == PL:
        for pl in ("p", "pl", "plural"):
            TENSES_ID[(tense, person, pl, mood, aspect, negated)] = i

# Map Penn Treebank tags to unique index.
for tag, tense in (
  ("VB", 0),    # infinitive
  ("VBP", 1),   # present 1 singular
  ("VBZ", 3),   # present 3 singular
  ("VBG", 8),   # present participle
  ("VBN", 24),  # past participle
  ("VBD", 25)): # past
    TENSES_ID[tag.lower()] = tense

# tense(tense=INFINITIVE)
# tense(tense=(PRESENT, 3, SINGULAR))
# tense(tense=PRESENT, person=3, number=SINGULAR, mood=INDICATIVE, aspect=IMPERFECTIVE, negated=False)


</t>
<t tx="karstenw.20230303131712.138">def tense_id(*args, **kwargs):
    """ Returns the tense id for a given (tense, person, number, mood, aspect, negated).
        Aliases and compound forms (e.g., IMPERFECT) are disambiguated.
    """
    # Unpack tense given as a tuple, e.g., tense((PRESENT, 1, SG)):
    if len(args) == 1 and isinstance(args[0], (list, tuple)):
        if args[0] not in ((PRESENT, PARTICIPLE), (PAST, PARTICIPLE)):
            args = args[0]
    # No parameters defaults to tense=INFINITIVE, tense=PRESENT otherwise.
    if len(args) == 0 and len(kwargs) == 0:
        t = INFINITIVE
    else:
        t = PRESENT
    # Set default values.
    tense   = kwargs.get("tense"  , args[0] if len(args) &gt; 0 else t)
    person  = kwargs.get("person" , args[1] if len(args) &gt; 1 else 3) or None
    number  = kwargs.get("number" , args[2] if len(args) &gt; 2 else SINGULAR)
    mood    = kwargs.get("mood"   , args[3] if len(args) &gt; 3 else INDICATIVE)
    aspect  = kwargs.get("aspect" , args[4] if len(args) &gt; 4 else IMPERFECTIVE)
    negated = kwargs.get("negated", args[5] if len(args) &gt; 5 else False)
    # Disambiguate wrong order of parameters.
    if mood in (PERFECTIVE, IMPERFECTIVE):
        mood, aspect = INDICATIVE, mood
    # Disambiguate INFINITIVE.
    # Disambiguate PARTICIPLE, IMPERFECT, PRETERITE.
    # These are often considered to be tenses but are in fact tense + aspect.
    if tense == INFINITIVE:
        person = number = mood = aspect = None
        negated = False
    if tense in ((PRESENT, PARTICIPLE), PRESENT + PARTICIPLE, PARTICIPLE, GERUND):
        tense, aspect = PRESENT, PROGRESSIVE
    if tense in ((PAST, PARTICIPLE), PAST + PARTICIPLE):
        tense, aspect = PAST, PROGRESSIVE
    if tense == IMPERFECT:
        tense, aspect = PAST, IMPERFECTIVE
    if tense == PRETERITE:
        tense, aspect = PAST, PERFECTIVE
    if aspect in (CONTINUOUS, PARTICIPLE, GERUND):
        aspect = PROGRESSIVE
    if aspect == PROGRESSIVE:
        person = number = None
    # Disambiguate CONDITIONAL.
    # In Spanish, the conditional is regarded as an indicative tense.
    if tense == CONDITIONAL and mood == INDICATIVE:
        tense, mood = PRESENT, CONDITIONAL
    # Disambiguate aliases: "pl" =&gt;
    # (PRESENT, None, PLURAL, INDICATIVE, IMPERFECTIVE, False).
    return TENSES_ID.get(tense.lower(),
           TENSES_ID.get((tense, person, number, mood, aspect, negated)))

tense = tense_id



</t>
<t tx="karstenw.20230303131712.139">#--- VERB CONJUGATIONS -----------------------------------------------------------------------------
# Verb conjugations based on a table of known verbs and rules for unknown verbs.
# Verb conjugations are useful to find the verb infinitive in the parser's lemmatizer.
# For unknown verbs, Verbs.find_lemma() and Verbs.find_lexeme() are called.
# These must be implemented in a subclass with rules for unknown verbs.

class Verbs(lazydict):

    @others
</t>
<t tx="karstenw.20230303131712.14">def __iter__(self):
    return self._lazy("__iter__")

</t>
<t tx="karstenw.20230303131712.140">def __init__(self, path="", format=[], default={}, language=None):
    """ A dictionary of verb infinitives, each linked to a list of conjugated forms.
        Each line in the file at the given path is one verb, with the tenses separated by a comma.
        The format defines the order of tenses (see TENSES).
        The default dictionary defines default tenses for omitted tenses.
    """
    self._path = path
    self._language = language
    self._format = dict((TENSES_ID[id], i) for i, id in enumerate(format))
    self._default = default
    self._inverse = {}

</t>
<t tx="karstenw.20230303131712.141">def load(self):
    # have,,,has,,having,,,,,had,had,haven't,,,hasn't,,,,,,,hadn't,hadn't
    # pdb.set_trace()
    id = self._format[TENSES_ID[INFINITIVE]]
    for v in _read(self._path):
        v = v.split(",")
        dict.__setitem__(self, v[id], v)
        for x in (x for x in v if x):
            self._inverse[x] = v[id]

</t>
<t tx="karstenw.20230303131712.142">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230303131712.143">@property
def language(self):
    return self._language

</t>
<t tx="karstenw.20230303131712.144">@property
def infinitives(self):
    """ Yields a dictionary of (infinitive, [inflections])-items.
    """
    if dict.__len__(self) == 0:
        self.load()
    return self

</t>
<t tx="karstenw.20230303131712.145">@property
def inflections(self):
    """ Yields a dictionary of (inflected, infinitive)-items.
    """
    if dict.__len__(self) == 0:
        self.load()
    return self._inverse

</t>
<t tx="karstenw.20230303131712.146">@property
def TENSES(self):
    """ Yields a list of tenses for this language, excluding negations.
        Each tense is a (tense, person, number, mood, aspect)-tuple.
    """
    a = set(TENSES[id] for id in self._format)
    a = a.union(set(TENSES[id] for id in self._default.keys()))
    a = a.union(set(TENSES[id] for id in self._default.values()))
    a = sorted(x[:-2] for x in a if x[-2] is False) # Exclude negation.
    return a

</t>
<t tx="karstenw.20230303131712.147">def lemma(self, verb, parse=True):
    """ Returns the infinitive form of the given verb, or None.
    """
    if dict.__len__(self) == 0:
        self.load()
    if verb.lower() in self._inverse:
        return self._inverse[verb.lower()]
    if verb in self._inverse:
        return self._inverse[verb]
    if parse is True: # rule-based
        return self.find_lemma(verb)

</t>
<t tx="karstenw.20230303131712.148">def lexeme(self, verb, parse=True):
    """ Returns a list of all possible inflections of the given verb.
    """
    a = []
    b = self.lemma(verb, parse=parse)
    if b in self:
        a = [x for x in self[b] if x != ""]
    elif parse is True: # rule-based
        a = self.find_lexeme(b)
    u = []
    [u.append(x) for x in a if x not in u]
    return u

</t>
<t tx="karstenw.20230303131712.149">def conjugate(self, verb, *args, **kwargs):
    """ Inflects the verb and returns the given tense (or None).
        For example: be
        - Verbs.conjugate("is", INFINITVE) =&gt; be
        - Verbs.conjugate("be", PRESENT, 1, SINGULAR) =&gt; I am
        - Verbs.conjugate("be", PRESENT, 1, PLURAL) =&gt; we are
        - Verbs.conjugate("be", PAST, 3, SINGULAR) =&gt; he was
        - Verbs.conjugate("be", PAST, aspect=PROGRESSIVE) =&gt; been
        - Verbs.conjugate("be", PAST, person=1, negated=True) =&gt; I wasn't
    """
    id = tense_id(*args, **kwargs)
    # Get the tense index from the format description (or a default).
    i1 = self._format.get(id)
    i2 = self._format.get(self._default.get(id))
    i3 = self._format.get(self._default.get(self._default.get(id)))
    b = self.lemma(verb, parse=kwargs.get("parse", True))
    v = []
    # Get the verb lexeme and return the requested index.
    if b in self:
        v = self[b]
        for i in (i1, i2, i3):
            if i is not None and 0 &lt;= i &lt; len(v) and v[i]:
                return v[i]
    if kwargs.get("parse", True) is True: # rule-based
        v = self.find_lexeme(b)
        for i in (i1, i2, i3):
            if i is not None and 0 &lt;= i &lt; len(v) and v[i]:
                return v[i]

</t>
<t tx="karstenw.20230303131712.15">def __contains__(self, *args):
    return self._lazy("__contains__", *args)

</t>
<t tx="karstenw.20230303131712.150">def tenses(self, verb, parse=True):
    """ Returns a list of possible tenses for the given inflected verb.
    """
    verb = verb.lower()
    a = set()
    b = self.lemma(verb, parse=parse)
    v = []
    if b in self:
        v = self[b]
    elif parse is True: # rule-based
        v = self.find_lexeme(b)
    # For each tense in the verb lexeme that matches the given tense,
    # 1) retrieve the tense tuple,
    # 2) retrieve the tense tuples for which that tense is a default.
    for i, tense in enumerate(v):
        if tense == verb:
            for id, index in self._format.items():
                if i == index:
                    a.add(id)
            for id1, id2 in self._default.items():
                if id2 in a:
                    a.add(id1)
            for id1, id2 in self._default.items():
                if id2 in a:
                    a.add(id1)

    a = list(TENSES[id][:-2] for id in a)

    # In Python 2, None is always smaller than anything else while in Python 3, comparison with incompatible types yield TypeError.
    # This is why we need to use a custom key function.
    a = Tenses(sorted(a, key = lambda x: 0 if x[1] is None else x[1]))

    return a

</t>
<t tx="karstenw.20230303131712.151">def find_lemma(self, verb):
    # Must be overridden in a subclass.
    # Must return the infinitive for the given conjugated (unknown) verb.
    return verb

</t>
<t tx="karstenw.20230303131712.152">def find_lexeme(self, verb):
    # Must be overridden in a subclass.
    # Must return the list of conjugations for the given (unknown) verb.
    return []


</t>
<t tx="karstenw.20230303131712.153">class Tenses(list):

    @others
</t>
<t tx="karstenw.20230303131712.154">def __contains__(self, tense):
    # t in tenses(verb) also works when t is an alias (e.g. "1sg").
    return list.__contains__(self, TENSES[tense_id(tense)][:-2])

</t>
<t tx="karstenw.20230303131712.155">def avg(list):
    return sum(list) / float(len(list) or 1)


</t>
<t tx="karstenw.20230303131712.156">class Score(tuple):

    @others
</t>
<t tx="karstenw.20230303131712.157">def __new__(self, polarity, subjectivity, assessments=[]):
    """ A (polarity, subjectivity)-tuple with an assessments property.
    """
    return tuple.__new__(self, [polarity, subjectivity])

</t>
<t tx="karstenw.20230303131712.158">def __init__(self, polarity, subjectivity, assessments=[]):
    self.assessments = assessments


</t>
<t tx="karstenw.20230303131712.159">class Sentiment(lazydict):

    @others


</t>
<t tx="karstenw.20230303131712.16">def __getitem__(self, *args):
    return self._lazy("__getitem__", *args)

</t>
<t tx="karstenw.20230303131712.160">def __init__(self, path="", language=None, synset=None, confidence=None, **kwargs):
    """ A dictionary of words (adjectives) and polarity scores (positive/negative).
        The value for each word is a dictionary of part-of-speech tags.
        The value for each word POS-tag is a tuple with values for
        polarity (-1.0-1.0), subjectivity (0.0-1.0) and intensity (0.5-2.0).
    """
    self._path       = path   # XML file path.
    self._language   = None   # XML language attribute ("en", "fr", ...)
    self._confidence = None   # XML confidence attribute threshold (&gt;=).
    self._synset     = synset # XML synset attribute ("wordnet_id", "cornetto_id", ...)
    self._synsets    = {}     # {"a-01123879": (1.0, 1.0, 1.0)}
    self.labeler     = {}     # {"dammit": "profanity"}
    self.tokenizer   = kwargs.get("tokenizer", find_tokens)
    self.negations   = kwargs.get("negations", ("no", "not", "n't", "never"))
    self.modifiers   = kwargs.get("modifiers", ("RB",))
    self.modifier    = kwargs.get("modifier", lambda w: w.endswith("ly"))
    self.ngrams      = kwargs.get("ngrams", 3)

</t>
<t tx="karstenw.20230303131712.161">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230303131712.162">@property
def language(self):
    return self._language

</t>
<t tx="karstenw.20230303131712.163">@property
def confidence(self):
    return self._confidence

</t>
<t tx="karstenw.20230303131712.164">def load(self, path=None):
    """ Loads the XML-file (with sentiment annotations) from the given path.
        By default, Sentiment.path is lazily loaded.
    """
    # &lt;word form="great" wordnet_id="a-01123879" pos="JJ" polarity="1.0" subjectivity="1.0" intensity="1.0" /&gt;
    # &lt;word form="damnmit" polarity="-0.75" subjectivity="1.0" label="profanity" /&gt;
    if not path:
        path = self._path
    if not os.path.exists(path):
        return
    words, synsets, labels = {}, {}, {}
    xml = cElementTree.parse(path)
    xml = xml.getroot()
    for w in xml.findall("word"):
        if self._confidence is None \
        or self._confidence &lt;= float(w.attrib.get("confidence", 0.0)):
            w, pos, p, s, i, label, synset = (
                w.attrib.get("form"),
                w.attrib.get("pos"),
                w.attrib.get("polarity", 0.0),
                w.attrib.get("subjectivity", 0.0),
                w.attrib.get("intensity", 1.0),
                w.attrib.get("label"),
                w.attrib.get(self._synset) # wordnet_id, cornetto_id, ...
            )
            psi = (float(p), float(s), float(i))
            if w:
                words.setdefault(w, {}).setdefault(pos, []).append(psi)
            if w and label:
                labels[w] = label
            if synset:
                synsets.setdefault(synset, []).append(psi)
    self._language = xml.attrib.get("language", self._language)
    # Average scores of all word senses per part-of-speech tag.
    for w in words:
        words[w] = dict((pos, list(map(avg, zip(*psi)))) for pos, psi in words[w].items())
    # Average scores of all part-of-speech tags.
    for w, pos in words.items():
        words[w][None] = list(map(avg, zip(*pos.values())))
    # Average scores of all synonyms per synset.
    for id, psi in synsets.items():
        synsets[id] = list(map(avg, zip(*psi)))
    dict.update(self, words)
    dict.update(self.labeler, labels)
    dict.update(self._synsets, synsets)

</t>
<t tx="karstenw.20230303131712.165">def synset(self, id, pos=ADJECTIVE):
    """ Returns a (polarity, subjectivity)-tuple for the given synset id.
        For example, the adjective "horrible" has id 193480 in WordNet:
        Sentiment.synset(193480, pos="JJ") =&gt; (-0.6, 1.0, 1.0).
    """
    id = str(id).zfill(8)
    if not id.startswith(("n-", "v-", "a-", "r-")):
        if pos == NOUN:
            id = "n-" + id
        if pos == VERB:
            id = "v-" + id
        if pos == ADJECTIVE:
            id = "a-" + id
        if pos == ADVERB:
            id = "r-" + id
    if dict.__len__(self) == 0:
        self.load()
    try:
        return tuple(self._synsets[id])[:2]
    except KeyError: # Some WordNet id's are not zero padded.
        return tuple(self._synsets.get(re.sub(r"-0+", "-", id), (0.0, 0.0))[:2])

</t>
<t tx="karstenw.20230303131712.166">def __call__(self, s, negation=True, ngrams=DEFAULT, **kwargs):
    """ Returns a (polarity, subjectivity)-tuple for the given sentence,
        with polarity between -1.0 and 1.0 and subjectivity between 0.0 and 1.0.
        The sentence can be a string, Synset, Text, Sentence, Chunk, Word, Document, Vector.
        An optional weight parameter can be given,
        as a function that takes a list of words and returns a weight.
    """
    def avg(assessments, weighted=lambda w: 1):
        s, n = 0, 0
        for words, score in assessments:
            w = weighted(words)
            s += w * score
            n += w
        return s / float(n or 1)
    ngrams = ngrams if ngrams != DEFAULT else self.ngrams
    # A pattern.en.wordnet.Synset.
    # Sentiment(synsets("horrible", "JJ")[0]) =&gt; (-0.6, 1.0)
    if hasattr(s, "gloss"):
        a = [(s.synonyms[0],) + self.synset(s.id, pos=s.pos) + (None,)]
    # A synset id.
    # Sentiment("a-00193480") =&gt; horrible =&gt; (-0.6, 1.0)   (English WordNet)
    # Sentiment("c_267") =&gt; verschrikkelijk =&gt; (-0.9, 1.0) (Dutch Cornetto)
    elif isinstance(s, str) and RE_SYNSET.match(s):
        a = [(s.synonyms[0],) + self.synset(s.id, pos=s.pos) + (None,)]
    # A string of words.
    # Sentiment("a horrible movie") =&gt; (-0.6, 1.0)
    elif isinstance(s, str):
        a = self.assessments(((w.lower(), None) for w in " ".join(self.tokenizer(s)).split()), negation, ngrams)
    # A pattern.en.Text.
    elif hasattr(s, "sentences"):
        a = self.assessments(((w.lemma or w.string.lower(), w.pos[:2]) for w in chain(*s)), negation, ngrams)
    # A pattern.en.Sentence or pattern.en.Chunk.
    elif hasattr(s, "lemmata"):
        a = self.assessments(((w.lemma or w.string.lower(), w.pos[:2]) for w in s.words), negation, ngrams)
    # A pattern.en.Word.
    elif hasattr(s, "lemma"):
        a = self.assessments(((s.lemma or s.string.lower(), s.pos[:2]),), negation, ngrams)
    # A pattern.vector.Document.
    # Average score = weighted average using feature weights.
    # Bag-of words is unordered: inject None between each two words
    # to stop assessments() from scanning for preceding negation &amp; modifiers.
    elif hasattr(s, "terms"):
        a = self.assessments(chain(*(((w, None), (None, None)) for w in s)), negation, ngrams)
        kwargs.setdefault("weight", lambda w: s.terms[w[0]])
    # A dict of (word, weight)-items.
    elif isinstance(s, dict):
        a = self.assessments(chain(*(((w, None), (None, None)) for w in s)), negation, ngrams)
        kwargs.setdefault("weight", lambda w: s[w[0]])
    # A list of words.
    elif isinstance(s, list):
        a = self.assessments(((w, None) for w in s), negation, ngrams)
    else:
        a = []
    weight = kwargs.get("weight", lambda w: 1)
    # Each "w" in "a" is a (words, polarity, subjectivity, label)-tuple.
    return Score(polarity = avg(map(lambda w: (w[0], w[1]), a), weight),
             subjectivity = avg(map(lambda w: (w[0], w[2]), a), weight),
              assessments = a)

</t>
<t tx="karstenw.20230303131712.167">def assessments(self, words=[], negation=True, ngrams=DEFAULT):
    """ Returns a list of (chunk, polarity, subjectivity, label)-tuples for the given list of words:
        where chunk is a list of successive words: a known word optionally
        preceded by a modifier ("very good") or a negation ("not good").
    """
    ngrams = ngrams if ngrams != DEFAULT else self.ngrams
    words = list(words)
    index = 0
    a = []
    m = None # Preceding modifier (i.e., adverb or adjective).
    n = None # Preceding negation (e.g., "not beautiful").
    while index &lt; len(words):
        w, pos = words[index]
        # Only assess known words, preferably by part-of-speech tag.
        # Including unknown words (polarity 0.0 and subjectivity 0.0) lowers the average.
        if w is None:
            index += 1
            continue
        for i in reversed(range(1, max(1, ngrams))):
            # Known idioms ("hit the spot").
            if index &lt; len(words) - i:
                idiom = words[index:index + i + 1]
                idiom = " ".join(w_pos[0] or "END-OF-NGRAM" for w_pos in idiom)
                if idiom in self:
                    w, pos = idiom, None
                    index += i
                    break
        if w in self and pos in self[w]:
            p, s, i = self[w][pos]
            # Known word not preceded by a modifier ("good").
            if m is None:
                a.append(dict(w=[w], p=p, s=s, i=i, n=1, x=self.labeler.get(w)))
            # Known word preceded by a modifier ("really good").
            if m is not None:
                a[-1]["w"].append(w)
                a[-1]["p"] = max(-1.0, min(p * a[-1]["i"], +1.0))
                a[-1]["s"] = max(-1.0, min(s * a[-1]["i"], +1.0))
                a[-1]["i"] = i
                a[-1]["x"] = self.labeler.get(w)
            # Known word preceded by a negation ("not really good").
            if n is not None:
                a[-1]["w"].insert(0, n)
                a[-1]["i"] = 1.0 / (a[-1]["i"] or 1)
                a[-1]["n"] = -1
            # Known word may be a negation.
            # Known word may be modifying the next word (i.e., it is a known adverb).
            m = None
            n = None
            if pos and pos in self.modifiers or any(map(self[w].__contains__, self.modifiers)):
                m = (w, pos)
            if negation and w in self.negations:
                n = w
        else:
            # Unknown word may be a negation ("not good").
            if negation and w in self.negations:
                n = w
            # Unknown word. Retain negation across small words ("not a good").
            elif n and len(w.strip("'")) &gt; 1:
                n = None
            # Unknown word may be a negation preceded by a modifier ("really not good").
            if n is not None and m is not None and (pos in self.modifiers or self.modifier(m[0])):
                a[-1]["w"].append(n)
                a[-1]["n"] = -1
                n = None
            # Unknown word. Retain modifier across small words ("really is a good").
            elif m and len(w) &gt; 2:
                m = None
            # Exclamation marks boost previous word.
            if w == "!" and len(a) &gt; 0:
                a[-1]["w"].append("!")
                a[-1]["p"] = max(-1.0, min(a[-1]["p"] * 1.25, +1.0))
            # Exclamation marks in parentheses indicate sarcasm.
            if w == "(!)":
                a.append(dict(w=[w], p=0.0, s=1.0, i=1.0, n=1, x=IRONY))
            # EMOTICONS: {("grin", +1.0): set((":-D", ":D"))}
            if w.isalpha() is False and len(w) &lt;= 5 and w not in PUNCTUATION: # speedup
                for E in (EMOTICONS, EMOJI):
                    for (type, p), e in E.items():
                        if w in map(lambda e: e.lower(), e):
                            a.append(dict(w=[w], p=p, s=1.0, i=1.0, n=1, x=MOOD))
                            break
        index += 1
    for i in range(len(a)):
        w = a[i]["w"]
        p = a[i]["p"]
        s = a[i]["s"]
        n = a[i]["n"]
        x = a[i]["x"]
        # "not good" = slightly bad, "not bad" = slightly good.
        a[i] = (w, p * -0.5 if n &lt; 0 else p, s, x)
    return a

</t>
<t tx="karstenw.20230303131712.168">def annotate(self, word, pos=None, polarity=0.0, subjectivity=0.0, intensity=1.0, label=None):
    """ Annotates the given word with polarity, subjectivity and intensity scores,
        and optionally a semantic label (e.g., MOOD for emoticons, IRONY for "(!)").
    """
    w = self.setdefault(word, {})
    w[pos] = w[None] = (polarity, subjectivity, intensity)
    if label:
        self.labeler[word] = label

</t>
<t tx="karstenw.20230303131712.169">def save(self, path):
    """ Saves the lexicon as an XML-file.
    """
    # WordNet id's, word sense descriptions and confidence scores
    # from a bundled XML (e.g., en/lexicon-en.xml) are not saved.
    a = []
    a.append("&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;")
    a.append("&lt;sentiment&gt;")
    for w in sorted(self):
        for pos, (p, s, i) in self[w].items():
            pos = pos or ""
            if pos or len(self[w]) == 1:
                a.append("\t&lt;word %s %s %s %s %s %s /&gt;" % (
                          "form=\"%s\""   % w,
                           "pos=\"%s\""   % pos,
                      "polarity=\"%.2f\"" % p,
                  "subjectivity=\"%.2f\"" % s,
                     "intensity=\"%.2f\"" % i,
                         "label=\"%s\""   % self.labeler.get(w, "")
                ))
    a.append("&lt;/sentiment&gt;")
    f = open(path, "w", encoding="utf-8")
    f.write(BOM_UTF8 + encode_utf8("\n".join(a)))
    f.close()

</t>
<t tx="karstenw.20230303131712.17">def __setitem__(self, *args):
    return self._lazy("__setitem__", *args)

</t>
<t tx="karstenw.20230303131712.170">#### SPELLING CORRECTION ###########################################################################
# Based on: Peter Norvig, "How to Write a Spelling Corrector", http://norvig.com/spell-correct.html

class Spelling(lazydict):

    # latin alphabet
    LATIN = "abcdefghijklmnopqrstuvwxyz"

    # cyrillic alphabet
    CYRILLIC = 'абвгдеёжзийклмнопрстуфхцчшщьыъэюя'

    @others
</t>
<t tx="karstenw.20230303131712.171">def __init__(self, path="", alphabet='LATIN'):
    self._path = path
    if alphabet == 'CYRILLIC':
        self.alphabet = Spelling.CYRILLIC
    else:
        self.alphabet = Spelling.LATIN

</t>
<t tx="karstenw.20230303131712.172">def load(self):
    for x in _read(self._path):
        x = x.split()
        dict.__setitem__(self, x[0], int(x[1]))

</t>
<t tx="karstenw.20230303131712.173">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230303131712.174">@property
def language(self):
    return self._language

</t>
<t tx="karstenw.20230303131712.175">@classmethod
def train(self, s, path="spelling.txt"):
    """ Counts the words in the given string and saves the probabilities at the given path.
        This can be used to generate a new model for the Spelling() constructor.
    """
    model = {}
    for w in re.findall("[a-z]+", s.lower()):
        model[w] = w in model and model[w] + 1 or 1
    model = ("%s %s" % (k, v) for k, v in sorted(model.items()))
    model = "\n".join(model)
    f = open(path, "w", encoding="utf-8")
    f.write(model)
    f.close()

</t>
<t tx="karstenw.20230303131712.176">def _edit1(self, w):
    """ Returns a set of words with edit distance 1 from the given word.
    """
    # Of all spelling errors, 80% is covered by edit distance 1.
    # Edit distance 1 = one character deleted, swapped, replaced or inserted.
    split = [(w[:i], w[i:]) for i in range(len(w) + 1)]
    delete, transpose, replace, insert = (
        [a + b[1:] for a, b in split if b],
        [a + b[1] + b[0] + b[2:] for a, b in split if len(b) &gt; 1],
        [a + c + b[1:] for a, b in split for c in self.alphabet if b],
        [a + c + b[0:] for a, b in split for c in self.alphabet]
    )
    return set(delete + transpose + replace + insert)

</t>
<t tx="karstenw.20230303131712.177">def _edit2(self, w):
    """ Returns a set of words with edit distance 2 from the given word
    """
    # Of all spelling errors, 99% is covered by edit distance 2.
    # Only keep candidates that are actually known words (20% speedup).
    return set(e2 for e1 in self._edit1(w) for e2 in self._edit1(e1) if e2 in self)

</t>
<t tx="karstenw.20230303131712.178">def _known(self, words=[]):
    """ Returns the given list of words filtered by known words.
    """
    return set(w for w in words if w in self)

</t>
<t tx="karstenw.20230303131712.179">def suggest(self, w):
    """ Return a list of (word, confidence) spelling corrections for the given word,
        based on the probability of known words with edit distance 1-2 from the given word.
    """
    if len(self) == 0:
        self.load()
    if len(w) == 1:
        return [(w, 1.0)] # I
    if w in PUNCTUATION:
        return [(w, 1.0)] # .?!
    if w.replace(".", "").isdigit():
        return [(w, 1.0)] # 1.5
    candidates = self._known([w]) \
              or self._known(self._edit1(w)) \
              or self._known(self._edit2(w)) \
              or [w]

    candidates = [(self.get(c, 0.0), c) for c in candidates]
    s = float(sum(p for p, w in candidates) or 1)
    candidates = sorted(((p / s, w) for p, w in candidates), reverse=True)
    candidates = [(w.istitle() and x.title() or x, p) for p, x in candidates] # case-sensitive
    return candidates

</t>
<t tx="karstenw.20230303131712.18">def __delitem__(self, *args):
    return self._lazy("__delitem__", *args)

</t>
<t tx="karstenw.20230303131712.180">def _module(language):
    """ Returns the given language module (e.g., "en" =&gt; pattern.en).
    """

    if sys.version &gt; '3':
        return _modules.setdefault(language, __import__(language, globals(), {}, [], 1))
    else:
        return _modules.setdefault(language, __import__(language, globals(), {}, [], -1))


</t>
<t tx="karstenw.20230303131712.181">def _multilingual(function, *args, **kwargs):
    """ Returns the value from the function with the given name in the given language module.
        By default, language="en".
    """
    return getattr(_module(kwargs.pop("language", "en")), function)(*args, **kwargs)


</t>
<t tx="karstenw.20230303131712.182">def language(s):
    """ Returns a (language, confidence)-tuple for the given string.
    """
    s = decode_utf8(s)
    s = set(w.strip(PUNCTUATION) for w in s.replace("'", "' ").split())
    n = float(len(s) or 1)
    p = {}
    for xx in LANGUAGES:
        lexicon = _module(xx).__dict__["lexicon"]
        p[xx] = sum(1 for w in s if w in lexicon) / n
    return max(p.items(), key=lambda kv: (kv[1], int(kv[0] == "en")))

lang = language


</t>
<t tx="karstenw.20230303131712.183">def tokenize(*args, **kwargs):
    return _multilingual("tokenize", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.184">def parse(*args, **kwargs):
    return _multilingual("parse", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.185">def parsetree(*args, **kwargs):
    return _multilingual("parsetree", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.186">def split(*args, **kwargs):
    return _multilingual("split", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.187">def tag(*args, **kwargs):
    return _multilingual("tag", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.188">def keywords(*args, **kwargs):
    return _multilingual("keywords", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.189">def suggest(*args, **kwargs):
    return _multilingual("suggest", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.19">def setdefault(self, *args):
    return self._lazy("setdefault", *args)

</t>
<t tx="karstenw.20230303131712.190">def sentiment(*args, **kwargs):
    return _multilingual("sentiment", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.191">def singularize(*args, **kwargs):
    return _multilingual("singularize", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.192">def pluralize(*args, **kwargs):
    return _multilingual("pluralize", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.193">def conjugate(*args, **kwargs):
    return _multilingual("conjugate", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.194">def predicative(*args, **kwargs):
    return _multilingual("predicative", *args, **kwargs)


</t>
<t tx="karstenw.20230303131712.195">def suggest(*args, **kwargs):
    return _multilingual("suggest", *args, **kwargs)
</t>
<t tx="karstenw.20230303131712.2">def ngramm(self, token_list, threshhold, indexes=[]):
    H = []
    res = [[i] for i in range(len(token_list))]
    for i in range(len(res) - 1):
        p1 = u'_'.join([token_list[w_i] for w_i in res[i]])
        p2 = u'_'.join([token_list[w_i] for w_i in res[i + 1]])
        score = self._significanceScore(p1, p2)
        H += [score]
    while len(res) &gt; 1:
        Best = max(H)
        best_ind = H.index(Best)
        if Best &gt; threshhold:
            new_res = res[:best_ind]
            new_res += [res[best_ind] + res[best_ind + 1]]
            new_res += res[best_ind + 2:]

            if best_ind == 0:
                new_H = []
            else:
                new_H = H[:best_ind - 1]
                p1 = u'_'.join([token_list[w_i] for w_i in new_res[best_ind - 1]])
                p2 = u'_'.join([token_list[w_i] for w_i in new_res[best_ind]])
                new_H += [self._significanceScore(p1, p2)]
            if best_ind != len(new_res) - 1:
                p1 = u'_'.join([token_list[w_i] for w_i in new_res[best_ind]])
                p2 = u'_'.join([token_list[w_i] for w_i in new_res[best_ind + 1]])
                new_H += [self._significanceScore(p1, p2)]
            new_H += H[best_ind + 2:]
            H = new_H
            res = new_res
        else:
            break
    ngrammed_doc = []
    for ngramm_ind in res:
        ngrammed_doc.append(u'_'.join([token_list[x] for x in ngramm_ind]))

    new_indexes = []
    if indexes:
        for i, ngramm_ind in enumerate(res):
            new_indexes += []
            start_ind = indexes[2 * ngramm_ind[0]]
            length = indexes[2 * ngramm_ind[-1]] + indexes[2 * ngramm_ind[-1] + 1] - start_ind
            new_indexes += (start_ind, length)

    return ngrammed_doc, new_indexes

</t>
<t tx="karstenw.20230303131712.20">def get(self, *args, **kwargs):
    return self._lazy("get", *args)

</t>
<t tx="karstenw.20230303131712.21">def items(self):
    return self._lazy("items")

</t>
<t tx="karstenw.20230303131712.22">def keys(self):
    return self._lazy("keys")

</t>
<t tx="karstenw.20230303131712.23">def values(self):
    return self._lazy("values")

</t>
<t tx="karstenw.20230303131712.24">def update(self, *args):
    return self._lazy("update", *args)

</t>
<t tx="karstenw.20230303131712.25">def pop(self, *args):
    return self._lazy("pop", *args)

</t>
<t tx="karstenw.20230303131712.26">def popitem(self, *args):
    return self._lazy("popitem", *args)

</t>
<t tx="karstenw.20230303131712.27">class lazylist(list):

    @others
#--- LAZY SET --------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303131712.28">def load(self):
    # Must be overridden in a subclass.
    # Must load data with list.append(self, v) instead of lazylist.append(v).
    pass

</t>
<t tx="karstenw.20230303131712.29">def _lazy(self, method, *args):
    """ If the list is empty, calls lazylist.load().
        Replaces lazylist.method() with list.method() and calls it.
    """
    if list.__len__(self) == 0:
        self.load()
        setattr(self, method, types.MethodType(getattr(list, method), self))
    return getattr(list, method)(self, *args)

</t>
<t tx="karstenw.20230303131712.3">def removeDelimiters(self, ngramm_list, indexes=[]):
    new_list = []
    new_indexes = []
    for i, w in enumerate(ngramm_list):
        if w in self._delimiters:
            continue
        flag = False
        for ptrn in self._delimiters_regex:
            if re.match(ptrn, w):
                flag = True
                break
        if flag:
            continue
        new_list.append(w)
        if indexes:
            new_indexes += (indexes[2 * i], indexes[2 * i + 1])
    return new_list, new_indexes

</t>
<t tx="karstenw.20230303131712.30">def __repr__(self):
    return self._lazy("__repr__")

</t>
<t tx="karstenw.20230303131712.31">def __len__(self):
    return self._lazy("__len__")

</t>
<t tx="karstenw.20230303131712.32">def __iter__(self):
    return self._lazy("__iter__")

</t>
<t tx="karstenw.20230303131712.33">def __contains__(self, *args):
    return self._lazy("__contains__", *args)

</t>
<t tx="karstenw.20230303131712.34">def __getitem__(self, *args):
    return self._lazy("__getitem__", *args)

</t>
<t tx="karstenw.20230303131712.35">def __setitem__(self, *args):
    return self._lazy("__setitem__", *args)

</t>
<t tx="karstenw.20230303131712.36">def __delitem__(self, *args):
    return self._lazy("__delitem__", *args)

</t>
<t tx="karstenw.20230303131712.37">def insert(self, *args):
    return self._lazy("insert", *args)

</t>
<t tx="karstenw.20230303131712.38">def append(self, *args):
    return self._lazy("append", *args)

</t>
<t tx="karstenw.20230303131712.39">def extend(self, *args):
    return self._lazy("extend", *args)

</t>
<t tx="karstenw.20230303131712.4">def saveAsJson(self, filename, with_delimiters=False):
    to_save = {u'lengthInWords': self._lengthInWords,
               u'phrase2freq': self._phrase2freq}
    if (with_delimiters):
        to_save[u'delimiters'] = self._delimiters
        to_save[u'delimiters_regex'] = [x.pattern for x in self._delimiters_regex]
    with open(filename, 'w') as fp:
        json.dump(to_save, fp)

</t>
<t tx="karstenw.20230303131712.40">def remove(self, *args):
    return self._lazy("remove", *args)

</t>
<t tx="karstenw.20230303131712.41">def pop(self, *args):
    return self._lazy("pop", *args)

</t>
<t tx="karstenw.20230303131712.42">def index(self, *args):
    return self._lazy("index", *args)

</t>
<t tx="karstenw.20230303131712.43">def count(self, *args):
    return self._lazy("count", *args)

</t>
<t tx="karstenw.20230303131712.44">class lazyset(set):

    @others
</t>
<t tx="karstenw.20230303131712.45">def load(self):
    # Must be overridden in a subclass.
    # Must load data with list.append(self, v) instead of lazylist.append(v).
    pass

</t>
<t tx="karstenw.20230303131712.46">def _lazy(self, method, *args):
    """ If the list is empty, calls lazylist.load().
        Replaces lazylist.method() with list.method() and calls it.
    """
    print("!")
    if set.__len__(self) == 0:
        self.load()
        setattr(self, method, types.MethodType(getattr(set, method), self))
    return getattr(set, method)(self, *args)

</t>
<t tx="karstenw.20230303131712.47">def __repr__(self):
    return self._lazy("__repr__")

</t>
<t tx="karstenw.20230303131712.48">def __len__(self):
    return self._lazy("__len__")

</t>
<t tx="karstenw.20230303131712.49">def __iter__(self):
    return self._lazy("__iter__")

</t>
<t tx="karstenw.20230303131712.5">def loadFromJson(self, filename, with_delimiters=False):
    with open(filename, 'r') as fp:
        loaded = json.load(fp)
    self._lengthInWords = loaded[u'lengthInWords']
    self._phrase2freq = loaded[u'phrase2freq']
    if (with_delimiters):
        self._delimiters = loaded[u'delimiters']
        self._delimiters_regex = [re.compile(p) for p in loaded[u'delimiters_regex']]



</t>
<t tx="karstenw.20230303131712.50">def __contains__(self, *args):
    return self._lazy("__contains__", *args)

</t>
<t tx="karstenw.20230303131712.51">def __sub__(self, *args):
    return self._lazy("__sub__", *args)

</t>
<t tx="karstenw.20230303131712.52">def __and__(self, *args):
    return self._lazy("__and__", *args)

</t>
<t tx="karstenw.20230303131712.53">def __or__(self, *args):
    return self._lazy("__or__", *args)

</t>
<t tx="karstenw.20230303131712.54">def __xor__(self, *args):
    return self._lazy("__xor__", *args)

</t>
<t tx="karstenw.20230303131712.55">def __isub__(self, *args):
    return self._lazy("__isub__", *args)

</t>
<t tx="karstenw.20230303131712.56">def __iand__(self, *args):
    return self._lazy("__iand__", *args)

</t>
<t tx="karstenw.20230303131712.57">def __ior__(self, *args):
    return self._lazy("__ior__", *args)

</t>
<t tx="karstenw.20230303131712.58">def __ixor__(self, *args):
    return self._lazy("__ixor__", *args)

</t>
<t tx="karstenw.20230303131712.59">def __gt__(self, *args):
    return self._lazy("__gt__", *args)

</t>
<t tx="karstenw.20230303131712.6">def deflood(s, n=3):
    """ Returns the string with no more than n repeated characters, e.g.,
        deflood("NIIIICE!!", n=1) =&gt; "Nice!"
        deflood("nice.....", n=3) =&gt; "nice..."
    """
    if n == 0:
        return s[0:0]
    return re.sub(r"((.)\2{%s,})" % (n - 1), lambda m: m.group(1)[0] * n, s)


</t>
<t tx="karstenw.20230303131712.60">def __lt__(self, *args):
    return self._lazy("__lt__", *args)

</t>
<t tx="karstenw.20230303131712.61">def __gte__(self, *args):
    return self._lazy("__gte__", *args)

</t>
<t tx="karstenw.20230303131712.62">def __lte__(self, *args):
    return self._lazy("__lte__", *args)

</t>
<t tx="karstenw.20230303131712.63">def add(self, *args):
    return self._lazy("add", *args)

</t>
<t tx="karstenw.20230303131712.64">def pop(self, *args):
    return self._lazy("pop", *args)

</t>
<t tx="karstenw.20230303131712.65">def remove(self, *args):
    return self._lazy("remove", *args)

</t>
<t tx="karstenw.20230303131712.66">def discard(self, *args):
    return self._lazy("discard", *args)

</t>
<t tx="karstenw.20230303131712.67">def isdisjoint(self, *args):
    return self._lazy("isdisjoint", *args)

</t>
<t tx="karstenw.20230303131712.68">def issubset(self, *args):
    return self._lazy("issubset", *args)

</t>
<t tx="karstenw.20230303131712.69">def issuperset(self, *args):
    return self._lazy("issuperset", *args)

</t>
<t tx="karstenw.20230303131712.7">def decamel(s, separator="_"):
    """ Returns the string with CamelCase converted to underscores, e.g.,
        decamel("TomDeSmedt", "-") =&gt; "tom-de-smedt"
        decamel("getHTTPResponse2) =&gt; "get_http_response2"
    """
    s = re.sub(r"([a-z0-9])([A-Z])", "\\1%s\\2" % separator, s)
    s = re.sub(r"([A-Z])([A-Z][a-z])", "\\1%s\\2" % separator, s)
    s = s.lower()
    return s


</t>
<t tx="karstenw.20230303131712.70">def union(self, *args):
    return self._lazy("union", *args)

</t>
<t tx="karstenw.20230303131712.71">def intersection(self, *args):
    return self._lazy("intersection", *args)

</t>
<t tx="karstenw.20230303131712.72">def difference(self, *args):
    return self._lazy("difference", *args)

</t>
<t tx="karstenw.20230303131712.73">def _read(path, encoding="utf-8", comment=";;;"):
    """ Returns an iterator over the lines in the file at the given path,
        strippping comments and decoding each line to Unicode.
    """
    if path:
        if isinstance(path, str) and os.path.exists(path):
            # From file path.
            f = open(path, "r", encoding="utf-8")
        elif isinstance(path, str):
            # From string.
            f = path.splitlines()
        else:
            # From file or buffer.
            f = path
        for i, line in enumerate(f):
            line = line.strip(BOM_UTF8) if i == 0 and isinstance(line, str) else line
            line = line.strip()
            line = decode_utf8(line, encoding)
            if not line or (comment and line.startswith(comment)):
                continue
            yield line
    # raise StopIteration


</t>
<t tx="karstenw.20230303131712.74">class Lexicon(lazydict):

    @others


</t>
<t tx="karstenw.20230303131712.75">def __init__(self, path=""):
    """ A dictionary of known words and their part-of-speech tags.
    """
    self._path = path

</t>
<t tx="karstenw.20230303131712.76">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230303131712.77">def load(self):
    # Arnold NNP x
    dict.update(self, (x.split(" ")[:2] for x in _read(self._path) if len(x.split(" ")) &gt; 1))

</t>
<t tx="karstenw.20230303131712.78">#--- FREQUENCY -------------------------------------------------------------------------------------
class Frequency(lazydict):

    @others
</t>
<t tx="karstenw.20230303131712.79">def __init__(self, path=""):
    """ A dictionary of words and their relative document frequency.
    """
    self._path = path

</t>
<t tx="karstenw.20230303131712.8">def pprint(string, token=[WORD, POS, CHUNK, PNP], column=4):
    """ Pretty-prints the output of Parser.parse() as a table with outlined columns.
        Alternatively, you can supply a tree.Text or tree.Sentence object.
    """
    if isinstance(string, str):
        print("\n\n".join([table(sentence, fill=column) for sentence in Text(string, token)]))
    if isinstance(string, Text):
        print("\n\n".join([table(sentence, fill=column) for sentence in string]))
    if isinstance(string, Sentence):
        print(table(string, fill=column))

#--- LAZY DICTIONARY -------------------------------------------------------------------------------
# A lazy dictionary is empty until one of its methods is called.
# This way many instances (e.g., lexicons) can be created without using memory until used.


</t>
<t tx="karstenw.20230303131712.80">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230303131712.81">def load(self):
    # and 0.4805
    for x in _read(self.path):
        x = x.split()
        dict.__setitem__(self, x[0], float(x[1]))

</t>
<t tx="karstenw.20230303131712.82">class Model(object):

    @others

</t>
<t tx="karstenw.20230303131712.83">def __init__(self, path="", classifier=None, known=set(), unknown=set()):
    """ A language model using a classifier (e.g., SLP, SVM) trained on morphology and context.
    """
    try:
        from pattern.vector import Classifier
        from pattern.vector import Perceptron
    except ImportError:
        sys.path.insert(0, os.path.join(MODULE, ".."))
        from vector import Classifier
        from vector import Perceptron
    self._path = path
    # Use a property instead of a subclass, so users can choose their own classifier.
    self._classifier = Classifier.load(path) if path else classifier or Perceptron()
    # Parser.lexicon entries can be ambiguous (e.g., about/IN  is RB 25% of the time).
    # Parser.lexicon entries also in Model.unknown are overruled by the model.
    # Parser.lexicon entries also in Model.known are not learned by the model
    # (only their suffix and context is learned, see Model._v() below).
    self.unknown = unknown | self._classifier._data.get("model_unknown", set())
    self.known = known

</t>
<t tx="karstenw.20230303131712.84">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230303131712.85">@classmethod
def load(self, path="", lexicon={}):
    return Model(path, lexicon)

</t>
<t tx="karstenw.20230303131712.86">def save(self, path, final=True):
    self._classifier._data["model_unknown"] = self.unknown
    self._classifier.save(path, final) # final = unlink training data (smaller file).

</t>
<t tx="karstenw.20230303131712.87">def train(self, token, tag, previous=None, next=None):
    """ Trains the model to predict the given tag for the given token,
        in context of the given previous and next (token, tag)-tuples.
    """
    self._classifier.train(self._v(token, previous, next), type=tag)

</t>
<t tx="karstenw.20230303131712.88">def classify(self, token, previous=None, next=None, **kwargs):
    """ Returns the predicted tag for the given token,
        in context of the given previous and next (token, tag)-tuples.
    """
    return self._classifier.classify(self._v(token, previous, next), **kwargs)

</t>
<t tx="karstenw.20230303131712.89">def apply(self, token, previous=(None, None), next=(None, None)):
    """ Returns a (token, tag)-tuple for the given token,
        in context of the given previous and next (token, tag)-tuples.
    """
    return [token[0], self._classifier.classify(self._v(token[0], previous, next))]

</t>
<t tx="karstenw.20230303131712.9">class lazydict(dict):

    @others
#--- LAZY LIST -------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303131712.90">def _v(self, token, previous=None, next=None):
    """ Returns a training vector for the given token and its context.
    """
    def f(v, *s):
        v[" ".join(s)] = 1
    p, n = previous, next
    p = ("", "") if not p else (p[0] or "", p[1] or "")
    n = ("", "") if not n else (n[0] or "", n[1] or "")
    v = {}
    f(v, "b", "b")         # Bias.
    f(v, "h", token[:1])   # Capitalization.
    f(v, "w", token[-6:] if token not in self.known or token in self.unknown else "")
    f(v, "x", token[-3:])  # Word suffix.
    f(v, "-x", p[0][-3:])   # Word suffix left.
    f(v, "+x", n[0][-3:])   # Word suffix right.
    f(v, "-t", p[1])        # Tag left.
    f(v, "-+", p[1] + n[1]) # Tag left + right.
    f(v, "+t", n[1])        # Tag right.
    return v

</t>
<t tx="karstenw.20230303131712.91">def _get_description(self):
    return self._classifier.description

</t>
<t tx="karstenw.20230303131712.92">def _set_description(self, s):
    self._classifier.description = s

description = property(_get_description, _set_description)

</t>
<t tx="karstenw.20230303131712.93">#--- MORPHOLOGICAL RULES ---------------------------------------------------------------------------
# Brill's algorithm generates lexical (i.e., morphological) rules in the following format:
# NN s fhassuf 1 NNS x =&gt; unknown words ending in -s and tagged NN change to NNS.
#     ly hassuf 2 RB x =&gt; unknown words ending in -ly change to RB.

class Morphology(lazylist):

    @others

</t>
<t tx="karstenw.20230303131712.94">def __init__(self, path="", known={}):
    """ A list of rules based on word morphology (prefix, suffix).
    """
    self.known = known
    self._path = path
    self._cmd = set((
            "word", # Word is x.
            "char", # Word contains x.
         "haspref", # Word starts with x.
          "hassuf", # Word end with x.
         "addpref", # x + word is in lexicon.
          "addsuf", # Word + x is in lexicon.
      "deletepref", # Word without x at the start is in lexicon.
       "deletesuf", # Word without x at the end is in lexicon.
        "goodleft", # Word preceded by word x.
       "goodright", # Word followed by word x.
    ))
    self._cmd.update([("f" + x) for x in self._cmd])

</t>
<t tx="karstenw.20230303131712.95">@property
def path(self):
    return self._path

</t>
<t tx="karstenw.20230303131712.96">def load(self):
    # ["NN", "s", "fhassuf", "1", "NNS", "x"]
    list.extend(self, (x.split() for x in _read(self._path)))

</t>
<t tx="karstenw.20230303131712.97">def apply(self, token, previous=(None, None), next=(None, None)):
    """ Applies lexical rules to the given token, which is a [word, tag] list.
    """
    w = token[0]
    for r in self:
        if r[1] in self._cmd: # Rule = ly hassuf 2 RB x
            f, x, pos, cmd = bool(0), r[0], r[-2], r[1].lower()
        if r[2] in self._cmd: # Rule = NN s fhassuf 1 NNS x
            f, x, pos, cmd = bool(1), r[1], r[-2], r[2].lower().lstrip("f")
        if f and token[1] != r[0]:
            continue
        if (cmd == "word"       and x == w) \
        or (cmd == "char"       and x in w) \
        or (cmd == "haspref"    and w.startswith(x)) \
        or (cmd == "hassuf"     and w.endswith(x)) \
        or (cmd == "addpref"    and x + w in self.known) \
        or (cmd == "addsuf"     and w + x in self.known) \
        or (cmd == "deletepref" and w.startswith(x) and w[len(x):] in self.known) \
        or (cmd == "deletesuf"  and w.endswith(x) and w[:-len(x)] in self.known) \
        or (cmd == "goodleft"   and x == next[0]) \
        or (cmd == "goodright"  and x == previous[0]):
            token[1] = pos
    return token

</t>
<t tx="karstenw.20230303131712.98">def insert(self, i, tag, affix, cmd="hassuf", tagged=None):
    """ Inserts a new rule that assigns the given tag to words with the given affix,
        e.g., Morphology.append("RB", "-ly").
    """
    if affix.startswith("-") and affix.endswith("-"):
        affix, cmd = affix[+1:-1], "char"
    if affix.startswith("-"):
        affix, cmd = affix[+1:-0], "hassuf"
    if affix.endswith("-"):
        affix, cmd = affix[+0:-1], "haspref"
    if tagged:
        r = [tagged, affix, "f" + cmd.lstrip("f"), tag, "x"]
    else:
        r = [affix, cmd.lstrip("f"), tag, "x"]
    lazylist.insert(self, i, r)

</t>
<t tx="karstenw.20230303131712.99">def append(self, *args, **kwargs):
    self.insert(len(self) - 1, *args, **kwargs)

</t>
<t tx="karstenw.20230303131752.1">#### PATTERN | TEXT | PATTERN MATCHING #############################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303131805.1">#### PATTERN | EN | PARSE TREE #####################################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################
# Text and Sentence objects to traverse words and chunks in parsed text.
# from pattern.en import parsetree
# for sentence in parsetree("The cat sat on the mat."):
#     for chunk in sentence.chunks:
#         for word in chunk.words:
#             print(word.string, word.tag, word.lemma)

# Terminology:
# - part-of-speech: the role that a word plays in a sentence: noun (NN), verb (VB), adjective, ...
# -    sentence: a unit of language, with a subject (e.g., "the cat") and a predicate ("jumped").
# -       token: a word in a sentence with a part-of-speech tag (e.g., "jump/VB" or "jump/NN").
# -        word: a string of characters that expresses a meaningful concept (e.g., "cat").
# -       lemma: the canonical word form ("jumped" =&gt; "jump").
# -      lexeme: the set of word forms ("jump", "jumps", "jumping", ...)
# -       chunk: a phrase, group of words that express a single thought (e.g., "the cat").
# -     subject: the phrase that the sentence is about, usually a noun phrase.
# -   predicate: the remainder of the sentence tells us what the subject does (jump).
# -      object: the phrase that is affected by the action (the cat jumped [the mouse]").
# - preposition: temporal, spatial or logical relationship ("the cat jumped [on the table]").
# -      anchor: the chunk to which the preposition is attached:
#                "the cat eats its snackerel with vigor" =&gt; eat with vigor?
#                                                     OR =&gt; vigorous snackerel?

# The Text and Sentece classes are containers:
# no parsing functionality should be added to it.

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303131812.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

from io import open

from itertools import chain

try:
    from config import SLASH
    from config import WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA
    MBSP = True # Memory-Based Shallow Parser for Python.
except:
    SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA = \
        "&amp;slash;", "word", "part-of-speech", "chunk", "preposition", "relation", "anchor", "lemma"
    MBSP = False

# B- marks the start of a chunk: the/DT/B-NP cat/NN/I-NP
# I- words are inside a chunk.
# O- words are outside a chunk (punctuation etc.).
IOB, BEGIN, INSIDE, OUTSIDE = "IOB", "B", "I", "O"

# -SBJ marks subjects: the/DT/B-NP-SBJ cat/NN/I-NP-SBJ
# -OBJ marks objects.
ROLE = "role"

SLASH0 = SLASH[0]

### LIST FUNCTIONS #################################################################################


</t>
<t tx="karstenw.20230303131812.10">def __repr__(self):
    return repr(list(iter(self)))

</t>
<t tx="karstenw.20230303131812.100">def __getitem__(self, index):
    return self.words[index]

</t>
<t tx="karstenw.20230303131812.101">def __len__(self):
    return len(self.words)

</t>
<t tx="karstenw.20230303131812.102">def __iter__(self):
    return self.words.__iter__()

</t>
<t tx="karstenw.20230303131812.103">def append(self, word, lemma=None, type=None, chunk=None, role=None, relation=None, pnp=None, anchor=None, iob=None, custom={}):
    """ Appends the next word to the sentence / chunk / preposition.
        For example: Sentence.append("clawed", "claw", "VB", "VP", role=None, relation=1)
        - word     : the current word,
        - lemma    : the canonical form of the word,
        - type     : part-of-speech tag for the word (NN, JJ, ...),
        - chunk    : part-of-speech tag for the chunk this word is part of (NP, VP, ...),
        - role     : the chunk's grammatical role (SBJ, OBJ, ...),
        - relation : an id shared by other related chunks (e.g., SBJ-1 &lt;=&gt; VP-1),
        - pnp      : PNP if this word is in a prepositional noun phrase (B- prefix optional),
        - iob      : BEGIN if the word marks the start of a new chunk,
                     INSIDE (optional) if the word is part of the previous chunk,
        - custom   : a dictionary of (tag, value)-items for user-defined word tags.
    """
    self._do_word(word, lemma, type)            # Append Word object.
    self._do_chunk(chunk, role, relation, iob)  # Append Chunk, or add last word to last chunk.
    self._do_conjunction()
    self._do_relation()
    self._do_pnp(pnp, anchor)
    self._do_anchor(anchor)
    self._do_custom(custom)

</t>
<t tx="karstenw.20230303131812.104">def parse_token(self, token, tags=[WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA]):
    """ Returns the arguments for Sentence.append() from a tagged token representation.
        The order in which token tags appear can be specified.
        The default order is (separated by slashes): 
        - word, 
        - part-of-speech, 
        - (IOB-)chunk, 
        - (IOB-)preposition, 
        - chunk(-relation)(-role), 
        - anchor, 
        - lemma.
        Examples:
        The/DT/B-NP/O/NP-SBJ-1/O/the
        cats/NNS/I-NP/O/NP-SBJ-1/O/cat
        clawed/VBD/B-VP/O/VP-1/A1/claw
        at/IN/B-PP/B-PNP/PP/P1/at
        the/DT/B-NP/I-PNP/NP/P1/the
        sofa/NN/I-NP/I-PNP/NP/P1/sofa
        ././O/O/O/O/.
        Returns a (word, lemma, type, chunk, role, relation, preposition, anchor, iob, custom)-tuple,
        which can be passed to Sentence.append(): Sentence.append(*Sentence.parse_token("cats/NNS/NP"))
        The custom value is a dictionary of (tag, value)-items of unrecognized tags in the token.
    """
    p = {WORD: "",
           POS: None,
           IOB: None,
         CHUNK: None,
           PNP: None,
           REL: None,
          ROLE: None,
        ANCHOR: None,
         LEMMA: None}
    # Split the slash-formatted token into separate tags in the given order.
    # Decode &amp;slash; characters (usually in words and lemmata).
    # Assume None for missing tags (except the word itself, which defaults to an empty string).
    custom = {}
    for k, v in zip(tags, token.split("/")):
        if SLASH0 in v:
            v = v.replace(SLASH, "/")
        if k == "pos":
            k = POS
        if k not in p:
            custom[k] = None
        if v != OUTSIDE or k == WORD or k == LEMMA: # "type O negative" =&gt; "O" != OUTSIDE.
            (p if k not in custom else custom)[k] = v
    # Split IOB-prefix from the chunk tag:
    # B- marks the start of a new chunk,
    # I- marks inside of a chunk.
    ch = p[CHUNK]
    if ch is not None and ch.startswith(("B-", "I-")):
        p[IOB], p[CHUNK] = ch[:1], ch[2:] # B-NP
    # Split the role from the relation:
    # NP-SBJ-1 =&gt; relation id is 1 and role is SBJ,
    # VP-1 =&gt; relation id is 1 with no role.
    # Tokens may be tagged with multiple relations (e.g., NP-OBJ-1*NP-OBJ-3).
    if p[REL] is not None:
        ch, p[REL], p[ROLE] = self._parse_relation(p[REL])
        # Infer a missing chunk tag from the relation tag (e.g., NP-SBJ-1 =&gt; NP).
        # For PP relation tags (e.g., PP-CLR-1), the first chunk is PP, the following chunks NP.
        if ch == "PP" \
         and self._previous \
         and self._previous[REL] == p[REL] \
         and self._previous[ROLE] == p[ROLE]:
            ch = "NP"
        if p[CHUNK] is None and ch != OUTSIDE:
            p[CHUNK] = ch
    self._previous = p
    # Return the tags in the right order for Sentence.append().
    return p[WORD], p[LEMMA], p[POS], p[CHUNK], p[ROLE], p[REL], p[PNP], p[ANCHOR], p[IOB], custom

</t>
<t tx="karstenw.20230303131812.105">def _parse_relation(self, tag):
    """ Parses the chunk tag, role and relation id from the token relation tag.
        - VP                =&gt; VP, [], []
        - VP-1              =&gt; VP, [1], [None]
        - ADJP-PRD          =&gt; ADJP, [None], [PRD]
        - NP-SBJ-1          =&gt; NP, [1], [SBJ]
        - NP-OBJ-1*NP-OBJ-2 =&gt; NP, [1,2], [OBJ,OBJ]
        - NP-SBJ;NP-OBJ-1   =&gt; NP, [1,1], [SBJ,OBJ]
    """
    chunk, relation, role = None, [], []
    if ";" in tag:
        # NP-SBJ;NP-OBJ-1 =&gt; 1 relates to both SBJ and OBJ.
        id = tag.split("*")[0][-2:]
        id = id if id.startswith("-") else ""
        tag = tag.replace(";", id + "*")
    if "*" in tag:
        tag = tag.split("*")
    else:
        tag = [tag]
    for s in tag:
        s = s.split("-")
        n = len(s)
        if n == 1:
            chunk = s[0]
        if n == 2:
            chunk = s[0]
            relation.append(s[1])
            role.append(None)
        if n &gt;= 3:
            chunk = s[0]
            relation.append(s[2])
            role.append(s[1])
        if n &gt; 1:
            id = relation[-1]
            if id.isdigit():
                relation[-1] = int(id)
            else:
                # Correct "ADJP-PRD":
                # (ADJP, [PRD], [None]) =&gt; (ADJP, [None], [PRD])
                relation[-1], role[-1] = None, id
    return chunk, relation, role

</t>
<t tx="karstenw.20230303131812.106">def _do_word(self, word, lemma=None, type=None):
    """ Adds a new Word to the sentence.
        Other Sentence._do_[tag] functions assume a new word has just been appended.
    """
    # Improve 3rd person singular "'s" lemma to "be", e.g., as in "he's fine".
    if lemma == "'s" and type in ("VB", "VBZ"):
        lemma = "be"
    self.words.append(Word(self, word, lemma, type, index=len(self.words)))

</t>
<t tx="karstenw.20230303131812.107">def _do_chunk(self, type, role=None, relation=None, iob=None):
    """ Adds a new Chunk to the sentence, or adds the last word to the previous chunk.
        The word is attached to the previous chunk if both type and relation match,
        and if the word's chunk tag does not start with "B-" (i.e., iob != BEGIN).
        Punctuation marks (or other "O" chunk tags) are not chunked.
    """
    if (type is None or type == OUTSIDE) and \
       (role is None or role == OUTSIDE) and (relation is None or relation == OUTSIDE):
        return
    if iob != BEGIN \
     and self.chunks \
     and self.chunks[-1].type == type \
     and self._relation == (relation, role) \
     and self.words[-2].chunk is not None: # "one, two" =&gt; "one" &amp; "two" different chunks.
        self.chunks[-1].append(self.words[-1])
    else:
        ch = Chunk(self, [self.words[-1]], type, role, relation)
        self.chunks.append(ch)
        self._relation = (relation, role)

</t>
<t tx="karstenw.20230303131812.108">def _do_relation(self):
    """ Attaches subjects, objects and verbs.
        If the previous chunk is a subject/object/verb, it is stored in Sentence.relations{}.
    """
    if self.chunks:
        ch = self.chunks[-1]
        for relation, role in ch.relations:
            if role == "SBJ" or role == "OBJ":
                self.relations[role][relation] = ch
        if ch.type in ("VP",):
            self.relations[ch.type][ch.relation] = ch

</t>
<t tx="karstenw.20230303131812.109">def _do_pnp(self, pnp, anchor=None):
    """ Attaches prepositional noun phrases.
        Identifies PNP's from either the PNP tag or the P-attachment tag.
        This does not determine the PP-anchor, it only groups words in a PNP chunk.
    """
    if anchor or pnp and pnp.endswith("PNP"):
        if anchor is not None:
            m = find(lambda x: x.startswith("P"), anchor)
        else:
            m = None
        if self.pnp \
         and pnp \
         and pnp != OUTSIDE \
         and pnp.startswith("B-") is False \
         and self.words[-2].pnp is not None:
            self.pnp[-1].append(self.words[-1])
        elif m is not None and m == self._attachment:
            self.pnp[-1].append(self.words[-1])
        else:
            ch = PNPChunk(self, [self.words[-1]], type="PNP")
            self.pnp.append(ch)
        self._attachment = m

</t>
<t tx="karstenw.20230303131812.11">def __getitem__(self, i):
    return self._f(self._a[i])

</t>
<t tx="karstenw.20230303131812.110">def _do_anchor(self, anchor):
    """ Collects preposition anchors and attachments in a dictionary.
        Once the dictionary has an entry for both the anchor and the attachment, they are linked.
    """
    if anchor:
        for x in anchor.split("-"):
            A, P = None, None
            if x.startswith("A") and len(self.chunks) &gt; 0: # anchor
                A, P = x, x.replace("A", "P")
                self._anchors[A] = self.chunks[-1]
            if x.startswith("P") and len(self.pnp) &gt; 0:    # attachment (PNP)
                A, P = x.replace("P", "A"), x
                self._anchors[P] = self.pnp[-1]
            if A in self._anchors and P in self._anchors and not self._anchors[P].anchor:
                pnp = self._anchors[P]
                pnp.anchor = self._anchors[A]
                pnp.anchor.attachments.append(pnp)

</t>
<t tx="karstenw.20230303131812.111">def _do_custom(self, custom):
    """ Adds the user-defined tags to the last word.
        Custom tags can be used to add extra semantical meaning or metadata to words.
    """
    if custom:
        self.words[-1].custom_tags.update(custom)

</t>
<t tx="karstenw.20230303131812.112">def _do_conjunction(self, _and=("and", "e", "en", "et", "und", "y")):
    """ Attach conjunctions.
        CC-words like "and" and "or" between two chunks indicate a conjunction.
    """
    w = self.words
    if len(w) &gt; 2 and w[-2].type == "CC" and w[-2].chunk is None:
        cc = w[-2].string.lower() in _and and AND or OR
        ch1 = w[-3].chunk
        ch2 = w[-1].chunk
        if ch1 is not None and \
           ch2 is not None:
            ch1.conjunctions.append(ch2, cc)
            ch2.conjunctions.append(ch1, cc)

</t>
<t tx="karstenw.20230303131812.113">def get(self, index, tag=LEMMA):
    """ Returns a tag for the word at the given index.
        The tag can be WORD, LEMMA, POS, CHUNK, PNP, RELATION, ROLE, ANCHOR or a custom word tag.
    """
    if tag == WORD:
        return self.words[index]
    if tag == LEMMA:
        return self.words[index].lemma
    if tag == POS or tag == "pos":
        return self.words[index].type
    if tag == CHUNK:
        return self.words[index].chunk
    if tag == PNP:
        return self.words[index].pnp
    if tag == REL:
        ch = self.words[index].chunk
        return ch and ch.relation
    if tag == ROLE:
        ch = self.words[index].chunk
        return ch and ch.role
    if tag == ANCHOR:
        ch = self.words[index].pnp
        return ch and ch.anchor
    if tag in self.words[index].custom_tags:
        return self.words[index].custom_tags[tag]
    return None

</t>
<t tx="karstenw.20230303131812.114">def loop(self, *tags):
    """ Iterates over the tags in the entire Sentence,
        For example, Sentence.loop(POS, LEMMA) yields tuples of the part-of-speech tags and lemmata. 
        Possible tags: WORD, LEMMA, POS, CHUNK, PNP, RELATION, ROLE, ANCHOR or a custom word tag.
        Any order or combination of tags can be supplied.
    """
    for i in range(len(self.words)):
        yield tuple([self.get(i, tag=tag) for tag in tags])

</t>
<t tx="karstenw.20230303131812.115">def indexof(self, value, tag=WORD):
    """ Returns the indices of tokens in the sentence where the given token tag equals the string.
        The string can contain a wildcard "*" at the end (this way "NN*" will match "NN" and "NNS").
        The tag can be WORD, LEMMA, POS, CHUNK, PNP, RELATION, ROLE, ANCHOR or a custom word tag.
        For example: Sentence.indexof("VP", tag=CHUNK) 
        returns the indices of all the words that are part of a VP chunk.
    """
    match = lambda a, b: a.endswith("*") and b.startswith(a[:-1]) or a == b
    indices = []
    for i in range(len(self.words)):
        if match(value, self.get(i, tag)):
            indices.append(i)
    return indices

</t>
<t tx="karstenw.20230303131812.116">def slice(self, start, stop):
    """ Returns a portion of the sentence from word start index to word stop index.
        The returned slice is a subclass of Sentence and a deep copy.
    """
    s = Slice(token=self.token, language=self.language)
    for i, word in enumerate(self.words[start:stop]):
        # The easiest way to copy (part of) a sentence
        # is by unpacking all of the token tags and passing them to Sentence.append().
        p0 = word.string                                                       # WORD
        p1 = word.lemma                                                        # LEMMA
        p2 = word.type                                                         # POS
        p3 = word.chunk is not None and word.chunk.type or None                # CHUNK
        p4 = word.pnp is not None and "PNP" or None                            # PNP
        p5 = word.chunk is not None and unzip(0, word.chunk.relations) or None # REL
        p6 = word.chunk is not None and unzip(1, word.chunk.relations) or None # ROLE
        p7 = word.chunk and word.chunk.anchor_id or None                       # ANCHOR
        p8 = word.chunk and word.chunk.start == start + i and BEGIN or None      # IOB
        p9 = word.custom_tags                                                  # User-defined tags.
        # If the given range does not contain the chunk head, remove the chunk tags.
        if word.chunk is not None and (word.chunk.stop &gt; stop):
            p3, p4, p5, p6, p7, p8 = None, None, None, None, None, None
        # If the word starts the preposition, add the IOB B-prefix (i.e., B-PNP).
        if word.pnp is not None and word.pnp.start == start + i:
            p4 = BEGIN + "-" + "PNP"
        # If the given range does not contain the entire PNP, remove the PNP tags.
        # The range must contain the entire PNP,
        # since it starts with the PP and ends with the chunk head (and is meaningless without these).
        if word.pnp is not None and (word.pnp.start &lt; start or word.chunk.stop &gt; stop):
            p4, p7 = None, None
        s.append(word=p0, lemma=p1, type=p2, chunk=p3, pnp=p4, relation=p5, role=p6, anchor=p7, iob=p8, custom=p9)
    s.parent = self
    s._start = start
    return s

</t>
<t tx="karstenw.20230303131812.117">def copy(self):
    return self.slice(0, len(self))

</t>
<t tx="karstenw.20230303131812.118">def chunked(self):
    return chunked(self)

</t>
<t tx="karstenw.20230303131812.119">def constituents(self, pnp=False):
    """ Returns an in-order list of mixed Chunk and Word objects.
        With pnp=True, also contains PNPChunk objects whenever possible.
    """
    a = []
    for word in self.words:
        if pnp and word.pnp is not None:
            if len(a) == 0 or a[-1] != word.pnp:
                a.append(word.pnp)
        elif word.chunk is not None:
            if len(a) == 0 or a[-1] != word.chunk:
                a.append(word.chunk)
        else:
            a.append(word)
    return a

# Sentence.string and unicode(Sentence) are Unicode strings.
# repr(Sentence) is a Python strings (with Unicode characters encoded).
</t>
<t tx="karstenw.20230303131812.12">def __len__(self):
    return len(self._a)

</t>
<t tx="karstenw.20230303131812.120">@property
def string(self):
    return " ".join(word.string for word in self)

</t>
<t tx="karstenw.20230303131812.121">def __str__(self):
    return self.string

</t>
<t tx="karstenw.20230303131812.122">def __repr__(self):
    return "Sentence(%s)" % repr(" ".join(["/".join(word.tags) for word in self.words]))

</t>
<t tx="karstenw.20230303131812.123">def __eq__(self, other):
    if not isinstance(other, Sentence):
        return False
    return len(self) == len(other) and repr(self) == repr(other)

# This is required because we overwrite the parent's __eq__() method.
# Otherwise objects will be unhashable in Python 3.
# More information: http://docs.python.org/3.6/reference/datamodel.html#object.__hash__
__hash__ = object.__hash__

</t>
<t tx="karstenw.20230303131812.124">@property
def xml(self):
    """ Yields the sentence as an XML-formatted string (plain bytestring, UTF-8 encoded).
    """
    return parse_xml(self, tab="\t", id=self.id or "")

</t>
<t tx="karstenw.20230303131812.125">@classmethod
def from_xml(cls, xml):
    """ Returns a new Text from the given XML string.
    """
    s = parse_string(xml)
    return Sentence(s.split("\n")[0], token=s.tags, language=s.language)

fromxml = from_xml

</t>
<t tx="karstenw.20230303131812.126">def nltk_tree(self):
    """ The sentence as an nltk.tree object.
    """
    return nltk_tree(self)


</t>
<t tx="karstenw.20230303131812.127">class Slice(Sentence):

    @others
#---------------------------------------------------------------------------------------------------
# s = Sentence(parse("black cats and white dogs"))
# s.words          =&gt; [Word('black/JJ'), Word('cats/NNS'), Word('and/CC'), Word('white/JJ'), Word('dogs/NNS')]
# s.chunks         =&gt; [Chunk('black cats/NP'), Chunk('white dogs/NP')]
# s.constituents() =&gt; [Chunk('black cats/NP'), Word('and/CC'), Chunk('white dogs/NP')]
# s.chunked(s)     =&gt; [Chunk('black cats/NP'), Chink('and/O'), Chunk('white dogs/NP')]


</t>
<t tx="karstenw.20230303131812.128">def __init__(self, *args, **kwargs):
    """ A portion of the sentence returned by Sentence.slice().
    """
    self._start = kwargs.pop("start", 0)
    Sentence.__init__(self, *args, **kwargs)

</t>
<t tx="karstenw.20230303131812.129">@property
def start(self):
    return self._start

</t>
<t tx="karstenw.20230303131812.13">def __iter__(self):
    i = 0
    while i &lt; len(self._a):
        yield self._f(self._a[i])
        i += 1

</t>
<t tx="karstenw.20230303131812.130">@property
def stop(self):
    return self._start + len(self.words)

</t>
<t tx="karstenw.20230303131812.131">def chunked(sentence):
    """ Returns a list of Chunk and Chink objects from the given sentence.
        Chink is a subclass of Chunk used for words that have Word.chunk == None
        (e.g., punctuation marks, conjunctions).
    """
    # For example, to construct a training vector with the head of previous chunks as a feature.
    # Doing this with Sentence.chunks would discard the punctuation marks and conjunctions
    # (Sentence.chunks only yields Chunk objects), which amy be useful features.
    chunks = []
    for word in sentence:
        if word.chunk is not None:
            if len(chunks) == 0 or chunks[-1] != word.chunk:
                chunks.append(word.chunk)
        else:
            ch = Chink(sentence)
            ch.append(word.copy(ch))
            chunks.append(ch)
    return chunks

#--- TEXT ------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303131812.132">class Text(list):

    @others
Tree = Text


</t>
<t tx="karstenw.20230303131812.133">def __init__(self, string, token=[WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA], language="en", encoding="utf-8"):
    """ A list of Sentence objects parsed from the given string.
        The string is the Unicode return value from parse().
    """
    self.encoding = encoding
    # Extract token format from TokenString if possible.
    if _is_tokenstring(string):
        token, language = string.tags, getattr(string, "language", language)
    if string:
        # From a string.
        if isinstance(string, str):
            string = string.splitlines()
        # From an iterable (e.g., string.splitlines(), open('parsed.txt')).
        self.extend(Sentence(s, token, language) for s in string)

</t>
<t tx="karstenw.20230303131812.134">def insert(self, index, sentence):
    list.insert(self, index, sentence)
    sentence.text = self

</t>
<t tx="karstenw.20230303131812.135">def append(self, sentence):
    list.append(self, sentence)
    sentence.text = self

</t>
<t tx="karstenw.20230303131812.136">def extend(self, sentences):
    list.extend(self, sentences)
    for s in sentences:
        s.text = self

</t>
<t tx="karstenw.20230303131812.137">def remove(self, sentence):
    list.remove(self, sentence)
    sentence.text = None

</t>
<t tx="karstenw.20230303131812.138">def pop(self, index):
    sentence = list.pop(self, index)
    sentence.text = None
    return sentence

</t>
<t tx="karstenw.20230303131812.139">@property
def sentences(self):
    return list(self)

</t>
<t tx="karstenw.20230303131812.14">class Word(object):

    @others
</t>
<t tx="karstenw.20230303131812.140">@property
def words(self):
    return list(chain(*self))

</t>
<t tx="karstenw.20230303131812.141">def copy(self):
    t = Text("", encoding=self.encoding)
    for sentence in self:
        t.append(sentence.copy())
    return t

# Text.string and unicode(Text) are Unicode strings.
</t>
<t tx="karstenw.20230303131812.142">@property
def string(self):
    return "\n".join(sentence.string for sentence in self)

</t>
<t tx="karstenw.20230303131812.143">def __str__(self):
    return self.string

#def __repr__(self):
#    return "\n".join([repr(sentence) for sentence in self])

</t>
<t tx="karstenw.20230303131812.144">@property
def xml(self):
    """ Yields the sentence as an XML-formatted string (plain bytestring, UTF-8 encoded).
        All the sentences in the XML are wrapped in a &lt;text&gt; element.
    """
    xml = []
    xml.append('&lt;?xml version="1.0" encoding="%s"?&gt;' % XML_ENCODING.get(self.encoding, self.encoding))
    xml.append("&lt;%s&gt;" % XML_TEXT)
    xml.extend([sentence.xml for sentence in self])
    xml.append("&lt;/%s&gt;" % XML_TEXT)
    return "\n".join(xml)

</t>
<t tx="karstenw.20230303131812.145">@classmethod
def from_xml(cls, xml):
    """ Returns a new Text from the given XML string.
    """
    return Text(parse_string(xml))

fromxml = from_xml

</t>
<t tx="karstenw.20230303131812.146">def tree(string, token=[WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA]):
    """ Transforms the output of parse() into a Text object.
        The token parameter lists the order of tags in each token in the input string.
    """
    return Text(string, token)

split = tree # Backwards compatibility.


</t>
<t tx="karstenw.20230303131812.147">def xml(string, token=[WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA]):
    """ Transforms the output of parse() into XML.
        The token parameter lists the order of tags in each token in the input string.
    """
    return Text(string, token).xml

### XML ############################################################################################

# Elements:
XML_TEXT     = "text"     # &lt;text&gt;, corresponds to Text object.
XML_SENTENCE = "sentence" # &lt;sentence&gt;, corresponds to Sentence object.
XML_CHINK    = "chink"    # &lt;chink&gt;, where word.chunk.type=None.
XML_CHUNK    = "chunk"    # &lt;chunk&gt;, corresponds to Chunk object.
XML_PNP      = "chunk"    # &lt;chunk type="PNP"&gt;, corresponds to PNP chunk object.
XML_WORD     = "word"     # &lt;word&gt;, corresponds to Word object

# Attributes:
XML_LANGUAGE = "language" # &lt;sentence language=""&gt;, defines the language used.
XML_TOKEN    = "token"    # &lt;sentence token=""&gt;, defines the order of tags in a token.
XML_TYPE     = "type"     # &lt;word type=""&gt;, &lt;chunk type=""&gt;
XML_RELATION = "relation" # &lt;chunk relation=""&gt;
XML_ID       = "id"       # &lt;chunk id=""&gt;
XML_OF       = "of"       # &lt;chunk of=""&gt; corresponds to id-attribute.
XML_ANCHOR   = "anchor"   # &lt;chunk anchor=""&gt; corresponds to id-attribute.
XML_LEMMA    = "lemma"    # &lt;word lemma=""&gt;

XML_ENCODING = {
            'utf8' : 'UTF-8',
           'utf-8' : 'UTF-8',
           'utf16' : 'UTF-16',
          'utf-16' : 'UTF-16',
           'latin' : 'ISO-8859-1',
          'latin1' : 'ISO-8859-1',
         'latin-1' : 'ISO-8859-1',
          'cp1252' : 'windows-1252',
    'windows-1252' : 'windows-1252'
}


</t>
<t tx="karstenw.20230303131812.148">def xml_encode(string):
    """ Returns the string with XML-safe special characters.
    """
    string = string.replace("&amp;", "&amp;amp;")
    string = string.replace("&lt;", "&amp;lt;")
    string = string.replace("&gt;", "&amp;gt;")
    string = string.replace("\"", "&amp;quot;")
    string = string.replace(SLASH, "/")
    return string


</t>
<t tx="karstenw.20230303131812.149">def xml_decode(string):
    """ Returns the string with special characters decoded.
    """
    string = string.replace("&amp;amp;", "&amp;")
    string = string.replace("&amp;lt;", "&lt;")
    string = string.replace("&amp;gt;", "&gt;")
    string = string.replace("&amp;quot;", "\"")
    string = string.replace("/", SLASH)
    return string

#--- SENTENCE TO XML -------------------------------------------------------------------------------

# Relation id's in the XML output are relative to the sentence id,
# so relation 1 in sentence 2 = "2.1".
_UID_SEPARATOR = "."


</t>
<t tx="karstenw.20230303131812.15">def __init__(self, sentence, string, lemma=None, type=None, index=0):
    """ A word in the sentence.
        - lemma: base form of the word; "was" =&gt; "be".
        -  type: the part-of-speech tag; "NN" =&gt; a noun.
        - chunk: the chunk (or phrase) this word belongs to.
        - index: the index in the sentence.
    """
    if not isinstance(string, str):
        try:
            string = string.decode("utf-8") # ensure Unicode
        except:
            pass
    self.sentence = sentence
    self.index    = index
    self.string   = string   # "was"
    self.lemma    = lemma    # "be"
    self.type     = type     # VB
    self.chunk    = None     # Chunk object this word belongs to (i.e., a VP).
    self.pnp      = None     # PNP chunk object this word belongs to.
                             # word.chunk and word.pnp are set in chunk.append().
    self._custom_tags = None # Tags object, created on request.

</t>
<t tx="karstenw.20230303131812.150">def parse_xml(sentence, tab="\t", id=""):
    """ Returns the given Sentence object as an XML-string (plain bytestring, UTF-8 encoded).
        The tab delimiter is used as indendation for nested elements.
        The id can be used as a unique identifier per sentence for chunk id's and anchors.
        For example: "I eat pizza with a fork." =&gt;

        &lt;sentence token="word, part-of-speech, chunk, preposition, relation, anchor, lemma" language="en"&gt;
            &lt;chunk type="NP" relation="SBJ" of="1"&gt;
                &lt;word type="PRP" lemma="i"&gt;I&lt;/word&gt;
            &lt;/chunk&gt;
            &lt;chunk type="VP" relation="VP" id="1" anchor="A1"&gt;
                &lt;word type="VBP" lemma="eat"&gt;eat&lt;/word&gt;
            &lt;/chunk&gt;
            &lt;chunk type="NP" relation="OBJ" of="1"&gt;
                &lt;word type="NN" lemma="pizza"&gt;pizza&lt;/word&gt;
            &lt;/chunk&gt;
            &lt;chunk type="PNP" of="A1"&gt;
                &lt;chunk type="PP"&gt;
                    &lt;word type="IN" lemma="with"&gt;with&lt;/word&gt;
                &lt;/chunk&gt;
                &lt;chunk type="NP"&gt;
                    &lt;word type="DT" lemma="a"&gt;a&lt;/word&gt;
                    &lt;word type="NN" lemma="fork"&gt;fork&lt;/word&gt;
                &lt;/chunk&gt;
            &lt;/chunk&gt;
            &lt;chink&gt;
                &lt;word type="." lemma="."&gt;.&lt;/word&gt;
            &lt;/chink&gt;
        &lt;/sentence&gt;
    """
    uid = lambda *parts: "".join([str(id), _UID_SEPARATOR] + [str(x) for x in parts]).lstrip(_UID_SEPARATOR)
    push = lambda indent: indent + tab         # push() increases the indentation.
    pop = lambda indent: indent[:-len(tab)] # pop() decreases the indentation.
    indent = tab
    xml = []
    # Start the sentence element:
    # &lt;sentence token="word, part-of-speech, chunk, preposition, relation, anchor, lemma"&gt;
    xml.append('&lt;%s%s %s="%s" %s="%s"&gt;' % (
        XML_SENTENCE,
        XML_ID and " %s=\"%s\"" % (XML_ID, str(id)) or "",
        XML_TOKEN, ", ".join(sentence.token),
        XML_LANGUAGE, sentence.language
    ))
    # Collect chunks that are PNP anchors and assign id.
    anchors = {}
    for chunk in sentence.chunks:
        if chunk.attachments:
            anchors[chunk.start] = len(anchors) + 1
    # Traverse all words in the sentence.
    for word in sentence.words:
        chunk = word.chunk
        pnp = word.chunk and word.chunk.pnp or None
        # Start the PNP element if the chunk is the first chunk in PNP:
        # &lt;chunk type="PNP" of="A1"&gt;
        if pnp and pnp.start == chunk.start and pnp.start == word.index:
            a = pnp.anchor and ' %s="%s"' % (XML_OF, uid("A", anchors.get(pnp.anchor.start, ""))) or ""
            xml.append(indent + '&lt;%s %s="PNP"%s&gt;' % (XML_CHUNK, XML_TYPE, a))
            indent = push(indent)
        # Start the chunk element if the word is the first word in the chunk:
        # &lt;chunk type="VP" relation="VP" id="1" anchor="A1"&gt;
        if chunk and chunk.start == word.index:
            if chunk.relations:
                # Create the shortest possible attribute values for multiple relations,
                # e.g., [(1,"OBJ"),(2,"OBJ")]) =&gt; relation="OBJ" id="1|2"
                r1 = unzip(0, chunk.relations) # Relation id's.
                r2 = unzip(1, chunk.relations) # Relation roles.
                r1 = [x is None and "-" or uid(x) for x in r1]
                r2 = [x is None and "-" or x for x in r2]
                r1 = not len(unique(r1)) == 1 and "|".join(r1) or (r1 + [None])[0]
                r2 = not len(unique(r2)) == 1 and "|".join(r2) or (r2 + [None])[0]
            xml.append(indent + '&lt;%s%s%s%s%s%s&gt;' % (
                XML_CHUNK,
                chunk.type and ' %s="%s"' % (XML_TYPE, chunk.type) or "",
                chunk.relations and chunk.role is not None and ' %s="%s"' % (XML_RELATION, r2) or "",
                chunk.relation and chunk.type == "VP" and ' %s="%s"' % (XML_ID, uid(chunk.relation)) or "",
                chunk.relation and chunk.type != "VP" and ' %s="%s"' % (XML_OF, r1) or "",
                chunk.attachments and ' %s="%s"' % (XML_ANCHOR, uid("A", anchors[chunk.start])) or ""
            ))
            indent = push(indent)
        # Words outside of a chunk are wrapped in a &lt;chink&gt; tag:
        # &lt;chink&gt;
        if not chunk:
            xml.append(indent + '&lt;%s&gt;' % XML_CHINK)
            indent = push(indent)
        # Add the word element:
        # &lt;word type="VBP" lemma="eat"&gt;eat&lt;/word&gt;
        xml.append(indent + '&lt;%s%s%s%s&gt;%s&lt;/%s&gt;' % (
            XML_WORD,
            word.type and ' %s="%s"' % (XML_TYPE, xml_encode(word.type)) or '',
            word.lemma and ' %s="%s"' % (XML_LEMMA, xml_encode(word.lemma)) or '',
            (" " + " ".join(['%s="%s"' % (k, v) for k, v in word.custom_tags.items() if v is not None])).rstrip(),
            xml_encode(word.string),
            XML_WORD
        ))
        if not chunk:
            # Close the &lt;chink&gt; element if outside of a chunk.
            indent = pop(indent)
            xml.append(indent + "&lt;/%s&gt;" % XML_CHINK)
        if chunk and chunk.stop - 1 == word.index:
            # Close the &lt;chunk&gt; element if this is the last word in the chunk.
            indent = pop(indent)
            xml.append(indent + "&lt;/%s&gt;" % XML_CHUNK)
        if pnp and pnp.stop - 1 == word.index:
            # Close the PNP element if this is the last word in the PNP.
            indent = pop(indent)
            xml.append(indent + "&lt;/%s&gt;" % XML_CHUNK)
    xml.append("&lt;/%s&gt;" % XML_SENTENCE)
    # Return as a plain str.
    return "\n".join(xml)

#--- XML TO SENTENCE(S) ----------------------------------------------------------------------------

# Classes XML and XMLNode provide an abstract interface to cElementTree.
# The advantage is that we can switch to a faster parser in the future
# (as we did when switching from xml.dom.minidom to xml.etree).
# cElemenTree is fast; but the fastest way is to simply store and reload the parsed Unicode string.
# The disadvantage is that we need to remember the token format, see (1) below:
# s = "..."
# s = parse(s, lemmata=True)
# open("parsed.txt",  "w", encoding="utf-8").write(s)
# s = open("parsed.txt", encoding="utf-8")
# s = Text(s, token=[WORD, POS, CHUNK, PNP, LEMMA]) # (1)


</t>
<t tx="karstenw.20230303131812.151">class XML(object):
    @others
</t>
<t tx="karstenw.20230303131812.152">def __init__(self, string):
    from xml.etree import cElementTree
    self.root = cElementTree.fromstring(string)

</t>
<t tx="karstenw.20230303131812.153">def __call__(self, tag):
    return self.root.tag == tag \
       and [XMLNode(self.root)] \
        or [XMLNode(e) for e in self.root.findall(tag)]


</t>
<t tx="karstenw.20230303131812.154">class XMLNode(object):
    @others
# The structure of linked anchor chunks and PNP attachments
# is collected from _parse_token() calls.
_anchors = {} # {'A1': [['eat', 'VBP', 'B-VP', 'O', 'VP-1', 'O', 'eat', 'O']]}
_attachments = {} # {'A1': [[['with', 'IN', 'B-PP', 'B-PNP', 'PP', 'O', 'with', 'O'],
                  #           ['a', 'DT', 'B-NP', 'I-PNP', 'NP', 'O', 'a', 'O'],
                  #           ['fork', 'NN', 'I-NP', 'I-PNP', 'NP', 'O', 'fork', 'O']]]}

# This is a fallback if for some reason we fail to import MBSP.TokenString,
# e.g., when tree.py is part of another project.


</t>
<t tx="karstenw.20230303131812.155">def __init__(self, element):
    self.element = element

</t>
<t tx="karstenw.20230303131812.156">@property
def tag(self):
    return self.element.tag

</t>
<t tx="karstenw.20230303131812.157">@property
def value(self):
    return self.element.text

</t>
<t tx="karstenw.20230303131812.158">def __iter__(self):
    return iter(XMLNode(e) for e in self.element)

</t>
<t tx="karstenw.20230303131812.159">def __getitem__(self, k):
    return self.element.attrib[k]

</t>
<t tx="karstenw.20230303131812.16">def copy(self, chunk=None, pnp=None):
    w = Word(
        self.sentence,
        self.string,
        self.lemma,
        self.type,
        self.index
    )
    w.chunk = chunk
    w.pnp = pnp
    if self._custom_tags:
        w._custom_tags = Tags(w, items=self._custom_tags)
    return w

</t>
<t tx="karstenw.20230303131812.160">def get(self, k, default=""):
    return self.element.attrib.get(k, default)

</t>
<t tx="karstenw.20230303131812.161">class TaggedString(str):
    @others
</t>
<t tx="karstenw.20230303131812.162">def __new__(cls, string, tags=["word"], language="en"):
    if isinstance(string, str) and hasattr(string, "tags"):
        tags, language = string.tags, getattr(string, "language", language)
    s = str.__new__(cls, string)
    s.tags = list(tags)
    s.language = language
    return s


</t>
<t tx="karstenw.20230303131812.163">def parse_string(xml):
    """ Returns a slash-formatted string from the given XML representation.
        The return value is a TokenString (for MBSP) or TaggedString (for Pattern).
    """
    string = ""
    # Traverse all the &lt;sentence&gt; elements in the XML.
    dom = XML(xml)
    for sentence in dom(XML_SENTENCE):
        _anchors.clear()     # Populated by calling _parse_tokens().
        _attachments.clear() # Populated by calling _parse_tokens().
        # Parse the language from &lt;sentence language=""&gt;.
        language = sentence.get(XML_LANGUAGE, "en")
        # Parse the token tag format from &lt;sentence token=""&gt;.
        # This information is returned in TokenString.tags,
        # so the format and order of the token tags is retained when exporting/importing as XML.
        format = sentence.get(XML_TOKEN, [WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA])
        format = not isinstance(format, str) and format or format.replace(" ", "").split(",")
        # Traverse all &lt;chunk&gt; and &lt;chink&gt; elements in the sentence.
        # Find the &lt;word&gt; elements inside and create tokens.
        tokens = []
        for chunk in sentence:
            tokens.extend(_parse_tokens(chunk, format))
        # Attach PNP's to their anchors.
        # Keys in _anchors have linked anchor chunks (each chunk is a list of tokens).
        # The keys correspond to the keys in _attachments, which have linked PNP chunks.
        if ANCHOR in format:
            A, P, a, i = _anchors, _attachments, 1, format.index(ANCHOR)
            for id in sorted(A.keys()):
                for token in A[id]:
                    token[i] += "-" + "-".join(["A" + str(a + p) for p in range(len(P[id]))])
                    token[i] = token[i].strip("O-")
                for p, pnp in enumerate(P[id]):
                    for token in pnp:
                        token[i] += "-" + "P" + str(a + p)
                        token[i] = token[i].strip("O-")
                a += len(P[id])
        # Collapse the tokens to string.
        # Separate multiple sentences with a new line.
        tokens = ["/".join([tag for tag in token]) for token in tokens]
        tokens = " ".join(tokens)
        string += tokens + "\n"
    # Return a TokenString, which is a unicode string that transforms easily
    # into a plain str, a list of tokens, or a Sentence.
    try:
        if MBSP:
            from mbsp import TokenString
        return TokenString(string.strip(), tags=format, language=language)
    except:
        return TaggedString(string.strip(), tags=format, language=language)


</t>
<t tx="karstenw.20230303131812.164">def _parse_tokens(chunk, format=[WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA]):
    """ Parses tokens from &lt;word&gt; elements in the given XML &lt;chunk&gt; element.
        Returns a flat list of tokens, in which each token is [WORD, POS, CHUNK, PNP, RELATION, ANCHOR, LEMMA].
        If a &lt;chunk type="PNP"&gt; is encountered, traverses all of the chunks in the PNP.
    """
    tokens = []
    # Only process &lt;chunk&gt; and &lt;chink&gt; elements,
    # text nodes in between return an empty list.
    if not (chunk.tag == XML_CHUNK or chunk.tag == XML_CHINK):
        return []
    type = chunk.get(XML_TYPE, "O")
    if type == "PNP":
        # For, &lt;chunk type="PNP"&gt;, recurse all the child chunks inside the PNP.
        for ch in chunk:
            tokens.extend(_parse_tokens(ch, format))
        # Tag each of them as part of the PNP.
        if PNP in format:
            i = format.index(PNP)
            for j, token in enumerate(tokens):
                token[i] = (j == 0 and "B-" or "I-") + "PNP"
        # Store attachments so we can construct anchor id's in parse_string().
        # This has to be done at the end, when all the chunks have been found.
        a = chunk.get(XML_OF).split(_UID_SEPARATOR)[-1]
        if a:
            _attachments.setdefault(a, [])
            _attachments[a].append(tokens)
        return tokens
    # For &lt;chunk type-"VP" id="1"&gt;, the relation is VP-1.
    # For &lt;chunk type="NP" relation="OBJ" of="1"&gt;, the relation is NP-OBJ-1.
    relation = _parse_relation(chunk, type)
    # Process all of the &lt;word&gt; elements in the chunk, for example:
    # &lt;word type="NN" lemma="pizza"&gt;pizza&lt;/word&gt; =&gt; [pizza, NN, I-NP, O, NP-OBJ-1, O, pizza]
    for word in filter(lambda n: n.tag == XML_WORD, chunk):
        tokens.append(_parse_token(word, chunk=type, relation=relation, format=format))
    # Add the IOB chunk tags:
    # words at the start of a chunk are marked with B-, words inside with I-.
    if CHUNK in format:
        i = format.index(CHUNK)
        for j, token in enumerate(tokens):
            token[i] = token[i] != "O" and ((j == 0 and "B-" or "I-") + token[i]) or "O"
    # The chunk can be the anchor of one or more PNP chunks.
    # Store anchors so we can construct anchor id's in parse_string().
    a = chunk.get(XML_ANCHOR, "").split(_UID_SEPARATOR)[-1]
    if a:
        _anchors[a] = tokens
    return tokens


</t>
<t tx="karstenw.20230303131812.165">def _parse_relation(chunk, type="O"):
    """ Returns a string of the roles and relations parsed from the given &lt;chunk&gt; element.
        The chunk type (which is part of the relation string) can be given as parameter.
    """
    r1 = chunk.get(XML_RELATION)
    r2 = chunk.get(XML_ID, chunk.get(XML_OF))
    r1 = [x != "-" and x or None for x in r1.split("|")] or [None]
    r2 = [x != "-" and x or None for x in r2.split("|")] or [None]
    r2 = [x is not None and x.split(_UID_SEPARATOR)[-1] or x for x in r2]
    if len(r1) &lt; len(r2):
        r1 = r1 + r1 * (len(r2) - len(r1)) # [1] ["SBJ", "OBJ"] =&gt; "SBJ-1;OBJ-1"
    if len(r2) &lt; len(r1):
        r2 = r2 + r2 * (len(r1) - len(r2)) # [2,4] ["OBJ"] =&gt; "OBJ-2;OBJ-4"
    return ";".join(["-".join([x for x in (type, r1, r2) if x]) for r1, r2 in zip(r1, r2)])


</t>
<t tx="karstenw.20230303131812.166">def _parse_token(word, chunk="O", pnp="O", relation="O", anchor="O",
                 format=[WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA]):
    """ Returns a list of token tags parsed from the given &lt;word&gt; element.
        Tags that are not attributes in a &lt;word&gt; (e.g., relation) can be given as parameters.
    """
    tags = []
    for tag in format:
        if tag == WORD:
            tags.append(xml_decode(word.value))
        elif tag == POS:
            tags.append(xml_decode(word.get(XML_TYPE, "O")))
        elif tag == CHUNK:
            tags.append(chunk)
        elif tag == PNP:
            tags.append(pnp)
        elif tag == REL:
            tags.append(relation)
        elif tag == ANCHOR:
            tags.append(anchor)
        elif tag == LEMMA:
            tags.append(xml_decode(word.get(XML_LEMMA, "")))
        else:
            # Custom tags when the parser has been extended, see also Word.custom_tags{}.
            tags.append(xml_decode(word.get(tag, "O")))
    return tags

### NLTK TREE ######################################################################################


</t>
<t tx="karstenw.20230303131812.167">def nltk_tree(sentence):
    """ Returns an NLTK nltk.tree.Tree object from the given Sentence.
        The NLTK module should be on the search path somewhere.
    """
    from nltk import tree

    def do_pnp(pnp):
        # Returns the PNPChunk (and the contained Chunk objects) in NLTK bracket format.
        s = ' '.join([do_chunk(ch) for ch in pnp.chunks])
        return '(PNP %s)' % s

    def do_chunk(ch):
        # Returns the Chunk in NLTK bracket format. Recurse attached PNP's.
        s = ' '.join(['(%s %s)' % (w.pos, w.string) for w in ch.words])
        s += ' '.join([do_pnp(pnp) for pnp in ch.attachments])
        return '(%s %s)' % (ch.type, s)

    T = ['(S']
    v = [] # PNP's already visited.
    for ch in sentence.chunked():
        if not ch.pnp and isinstance(ch, Chink):
            T.append('(%s %s)' % (ch.words[0].pos, ch.words[0].string))
        elif not ch.pnp:
            T.append(do_chunk(ch))
        #elif ch.pnp not in v:
        elif ch.pnp.anchor is None and ch.pnp not in v:
            # The chunk is part of a PNP without an anchor.
            T.append(do_pnp(ch.pnp))
            v.append(ch.pnp)
    T.append(')')
    return tree.bracket_parse(' '.join(T))

### GRAPHVIZ DOT ###################################################################################

BLUE = {
       '' : ("#f0f5ff", "#000000"),
     'VP' : ("#e6f0ff", "#000000"),
    'SBJ' : ("#64788c", "#ffffff"),
    'OBJ' : ("#64788c", "#ffffff"),
}


</t>
<t tx="karstenw.20230303131812.168">def _colorize(x, colors):
    s = ''
    if isinstance(x, Word):
        x = x.chunk
    if isinstance(x, Chunk):
        s = ',style=filled, fillcolor="%s", fontcolor="%s"' % ( \
            colors.get(x.role) or \
            colors.get(x.type) or \
            colors.get('') or ("none", "black"))
    return s


</t>
<t tx="karstenw.20230303131812.169">def graphviz_dot(sentence, font="Arial", colors=BLUE):
    """ Returns a dot-formatted string that can be visualized as a graph in GraphViz.
    """
    s = 'digraph sentence {\n'
    s += '\tranksep=0.75;\n'
    s += '\tnodesep=0.15;\n'
    s += '\tnode [penwidth=1, fontname="%s", shape=record, margin=0.1, height=0.35];\n' % font
    s += '\tedge [penwidth=1];\n'
    s += '\t{ rank=same;\n'
    # Create node groups for words, chunks and PNP chunks.
    for w in sentence.words:
        s += '\t\tword%s [label="&lt;f0&gt;%s|&lt;f1&gt;%s"%s];\n' % (w.index, w.string, w.type, _colorize(w, colors))
    for w in sentence.words[:-1]:
        # Invisible edges forces the words into the right order:
        s += '\t\tword%s -&gt; word%s [color=none];\n' % (w.index, w.index + 1)
    s += '\t}\n'
    s += '\t{ rank=same;\n'
    for i, ch in enumerate(sentence.chunks):
        s += '\t\tchunk%s [label="&lt;f0&gt;%s"%s];\n' % (i + 1, "-".join([x for x in (
            ch.type, ch.role, str(ch.relation or '')) if x]) or '-', _colorize(ch, colors))
    for i, ch in enumerate(sentence.chunks[:-1]):
        # Invisible edges forces the chunks into the right order:
        s += '\t\tchunk%s -&gt; chunk%s [color=none];\n' % (i + 1, i + 2)
    s += '}\n'
    s += '\t{ rank=same;\n'
    for i, ch in enumerate(sentence.pnp):
        s += '\t\tpnp%s [label="&lt;f0&gt;PNP"%s];\n' % (i + 1, _colorize(ch, colors))
    s += '\t}\n'
    s += '\t{ rank=same;\n S [shape=circle, margin=0.25, penwidth=2]; }\n'
    # Connect words to chunks.
    # Connect chunks to PNP or S.
    for i, ch in enumerate(sentence.chunks):
        for w in ch:
            s += '\tword%s -&gt; chunk%s;\n' % (w.index, i + 1)
        if ch.pnp:
            s += '\tchunk%s -&gt; pnp%s;\n' % (i + 1, sentence.pnp.index(ch.pnp) + 1)
        else:
            s += '\tchunk%s -&gt; S;\n' % (i + 1)
        if ch.type == 'VP':
            # Indicate related chunks with a dotted
            for r in ch.related:
                s += '\tchunk%s -&gt; chunk%s [style=dotted, arrowhead=none];\n' % (
                    i + 1, sentence.chunks.index(r) + 1)
    # Connect PNP to anchor chunk or S.
    for i, ch in enumerate(sentence.pnp):
        if ch.anchor:
            s += '\tpnp%s -&gt; chunk%s;\n' % (i + 1, sentence.chunks.index(ch.anchor) + 1)
            s += '\tpnp%s -&gt; S [color=none];\n' % (i + 1)
        else:
            s += '\tpnp%s -&gt; S;\n' % (i + 1)
    s += "}"
    return s

### STDOUT TABLE ###################################################################################


</t>
<t tx="karstenw.20230303131812.17">def _get_tag(self):
    return self.type

</t>
<t tx="karstenw.20230303131812.170">def table(sentence, fill=1, placeholder="-"):
    """ Returns a string where the tags of tokens in the sentence are organized in outlined columns.
    """
    tags = [WORD, POS, IOB, CHUNK, ROLE, REL, PNP, ANCHOR, LEMMA]
    tags += [tag for tag in sentence.token if tag not in tags]

    def format(token, tag):
        # Returns the token tag as a string.
        if tag == WORD:
            s = token.string
        elif tag == POS:
            s = token.type
        elif tag == IOB:
            s = token.chunk and (token.index == token.chunk.start and "B" or "I")
        elif tag == CHUNK:
            s = token.chunk and token.chunk.type
        elif tag == ROLE:
            s = token.chunk and token.chunk.role
        elif tag == REL:
            s = token.chunk and token.chunk.relation and str(token.chunk.relation)
        elif tag == PNP:
            s = token.chunk and token.chunk.pnp and token.chunk.pnp.type
        elif tag == ANCHOR:
            s = token.chunk and token.chunk.anchor_id
        elif tag == LEMMA:
            s = token.lemma
        else:
            s = token.custom_tags.get(tag)
        return s or placeholder

    def outline(column, fill=1, padding=3, align="left"):
        # Add spaces to each string in the column so they line out to the highest width.
        n = max([len(x) for x in column] + [fill])
        if align == "left":
            return [x + " " * (n - len(x)) + " " * padding for x in column]
        if align == "right":
            return [" " * (n - len(x)) + x + " " * padding for x in column]

    # Gather the tags of the tokens in the sentece per column.
    # If the IOB-tag is I-, mark the chunk tag with "^".
    # Add the tag names as headers in each column.
    columns = [[format(token, tag) for token in sentence] for tag in tags]
    columns[3] = [columns[3][i] + (iob == "I" and " ^" or "") for i, iob in enumerate(columns[2])]
    del columns[2]
    for i, header in enumerate(['word', 'tag', 'chunk', 'role', 'id', 'pnp', 'anchor', 'lemma'] + tags[9:]):
        columns[i].insert(0, "")
        columns[i].insert(0, header.upper())
    # The left column (the word itself) is outlined to the right,
    # and has extra spacing so that words across sentences line out nicely below each other.
    for i, column in enumerate(columns):
        columns[i] = outline(column, fill + 10 * (i == 0), align=("left", "right")[i == 0])
    # Anchor column is useful in MBSP but not in pattern.en.
    if not MBSP:
        del columns[6]
    # Create a string with one row (i.e., one token) per line.
    return "\n".join(["".join([x[i] for x in columns]) for i in range(len(columns[0]))])
</t>
<t tx="karstenw.20230303131812.18">def _set_tag(self, v):
    self.type = v

tag = pos = part_of_speech = property(_get_tag, _set_tag)

</t>
<t tx="karstenw.20230303131812.19">@property
def phrase(self):
    return self.chunk

</t>
<t tx="karstenw.20230303131812.2">def find(function, iterable):
    """ Returns the first item in the list for which function(item) is True, None otherwise.
    """
    for x in iterable:
        if function(x):
            return x


</t>
<t tx="karstenw.20230303131812.20">@property
def prepositional_phrase(self):
    return self.pnp

prepositional_noun_phrase = prepositional_phrase

</t>
<t tx="karstenw.20230303131812.21">@property
def tags(self):
    """ Yields a list of all the token tags as they appeared when the word was parsed.
        For example: ["was", "VBD", "B-VP", "O", "VP-1", "A1", "be"]
    """
    # See also. Sentence.__repr__().
    ch, I, O, B = self.chunk, INSIDE + "-", OUTSIDE, BEGIN + "-"
    tags = [OUTSIDE for i in range(len(self.sentence.token))]
    for i, tag in enumerate(self.sentence.token): # Default: [WORD, POS, CHUNK, PNP, RELATION, ANCHOR, LEMMA]
        if tag == WORD:
            tags[i] = encode_entities(self.string)
        elif tag == POS or tag == "pos" and self.type:
            tags[i] = self.type
        elif tag == CHUNK and ch and ch.type:
            tags[i] = (self == ch[0] and B or I) + ch.type
        elif tag == PNP and self.pnp:
            tags[i] = (self == self.pnp[0] and B or I) + "PNP"
        elif tag == REL and ch and len(ch.relations) &gt; 0:
            tags[i] = ["-".join([str(x) for x in [ch.type] + list(reversed(r)) if x]) for r in ch.relations]
            tags[i] = "*".join(tags[i])
        elif tag == ANCHOR and ch:
            tags[i] = ch.anchor_id or OUTSIDE
        elif tag == LEMMA:
            tags[i] = encode_entities(self.lemma or "")
        elif tag in self.custom_tags:
            tags[i] = self.custom_tags.get(tag) or OUTSIDE
    return tags

</t>
<t tx="karstenw.20230303131812.22">@property
def custom_tags(self):
    if not self._custom_tags:
        self._custom_tags = Tags(self)
    return self._custom_tags

</t>
<t tx="karstenw.20230303131812.23">def next(self, type=None):
    """ Returns the next word in the sentence with the given type.
    """
    i = self.index + 1
    s = self.sentence
    while i &lt; len(s):
        if type in (s[i].type, None):
            return s[i]
        i += 1

</t>
<t tx="karstenw.20230303131812.24">def previous(self, type=None):
    """ Returns the next previous word in the sentence with the given type.
    """
    i = self.index - 1
    s = self.sentence
    while i &gt; 0:
        if type in (s[i].type, None):
            return s[i]
        i -= 1

# User-defined tags are available as Word.[tag] attributes.
</t>
<t tx="karstenw.20230303131812.25">def __getattr__(self, tag):
    d = self.__dict__.get("_custom_tags", None)
    if d and tag in d:
        return d[tag]
    raise AttributeError("Word instance has no attribute '%s'" % tag)

# Word.string and unicode(Word) are Unicode strings.
# repr(Word) is a Python string (with Unicode characters encoded).
</t>
<t tx="karstenw.20230303131812.26">def __str__(self):
    return self.string

</t>
<t tx="karstenw.20230303131812.27">def __repr__(self):
    return "Word(%s)" % repr("%s/%s" % (
        encode_entities(self.string),
        self.type is not None and self.type or OUTSIDE))

</t>
<t tx="karstenw.20230303131812.28">def __eq__(self, word):
    return id(self) == id(word)

</t>
<t tx="karstenw.20230303131812.29">def __ne__(self, word):
    return id(self) != id(word)

# This is required because we overwrite the parent's __eq__() method.
# Otherwise objects will be unhashable in Python 3.
# More information: http://docs.python.org/3.6/reference/datamodel.html#object.__hash__
__hash__ = object.__hash__


</t>
<t tx="karstenw.20230303131812.3">def intersects(iterable1, iterable2):
    """ Returns True if the given lists have at least one item in common.
    """
    return find(lambda x: x in iterable1, iterable2) is not None


</t>
<t tx="karstenw.20230303131812.30">class Tags(dict):

    @others
#--- CHUNK -----------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303131812.31">def __init__(self, word, items=[]):
    """ A dictionary of custom word tags.
        A word may be annotated with its part-of-speech tag (e.g., "cat/NN"), 
        phrase tag (e.g., "cat/NN/NP"), the prepositional noun phrase it is part of etc.
        An example of an extra custom slot is its semantic type, 
        e.g., gene type, topic, and so on: "cat/NN/NP/genus_felis"
    """
    if items:
        dict.__init__(self, items)
    self.word = word

</t>
<t tx="karstenw.20230303131812.32">def __setitem__(self, k, v):
    # Ensure that the custom tag is also in Word.sentence.token,
    # so that it is not forgotten when exporting or importing XML.
    dict.__setitem__(self, k, v)
    if k not in reversed(self.word.sentence.token):
        self.word.sentence.token.append(k)

</t>
<t tx="karstenw.20230303131812.33">def setdefault(self, k, v):
    if k not in self:
        self.__setitem__(k, v)
        return self[k]

</t>
<t tx="karstenw.20230303131812.34">class Chunk(object):

    @others
# Chinks are non-chunks,
# see also the chunked() function:


</t>
<t tx="karstenw.20230303131812.35">def __init__(self, sentence, words=[], type=None, role=None, relation=None):
    """ A list of words that make up a phrase in the sentence.
        - type: the phrase tag; "NP" =&gt; a noun phrase (e.g., "the black cat").
        - role: the function of the phrase; "SBJ" =&gt; sentence subject.
        - relation: an id shared with other phrases, linking subject to object in the sentence.
    """
    # A chunk can have multiple roles or relations in the sentence,
    # so role and relation can also be given as lists.
    b1 = isinstance(relation, (list, tuple))
    b2 = isinstance(role, (list, tuple))
    if not b1 and not b2:
        r = [(relation, role)]
    elif b1 and b2:
        r = list(zip(relation, role))
    elif b1:
        r = list(zip(relation, [role] * len(relation)))
    elif b2:
        r = list(zip([relation] * len(role), role))
    r = [(a, b) for a, b in r if a is not None or b is not None]
    self.sentence      = sentence
    self.words         = []
    self.type          = type  # NP, VP, ADJP ...
    self.relations     = r     # NP-SBJ-1 =&gt; [(1, SBJ)]
    self.pnp           = None  # PNP chunk object this chunk belongs to.
    self.anchor        = None  # PNP chunk's anchor.
    self.attachments   = []    # PNP chunks attached to this anchor.
    self._conjunctions = None  # Conjunctions object, created on request.
    self._modifiers    = None
    self.extend(words)

</t>
<t tx="karstenw.20230303131812.36">def extend(self, words):
    for w in words:
        self.append(w)

</t>
<t tx="karstenw.20230303131812.37">def append(self, word):
    self.words.append(word)
    word.chunk = self

</t>
<t tx="karstenw.20230303131812.38">def __getitem__(self, index):
    return self.words[index]

</t>
<t tx="karstenw.20230303131812.39">def __len__(self):
    return len(self.words)

</t>
<t tx="karstenw.20230303131812.4">def unique(iterable):
    """ Returns a list copy in which each item occurs only once (in-order).
    """
    seen = set()
    return [x for x in iterable if x not in seen and not seen.add(x)]

_zip = zip


</t>
<t tx="karstenw.20230303131812.40">def __iter__(self):
    return self.words.__iter__()

</t>
<t tx="karstenw.20230303131812.41">def _get_tag(self):
    return self.type

</t>
<t tx="karstenw.20230303131812.42">def _set_tag(self, v):
    self.type = v

tag = pos = part_of_speech = property(_get_tag, _set_tag)

</t>
<t tx="karstenw.20230303131812.43">@property
def start(self):
    return self.words[0].index

</t>
<t tx="karstenw.20230303131812.44">@property
def stop(self):
    return self.words[-1].index + 1

</t>
<t tx="karstenw.20230303131812.45">@property
def range(self):
    return range(self.start, self.stop)

</t>
<t tx="karstenw.20230303131812.46">@property
def span(self):
    return (self.start, self.stop)

</t>
<t tx="karstenw.20230303131812.47">@property
def lemmata(self):
    return [word.lemma for word in self.words]

</t>
<t tx="karstenw.20230303131812.48">@property
def tagged(self):
    return [(word.string, word.type) for word in self.words]

</t>
<t tx="karstenw.20230303131812.49">@property
def head(self):
    """ Yields the head of the chunk (usually, the last word in the chunk).
    """
    if self.type == "NP" and any(w.type.startswith("NNP") for w in self):
        w = find(lambda w: w.type.startswith("NNP"), reversed(self))
    elif self.type == "NP":  # "the cat" =&gt; "cat"
        w = find(lambda w: w.type.startswith("NN"), reversed(self))
    elif self.type == "VP":  # "is watching" =&gt; "watching"
        w = find(lambda w: w.type.startswith("VB"), reversed(self))
    elif self.type == "PP":  # "from up on" =&gt; "from"
        w = find(lambda w: w.type.startswith(("IN", "PP")), self)
    elif self.type == "PNP": # "from up on the roof" =&gt; "roof"
        w = find(lambda w: w.type.startswith("NN"), reversed(self))
    else:
        w = None
    if w is None:
        w = self[-1]
    return w

</t>
<t tx="karstenw.20230303131812.5">def zip(*args, **kwargs):
    """ Returns a list of tuples, where the i-th tuple contains the i-th element 
        from each of the argument sequences or iterables (or default if too short).
    """
    args = [list(iterable) for iterable in args]
    n = max(map(len, args))
    v = kwargs.get("default", None)
    return list(_zip(*[i + [v] * (n - len(i)) for i in args]))


</t>
<t tx="karstenw.20230303131812.50">@property
def relation(self):
    """ Yields the first relation id of the chunk.
    """
    # [(2,OBJ), (3,OBJ)])] =&gt; 2
    return len(self.relations) &gt; 0 and self.relations[0][0] or None

</t>
<t tx="karstenw.20230303131812.51">@property
def role(self):
    """ Yields the first role of the chunk (SBJ, OBJ, ...).
    """
    # [(1,SBJ), (1,OBJ)])] =&gt; SBJ
    return len(self.relations) &gt; 0 and self.relations[0][1] or None

</t>
<t tx="karstenw.20230303131812.52">@property
def subject(self):
    ch = self.sentence.relations["SBJ"].get(self.relation, None)
    if ch != self:
        return ch

</t>
<t tx="karstenw.20230303131812.53">@property
def object(self):
    ch = self.sentence.relations["OBJ"].get(self.relation, None)
    if ch != self:
        return ch

</t>
<t tx="karstenw.20230303131812.54">@property
def verb(self):
    ch = self.sentence.relations["VP"].get(self.relation, None)
    if ch != self:
        return ch

</t>
<t tx="karstenw.20230303131812.55">@property
def related(self):
    """ Yields a list of all chunks in the sentence with the same relation id.
    """
    return [ch for ch in self.sentence.chunks
                if ch != self and intersects(unzip(0, ch.relations), unzip(0, self.relations))]

</t>
<t tx="karstenw.20230303131812.56">@property
def prepositional_phrase(self):
    return self.pnp

prepositional_noun_phrase = prepositional_phrase

</t>
<t tx="karstenw.20230303131812.57">@property
def anchor_id(self):
    """ Yields the anchor tag as parsed from the original token.
        Chunks that are anchors have a tag with an "A" prefix (e.g., "A1").
        Chunks that are PNP attachmens (or chunks inside a PNP) have "P" (e.g., "P1").
        Chunks inside a PNP can be both anchor and attachment (e.g., "P1-A2"),
        as in: "clawed/A1 at/P1 mice/P1-A2 in/P2 the/P2 wall/P2"
    """
    id = ""
    f = lambda ch: list(filter(lambda k: self.sentence._anchors[k] == ch, self.sentence._anchors))
    if self.pnp and self.pnp.anchor:
        id += "-" + "-".join(f(self.pnp))
    if self.anchor:
        id += "-" + "-".join(f(self))
    if self.attachments:
        id += "-" + "-".join(f(self))
    return id.strip("-") or None

</t>
<t tx="karstenw.20230303131812.58">@property
def conjunctions(self):
    if not self._conjunctions:
        self._conjunctions = Conjunctions(self)
    return self._conjunctions

</t>
<t tx="karstenw.20230303131812.59">@property
def modifiers(self):
    """ For verb phrases (VP), yields a list of the nearest adjectives and adverbs.
    """
    if self._modifiers is None:
        # Iterate over all the chunks and attach modifiers to their VP-anchor.
        is_modifier = lambda ch: ch.type in ("ADJP", "ADVP") and ch.relation is None
        for chunk in self.sentence.chunks:
            chunk._modifiers = []
        for chunk in filter(is_modifier, self.sentence.chunks):
            anchor = chunk.nearest("VP")
            if anchor:
                anchor._modifiers.append(chunk)
    return self._modifiers

</t>
<t tx="karstenw.20230303131812.6">def unzip(i, iterable):
    """ Returns the item at the given index from inside each tuple in the list.
    """
    return [x[i] for x in iterable]


</t>
<t tx="karstenw.20230303131812.60">def nearest(self, type="VP"):
    """ Returns the nearest chunk in the sentence with the given type.
        This can be used (for example) to find adverbs and adjectives related to verbs,
        as in: "the cat is ravenous" =&gt; is what? =&gt; "ravenous".
    """
    candidate, d = None, len(self.sentence.chunks)
    if isinstance(self, PNPChunk):
        i = self.sentence.chunks.index(self.chunks[0])
    else:
        i = self.sentence.chunks.index(self)
    for j, chunk in enumerate(self.sentence.chunks):
        if chunk.type.startswith(type) and abs(i - j) &lt; d:
            candidate, d = chunk, abs(i - j)
    return candidate

</t>
<t tx="karstenw.20230303131812.61">def next(self, type=None):
    """ Returns the next chunk in the sentence with the given type.
    """
    i = self.stop
    s = self.sentence
    while i &lt; len(s):
        if s[i].chunk is not None and type in (s[i].chunk.type, None):
            return s[i].chunk
        i += 1

</t>
<t tx="karstenw.20230303131812.62">def previous(self, type=None):
    """ Returns the next previous chunk in the sentence with the given type.
    """
    i = self.start - 1
    s = self.sentence
    while i &gt; 0:
        if s[i].chunk is not None and type in (s[i].chunk.type, None):
            return s[i].chunk
        i -= 1

# Chunk.string and unicode(Chunk) are Unicode strings.
# repr(Chunk) is a Python string (with Unicode characters encoded).
</t>
<t tx="karstenw.20230303131812.63">@property
def string(self):
    return " ".join(word.string for word in self.words)

</t>
<t tx="karstenw.20230303131812.64">def __str__(self):
    return self.string

</t>
<t tx="karstenw.20230303131812.65">def __repr__(self):
    return "Chunk(%s)" % repr("%s/%s%s%s") % (
            self.string,
            self.type is not None and self.type or OUTSIDE,
            self.role is not None and ("-" + self.role) or "",
        self.relation is not None and ("-" + str(self.relation)) or "")

</t>
<t tx="karstenw.20230303131812.66">def __eq__(self, chunk):
    return id(self) == id(chunk)

</t>
<t tx="karstenw.20230303131812.67">def __ne__(self, chunk):
    return id(self) != id(chunk)

# This is required because we overwrite the parent's __eq__() method.
# Otherwise objects will be unhashable in Python 3.
# More information: http://docs.python.org/3.6/reference/datamodel.html#object.__hash__
__hash__ = object.__hash__

</t>
<t tx="karstenw.20230303131812.68">class Chink(Chunk):
    @others
#--- PNP CHUNK -------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303131812.69">def __repr__(self):
    return Chunk.__repr__(self).replace("Chunk(", "Chink(", 1)

</t>
<t tx="karstenw.20230303131812.7">class Map(list):
    """ A stored imap() on a list.
        The list is referenced instead of copied, and the items are mapped on-the-fly.
    """

    @others
### SENTENCE #######################################################################################

# The output of parse() is a slash-formatted string (e.g., "the/DT cat/NN"),
# so slashes in words themselves are encoded as &amp;slash;

encode_entities = lambda string: string.replace("/", SLASH)
decode_entities = lambda string: string.replace(SLASH, "/")

#--- WORD ------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303131812.70">class PNPChunk(Chunk):

    @others
#--- CONJUNCTION -----------------------------------------------------------------------------------

CONJUNCT = AND = "AND"
DISJUNCT = OR = "OR"


</t>
<t tx="karstenw.20230303131812.71">def __init__(self, *args, **kwargs):
    """ A chunk of chunks that make up a prepositional noun phrase (i.e., PP + NP).
        When the output of the parser includes PP-attachment,
        PNPChunck.anchor will yield the chunk that is clarified by the preposition.
        For example: "the cat went [for the mouse] [with its claws]":
        - [went] what? =&gt; for the mouse,
        - [went] how? =&gt; with its claws.
    """
    self.anchor = None # The anchor chunk (e.g., "for the mouse" =&gt; "went").
    self.chunks = []   # List of chunks in the prepositional noun phrase.
    Chunk.__init__(self, *args, **kwargs)

</t>
<t tx="karstenw.20230303131812.72">def append(self, word):
    self.words.append(word)
    word.pnp = self
    if word.chunk is not None:
        word.chunk.pnp = self
        if word.chunk not in self.chunks:
            self.chunks.append(word.chunk)

</t>
<t tx="karstenw.20230303131812.73">@property
def preposition(self):
    """ Yields the first chunk in the prepositional noun phrase, usually a PP-chunk.
        PP-chunks contain words such as "for", "with", "in", ...
    """
    return self.chunks[0]

pp = preposition

</t>
<t tx="karstenw.20230303131812.74">@property
def phrases(self):
    return self.chunks

</t>
<t tx="karstenw.20230303131812.75">def guess_anchor(self):
    """ Returns an anchor chunk for this prepositional noun phrase (without a PP-attacher).
        Often, the nearest verb phrase is a good candidate.
    """
    return self.nearest("VP")

</t>
<t tx="karstenw.20230303131812.76">class Conjunctions(list):

    @others
#--- SENTENCE --------------------------------------------------------------------------------------

_UID = 0


</t>
<t tx="karstenw.20230303131812.77">def __init__(self, chunk):
    """ Chunk.conjunctions is a list of other chunks participating in a conjunction.
        Each item in the list is a (chunk, conjunction)-tuple, with conjunction either AND or OR.
    """
    self.anchor = chunk

</t>
<t tx="karstenw.20230303131812.78">def append(self, chunk, type=CONJUNCT):
    list.append(self, (chunk, type))

</t>
<t tx="karstenw.20230303131812.79">def _uid():
    global _UID
    _UID += 1
    return _UID


</t>
<t tx="karstenw.20230303131812.8">def __init__(self, function=lambda x: x, items=[]):
    self._f = function
    self._a = items

</t>
<t tx="karstenw.20230303131812.80">def _is_tokenstring(string):
    # The class mbsp.TokenString stores the format of tags for each token.
    # Since it comes directly from MBSP.parse(), this format is always correct,
    # regardless of the given token format parameter for Sentence() or Text().
    return isinstance(string, str) and hasattr(string, "tags")


</t>
<t tx="karstenw.20230303131812.81">class Sentence(object):

    @others
</t>
<t tx="karstenw.20230303131812.82">def __init__(self, string="", token=[WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA], language="en"):
    """ A nested tree of sentence words, chunks and prepositions.
        The input is a tagged string from parse(). 
        The order in which token tags appear can be specified.
    """
    # Extract token format from TokenString or TaggedString if possible.
    if _is_tokenstring(string):
        token, language = string.tags, getattr(string, "language", language)
    # Convert to Unicode.
    if not isinstance(string, str):
        for encoding in (("utf-8",), ("windows-1252",), ("utf-8", "ignore")):
            try:
                string = string.decode(*encoding)
            except:
                pass
    self.parent      = None # A Slice refers to the Sentence it is part of.
    self.text        = None # A Sentence refers to the Text it is part of.
    self.language    = language
    self.id          = _uid()
    self.token       = list(token)
    self.words       = []
    self.chunks      = [] # Words grouped into chunks.
    self.pnp         = [] # Words grouped into PNP chunks.
    self._anchors    = {} # Anchor tags related to anchor chunks or attached PNP's.
    self._relation   = None # Helper variable: the last chunk's relation and role.
    self._attachment = None # Helper variable: the last attachment tag (e.g., "P1") parsed in _do_pnp().
    self._previous   = None # Helper variable: the last token parsed in parse_token().
    self.relations   = {"SBJ": {}, "OBJ": {}, "VP": {}}
    # Split the slash-formatted token into the separate tags in the given order.
    # Append Word and Chunk objects according to the token's tags.
    for chars in string.split(" "):
        if chars:
            self.append(*self.parse_token(chars, token))

</t>
<t tx="karstenw.20230303131812.83">@property
def word(self):
    return self.words

</t>
<t tx="karstenw.20230303131812.84">@property
def lemmata(self):
    return Map(lambda w: w.lemma, self.words)
    #return [word.lemma for word in self.words]

lemma = lemmata

</t>
<t tx="karstenw.20230303131812.85">@property
def parts_of_speech(self):
    return Map(lambda w: w.type, self.words)
    #return [word.type for word in self.words]

pos = parts_of_speech

</t>
<t tx="karstenw.20230303131812.86">@property
def tagged(self):
    return [(word.string, word.type) for word in self]

</t>
<t tx="karstenw.20230303131812.87">@property
def phrases(self):
    return self.chunks

chunk = phrases

</t>
<t tx="karstenw.20230303131812.88">@property
def prepositional_phrases(self):
    return self.pnp

prepositional_noun_phrases = prepositional_phrases

</t>
<t tx="karstenw.20230303131812.89">@property
def start(self):
    return 0

</t>
<t tx="karstenw.20230303131812.9">@property
def items(self):
    return self._a

</t>
<t tx="karstenw.20230303131812.90">@property
def stop(self):
    return self.start + len(self.words)

</t>
<t tx="karstenw.20230303131812.91">@property
def nouns(self):
    return [word for word in self if word.type.startswith("NN")]

</t>
<t tx="karstenw.20230303131812.92">@property
def verbs(self):
    return [word for word in self if word.type.startswith("VB")]

</t>
<t tx="karstenw.20230303131812.93">@property
def adjectives(self):
    return [word for word in self if word.type.startswith("JJ")]

</t>
<t tx="karstenw.20230303131812.94">@property
def subjects(self):
    return list(self.relations["SBJ"].values())

</t>
<t tx="karstenw.20230303131812.95">@property
def objects(self):
    return list(self.relations["OBJ"].values())

</t>
<t tx="karstenw.20230303131812.96">@property
def verbs(self):
    return list(self.relations["VP"].values())

</t>
<t tx="karstenw.20230303131812.97">@property
def anchors(self):
    return [chunk for chunk in self.chunks if len(chunk.attachments) &gt; 0]

</t>
<t tx="karstenw.20230303131812.98">@property
def is_question(self):
    return len(self) &gt; 0 and str(self[-1]) == "?"

</t>
<t tx="karstenw.20230303131812.99">@property
def is_exclamation(self):
    return len(self) &gt; 0 and str(self[-1]) == "!"

</t>
<t tx="karstenw.20230303131819.1">from __future__ import unicode_literals
from __future__ import absolute_import
from __future__ import division

from io import open

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import re
import itertools

from functools import cmp_to_key

#--- TEXT, SENTENCE AND WORD -----------------------------------------------------------------------
# The search() and match() functions work on Text, Sentence and Word objects (see pattern.text.tree),
# i.e., the parse tree including part-of-speech tags and phrase chunk tags.

# The pattern.text.search Match object will contain matched Word objects,
# emulated with the following classes if the original input was a plain string:

PUNCTUATION = ".,;:!?()[]{}`'\"@#$^&amp;*+-|=~_"

RE_PUNCTUATION = "|".join(map(re.escape, PUNCTUATION))
RE_PUNCTUATION = re.compile("(%s)" % RE_PUNCTUATION)


</t>
<t tx="karstenw.20230303131819.10">@property
def chunks(self):
    return []


</t>
<t tx="karstenw.20230303131819.100">@property
def start(self):
    return self and self[0].index or None

</t>
<t tx="karstenw.20230303131819.101">@property
def stop(self):
    return self and self[-1].index + 1 or None

</t>
<t tx="karstenw.20230303131819.102">@property
def string(self):
    return " ".join(w.string for w in self)
</t>
<t tx="karstenw.20230303131819.11">class Word(object):

    @others
#--- STRING MATCHING -------------------------------------------------------------------------------

WILDCARD = "*"
regexp = type(re.compile(r"."))


</t>
<t tx="karstenw.20230303131819.12">def __init__(self, sentence, string, tag=None, index=0):
    """ A word with a position in a sentence.
    """
    self.sentence, self.string, self.tag, self.index = sentence, string, tag, index

</t>
<t tx="karstenw.20230303131819.13">def __repr__(self):
    return "Word(%s)" % repr(self.string)

</t>
<t tx="karstenw.20230303131819.14">def _get_type(self):
    return self.tag

</t>
<t tx="karstenw.20230303131819.15">def _set_type(self, v):
    self.tag = v

type = property(_get_type, _set_type)

</t>
<t tx="karstenw.20230303131819.16">@property
def chunk(self):
    return None

</t>
<t tx="karstenw.20230303131819.17">@property
def lemma(self):
    return None

</t>
<t tx="karstenw.20230303131819.18">def _match(string, pattern):
    """ Returns True if the pattern matches the given word string.
        The pattern can include a wildcard (*front, back*, *both*, in*side),
        or it can be a compiled regular expression.
    """
    p = pattern
    try:
        if p[:1] == WILDCARD and (p[-1:] == WILDCARD and p[1:-1] in string or string.endswith(p[1:])):
            return True
        if p[-1:] == WILDCARD and not p[-2:-1] == "\\" and string.startswith(p[:-1]):
            return True
        if p == string:
            return True
        if WILDCARD in p[1:-1]:
            p = p.split(WILDCARD)
            return string.startswith(p[0]) and string.endswith(p[-1])
    except:
        # For performance, calling isinstance() last is 10% faster for plain strings.
        if isinstance(p, regexp):
            return p.search(string) is not None
    return False

#--- LIST FUNCTIONS --------------------------------------------------------------------------------
# Search patterns can contain optional constraints,
# so we need to find all possible variations of a pattern.


</t>
<t tx="karstenw.20230303131819.19">def unique(iterable):
    """ Returns a list copy in which each item occurs only once (in-order).
    """
    seen = set()
    return [x for x in iterable if x not in seen and not seen.add(x)]


</t>
<t tx="karstenw.20230303131819.2">class Text(list):

    @others
</t>
<t tx="karstenw.20230303131819.20">def find(function, iterable):
    """ Returns the first item in the list for which function(item) is True, None otherwise.
    """
    for x in iterable:
        if function(x) is True:
            return x


</t>
<t tx="karstenw.20230303131819.21">def combinations(iterable, n):
    # Backwards compatibility.
    return product(iterable, repeat=n)


</t>
<t tx="karstenw.20230303131819.22">def product(*args, **kwargs):
    """ Yields all permutations with replacement:
        list(product("cat", repeat=2)) =&gt; 
        [("c", "c"), 
         ("c", "a"), 
         ("c", "t"), 
         ("a", "c"), 
         ("a", "a"), 
         ("a", "t"), 
         ("t", "c"), 
         ("t", "a"), 
         ("t", "t")]
    """
    p = [[]]
    for iterable in map(tuple, args) * kwargs.get("repeat", 1):
        p = [x + [y] for x in p for y in iterable]
    for p in p:
        yield tuple(p)

try:
    from itertools import product
except:
    pass


</t>
<t tx="karstenw.20230303131819.23">def variations(iterable, optional=lambda x: False):
    """ Returns all possible variations of a sequence with optional items.
    """
    # For example: variations(["A?", "B?", "C"], optional=lambda s: s.endswith("?"))
    # defines a sequence where constraint A and B are optional:
    # [("A?", "B?", "C"), ("B?", "C"), ("A?", "C"), ("C")]
    iterable = tuple(iterable)
    # Create a boolean sequence where True means optional:
    # ("A?", "B?", "C") =&gt; [True, True, False]
    o = [optional(x) for x in iterable]
    # Find all permutations of the boolean sequence:
    # [True, False, True], [True, False, False], [False, False, True], [False, False, False].
    # Map to sequences of constraints whose index in the boolean sequence yields True.
    a = set()
    for p in product([False, True], repeat=sum(o)):
        p = list(p)
        v = [b and (b and p.pop(0)) for b in o]
        v = tuple(iterable[i] for i in range(len(v)) if not v[i])
        a.add(v)
    # Longest-first.
    f = lambda x, y: len(y) - len(x)
    return sorted(a, key=cmp_to_key(f))

#### TAXONOMY ######################################################################################

#--- ORDERED DICTIONARY ----------------------------------------------------------------------------
# A taxonomy is based on an ordered dictionary
# (i.e., if a taxonomy term has multiple parents, the most recent parent is the default).


</t>
<t tx="karstenw.20230303131819.24">class odict(dict):

    @others
#--- TAXONOMY --------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303131819.25">def __init__(self, items=[]):
    """ A dictionary with ordered keys (first-in last-out).
    """
    dict.__init__(self)
    self._o = [] # List of ordered keys.
    if isinstance(items, dict):
        items = reversed(list(items.items()))
    for k, v in items:
        self.__setitem__(k, v)

</t>
<t tx="karstenw.20230303131819.26">@classmethod
def fromkeys(cls, keys=[], v=None):
    return cls((k, v) for k in keys)

</t>
<t tx="karstenw.20230303131819.27">def push(self, kv):
    """ Adds a new item from the given (key, value)-tuple.
        If the key exists, pushes the updated item to the head of the dict.
    """
    if kv[0] in self:
        self.__delitem__(kv[0])
    self.__setitem__(kv[0], kv[1])
append = push

</t>
<t tx="karstenw.20230303131819.28">def __iter__(self):
    return reversed(self._o)

</t>
<t tx="karstenw.20230303131819.29">def __setitem__(self, k, v):
    if k not in self:
        self._o.append(k)
    dict.__setitem__(self, k, v)

</t>
<t tx="karstenw.20230303131819.3">def __init__(self, string="", token=["word"]):
    """ A list of sentences, where each sentence is separated by a period.
    """
    list.__init__(self, (Sentence(s + ".", token) for s in string.split(".")))

</t>
<t tx="karstenw.20230303131819.30">def __delitem__(self, k):
    self._o.remove(k)
    dict.__delitem__(self, k)

</t>
<t tx="karstenw.20230303131819.31">def update(self, d):
    for k, v in reversed(list(d.items())):
        self.__setitem__(k, v)

</t>
<t tx="karstenw.20230303131819.32">def setdefault(self, k, v=None):
    if k not in self:
        self.__setitem__(k, v)
    return self[k]

</t>
<t tx="karstenw.20230303131819.33">def pop(self, k, *args, **kwargs):
    if k in self:
        self._o.remove(k)
    return dict.pop(self, k, *args, **kwargs)

</t>
<t tx="karstenw.20230303131819.34">def popitem(self):
    k = self._o[-1] if self._o else None
    return (k, self.pop(k))

</t>
<t tx="karstenw.20230303131819.35">def clear(self):
    self._o = []
    dict.clear(self)

</t>
<t tx="karstenw.20230303131819.36">def iterkeys(self):
    return reversed(self._o)

</t>
<t tx="karstenw.20230303131819.37">def itervalues(self):
    return map(self.__getitem__, reversed(self._o))

</t>
<t tx="karstenw.20230303131819.38">def iteritems(self):
    return iter(zip(self.iterkeys(), self.itervalues()))

</t>
<t tx="karstenw.20230303131819.39">def keys(self):
    return list(self.iterkeys())

</t>
<t tx="karstenw.20230303131819.4">@property
def sentences(self):
    return self

</t>
<t tx="karstenw.20230303131819.40">def values(self):
    return list(self.itervalues())

</t>
<t tx="karstenw.20230303131819.41">def items(self):
    return list(self.iteritems())

</t>
<t tx="karstenw.20230303131819.42">def copy(self):
    return self.__class__(reversed(list(self.items())))

</t>
<t tx="karstenw.20230303131819.43">def __repr__(self):
    return "{%s}" % ", ".join("%s: %s" % (repr(k), repr(v)) for k, v in self.items())

</t>
<t tx="karstenw.20230303131819.44">class Taxonomy(dict):

    @others
# Global taxonomy:
TAXONOMY = taxonomy = Taxonomy()

#taxonomy.append("rose", type="flower")
#taxonomy.append("daffodil", type="flower")
#taxonomy.append("flower", type="plant")
#print(taxonomy.classify("rose"))
#print(taxonomy.children("plant", recursive=True))

#c = Classifier(parents=lambda term: term.endswith("ness") and ["quality"] or [])
#taxonomy.classifiers.append(c)
#print(taxonomy.classify("roughness"))

#--- TAXONOMY CLASSIFIER ---------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303131819.45">def __init__(self):
    """ Hierarchical tree of words classified by semantic type.
        For example: "rose" and "daffodil" can be classified as "flower":
        &gt;&gt;&gt; taxonomy.append("rose", type="flower")
        &gt;&gt;&gt; taxonomy.append("daffodil", type="flower")
        &gt;&gt;&gt; print(taxonomy.children("flower"))
        Taxonomy terms can be used in a Pattern:
        FLOWER will match "flower" as well as "rose" and "daffodil".
        The taxonomy is case insensitive by default.
    """
    self.case_sensitive = False
    self._values = {}
    self.classifiers = []

</t>
<t tx="karstenw.20230303131819.46">def _normalize(self, term):
    try:
        return not self.case_sensitive and term.lower() or term
    except: # Not a string.
        return term

</t>
<t tx="karstenw.20230303131819.47">def __contains__(self, term):
    # Check if the term is in the dictionary.
    # If the term is not in the dictionary, check the classifiers.
    term = self._normalize(term)
    if dict.__contains__(self, term):
        return True
    for classifier in self.classifiers:
        if classifier.parents(term) \
        or classifier.children(term):
            return True
    return False

</t>
<t tx="karstenw.20230303131819.48">def append(self, term, type=None, value=None):
    """ Appends the given term to the taxonomy and tags it as the given type.
        Optionally, a disambiguation value can be supplied.
        For example: taxonomy.append("many", "quantity", "50-200")
    """
    term = self._normalize(term)
    type = self._normalize(type)
    self.setdefault(term, (odict(), odict()))[0].push((type, True))
    self.setdefault(type, (odict(), odict()))[1].push((term, True))
    self._values[term] = value

</t>
<t tx="karstenw.20230303131819.49">def classify(self, term, **kwargs):
    """ Returns the (most recently added) semantic type for the given term ("many" =&gt; "quantity").
        If the term is not in the dictionary, try Taxonomy.classifiers.
    """
    term = self._normalize(term)
    if dict.__contains__(self, term):
        return list(self[term][0].keys())[-1]
    # If the term is not in the dictionary, check the classifiers.
    # Returns the first term in the list returned by a classifier.
    for classifier in self.classifiers:
        # **kwargs are useful if the classifier requests extra information,
        # for example the part-of-speech tag.
        v = classifier.parents(term, **kwargs)
        if v:
            return v[0]

</t>
<t tx="karstenw.20230303131819.5">@property
def words(self):
    return list(chain(*self))


</t>
<t tx="karstenw.20230303131819.50">def parents(self, term, recursive=False, **kwargs):
    """ Returns a list of all semantic types for the given term.
        If recursive=True, traverses parents up to the root.
    """
    def dfs(term, recursive=False, visited={}, **kwargs):
        if term in visited: # Break on cyclic relations.
            return []
        visited[term], a = True, []
        if dict.__contains__(self, term):
            a = list(self[term][0].keys())
        for classifier in self.classifiers:
            a.extend(classifier.parents(term, **kwargs) or [])
        if recursive:
            for w in a:
                a += dfs(w, recursive, visited, **kwargs)
        return a
    return unique(dfs(self._normalize(term), recursive, {}, **kwargs))

</t>
<t tx="karstenw.20230303131819.51">def children(self, term, recursive=False, **kwargs):
    """ Returns all terms of the given semantic type: "quantity" =&gt; ["many", "lot", "few", ...]
        If recursive=True, traverses children down to the leaves.
    """
    def dfs(term, recursive=False, visited={}, **kwargs):
        if term in visited: # Break on cyclic relations.
            return []
        visited[term], a = True, []
        if dict.__contains__(self, term):
            a = list(self[term][1].keys())
        for classifier in self.classifiers:
            a.extend(classifier.children(term, **kwargs) or [])
        if recursive:
            for w in a:
                a += dfs(w, recursive, visited, **kwargs)
        return a
    return unique(dfs(self._normalize(term), recursive, {}, **kwargs))

</t>
<t tx="karstenw.20230303131819.52">def value(self, term, **kwargs):
    """ Returns the value of the given term ("many" =&gt; "50-200")
    """
    term = self._normalize(term)
    if term in self._values:
        return self._values[term]
    for classifier in self.classifiers:
        v = classifier.value(term, **kwargs)
        if v is not None:
            return v

</t>
<t tx="karstenw.20230303131819.53">def remove(self, term):
    if dict.__contains__(self, term):
        for w in self.parents(term):
            self[w][1].pop(term)
        dict.pop(self, term)

</t>
<t tx="karstenw.20230303131819.54">class Classifier(object):

    @others
# Classifier(parents=lambda word: word.endswith("ness") and ["quality"] or [])
# Classifier(parents=lambda word, chunk=None: chunk=="VP" and [ACTION] or [])


</t>
<t tx="karstenw.20230303131819.55">def __init__(self, parents=lambda term: [], children=lambda term: [], value=lambda term: None):
    """ A classifier uses a rule-based approach to enrich the taxonomy, for example:
        c = Classifier(parents=lambda term: term.endswith("ness") and ["quality"] or [])
        taxonomy.classifiers.append(c)
        This tags any word ending in -ness as "quality".
        This is much shorter than manually adding "roughness", "sharpness", ...
        Other examples of useful classifiers: calling en.wordnet.Synset.hyponyms() or en.number().
    """
    self.parents = parents
    self.children = children
    self.value = value

</t>
<t tx="karstenw.20230303131819.56">class WordNetClassifier(Classifier):

    @others
#from en import wordnet
#taxonomy.classifiers.append(WordNetClassifier(wordnet))
#print(taxonomy.parents("ponder", pos="VB"))
#print(taxonomy.children("computer"))

#### PATTERN #######################################################################################

#--- PATTERN CONSTRAINT ----------------------------------------------------------------------------

# Allowed chunk, role and part-of-speech tags (Penn Treebank II):
CHUNKS = dict.fromkeys(["NP", "PP", "VP", "ADVP", "ADJP", "SBAR", "PRT", "INTJ"], True)
ROLES = dict.fromkeys(["SBJ", "OBJ", "PRD", "TMP", "CLR", "LOC", "DIR", "EXT", "PRP"], True)
TAGS = dict.fromkeys(["CC", "CD", "CJ", "DT", "EX", "FW", "IN", "JJ", "JJR", "JJS", "JJ*",
                        "LS", "MD", "NN", "NNS", "NNP", "NNP*", "NNPS", "NN*", "NO", "PDT", "PR",
                        "PRP", "PRP$", "PR*", "PRP*", "PT", "RB", "RBR", "RBS", "RB*", "RP",
                        "SYM", "TO", "UH", "VB", "VBZ", "VBP", "VBD", "VBN", "VBG", "VB*",
                        "WDT", "WP*", "WRB", "X", ".", ",", ":", "(", ")"], True)

ALPHA = re.compile("[a-zA-Z]")
has_alpha = lambda string: ALPHA.match(string) is not None


</t>
<t tx="karstenw.20230303131819.57">def __init__(self, wordnet=None):
    if wordnet is None:
        try:
            from pattern.en import wordnet
        except:
            try:
                from .en import wordnet
            except:
                pass
    Classifier.__init__(self, self._parents, self._children)
    self.wordnet = wordnet

</t>
<t tx="karstenw.20230303131819.58">def _children(self, word, pos="NN"):
    try:
        return [w.synonyms[0] for w in self.wordnet.synsets(word, pos[:2])[0].hyponyms()]
    except:
        pass

</t>
<t tx="karstenw.20230303131819.59">def _parents(self, word, pos="NN"):
    try:
        return [w.synonyms[0] for w in self.wordnet.synsets(word, pos[:2])[0].hypernyms()]
    except:
        pass

</t>
<t tx="karstenw.20230303131819.6">class Sentence(list):

    @others
</t>
<t tx="karstenw.20230303131819.60">class Constraint(object):

    @others
#--- PATTERN ---------------------------------------------------------------------------------------

STRICT = "strict"
GREEDY = "greedy"


</t>
<t tx="karstenw.20230303131819.61">def __init__(self, words=[], tags=[], chunks=[], roles=[], taxa=[], optional=False, multiple=False, first=False, taxonomy=TAXONOMY, exclude=None, custom=None):
    """ A range of words, tags and taxonomy terms that matches certain words in a sentence.        
        For example: 
        Constraint.fromstring("with|of") matches either "with" or "of".
        Constraint.fromstring("(JJ)") optionally matches an adjective.
        Constraint.fromstring("NP|SBJ") matches subject noun phrases.
        Constraint.fromstring("QUANTITY|QUALITY") matches quantity-type and quality-type taxa.
    """
    self.index    = 0
    self.words    = list(words)  # Allowed words/lemmata (of, with, ...)
    self.tags     = list(tags)   # Allowed parts-of-speech (NN, JJ, ...)
    self.chunks   = list(chunks) # Allowed chunk types (NP, VP, ...)
    self.roles    = list(roles)  # Allowed chunk roles (SBJ, OBJ, ...)
    self.taxa     = list(taxa)   # Allowed word categories.
    self.taxonomy = taxonomy
    self.optional = optional
    self.multiple = multiple
    self.first    = first
    self.exclude  = exclude      # Constraint of words that are *not* allowed, or None.
    self.custom   = custom       # Custom function(Word) returns True if word matches constraint.

</t>
<t tx="karstenw.20230303131819.62">@classmethod
def fromstring(cls, s, **kwargs):
    """ Returns a new Constraint from the given string.
        Uppercase words indicate either a tag ("NN", "JJ", "VP")
        or a taxonomy term (e.g., "PRODUCT", "PERSON").
        Syntax:
        ( defines an optional constraint, e.g., "(JJ)".
        [ defines a constraint with spaces, e.g., "[Mac OS X | Windows Vista]".
        _ is converted to spaces, e.g., "Windows_Vista".
        | separates different options, e.g., "ADJP|ADVP".
        ! can be used as a word prefix to disallow it.
        * can be used as a wildcard character, e.g., "soft*|JJ*".
        ? as a suffix defines a constraint that is optional, e.g., "JJ?".
        + as a suffix defines a constraint that can span multiple words, e.g., "JJ+".
        ^ as a prefix defines a constraint that can only match the first word.
        These characters need to be escaped if used as content: "\(".
    """
    C = cls(**kwargs)
    s = s.strip()
    s = s.strip("{}")
    s = s.strip()
    for i in range(3):
        # Wrapping order of control characters is ignored:
        # (NN+) == (NN)+ == NN?+ == NN+? == [NN+?] == [NN]+?
        if s.startswith("^"):
            s = s[1:]; C.first = True
        if s.endswith("+") and not s.endswith("\+"):
            s = s[0:-1]; C.multiple = True
        if s.endswith("?") and not s.endswith("\?"):
            s = s[0:-1]; C.optional = True
        if s.startswith("(") and s.endswith(")"):
            s = s[1:-1]; C.optional = True
        if s.startswith("[") and s.endswith("]"):
            s = s[1:-1]
    s = re.sub(r"^\\\^", "^", s)
    s = re.sub(r"\\\+$", "+", s)
    s = s.replace("\_", "&amp;uscore;")
    s = s.replace("_", " ")
    s = s.replace("&amp;uscore;", "_")
    s = s.replace("&amp;lparen;", "(")
    s = s.replace("&amp;rparen;", ")")
    s = s.replace("&amp;lbrack;", "[")
    s = s.replace("&amp;rbrack;", "]")
    s = s.replace("&amp;lcurly;", "{")
    s = s.replace("&amp;rcurly;", "}")
    s = s.replace("\(", "(")
    s = s.replace("\)", ")")
    s = s.replace("\[", "[")
    s = s.replace("\]", "]")
    s = s.replace("\{", "{")
    s = s.replace("\}", "}")
    s = s.replace("\*", "*")
    s = s.replace("\?", "?")
    s = s.replace("\+", "+")
    s = s.replace("\^", "^")
    s = s.replace("\|", "&amp;vdash;")
    s = s.split("|")
    s = [v.replace("&amp;vdash;", "|").strip() for v in s]
    for v in s:
        C._append(v)
    return C

</t>
<t tx="karstenw.20230303131819.63">def _append(self, v):
    if v.startswith("!") and self.exclude is None:
        self.exclude = Constraint()
    if v.startswith("!"):
        self.exclude._append(v[1:]); return
    if "!" in v:
        v = v.replace("\!", "!")
    if v != v.upper():
        self.words.append(v.lower())
    elif v in TAGS:
        self.tags.append(v)
    elif v in CHUNKS:
        self.chunks.append(v)
    elif v in ROLES:
        self.roles.append(v)
    elif v in self.taxonomy or has_alpha(v):
        self.taxa.append(v.lower())
    else:
        # Uppercase words indicate tags or taxonomy terms.
        # However, this also matches "*" or "?" or "0.25".
        # Unless such punctuation is defined in the taxonomy, it is added to Range.words.
        self.words.append(v.lower())

</t>
<t tx="karstenw.20230303131819.64">def match(self, word):
    """ Return True if the given Word is part of the constraint:
        - the word (or lemma) occurs in Constraint.words, OR
        - the word (or lemma) occurs in Constraint.taxa taxonomy tree, AND
        - the word and/or chunk tags match those defined in the constraint.
        Individual terms in Constraint.words or the taxonomy can contain wildcards (*).
        Some part-of-speech-tags can also contain wildcards: NN*, VB*, JJ*, RB*, PR*.
        If the given word contains spaces (e.g., proper noun),
        the entire chunk will also be compared.
        For example: Constraint(words=["Mac OS X*"]) 
        matches the word "Mac" if the word occurs in a Chunk("Mac OS X 10.5").
    """
    # If the constraint has a custom function it must return True.
    if self.custom is not None and self.custom(word) is False:
        return False
    # If the constraint can only match the first word, Word.index must be 0.
    if self.first and word.index &gt; 0:
        return False
    # If the constraint defines excluded options, Word can not match any of these.
    if self.exclude and self.exclude.match(word):
        return False
    # If the constraint defines allowed tags, Word.tag needs to match one of these.
    if self.tags:
        if find(lambda w: _match(word.tag, w), self.tags) is None:
            return False
    # If the constraint defines allowed chunks, Word.chunk.tag needs to match one of these.
    if self.chunks:
        ch = word.chunk and word.chunk.tag or None
        if find(lambda w: _match(ch, w), self.chunks) is None:
            return False
    # If the constraint defines allowed role, Word.chunk.tag needs to match one of these.
    if self.roles:
        R = word.chunk and [r2 for r1, r2 in word.chunk.relations] or []
        if find(lambda w: w in R, self.roles) is None:
            return False
    # If the constraint defines allowed words,
    # Word.string.lower() OR Word.lemma needs to match one of these.
    b = True # b==True when word in constraint (or Constraints.words=[]).
    if len(self.words) + len(self.taxa) &gt; 0:
        s1 = word.string.lower()
        s2 = word.lemma
        b = False
        for w in itertools.chain(self.words, self.taxa):
            # If the constraint has a word with spaces (e.g., a proper noun),
            # compare it to the entire chunk.
            try:
                if " " in w and (s1 in w or s2 and s2 in w or "*" in w):
                    s1 = word.chunk and word.chunk.string.lower() or s1
                    s2 = word.chunk and " ".join(x or "" for x in word.chunk.lemmata) or s2
            except Exception as e:
                s1 = s1
                s2 = None
            # Compare the word to the allowed words (which can contain wildcards).
            if _match(s1, w):
                b = True
                break
            # Compare the word lemma to the allowed words, e.g.,
            # if "was" is not in the constraint, perhaps "be" is, which is a good match.
            if s2 and _match(s2, w):
                b = True
                break

    # If the constraint defines allowed taxonomy terms,
    # and the given word did not match an allowed word, traverse the taxonomy.
    # The search goes up from the given word to its parents in the taxonomy.
    # This is faster than traversing all the children of terms in Constraint.taxa.
    # The drawback is that:
    # 1) Wildcards in the taxonomy are not detected (use classifiers instead),
    # 2) Classifier.children() has no effect, only Classifier.parent().
    if self.taxa and (not self.words or (self.words and not b)):
        for s in (
          word.string, # "ants"
          word.lemma,  # "ant"
          word.chunk and word.chunk.string or None, # "army ants"
          word.chunk and " ".join([x or "" for x in word.chunk.lemmata]) or None): # "army ant"
            if s is not None:
                if self.taxonomy.case_sensitive is False:
                    s = s.lower()
                # Compare ancestors of the word to each term in Constraint.taxa.
                for p in self.taxonomy.parents(s, recursive=True):
                    if find(lambda s: p == s, self.taxa): # No wildcards.
                        return True
    return b

</t>
<t tx="karstenw.20230303131819.65">def __repr__(self):
    s = []
    for k, v in (
      ( "words", self.words),
      (  "tags", self.tags),
      ("chunks", self.chunks),
      ( "roles", self.roles),
      (  "taxa", self.taxa)):
        if v:
            s.append("%s=%s" % (k, repr(v)))
    return "Constraint(%s)" % ", ".join(s)

</t>
<t tx="karstenw.20230303131819.66">@property
def string(self):
    a = self.words + self.tags + self.chunks + self.roles + [w.upper() for w in self.taxa]
    a = (escape(s) for s in a)
    a = (s.replace("\\*", "*") for s in a)
    a = [s.replace(" ", "_") for s in a]
    if self.exclude:
        a.extend("!" + s for s in self.exclude.string[1:-1].split("|"))
    return (self.optional and "%s(%s)%s" or "%s[%s]%s") % (
        self.first and "^" or "", "|".join(a), self.multiple and "+" or "")

</t>
<t tx="karstenw.20230303131819.67">class Pattern(object):

    @others
_cache = {}
_CACHE_SIZE = 100 # Number of dynamic Pattern objects to keep in cache.


</t>
<t tx="karstenw.20230303131819.68">def __init__(self, sequence=[], *args, **kwargs):
    """ A sequence of constraints that matches certain phrases in a sentence.
        The given list of Constraint objects can contain nested lists (groups).
    """
    # Parse nested lists and tuples from the sequence into groups.
    # [DT [JJ NN]] =&gt; Match.group(1) will yield the JJ NN sequences.
    def _ungroup(sequence, groups=None):
        for v in sequence:
            if isinstance(v, (list, tuple)):
                if groups is not None:
                    groups.append(list(_ungroup(v, groups=None)))
                for v in _ungroup(v, groups):
                    yield v
            else:
                yield v
    self.groups = []
    self.sequence = list(_ungroup(sequence, groups=self.groups))
    # Assign Constraint.index:
    i = 0
    for constraint in self.sequence:
        constraint.index = i
        i += 1
    # There are two search modes: STRICT and GREEDY.
    # - In STRICT, "rabbit" matches only the string "rabbit".
    # - In GREEDY, "rabbit|NN" matches the string "rabbit" tagged "NN".
    # - In GREEDY, "rabbit" matches "the big white rabbit" (the entire chunk is a match).
    # - Pattern.greedy(chunk, constraint) determines (True/False) if a chunk is a match.
    self.strict = kwargs.get("strict", STRICT in args and GREEDY not in args)
    self.greedy = kwargs.get("greedy", lambda chunk, constraint: True)

</t>
<t tx="karstenw.20230303131819.69">def __iter__(self):
    return iter(self.sequence)

</t>
<t tx="karstenw.20230303131819.7">def __init__(self, string="", token=["word"]):
    """ A list of words, where punctuation marks are split from words.
    """
    s = RE_PUNCTUATION.sub(" \\1 ", string) # Naive tokenization.
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r" ' (d|m|s|ll|re|ve)", " '\\1", s)
    s = s.replace("n ' t", " n't")
    s = s.split(" ")
    list.__init__(self, (Word(self, w, index=i) for i, w in enumerate(s)))

</t>
<t tx="karstenw.20230303131819.70">def __len__(self):
    return len(self.sequence)

</t>
<t tx="karstenw.20230303131819.71">def __getitem__(self, i):
    return self.sequence[i]

</t>
<t tx="karstenw.20230303131819.72">@classmethod
def fromstring(cls, s, *args, **kwargs):
    """ Returns a new Pattern from the given string.
        Constraints are separated by a space.
        If a constraint contains a space, it must be wrapped in [].
    """
    s = s.replace("\(", "&amp;lparen;")
    s = s.replace("\)", "&amp;rparen;")
    s = s.replace("\[", "&amp;lbrack;")
    s = s.replace("\]", "&amp;rbrack;")
    s = s.replace("\{", "&amp;lcurly;")
    s = s.replace("\}", "&amp;rcurly;")
    p = []
    i = 0
    for m in re.finditer(r"\[.*?\]|\(.*?\)", s):
        # Spaces in a range encapsulated in square brackets are encoded.
        # "[Windows Vista]" is one range, don't split on space.
        p.append(s[i:m.start()])
        p.append(s[m.start():m.end()].replace(" ", "&amp;space;")); i = m.end()
    p.append(s[i:])
    s = "".join(p)
    s = s.replace("][", "] [")
    s = s.replace(")(", ") (")
    s = s.replace("\|", "&amp;vdash;")
    s = re.sub(r"\s+\|\s+", "|", s)
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"\{\s+", "{", s)
    s = re.sub(r"\s+\}", "}", s)
    s = s.split(" ")
    s = [v.replace("&amp;space;", " ") for v in s]
    P = cls([], *args, **kwargs)
    G, O, i = [], [], 0
    for s in s:
        constraint = Constraint.fromstring(s.strip("{}"), taxonomy=kwargs.get("taxonomy", TAXONOMY))
        constraint.index = len(P.sequence)
        P.sequence.append(constraint)
        # Push a new group on the stack if string starts with "{".
        # Parse constraint from string, add it to all open groups.
        # Pop latest group from stack if string ends with "}".
        # Insert groups in opened-first order (i).
        while s.startswith("{"):
            s = s[1:]
            G.append((i, []))
            i += 1
            O.append([])
        for g in G:
            g[1].append(constraint)
        while s.endswith("}"):
            s = s[:-1]
            if G:
                O[G[-1][0]] = G[-1][1]
                G.pop()
    P.groups = [g for g in O if g]
    return P

</t>
<t tx="karstenw.20230303131819.73">def scan(self, string):
    """ Returns True if search(Sentence(string)) may yield matches.
        If is often faster to scan prior to creating a Sentence and searching it.
    """
    # In the following example, first scan the string for "good" and "bad":
    # p = Pattern.fromstring("good|bad NN")
    # for s in open("parsed.txt"):
    #     if p.scan(s):
    #         s = Sentence(s)
    #         m = p.search(s)
    #         if m:
    #             print(m)
    w = (constraint.words for constraint in self.sequence if not constraint.optional)
    w = itertools.chain(*w)
    w = [w.strip(WILDCARD) for w in w if WILDCARD not in w[1:-1]]
    if w and not any(w in string.lower() for w in w):
        return False
    return True

</t>
<t tx="karstenw.20230303131819.74">def search(self, sentence):
    """ Returns a list of all matches found in the given sentence.
    """
    if sentence.__class__.__name__ == "Sentence":
        pass
    elif isinstance(sentence, list) or sentence.__class__.__name__ == "Text":
        a = []
        [a.extend(self.search(s)) for s in sentence]
        return a
    elif isinstance(sentence, str):
        sentence = Sentence(sentence)
    elif isinstance(sentence, Match) and len(sentence) &gt; 0:
        sentence = sentence[0].sentence.slice(sentence[0].index, sentence[-1].index + 1)
    a = []
    v = self._variations()
    u = {}
    m = self.match(sentence, _v=v)
    while m:
        a.append(m)
        m = self.match(sentence, start=m.words[-1].index + 1, _v=v, _u=u)
    return a

</t>
<t tx="karstenw.20230303131819.75">def match(self, sentence, start=0, _v=None, _u=None):
    """ Returns the first match found in the given sentence, or None.
    """
    if sentence.__class__.__name__ == "Sentence":
        pass
    elif isinstance(sentence, list) or sentence.__class__.__name__ == "Text":
        return find(lambda m: m is not None, (self.match(s, start, _v) for s in sentence))
    elif isinstance(sentence, str):
        sentence = Sentence(sentence)
    elif isinstance(sentence, Match) and len(sentence) &gt; 0:
        sentence = sentence[0].sentence.slice(sentence[0].index, sentence[-1].index + 1)
    # Variations (_v) further down the list may match words more to the front.
    # We need to check all of them. Unmatched variations are blacklisted (_u).
    # Pattern.search() calls Pattern.match() with a persistent blacklist (1.5x faster).
    a = []
    for sequence in (_v is not None and _v or self._variations()):
        if _u is not None and id(sequence) in _u:
            continue
        m = self._match(sequence, sentence, start)
        if m is not None:
            a.append((m.words[0].index, len(m.words), m))
        if m is not None and m.words[0].index == start:
            return m
        if m is None and _u is not None:
            _u[id(sequence)] = False
    # Return the leftmost-longest.
    if len(a) &gt; 0:
        return sorted(a, key = lambda x: (x[0], -x[1]))[0][-1]

</t>
<t tx="karstenw.20230303131819.76">def _variations(self):
    v = variations(self.sequence, optional=lambda constraint: constraint.optional)
    v = sorted(v, key=len, reverse=True)
    return v

</t>
<t tx="karstenw.20230303131819.77">def _match(self, sequence, sentence, start=0, i=0, w0=None, map=None, d=0):
    # Backtracking tree search.
    # Finds the first match in the sentence of the given sequence of constraints.
    # start : the current word index.
    #     i : the current constraint index.
    #    w0 : the first word that matches a constraint.
    #   map : a dictionary of (Word index, Constraint) items.
    #     d : recursion depth.

    # XXX - We can probably rewrite all of this using (faster) regular expressions.

    if map is None:
        map = {}

    n = len(sequence)

    # --- MATCH ----------
    if i == n:
        if w0 is not None:
            w1 = sentence.words[start - 1]
            # Greedy algorithm:
            # - "cat" matches "the big cat" if "cat" is head of the chunk.
            # - "Tom" matches "Tom the cat" if "Tom" is head of the chunk.
            # - This behavior is ignored with POS-tag constraints:
            #   "Tom|NN" can only match single words, not chunks.
            # - This is also True for negated POS-tags (e.g., !NN).
            w01 = [w0, w1]
            for j in (0, -1):
                constraint, w = sequence[j], w01[j]
                if self.strict is False and w.chunk is not None:
                    if not constraint.tags:
                        if not constraint.exclude or not constraint.exclude.tags:
                            if constraint.match(w.chunk.head):
                                w01[j] = w.chunk.words[j]
                            if constraint.exclude and constraint.exclude.match(w.chunk.head):
                                return None
                            if self.greedy(w.chunk, constraint) is False: # User-defined.
                                return None
            w0, w1 = w01
            # Update map for optional chunk words (see below).
            words = sentence.words[w0.index:w1.index + 1]
            for w in words:
                if w.index not in map and w.chunk:
                    wx = find(lambda w: w.index in map, reversed(w.chunk.words))
                    if wx:
                        map[w.index] = map[wx.index]
            # Return matched word range, we'll need the map to build Match.constituents().
            return Match(self, words, map)
        return None

    # --- RECURSION --------
    constraint = sequence[i]
    for w in sentence.words[start:]:
        #print(" "*d, "match?", w, sequence[i].string) # DEBUG
        if i &lt; n and constraint.match(w):
            #print(" "*d, "match!", w, sequence[i].string) # DEBUG
            map[w.index] = constraint
            if constraint.multiple:
                # Next word vs. same constraint if Constraint.multiple=True.
                m = self._match(sequence, sentence, w.index + 1, i, w0 or w, map, d + 1)
                if m:
                    return m
            # Next word vs. next constraint.
            m = self._match(sequence, sentence, w.index + 1, i + 1, w0 or w, map, d + 1)
            if m:
                return m
        # Chunk words other than the head are optional:
        # - Pattern.fromstring("cat") matches "cat" but also "the big cat" (overspecification).
        # - Pattern.fromstring("cat|NN") does not match "the big cat" (explicit POS-tag).
        if w0 and not constraint.tags:
            if not constraint.exclude and not self.strict and w.chunk and w.chunk.head != w:
                continue
            break
        # Part-of-speech tags match one single word.
        if w0 and constraint.tags:
            break
        if w0 and constraint.exclude and constraint.exclude.tags:
            break

</t>
<t tx="karstenw.20230303131819.78">@property
def string(self):
    return " ".join(constraint.string for constraint in self.sequence)

</t>
<t tx="karstenw.20230303131819.79">def compile(pattern, *args, **kwargs):
    """ Returns a Pattern from the given string or regular expression.
        Recently compiled patterns are kept in cache
        (if they do not use taxonomies, which are mutable dicts).
    """
    id, p = repr(pattern) + repr(args), pattern
    if id in _cache and not kwargs:
        return _cache[id]
    if isinstance(pattern, str):
        p = Pattern.fromstring(pattern, *args, **kwargs)
    if isinstance(pattern, regexp):
        p = Pattern([Constraint(words=[pattern], taxonomy=kwargs.get("taxonomy", TAXONOMY))], *args, **kwargs)
    if len(_cache) &gt; _CACHE_SIZE:
        _cache.clear()
    if isinstance(p, Pattern) and not kwargs:
        _cache[id] = p
    if isinstance(p, Pattern):
        return p
    else:
        raise TypeError("can't compile '%s' object" % pattern.__class__.__name__)


</t>
<t tx="karstenw.20230303131819.8">@property
def string(self):
    return " ".join(w.string for w in self)

</t>
<t tx="karstenw.20230303131819.80">def scan(pattern, string, *args, **kwargs):
    """ Returns True if pattern.search(Sentence(string)) may yield matches.
        If is often faster to scan prior to creating a Sentence and searching it.
    """
    return compile(pattern, *args, **kwargs).scan(string)


</t>
<t tx="karstenw.20230303131819.81">def match(pattern, sentence, *args, **kwargs):
    """ Returns the first match found in the given sentence, or None.
    """
    return compile(pattern, *args, **kwargs).match(sentence)


</t>
<t tx="karstenw.20230303131819.82">def search(pattern, sentence, *args, **kwargs):
    """ Returns a list of all matches found in the given sentence.
    """
    return compile(pattern, *args, **kwargs).search(sentence)


</t>
<t tx="karstenw.20230303131819.83">def escape(string):
    """ Returns the string with control characters for Pattern syntax escaped.
        For example: "hello!" =&gt; "hello\!".
    """
    for ch in ("{", "}", "[", "]", "(", ")", "_", "|", "!", "*", "+", "^"):
        string = string.replace(ch, "\\" + ch)
    return string

#--- PATTERN MATCH ---------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303131819.84">class Match(object):

    @others
#--- PATTERN MATCH GROUP ---------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303131819.85">def __init__(self, pattern, words=[], map={}):
    """ Search result returned from Pattern.match(sentence),
        containing a sequence of Word objects.
    """
    self.pattern = pattern
    self.words = words
    self._map1 = dict() # Word index to Constraint.
    self._map2 = dict() # Constraint index to list of Word indices.
    for w in self.words:
        self._map1[w.index] = map[w.index]
    for k, v in self._map1.items():
        self._map2.setdefault(self.pattern.sequence.index(v), []).append(k)
    for k, v in self._map2.items():
        v.sort()

</t>
<t tx="karstenw.20230303131819.86">def __len__(self):
    return len(self.words)

</t>
<t tx="karstenw.20230303131819.87">def __iter__(self):
    return iter(self.words)

</t>
<t tx="karstenw.20230303131819.88">def __getitem__(self, i):
    return self.words.__getitem__(i)

</t>
<t tx="karstenw.20230303131819.89">@property
def start(self):
    return self.words and self.words[0].index or None

</t>
<t tx="karstenw.20230303131819.9">@property
def words(self):
    return self

</t>
<t tx="karstenw.20230303131819.90">@property
def stop(self):
    return self.words and self.words[-1].index + 1 or None

</t>
<t tx="karstenw.20230303131819.91">def constraint(self, word):
    """ Returns the constraint that matches the given Word, or None.
    """
    if word.index in self._map1:
        return self._map1[word.index]

</t>
<t tx="karstenw.20230303131819.92">def constraints(self, chunk):
    """ Returns a list of constraints that match the given Chunk.
    """
    a = [self._map1[w.index] for w in chunk.words if w.index in self._map1]
    b = []
    [b.append(constraint) for constraint in a if constraint not in b]
    return b

</t>
<t tx="karstenw.20230303131819.93">def constituents(self, constraint=None):
    """ Returns a list of Word and Chunk objects, 
        where words have been grouped into their chunks whenever possible.
        Optionally, returns only chunks/words that match given constraint(s), or constraint index.
    """
    # Select only words that match the given constraint.
    # Note: this will only work with constraints from Match.pattern.sequence.
    W = self.words
    n = len(self.pattern.sequence)
    if isinstance(constraint, (int, Constraint)):
        if isinstance(constraint, int):
            i = constraint
            i = i &lt; 0 and i % n or i
        else:
            i = self.pattern.sequence.index(constraint)
        W = self._map2.get(i, [])
        W = [self.words[i - self.words[0].index] for i in W]
    if isinstance(constraint, (list, tuple)):
        W = []
        [W.extend(self._map2.get(j &lt; 0 and j % n or j, [])) for j in constraint]
        W = [self.words[i - self.words[0].index] for i in W]
        W = unique(W)
    a = []
    i = 0
    while i &lt; len(W):
        w = W[i]
        if w.chunk and W[i:i + len(w.chunk)] == w.chunk.words:
            i += len(w.chunk) - 1
            a.append(w.chunk)
        else:
            a.append(w)
        i += 1
    return a

</t>
<t tx="karstenw.20230303131819.94">def group(self, index, chunked=False):
    """ Returns a list of Word objects that match the given group.
        With chunked=True, returns a list of Word + Chunk objects - see Match.constituents().
        A group consists of consecutive constraints wrapped in { }, e.g.,
        search("{JJ JJ} NN", Sentence(parse("big black cat"))).group(1) =&gt; big black.
    """
    if index &lt; 0 or index &gt; len(self.pattern.groups):
        raise IndexError("no such group")
    if index &gt; 0 and index &lt;= len(self.pattern.groups):
        g = self.pattern.groups[index - 1]
    if index == 0:
        g = self.pattern.sequence
    if chunked is True:
        return Group(self, self.constituents(constraint=[self.pattern.sequence.index(x) for x in g]))
    return Group(self, [w for w in self.words if self.constraint(w) in g])

</t>
<t tx="karstenw.20230303131819.95">@property
def string(self):
    return " ".join(w.string for w in self.words)

</t>
<t tx="karstenw.20230303131819.96">def __repr__(self):
    return "Match(words=%s)" % repr(self.words)

</t>
<t tx="karstenw.20230303131819.97">class Group(list):

    @others
</t>
<t tx="karstenw.20230303131819.98">def __init__(self, match, words):
    list.__init__(self, words)
    self.match = match

</t>
<t tx="karstenw.20230303131819.99">@property
def words(self):
    return list(self)

</t>
<t tx="karstenw.20230303132508.1">#### PATTERN | DE ##################################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2012 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################
# German linguistical tools using fast regular expressions.

@others
if __name__ == "__main__":
    commandline(parse)
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303132547.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

# Import parser base classes.
from pattern.text import (
    Lexicon, Model, Morphology, Context, Parser as _Parser, ngrams, pprint, commandline,
    PUNCTUATION
)
# Import parser universal tagset.
from pattern.text import (
    penntreebank2universal,
    PTB, PENN, UNIVERSAL,
    NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X
)
# Import parse tree base classes.
from pattern.text.tree import (
    Tree, Text, Sentence, Slice, Chunk, PNPChunk, Chink, Word, table,
    SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA, AND, OR
)
# Import sentiment analysis base classes.
from pattern.text import (
    Sentiment, NOUN, VERB, ADJECTIVE, ADVERB
)
# Import spelling base class.
from pattern.text import (
    Spelling
)
# Import verb tenses.
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    INDICATIVE, IMPERATIVE, SUBJUNCTIVE,
    PROGRESSIVE,
    PARTICIPLE, GERUND
)
# Import inflection functions.
from pattern.text.de.inflect import (
    article, referenced, DEFINITE, INDEFINITE,
    pluralize, singularize, NOUN, VERB, ADJECTIVE,
    grade, comparative, superlative, COMPARATIVE, SUPERLATIVE,
    verbs, conjugate, lemma, lexeme, tenses,
    predicative, attributive,
    gender, MASCULINE, MALE, FEMININE, FEMALE, NEUTER, NEUTRAL, PLURAL, M, F, N, PL,
            NOMINATIVE, ACCUSATIVE, DATIVE, GENITIVE, SUBJECT, OBJECT, INDIRECT, PROPERTY
)
# Import all submodules.
from pattern.text.de import inflect

sys.path.pop(0)

#--- GERMAN PARSER ---------------------------------------------------------------------------------
# The German parser (accuracy 96% for known words) is based on Schneider &amp; Volk's language model:
# Schneider, G. &amp; Volk, M. (1998).
# Adding Manual Constraints and Lexical Look-up to a Brill-Tagger for German.
# Proceedings of the ESSLLI workshop on recent advances in corpus annotation. Saarbrucken, Germany.
# http://www.zora.uzh.ch/28579/

# The lexicon uses the Stuttgart/Tubinger Tagset (STTS):
# https://files.ifi.uzh.ch/cl/tagger/UIS-STTS-Diffs.html
STTS = "stts"
stts = tagset = {
      "ADJ": "JJ",
     "ADJA": "JJ",   # das große Haus
     "ADJD": "JJ",   # er ist schnell
      "ADV": "RB",   # schon
     "APPR": "IN",   # in der Stadt
  "APPRART": "IN",   # im Haus
     "APPO": "IN",   # der Sache wegen
     "APZR": "IN",   # von jetzt an
      "ART": "DT",   # der, die, eine
   "ARTDEF": "DT",   # der, die
   "ARTIND": "DT",   # eine
     "CARD": "CD",   # zwei
  "CARDNUM": "CD",   # 3
     "KOUI": "IN",   # [um] zu leben
     "KOUS": "IN",   # weil, damit, ob
      "KON": "CC",   # und, oder, aber
    "KOKOM": "IN",   # als, wie
     "KONS": "IN",   # usw.
       "NN": "NN",   # Tisch, Herr
      "NNS": "NNS",  # Tischen, Herren
       "NE": "NNP",  # Hans, Hamburg
      "PDS": "DT",   # dieser, jener
     "PDAT": "DT",   # jener Mensch
      "PIS": "DT",   # keiner, viele, niemand
     "PIAT": "DT",   # kein Mensch
    "PIDAT": "DT",   # die beiden Brüder
     "PPER": "PRP",  # ich, er, ihm, mich, dir
     "PPOS": "PRP$", # meins, deiner
   "PPOSAT": "PRP$", # mein Buch, deine Mutter
    "PRELS": "WDT",  # der Hund, [der] bellt
   "PRELAT": "WDT",  # der Mann, [dessen] Hund bellt
      "PRF": "PRP",  # erinnere [dich]
      "PWS": "WP",   # wer
     "PWAT": "WP",   # wessen, welche
     "PWAV": "WRB",  # warum, wo, wann
      "PAV": "RB",   # dafur, dabei, deswegen, trotzdem
    "PTKZU": "TO",   # zu gehen, zu sein
   "PTKNEG": "RB",   # nicht
    "PTKVZ": "RP",   # pass [auf]!
   "PTKANT": "UH",   # ja, nein, danke, bitte
     "PTKA": "RB",   # am schönsten, zu schnell
    "VVFIN": "VB",   # du [gehst], wir [kommen] an
    "VAFIN": "VB",   # du [bist], wir [werden]
    "VVINF": "VB",   # gehen, ankommen
    "VAINF": "VB",   # werden, sein
    "VVIZU": "VB",   # anzukommen
    "VVIMP": "VB",   # [komm]!
    "VAIMP": "VB",   # [sei] ruhig!
     "VVPP": "VBN",  # gegangen, angekommen
     "VAPP": "VBN",  # gewesen
    "VMFIN": "MD",   # dürfen
    "VMINF": "MD",   # wollen
     "VMPP": "MD",   # gekonnt
     "SGML": "SYM",  #
       "FM": "FW",   #
      "ITJ": "UH",   # ach, tja
       "XY": "NN",   #
       "XX": "NN",   #
    "LINUM": "LS",   # 1.
        "C": ",",    # ,
       "Co": ":",    # :
       "Ex": ".",    # !
       "Pc": ")",    # )
       "Po": "(",    # (
        "Q": ".",    # ?
      "QMc": "\"",   # "
      "QMo": "\"",   # "
        "S": ".",    # .
       "Se": ":",    # ;
}


</t>
<t tx="karstenw.20230303132547.10">def parse(s, *args, **kwargs):
    """ Returns a tagged Unicode string.
    """
    return parser.parse(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303132547.11">def parsetree(s, *args, **kwargs):
    """ Returns a parsed Text from the given string.
    """
    return Text(parse(s, *args, **kwargs))


</t>
<t tx="karstenw.20230303132547.12">def tree(s, token=[WORD, POS, CHUNK, PNP, REL, LEMMA]):
    """ Returns a parsed Text from the given parsed string.
    """
    return Text(s, token)


</t>
<t tx="karstenw.20230303132547.13">def tag(s, tokenize=True, encoding="utf-8", **kwargs):
    """ Returns a list of (token, tag)-tuples from the given string.
    """
    tags = []
    for sentence in parse(s, tokenize, True, False, False, False, encoding, **kwargs).split():
        for token in sentence:
            tags.append((token[0], token[1]))
    return tags


</t>
<t tx="karstenw.20230303132547.14">def keywords(s, top=10, **kwargs):
    """ Returns a sorted list of keywords in the given string.
    """
    return parser.find_keywords(s, **dict({
        "frequency": parser.frequency,
              "top": top,
              "pos": ("NN",),
           "ignore": ("rt",)}, **kwargs))


</t>
<t tx="karstenw.20230303132547.15">def suggest(w):
    """ Returns a list of (word, confidence)-tuples of spelling corrections.
    """
    return spelling.suggest(w)

split = tree # Backwards compatibility.

#---------------------------------------------------------------------------------------------------
# python -m pattern.de xml -s "Ein Unglück kommt selten allein." -OTCL

</t>
<t tx="karstenw.20230303132547.2">def stts2penntreebank(token, tag):
    """ Converts an STTS tag to a Penn Treebank II tag.
        For example: ohne/APPR =&gt; ohne/IN
    """
    return (token, stts.get(tag, tag))


</t>
<t tx="karstenw.20230303132547.3">def stts2universal(token, tag):
    """ Converts an STTS tag to a universal tag.
        For example: ohne/APPR =&gt; ohne/PREP
    """
    if tag in ("KON", "KOUI", "KOUS", "KOKOM"):
        return (token, CONJ)
    if tag in ("PTKZU", "PTKNEG", "PTKVZ", "PTKANT"):
        return (token, PRT)
    if tag in ("PDF", "PDAT", "PIS", "PIAT", "PIDAT", "PPER", "PPOS", "PPOSAT"):
        return (token, PRON)
    if tag in ("PRELS", "PRELAT", "PRF", "PWS", "PWAT", "PWAV", "PAV"):
        return (token, PRON)
    return penntreebank2universal(*stts2penntreebank(token, tag))

ABBREVIATIONS = set((
    "Abs.", "Abt.", "Ass.", "Br.", "Ch.", "Chr.", "Cie.", "Co.", "Dept.", "Diff.",
    "Dr.", "Eidg.", "Exp.", "Fam.", "Fr.", "Hrsg.", "Inc.", "Inv.", "Jh.", "Jt.", "Kt.",
    "Mio.", "Mrd.", "Mt.", "Mte.", "Nr.", "Nrn.", "Ord.", "Ph.", "Phil.", "Pkt.",
    "Prof.", "Pt.", " S.", "St.", "Stv.", "Tit.", "VII.", "al.", "begr.", "bzw.",
    "chem.", "dent.", "dipl.", "e.g.", "ehem.", "etc.", "excl.", "exkl.", "hum.",
    "i.e.", "incl.", "ing.", "inkl.", "int.", "iur.", "lic.", "med.", "no.", "oec.",
    "phil.", "phys.", "pp.", "psych.", "publ.", "rer.", "sc.", "soz.", "spez.", "stud.",
    "theol.", "usw.", "vet.", "vgl.", "vol.", "wiss.",
    "d.h.", "h.c.", "o.ä.", "u.a.", "z.B.", "z.T.", "z.Zt."
))


</t>
<t tx="karstenw.20230303132547.4">def find_lemmata(tokens):
    """ Annotates the tokens with lemmata for plural nouns and conjugated verbs,
        where each token is a [word, part-of-speech] list.
    """
    for token in tokens:
        word, pos, lemma = token[0], token[1], token[0]
        if pos.startswith(("DT", "JJ")):
            lemma = predicative(word)
        if pos == "NNS":
            lemma = singularize(word)
        if pos.startswith(("VB", "MD")):
            lemma = conjugate(word, INFINITIVE) or word
        token.append(lemma.lower())
    return tokens


</t>
<t tx="karstenw.20230303132547.5">class Parser(_Parser):

    @others
parser = Parser(
     lexicon = os.path.join(MODULE, "de-lexicon.txt"),
   frequency = os.path.join(MODULE, "de-frequency.txt"),
  morphology = os.path.join(MODULE, "de-morphology.txt"),
     context = os.path.join(MODULE, "de-context.txt"),
     default = ("NN", "NE", "CARDNUM"),
    language = "de"
)

lexicon = parser.lexicon # Expose lexicon.

spelling = Spelling(
        path = os.path.join(MODULE, "de-spelling.txt")
)


</t>
<t tx="karstenw.20230303132547.6">def find_tokens(self, tokens, **kwargs):
    kwargs.setdefault("abbreviations", ABBREVIATIONS)
    kwargs.setdefault("replace", {})
    return _Parser.find_tokens(self, tokens, **kwargs)

</t>
<t tx="karstenw.20230303132547.7">def find_lemmata(self, tokens, **kwargs):
    return find_lemmata(tokens)

</t>
<t tx="karstenw.20230303132547.8">def find_tags(self, tokens, **kwargs):
    if kwargs.get("tagset") in (PENN, None):
        kwargs.setdefault("map", lambda token, tag: stts2penntreebank(token, tag))
    if kwargs.get("tagset") == UNIVERSAL:
        kwargs.setdefault("map", lambda token, tag: stts2universal(token, tag))
    if kwargs.get("tagset") is STTS:
        kwargs.setdefault("map", lambda token, tag: (token, tag))
    # The lexicon uses Swiss spelling: "ss" instead of "ß".
    # We restore the "ß" after parsing.
    tokens_ss = [t.replace("ß", "ss") for t in tokens]
    tokens_ss = _Parser.find_tags(self, tokens_ss, **kwargs)
    return [[w] + tokens_ss[i][1:] for i, w in enumerate(tokens)]

</t>
<t tx="karstenw.20230303132547.9">def tokenize(s, *args, **kwargs):
    """ Returns a list of sentences, where punctuation marks have been split from words.
    """
    return parser.find_tokens(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303132553.1"></t>
<t tx="karstenw.20230303132600.1"></t>
<t tx="karstenw.20230303132610.1"></t>
<t tx="karstenw.20230303132624.1">#### PATTERN | DE | RULE-BASED SHALLOW PARSER ######################################################
# Copyright (c) 2012 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303132635.1">from __future__ import absolute_import

from .__init__ import parse, commandline
commandline(parse)
</t>
<t tx="karstenw.20230303132715.1">#### PATTERN | DE | INFLECT ####################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2012 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).

################################################################################
# Regular expressions-based rules for German word inflection:
# - pluralization and singularization of nouns and adjectives,
# - conjugation of verbs,
# - attributive and predicative of adjectives,
# - comparative and superlative of adjectives.

# Accuracy (measured on CELEX German morphology word forms):
# 75% for gender()
# 72% for pluralize()
# 84% for singularize() (for nominative)
# 87% for Verbs.find_lemma()
# 87% for Verbs.find_lexeme()
# 98% for predicative

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303132723.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
import re

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

from pattern.text import Verbs as _Verbs
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    INDICATIVE, IMPERATIVE, SUBJUNCTIVE,
    PROGRESSIVE,
    PARTICIPLE, GERUND
)

sys.path.pop(0)

VERB, NOUN, ADJECTIVE, ADVERB = "VB", "NN", "JJ", "RB"

VOWELS = "aeiouy"
re_vowel = re.compile(r"a|e|i|o|u|y", re.I)
is_vowel = lambda ch: ch in VOWELS

#### ARTICLE ###################################################################
# German inflection of depends on gender, role and number + the
# determiner (if any).

# Inflection gender.
# Masculine is the most common, so it is the default for all functions.
MASCULINE, FEMININE, NEUTER, PLURAL = \
    MALE, FEMALE, NEUTRAL, PLURAL = \
        M, F, N, PL = "m", "f", "n", "p"

# Inflection role.
# - nom = subject, "Der Hund bellt" (the dog barks).
# - acc = object, "Das Mädchen küsst den Hund" (the girl kisses the dog).
# - dat = object (indirect), "Der Mann gibt einen Knochen zum Hund" (the man gives the dog a bone).
# - gen = property, "die Knochen des Hundes" (the dog's bone).

NOMINATIVE = SUBJECT = "nominative"
ACCUSATIVE = OBJECT = "accusative"
DATIVE = INDIRECT = "dative"
GENITIVE = PROPERTY = "genitive"
    

article_definite = {
    ("m", "nom"): "der", ("f", "nom"): "die", ("n", "nom"): "das", ("p", "nom"): "die",
    ("m", "acc"): "den", ("f", "acc"): "die", ("n", "acc"): "das", ("p", "acc"): "die",
    ("m", "dat"): "dem", ("f", "dat"): "der", ("n", "dat"): "dem", ("p", "dat"): "den",
    ("m", "gen"): "des", ("f", "gen"): "der", ("n", "gen"): "des", ("p", "gen"): "der",
}

article_indefinite = {
    ("m", "nom"): "ein"  , ("f", "nom"): "eine" , ("n", "nom"): "ein"  , ("p", "nom"): "eine",
    ("m", "acc"): "einen", ("f", "acc"): "eine" , ("n", "acc"): "ein"  , ("p", "acc"): "eine",
    ("m", "dat"): "einem", ("f", "dat"): "einer", ("n", "dat"): "einem", ("p", "dat"): "einen",
    ("m", "gen"): "eines", ("f", "gen"): "einer", ("n", "gen"): "eines", ("p", "gen"): "einer",
}


</t>
<t tx="karstenw.20230303132723.10">def decode_sz(s):
    return s.replace("ss", "ß")


</t>
<t tx="karstenw.20230303132723.11">class Verbs(_Verbs):

    @others
verbs = Verbs()

conjugate, lemma, lexeme, tenses = \
    verbs.conjugate, verbs.lemma, verbs.lexeme, verbs.tenses

#### ATTRIBUTIVE &amp; PREDICATIVE #####################################################################

# Strong inflection: no article.
adjectives_strong = {
    ("m", "nom"): "er", ("f", "nom"): "e" , ("n", "nom"): "es", ("p", "nom"): "e",
    ("m", "acc"): "en", ("f", "acc"): "e" , ("n", "acc"): "es", ("p", "acc"): "e",
    ("m", "dat"): "em", ("f", "dat"): "er", ("n", "dat"): "em", ("p", "dat"): "en",
    ("m", "gen"): "en", ("f", "gen"): "er", ("n", "gen"): "en", ("p", "gen"): "er",
}

# Mixed inflection: after indefinite article ein &amp; kein and possessive determiners.
adjectives_mixed = {
    ("m", "nom"): "er", ("f", "nom"): "e" , ("n", "nom"): "es", ("p", "nom"): "en",
    ("m", "acc"): "en", ("f", "acc"): "e" , ("n", "acc"): "es", ("p", "acc"): "en",
    ("m", "dat"): "en", ("f", "dat"): "en", ("n", "dat"): "en", ("p", "dat"): "en",
    ("m", "gen"): "en", ("f", "gen"): "en", ("n", "gen"): "en", ("p", "gen"): "en",
}

# Weak inflection: after definite article.
adjectives_weak = {
    ("m", "nom"): "e",  ("f", "nom"): "e" , ("n", "nom"): "e",  ("p", "nom"): "en",
    ("m", "acc"): "en", ("f", "acc"): "e" , ("n", "acc"): "e",  ("p", "acc"): "en",
    ("m", "dat"): "en", ("f", "dat"): "en", ("n", "dat"): "en", ("p", "dat"): "en",
    ("m", "gen"): "en", ("f", "gen"): "en", ("n", "gen"): "en", ("p", "gen"): "en",
}

# Uninflected + exceptions.
adjective_attributive = {
    "etwas" : "etwas",
    "genug" : "genug",
    "viel"  : "viel",
    "wenig" : "wenig"
}


</t>
<t tx="karstenw.20230303132723.12">def __init__(self):
    _Verbs.__init__(self, os.path.join(MODULE, "de-verbs.txt"),
        language = "de",
          format = [0, 1, 2, 3, 4, 5, 8, 17, 18, 19, 20, 21, 24, 52, 54,
                    53, 55, 56, 58, 59, 67, 68, 70, 71],
         default = {6: 4, 22: 20, 57: 55, 60: 58, 69: 67, 72: 70}
        )

</t>
<t tx="karstenw.20230303132723.13">def find_lemma(self, verb):
    """ Returns the base form of the given inflected verb, using
    a rule-based approach.
    """
    v = verb.lower()
    # Common prefixes: be-finden and emp-finden probably inflect like finden.
    if not (v.startswith("ge") and v.endswith("t")): # Probably gerund.
        for prefix in prefixes:
            if v.startswith(prefix) and v[len(prefix):] in self.inflections:
                return prefix + self.inflections[v[len(prefix):]]
    # Common sufixes: setze nieder =&gt; niedersetzen.
    b, suffix = " " in v and v.split()[:2] or (v, "")
    # Infinitive -ln: trommeln.
    if b.endswith(("ln", "rn")):
        return b
    # Lemmatize regular inflections.
    for x in ("test", "est", "end", "ten", "tet", "en", "et", "te", "st", "e", "t"):
        if b.endswith(x):
            b = b[:-len(x)]; break
    # Subjunctive: hielte =&gt; halten, schnitte =&gt; schneiden.
    for x, y in (
      ("ieb", "eib"), ( "ied", "eid"), ( "ief", "auf" ), ( "ieg", "eig" ), ("iel", "alt"),
      ("ien", "ein"), ("iess", "ass"), ( "ieß", "aß"  ), ( "iff", "eif" ), ("iss", "eiss"),
      ( "iß", "eiß"), (  "it", "eid"), ( "oss", "iess"), ( "öss", "iess")):
        if b.endswith(x):
            b = b[:-len(x)] + y; break
    b = b.replace("eeiss", "eiss")
    b = b.replace("eeid", "eit")
    # Subjunctive: wechselte =&gt; wechseln
    if not b.endswith(("e", "l")) and not (b.endswith("er") and len(b) &gt;= 3 and not b[-3] in VOWELS):
        b = b + "e"
    # abknallst != abknalln =&gt; abknallen
    if b.endswith(("hl", "ll", "ul", "eil")):
        b = b + "e"
    # Strip ge- from (likely) gerund:
    if b.startswith("ge") and v.endswith("t"):
        b = b[2:]
    # Corrections (these add about 1.5% accuracy):
    if b.endswith(("lnde", "rnde")):
        b = b[:-3]
    if b.endswith(("ae", "al", "öe", "üe")):
        b = b.rstrip("e") + "te"
    if b.endswith("äl"):
        b = b + "e"
    return suffix + b + "n"

</t>
<t tx="karstenw.20230303132723.14">def find_lexeme(self, verb):
    """ For a regular verb (base form), returns the forms using a rule-based approach.
    """
    v = verb.lower()
    # Stem = infinitive minus -en, -ln, -rn.
    b = b0 = re.sub("en$", "", re.sub("ln$", "l", re.sub("rn$", "r", v)))
    # Split common prefixes.
    x, x1, x2 = "", "", ""
    for prefix in prefix_separable:
        if v.startswith(prefix):
            b, x = b[len(prefix):], prefix
            x1 = (" " + x).rstrip()
            x2 = x + "ge"
            break
    # Present tense 1sg and subjunctive -el: handeln =&gt; ich handle, du handlest.
    pl = b.endswith("el") and b[:-2] + "l" or b
    # Present tense 1pl -el: handeln =&gt; wir handeln
    pw = v.endswith(("ln", "rn")) and v or b + "en"
    # Present tense ending in -d or -t gets -e:
    pr = b.endswith(("d", "t")) and b + "e" or b
    # Present tense 2sg gets -st, unless stem ends with -s or -z.
    p2 = pr.endswith(("s", "z")) and pr + "t" or pr + "st"
    # Present participle: spiel + -end, arbeiten + -d:
    pp = v.endswith(("en", "ln", "rn")) and v + "d" or v + "end"
    # Past tense regular:
    pt = encode_sz(pr) + "t"
    # Past participle: haushalten =&gt; hausgehalten
    ge = (v.startswith(prefix_inseparable) or b.endswith(("r", "t"))) and pt or "ge" + pt
    ge = x and x + "ge" + pt or ge
    # Present subjunctive: stem + -e, -est, -en, -et:
    s1 = encode_sz(pl)
    # Past subjunctive: past (usually with Umlaut) + -e, -est, -en, -et:
    s2 = encode_sz(pt)
    # Construct the lexeme:
    lexeme = a = [
        v,
        pl + "e" + x1, p2 + x1, pr + "t" + x1, pw + x1, pr + "t" + x1, pp,                 # present
        pt + "e" + x1, pt + "est" + x1, pt + "e" + x1, pt + "en" + x1, pt + "et" + x1, ge, # past
        b + "e" + x1, pr + "t" + x1, x + pw,                                               # imperative
        s1 + "e" + x1, s1 + "est" + x1, s1 + "en" + x1, s1 + "et" + x1,                    # subjunctive I
        s2 + "e" + x1, s2 + "est" + x1, s2 + "en" + x1, s2 + "et" + x1                     # subjunctive II
    ]
    # Encode Eszett (ß) and attempt to retrieve from the lexicon.
    # Decode Eszett for present and imperative.
    if encode_sz(v) in self:
        a = self[encode_sz(v)]
        a = [decode_sz(v) for v in a[:7]] + a[7:13] + [decode_sz(v) for v in a[13:20]] + a[20:]
    # Since the lexicon does not contain imperative for all verbs, don't simply return it.
    # Instead, update the rule-based lexeme with inflections from the lexicon.
    return [a[i] or lexeme[i] for i in range(len(a))]

</t>
<t tx="karstenw.20230303132723.15">def tenses(self, verb, parse=True):
    """ Returns a list of possible tenses for the given inflected verb.
    """
    tenses = _Verbs.tenses(self, verb, parse)
    if len(tenses) == 0:
        # auswirkte =&gt; wirkte aus
        for prefix in prefix_separable:
            if verb.startswith(prefix):
                tenses = _Verbs.tenses(self, verb[len(prefix):] + " " + prefix, parse)
                break
    return tenses

</t>
<t tx="karstenw.20230303132723.16">def attributive(adjective, gender=MALE, role=SUBJECT, article=None):
    """ For a predicative adjective, returns the attributive form (lowercase).
        In German, the attributive is formed with -e, -em, -en, -er or -es,
        depending on gender (masculine, feminine, neuter or plural) and role
        (nominative, accusative, dative, genitive).
    """
    w, g, c, a = \
        adjective.lower(), gender[:1].lower(), role[:3].lower(), article and article.lower() or None
    if w in adjective_attributive:
        return adjective_attributive[w]
    if a is None \
    or a in ("mir", "dir", "ihm") \
    or a in ("ein", "etwas", "mehr") \
    or a.startswith(("all", "mehrer", "wenig", "viel")):
        return w + adjectives_strong.get((g, c), "")
    if a.startswith(("ein", "kein")) \
    or a.startswith(("mein", "dein", "sein", "ihr", "Ihr", "unser", "euer")):
        return w + adjectives_mixed.get((g, c), "")
    if a in ("arm", "alt", "all", "der", "die", "das", "den", "dem", "des") \
    or a.startswith((
      "derselb", "derjenig", "jed", "jeglich", "jen", "manch",
      "dies", "solch", "welch")):
        return w + adjectives_weak.get((g, c), "")
    # Default to strong inflection.
    return w + adjectives_strong.get((g, c), "")


</t>
<t tx="karstenw.20230303132723.17">def predicative(adjective):
    """ Returns the predicative adjective (lowercase).
        In German, the attributive form preceding a noun is always used:
        "ein kleiner Junge" =&gt; strong, masculine, nominative,
        "eine schöne Frau" =&gt; mixed, feminine, nominative,
        "der kleine Prinz" =&gt; weak, masculine, nominative, etc.
        The predicative is useful for lemmatization.
    """
    w = adjective.lower()
    if len(w) &gt; 3:
        for suffix in ("em", "en", "er", "es", "e"):
            if w.endswith(suffix):
                b = w[:max(-len(suffix), -(len(w) - 3))]
                if b.endswith("bl"): # plausibles =&gt; plausibel
                    b = b[:-1] + "el"
                if b.endswith("pr"): # propres =&gt; proper
                    b = b[:-1] + "er"
                return b
    return w

#### COMPARATIVE &amp; SUPERLATIVE #####################################################################

COMPARATIVE = "er"
SUPERLATIVE = "st"


</t>
<t tx="karstenw.20230303132723.18">def grade(adjective, suffix=COMPARATIVE):
    """ Returns the comparative or superlative form of the given (inflected) adjective.
    """
    b = predicative(adjective)
    # groß =&gt; großt, schön =&gt; schönst
    if suffix == SUPERLATIVE and b.endswith(("s", "ß")):
        suffix = suffix[1:]
    # große =&gt; großere, schönes =&gt; schöneres
    return adjective[:len(b)] + suffix + adjective[len(b):]


</t>
<t tx="karstenw.20230303132723.19">def comparative(adjective):
    return grade(adjective, COMPARATIVE)


</t>
<t tx="karstenw.20230303132723.2">def definite_article(word, gender=MALE, role=SUBJECT):
    """ Returns the definite article (der/die/das/die) for a given word.
    """
    return article_definite.get((gender[:1].lower(), role[:3].lower()))


</t>
<t tx="karstenw.20230303132723.20">def superlative(adjective):
    return grade(adjective, SUPERLATIVE)

#print(comparative("schönes"))
#print(superlative("schönes"))
#print(superlative("große"))
</t>
<t tx="karstenw.20230303132723.3">def indefinite_article(word, gender=MALE, role=SUBJECT):
    """ Returns the indefinite article (ein) for a given word.
    """
    return article_indefinite.get((gender[:1].lower(), role[:3].lower()))

DEFINITE = "definite"
INDEFINITE = "indefinite"


</t>
<t tx="karstenw.20230303132723.4">def article(word, function=INDEFINITE, gender=MALE, role=SUBJECT):
    """ Returns the indefinite (ein) or definite (der/die/das/die) article for
    the given word.
    """
    return (    function == DEFINITE
            and definite_article(word, gender, role)
             or indefinite_article(word, gender, role) )
_article = article


</t>
<t tx="karstenw.20230303132723.5">def referenced(word, article=INDEFINITE, gender=MALE, role=SUBJECT):
    """ Returns a string with the article + the word.
    """
    return "%s %s" % (_article(word, article, gender, role), word)

#### GENDER ####################################################################

gender_masculine = (
    "ant", "ast", "ich", "ig", "ismus", "ling", "or", "us"
)
gender_feminine = (
    "a", "anz", "ei", "enz", "heit", "ie", "ik", "in", "keit", "schaf", "sion",
    "sis", "tät", "tion", "ung", "ur"
)
gender_neuter = (
    "chen", "icht", "il", "it", "lein", "ma", "ment", "tel", "tum", "um", "al",
    "an", "ar", "ät", "ent", "ett", "ier", "iv", "o", "on", "nis", "sal"
)
gender_majority_vote = {
    MASCULINE: (
        "ab", "af", "ag", "ak", "am", "an", "ar", "at", "au", "ch", "ck", "eb",
        "ef", "eg", "el", "er", "es", "ex", "ff", "go", "hn", "hs", "ib", "if",
        "ig", "ir", "kt", "lf", "li", "ll", "lm", "ls", "lt", "mi", "nd", "nk",
        "nn", "nt", "od", "of", "og", "or", "pf", "ph", "pp", "ps", "rb", "rd",
        "rf", "rg", "ri", "rl", "rm", "rr", "rs", "rt", "rz", "ss", "st", "tz",
        "ub", "uf", "ug", "uh", "un", "us", "ut", "xt", "zt"
    ),
    FEMININE: (
        "be", "ce", "da", "de", "dt", "ee", "ei", "et", "eu", "fe", "ft", "ge",
        "he", "hr", "ht", "ia", "ie", "ik", "in", "it", "iz", "ka", "ke", "la",
        "le", "me", "na", "ne", "ng", "nz", "on", "pe", "ra", "re", "se", "ta",
        "te", "ue", "ur", "ve", "ze"
    ),

    NEUTER: (
        "ad", "al", "as", "do", "ed", "eh", "em", "en", "hl", "id", "il", "im",
        "io", "is", "iv", "ix", "ld", "lk", "lo", "lz", "ma", "md", "mm", "mt",
        "no", "ns", "ol", "om", "op", "os", "ot", "pt", "rk", "rn", "ro", "to",
        "tt", "ul", "um", "uz"
    )
}


</t>
<t tx="karstenw.20230303132723.6">def gender(word, pos=NOUN):
    """ Returns the gender (MALE, FEMALE or NEUTRAL) for nouns (majority vote).
        Returns None for words that are not nouns.
    """
    w = word.lower()
    if pos == NOUN:
        # Default rules (baseline = 32%).
        if w.endswith(gender_masculine):
            return MASCULINE
        if w.endswith(gender_feminine):
            return FEMININE
        if w.endswith(gender_neuter):
            return NEUTER
        # Majority vote.
        for g in gender_majority_vote:
            if w.endswith(gender_majority_vote[g]):
                return g

#### PLURALIZE #################################################################

plural_inflections = [
    ("aal", "äle"   ), ("aat", "aaten"), ("abe", "aben" ), ("ach", "ächer"),
    ("ade", "aden"  ), ("age", "agen" ), ("ahn", "ahnen"), ("ahr", "ahre" ),
    ("akt", "akte"  ), ("ale", "alen" ), ("ame", "amen" ), ("amt", "ämter"),
    ("ane", "anen"  ), ("ang", "änge" ), ("ank", "änke" ), ("ann", "änner"),
    ("ant", "anten" ), ("aph", "aphen"), ("are", "aren" ), ("arn", "arne" ),
    ("ase", "asen"  ), ("ate", "aten" ), ("att", "ätter"), ("atz", "ätze" ),
    ("aum", "äume"  ), ("aus", "äuser"), ("bad", "bäder"), ("bel", "bel"  ),
    ("ben", "ben"   ), ("ber", "ber"  ), ("bot", "bote" ), ("che", "chen" ),
    ("chs", "chse"  ), ("cke", "cken" ), ("del", "del"  ), ("den", "den"  ),
    ("der", "der"   ), ("ebe", "ebe"  ), ("ede", "eden" ), ("ehl", "ehle" ),
    ("ehr", "ehr"   ), ("eil", "eile" ), ("eim", "eime" ), ("eis", "eise" ), ("eit", "eit"   ),
    ("ekt", "ekte"  ), ("eld", "elder"), ("ell", "elle" ), ("ene", "enen" ), ("enz", "enzen" ),
    ("erd", "erde"  ), ("ere", "eren" ), ("erk", "erke" ), ("ern", "erne" ), ("ert", "erte"  ),
    ("ese", "esen"  ), ("ess", "esse" ), ("est", "este" ), ("etz", "etze" ), ("eug", "euge"  ),
    ("eur", "eure"  ), ("fel", "fel"  ), ("fen", "fen"  ), ("fer", "fer"  ), ("ffe", "ffen"  ),
    ("gel", "gel"   ), ("gen", "gen"  ), ("ger", "ger"  ), ("gie", "gie"  ), ("hen", "hen"   ),
    ("her", "her"   ), ("hie", "hien" ), ("hle", "hlen" ), ("hme", "hmen" ), ("hne", "hnen"  ),
    ("hof", "höfe"  ), ("hre", "hren" ), ("hrt", "hrten"), ("hse", "hsen" ), ("hte", "hten"  ),
    ("ich", "iche"  ), ("ick", "icke" ), ("ide", "iden" ), ("ieb", "iebe" ), ("ief", "iefe"  ),
    ("ieg", "iege"  ), ("iel", "iele" ), ("ien", "ium"  ), ("iet", "iete" ), ("ife", "ifen"  ),
    ("iff", "iffe"  ), ("ift", "iften"), ("ige", "igen" ), ("ika", "ikum" ), ("ild", "ilder" ),
    ("ilm", "ilme"  ), ("ine", "inen" ), ("ing", "inge" ), ("ion", "ionen"), ("ise", "isen"  ),
    ("iss", "isse"  ), ("ist", "isten"), ("ite", "iten" ), ("itt", "itte" ), ("itz", "itze"  ),
    ("ium", "ium"   ), ("kel", "kel"  ), ("ken", "ken"  ), ("ker", "ker"  ), ("lag", "läge"  ),
    ("lan", "läne"  ), ("lar", "lare" ), ("lei", "leien"), ("len", "len"  ), ("ler", "ler"   ),
    ("lge", "lgen"  ), ("lie", "lien" ), ("lle", "llen" ), ("mel", "mel"  ), ("mer", "mer"   ),
    ("mme", "mmen"  ), ("mpe", "mpen" ), ("mpf", "mpfe" ), ("mus", "mus"  ), ("mut", "mut"   ),
    ("nat", "nate"  ), ("nde", "nden" ), ("nen", "nen"  ), ("ner", "ner"  ), ("nge", "ngen"  ),
    ("nie", "nien"  ), ("nis", "nisse"), ("nke", "nken" ), ("nkt", "nkte" ), ("nne", "nnen"  ),
    ("nst", "nste"  ), ("nte", "nten" ), ("nze", "nzen" ), ("ock", "öcke" ), ("ode", "oden"  ),
    ("off", "offe"  ), ("oge", "ogen" ), ("ohn", "öhne" ), ("ohr", "ohre" ), ("olz", "ölzer" ),
    ("one", "onen"  ), ("oot", "oote" ), ("opf", "öpfe" ), ("ord", "orde" ), ("orm", "ormen" ),
    ("orn", "örner" ), ("ose", "osen" ), ("ote", "oten" ), ("pel", "pel"  ), ("pen", "pen"   ),
    ("per", "per"   ), ("pie", "pien" ), ("ppe", "ppen" ), ("rag", "räge" ), ("rau", "raün"  ),
    ("rbe", "rben"  ), ("rde", "rden" ), ("rei", "reien"), ("rer", "rer"  ), ("rie", "rien"  ),
    ("rin", "rinnen"), ("rke", "rken" ), ("rot", "rote" ), ("rre", "rren" ), ("rte", "rten"  ),
    ("ruf", "rufe"  ), ("rzt", "rzte" ), ("sel", "sel"  ), ("sen", "sen"  ), ("ser", "ser"   ),
    ("sie", "sien"  ), ("sik", "sik"  ), ("sse", "ssen" ), ("ste", "sten" ), ("tag", "tage"  ),
    ("tel", "tel"   ), ("ten", "ten"  ), ("ter", "ter"  ), ("tie", "tien" ), ("tin", "tinnen"),
    ("tiv", "tive"  ), ("tor", "toren"), ("tte", "tten" ), ("tum", "tum"  ), ("tur", "turen" ),
    ("tze", "tzen"  ), ("ube", "uben" ), ("ude", "uden" ), ("ufe", "ufen" ), ("uge", "ugen"  ),
    ("uhr", "uhren" ), ("ule", "ulen" ), ("ume", "umen" ), ("ung", "ungen"), ("use", "usen"  ),
    ("uss", "üsse"  ), ("ute", "uten" ), ("utz", "utz"  ), ("ver", "ver"  ), ("weg", "wege"  ),
    ("zer", "zer"   ), ("zug", "züge" ), ("ück", "ücke" )
]


</t>
<t tx="karstenw.20230303132723.7">def pluralize(word, pos=NOUN, gender=MALE, role=SUBJECT, custom={}):
    """ Returns the plural of a given word.
        The inflection is based on probability rather than gender and role.
    """
    w = word.lower().capitalize()
    if word in custom:
        return custom[word]
    if pos == NOUN:
        for a, b in plural_inflections:
            if w.endswith(a):
                return w[:-len(a)] + b
        # Default rules (baseline = 69%).
        if w.startswith("ge"):
            return w
        if w.endswith("gie"):
            return w
        if w.endswith("e"):
            return w + "n"
        if w.endswith("ien"):
            return w[:-2] + "um"
        if w.endswith( ("au", "ein", "eit", "er", "en", "el", "chen", "mus",
                        "tät", "tik", "tum", "u")):
            return w
        if w.endswith(( "ant", "ei", "enz", "ion", "ist", "or", "schaft",
                        "tur", "ung")):
            return w + "en"
        if w.endswith("in"):
            return w + "nen"
        if w.endswith("nis"):
            return w + "se"
        if w.endswith(("eld", "ild", "ind")):
            return w + "er"
        if w.endswith("o"):
            return w + "s"
        if w.endswith("a"):
            return w[:-1] + "en"
        # Inflect common umlaut vowels: Kopf =&gt; Köpfe.
        if w.endswith(( "all", "and", "ang", "ank", "atz", "auf", "ock", "opf",
                        "uch", "uss")):
            umlaut = w[-3]
            umlaut = umlaut.replace("a", "ä")
            umlaut = umlaut.replace("o", "ö")
            umlaut = umlaut.replace("u", "ü")
            return w[:-3] + umlaut + w[-2:] + "e"
        for a, b in (
          ("ag",  "äge"),
          ("ann", "änner"),
          ("aum", "äume"),
          ("aus", "äuser"),
          ("zug", "züge")):
            if w.endswith(a):
                return w[:-len(a)] + b
        return w + "e"
    return w

#### SINGULARIZE ###################################################################################

singular_inflections = [
    ( "innen", "in" ), ( "täten", "tät"), ( "ahnen", "ahn"), ( "enten", "ent"), ( "räser", "ras"),
    ( "hrten", "hrt"), ( "ücher", "uch"), ( "örner", "orn"), ( "änder", "and"), ( "ürmer", "urm"),
    ( "ahlen", "ahl"), ( "uhren", "uhr"), ( "ätter", "att"), ( "suren", "sur"), ( "chten", "cht"),
    ( "kuren", "kur"), ( "erzen", "erz"), ( "güter", "gut"), ( "soren", "sor"), ( "änner", "ann"),
    ( "äuser", "aus"), ( "taten", "tat"), ( "isten", "ist"), ( "bäder", "bad"), ( "ämter", "amt"),
    ( "eiten", "eit"), ( "raten", "rat"), ( "ormen", "orm"), ( "ionen", "ion"), ( "nisse", "nis"),
    ( "ölzer", "olz"), ( "ungen", "ung"), ( "läser", "las"), ( "ächer", "ach"), ( "urten", "urt"),
    ( "enzen", "enz"), ( "aaten", "aat"), ( "aphen", "aph"), ( "öcher", "och"), ( "türen", "tür"),
    ( "sonen", "son"), ( "ühren", "ühr"), ( "ühner", "uhn"), ( "toren", "tor"), ( "örter", "ort"),
    ( "anten", "ant"), ( "räder", "rad"), ( "turen", "tur"), ( "äuler", "aul"), (  "änze", "anz"),
    (  "tten", "tte"), (  "mben", "mbe"), (  "ädte", "adt"), (  "llen", "lle"), (  "ysen", "yse"),
    (  "rben", "rbe"), (  "hsen", "hse"), (  "raün", "rau"), (  "rven", "rve"), (  "rken", "rke"),
    (  "ünge", "ung"), (  "üten", "üte"), (  "usen", "use"), (  "tien", "tie"), (  "läne", "lan"),
    (  "iben", "ibe"), (  "ifen", "ife"), (  "ssen", "sse"), (  "gien", "gie"), (  "eten", "ete"),
    (  "rden", "rde"), (  "öhne", "ohn"), (  "ärte", "art"), (  "ncen", "nce"), (  "ünde", "und"),
    (  "uben", "ube"), (  "lben", "lbe"), (  "üsse", "uss"), (  "agen", "age"), (  "räge", "rag"),
    (  "ogen", "oge"), (  "anen", "ane"), (  "sken", "ske"), (  "eden", "ede"), (  "össe", "oss"),
    (  "ürme", "urm"), (  "ggen", "gge"), (  "üren", "üre"), (  "nten", "nte"), (  "ühle", "ühl"),
    (  "änge", "ang"), (  "mmen", "mme"), (  "igen", "ige"), (  "nken", "nke"), (  "äcke", "ack"),
    (  "oden", "ode"), (  "oben", "obe"), (  "ähne", "ahn"), (  "änke", "ank"), (  "inen", "ine"),
    (  "seen", "see"), (  "äfte", "aft"), (  "ulen", "ule"), (  "äste", "ast"), (  "hren", "hre"),
    (  "öcke", "ock"), (  "aben", "abe"), (  "öpfe", "opf"), (  "ugen", "uge"), (  "lien", "lie"),
    (  "ände", "and"), (  "ücke", "ück"), (  "asen", "ase"), (  "aden", "ade"), (  "dien", "die"),
    (  "aren", "are"), (  "tzen", "tze"), (  "züge", "zug"), (  "üfte", "uft"), (  "hien", "hie"),
    (  "nden", "nde"), (  "älle", "all"), (  "hmen", "hme"), (  "ffen", "ffe"), (  "rmen", "rma"),
    (  "olen", "ole"), (  "sten", "ste"), (  "amen", "ame"), (  "höfe", "hof"), (  "üste", "ust"),
    (  "hnen", "hne"), (  "ähte", "aht"), (  "umen", "ume"), (  "nnen", "nne"), (  "alen", "ale"),
    (  "mpen", "mpe"), (  "mien", "mie"), (  "rten", "rte"), (  "rien", "rie"), (  "äute", "aut"),
    (  "uden", "ude"), (  "lgen", "lge"), (  "ngen", "nge"), (  "iden", "ide"), (  "ässe", "ass"),
    (  "osen", "ose"), (  "lken", "lke"), (  "eren", "ere"), (  "üche", "uch"), (  "lüge", "lug"),
    (  "hlen", "hle"), (  "isen", "ise"), (  "ären", "äre"), (  "töne", "ton"), (  "onen", "one"),
    (  "rnen", "rne"), (  "üsen", "üse"), (  "haün", "hau"), (  "pien", "pie"), (  "ihen", "ihe"),
    (  "ürfe", "urf"), (  "esen", "ese"), (  "ätze", "atz"), (  "sien", "sie"), (  "läge", "lag"),
    (  "iven", "ive"), (  "ämme", "amm"), (  "äufe", "auf"), (  "ppen", "ppe"), (  "enen", "ene"),
    (  "lfen", "lfe"), (  "äume", "aum"), (  "nien", "nie"), (  "unen", "une"), (  "cken", "cke"),
    (  "oten", "ote"), (   "mie", "mie"), (   "rie", "rie"), (   "sis", "sen"), (   "rin", "rin"),
    (   "ein", "ein"), (   "age", "age"), (   "ern", "ern"), (   "ber", "ber"), (   "ion", "ion"),
    (   "inn", "inn"), (   "ben", "ben"), (   "äse", "äse"), (   "eis", "eis"), (   "hme", "hme"),
    (   "iss", "iss"), (   "hen", "hen"), (   "fer", "fer"), (   "gie", "gie"), (   "fen", "fen"),
    (   "her", "her"), (   "ker", "ker"), (   "nie", "nie"), (   "mer", "mer"), (   "ler", "ler"),
    (   "men", "men"), (   "ass", "ass"), (   "ner", "ner"), (   "per", "per"), (   "rer", "rer"),
    (   "mus", "mus"), (   "abe", "abe"), (   "ter", "ter"), (   "ser", "ser"), (   "äle", "aal"),
    (   "hie", "hie"), (   "ger", "ger"), (   "tus", "tus"), (   "gen", "gen"), (   "ier", "ier"),
    (   "ver", "ver"), (   "zer", "zer"),
]

singular = {
    "Löwen": "Löwe",
}


</t>
<t tx="karstenw.20230303132723.8">def singularize(word, pos=NOUN, gender=MALE, role=SUBJECT, custom={}):
    """ Returns the singular of a given word.
        The inflection is based on probability rather than gender and role.
    """
    w = word.lower().capitalize()
    if word in custom:
        return custom[word]
    if word in singular:
        return singular[word]
    if pos == NOUN:
        for a, b in singular_inflections:
            if w.endswith(a):
                return w[:-len(a)] + b
        # Default rule: strip known plural suffixes (baseline = 51%).
        for suffix in ("nen", "en", "n", "e", "er", "s"):
            if w.endswith(suffix):
                w = w[:-len(suffix)]
                break
        # Corrections (these add about 1% accuracy):
        if w.endswith(("rr", "rv", "nz")):
            return w + "e"
        return w
    return w

#### VERB CONJUGATION ##########################################################
# The verb table was trained on CELEX and contains the top 2000 most frequent verbs.

prefix_inseparable = ( "be", "emp", "ent", "er", "ge", "miss", "über", "unter",
                       "ver", "voll", "wider", "zer" )

prefix_separable = (
    "ab", "an", "auf", "aus", "bei", "durch", "ein", "fort", "mit", "nach",
    "vor", "weg", "zurück", "zusammen", "zu", "dabei", "daran", "da", "empor",
    "entgegen", "entlang", "fehl", "fest", "gegenüber", "gleich", "herab",
    "heran", "herauf", "heraus", "herum", "her", "hinweg", "hinzu", "hin",
    "los", "nieder", "statt", "umher", "um", "weg", "weiter", "wieder",
    "zwischen"
) + ( # There are many more...
     "dort", "fertig", "frei", "gut", "heim", "hoch", "klein", "klar",
     "nahe", "offen", "richtig"
)
prefixes = prefix_inseparable + prefix_separable


</t>
<t tx="karstenw.20230303132723.9">def encode_sz(s):
    return s.replace("ß", "ss")


</t>
<t tx="karstenw.20230303132745.1"></t>
<t tx="karstenw.20230303132747.1">#### PATTERN | EN ##############################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

################################################################################
# English linguistical tools using fast regular expressions.

@others
if __name__ == "__main__":
    commandline(parse)
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303132805.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

# Import parser base classes.
from pattern.text import (
         Lexicon, Model, Morphology, Context, Parser
     as _Parser, ngrams, pprint, commandline, PUNCTUATION )

# Import parser universal tagset.
from pattern.text import (
    penntreebank2universal,
    PTB, PENN, UNIVERSAL,
    NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X
)
# Import parse tree base classes.
from pattern.text.tree import (
    Tree, Text, Sentence, Slice, Chunk, PNPChunk, Chink, Word, table,
    SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA, AND, OR
)
# Import sentiment analysis base classes.
from pattern.text import (
    Sentiment as _Sentiment, NOUN, VERB, ADJECTIVE, ADVERB
)
# Import spelling base class.
from pattern.text import (
    Spelling
)
# Import verb tenses.
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    PROGRESSIVE,
    PARTICIPLE
)
# Import inflection functions.
from pattern.text.en.inflect import (
    article, referenced, DEFINITE, INDEFINITE,
    pluralize, singularize, NOUN, VERB, ADJECTIVE,
    grade, comparative, superlative, COMPARATIVE, SUPERLATIVE,
    verbs, conjugate, lemma, lexeme, tenses,
    predicative, attributive
)
# Import quantification functions.
from pattern.text.en.inflect_quantify import (
    number, numerals, quantify, reflect
)
# Import mood &amp; modality functions.
from pattern.text.en.modality import (
    mood, INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE,
    modality, uncertain, EPISTEMIC,
    negated
)
# Import all submodules.
from pattern.text.en import inflect
from pattern.text.en import wordnet
from pattern.text.en import wordlist

sys.path.pop(0)

#--- ENGLISH PARSER ------------------------------------------------------------


</t>
<t tx="karstenw.20230303132805.10">def parsetree(s, *args, **kwargs):
    """ Returns a parsed Text from the given string.
    """
    return Text(parse(s, *args, **kwargs))


</t>
<t tx="karstenw.20230303132805.11">def tree(s, token=[WORD, POS, CHUNK, PNP, REL, LEMMA]):
    """ Returns a parsed Text from the given parsed string.
    """
    return Text(s, token)


</t>
<t tx="karstenw.20230303132805.12">def tag(s, tokenize=True, encoding="utf-8", **kwargs):
    """ Returns a list of (token, tag)-tuples from the given string.
    """
    tags = []
    for sentence in parse(s, tokenize, True, False, False, False, encoding, **kwargs).split():
        for token in sentence:
            tags.append((token[0], token[1]))
    return tags


</t>
<t tx="karstenw.20230303132805.13">def keywords(s, top=10, **kwargs):
    """ Returns a sorted list of keywords in the given string.
    """
    return parser.find_keywords(s, **dict({
        "frequency": parser.frequency,
              "top": top,
              "pos": ("NN",),
           "ignore": ("rt",)}, **kwargs))


</t>
<t tx="karstenw.20230303132805.14">def suggest(w):
    """ Returns a list of (word, confidence)-tuples of spelling corrections.
    """
    return spelling.suggest(w)


</t>
<t tx="karstenw.20230303132805.15">def polarity(s, **kwargs):
    """ Returns the sentence polarity (positive/negative) between -1.0 and 1.0.
    """
    return sentiment(s, **kwargs)[0]


</t>
<t tx="karstenw.20230303132805.16">def subjectivity(s, **kwargs):
    """ Returns the sentence subjectivity (objective/subjective) between 0.0 and 1.0.
    """
    return sentiment(s, **kwargs)[1]


</t>
<t tx="karstenw.20230303132805.17">def positive(s, threshold=0.1, **kwargs):
    """ Returns True if the given sentence has a positive sentiment (polarity &gt;= threshold).
    """
    return polarity(s, **kwargs) &gt;= threshold

split = tree # Backwards compatibility.

#---------------------------------------------------------------------------------------------------
# python -m pattern.en xml -s "The cat sat on the mat." -OTCL

</t>
<t tx="karstenw.20230303132805.2">def find_lemmata(tokens):
    """ Annotates the tokens with lemmata for plural nouns and conjugated verbs,
        where each token is a [word, part-of-speech] list.
    """
    for token in tokens:
        word, pos, lemma = token[0], token[1], token[0]
        # cats =&gt; cat
        if pos == "NNS":
            lemma = singularize(word)
        # sat =&gt; sit
        if pos.startswith(("VB", "MD")):
            lemma = conjugate(word, INFINITIVE) or word
        token.append(lemma.lower())
    return tokens


</t>
<t tx="karstenw.20230303132805.3">class Parser(_Parser):

    @others
</t>
<t tx="karstenw.20230303132805.4">def find_lemmata(self, tokens, **kwargs):
    return find_lemmata(tokens)

</t>
<t tx="karstenw.20230303132805.5">def find_tags(self, tokens, **kwargs):
    if kwargs.get("tagset") in (PENN, None):
        kwargs.setdefault("map", lambda token, tag: (token, tag))
    if kwargs.get("tagset") == UNIVERSAL:
        kwargs.setdefault("map",
                    lambda token, tag: penntreebank2universal(token, tag))
    return _Parser.find_tags(self, tokens, **kwargs)


</t>
<t tx="karstenw.20230303132805.6">class Sentiment(_Sentiment):

    @others
</t>
<t tx="karstenw.20230303132805.7">def load(self, path=None):
    _Sentiment.load(self, path)
    # Map "terrible" to adverb "terribly" (+1% accuracy)
    if not path:
        for w, pos in list(dict.items(self)):
            if "JJ" in pos:
                if w.endswith("y"):
                    w = w[:-1] + "i"
                if w.endswith("le"):
                    w = w[:-2]
                p, s, i = pos["JJ"]
                self.annotate(w + "ly", "RB", p, s, i)

</t>
<t tx="karstenw.20230303132805.8">def tokenize(s, *args, **kwargs):
    """ Returns a list of sentences, where punctuation marks have been split from words.
    """
    return parser.find_tokens(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303132805.9">def parse(s, *args, **kwargs):
    """ Returns a tagged Unicode string.
    """
    return parser.parse(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303132812.1">#### PATTERN | EN | PARSER COMMAND-LINE ############################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303132818.1">from __future__ import absolute_import

from .__init__ import parse, commandline
commandline(parse)
</t>
<t tx="karstenw.20230303132820.1">#### PATTERN | EN | QUANTIFY #######################################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).

####################################################################################################
# Transforms numeral strings to numbers, and numbers (int, float) to numeral strings.
# Approximates quantities of objects ("dozens of chickens" etc.)

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303132841.1">#### PATTERN | EN | INFLECT ####################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).

################################################################################
# Regular expressions-based rules for English word inflection:
# - pluralization and singularization of nouns and adjectives,
# - conjugation of verbs,
# - comparative and superlative of adjectives.

# Accuracy (measured on CELEX English morphology word forms):
# 95% for pluralize()
# 96% for singularize()
# 95% for Verbs.find_lemma() (for regular verbs)
# 96% for Verbs.find_lexeme() (for regular verbs)

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303132850.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
import re

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

from pattern.text import Verbs as _Verbs
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    PROGRESSIVE,
    PARTICIPLE
)

sys.path.pop(0)

VERB, NOUN, ADJECTIVE, ADVERB = "VB", "NN", "JJ", "RB"

VOWELS = "aeiouy"
re_vowel = re.compile(r"a|e|i|o|u|y", re.I)
is_vowel = lambda ch: ch in VOWELS

#### ARTICLE ###################################################################
# Based on the Ruby Linguistics module by Michael Granger:
# http://www.deveiate.org/projects/Linguistics/wiki/English

RE_ARTICLE = list(map(lambda x: (re.compile(x[0]), x[1]), (
    # exceptions: an hour, an honor
    (r"euler|hour(?!i)|heir|honest|hono", "an"),
    # Abbreviations:
    # strings of capitals starting with a vowel-sound consonant followed by
    # another consonant, which are not likely to be real words.
    (r"(?!FJO|[HLMNS]Y.|RY[EO]|SQU|(F[LR]?|[HL]|MN?|N|RH?|S[CHKLMNPTVW]?|X(YL)?)[AEIOU])[FHLMNRSX][A-Z]", "an"),
    (r"^[aefhilmnorsx][.-]"  , "an"), # hyphenated: an f-16, an e-mail
    (r"^[a-z][.-]"           , "a" ), # hyphenated: a b-52
    (r"^[^aeiouy]"           , "a" ), # consonants: a bear
    (r"^e[uw]"               , "a" ), # -eu like "you": a european
    (r"^onc?e"               , "a" ), #  -o like "wa" : a one-liner
    (r"uni([^nmd]|mo)"       , "a" ), #  -u like "you": a university
    (r"^u[bcfhjkqrst][aeiou]", "a" ), #  -u like "you": a uterus
    (r"^[aeiou]"             , "an"), # vowels: an owl
    (r"y(b[lor]|cl[ea]|fere|gg|p[ios]|rou|tt)", "an"), # y like "i": an yclept, a year
    (r""                     , "a" )  # guess "a"
)))


</t>
<t tx="karstenw.20230303132850.10">def find_lemma(self, verb):
    """ Returns the base form of the given inflected verb, using a rule-based approach.
        This is problematic if a verb ending in -e is given in the past tense or gerund.
    """
    v = verb.lower()
    b = False
    if v in ("'m", "'re", "'s", "n't"):
        return "be"
    if v in ("'d", "'ll"):
        return "will"
    if v in ("'ve"):
        return "have"
    if v.endswith("s"):
        if v.endswith("ies") and len(v) &gt; 3 and v[-4] not in VOWELS:
            return v[:-3] + "y" # complies =&gt; comply
        if v.endswith(("sses", "shes", "ches", "xes")):
            return v[:-2]     # kisses =&gt; kiss
        return v[:-1]
    if v.endswith("ied") and re_vowel.search(v[:-3]) is not None:
        return v[:-3] + "y"     # envied =&gt; envy
    if v.endswith("ing") and re_vowel.search(v[:-3]) is not None:
        v = v[:-3]; b = True;   # chopping =&gt; chopp
    if v.endswith("ed") and re_vowel.search(v[:-2]) is not None:
        v = v[:-2]; b = True;   # danced =&gt; danc
    if b:
        # Doubled consonant after short vowel: chopp =&gt; chop.
        if len(v) &gt; 3 and v[-1] == v[-2] and v[-3] in VOWELS and v[-4] not in VOWELS and not v.endswith("ss"):
            return v[:-1]
        if v.endswith(("ick", "ack")):
            return v[:-1]     # panick =&gt; panic
        # Guess common cases where the base form ends in -e:
        if v.endswith(("v", "z", "c", "i")):
            return v + "e"      # danc =&gt; dance
        if v.endswith("g") and v.endswith(("dg", "lg", "ng", "rg")):
            return v + "e"      # indulg =&gt; indulge
        if v.endswith(("b", "d", "g", "k", "l", "m", "r", "s", "t")) \
          and len(v) &gt; 2 and v[-2] in VOWELS and not v[-3] in VOWELS \
          and not v.endswith("er"):
            return v + "e"      # generat =&gt; generate
        if v.endswith("n") and v.endswith(("an", "in")) and not v.endswith(("ain", "oin", "oan")):
            return v + "e"      # imagin =&gt; imagine
        if v.endswith("l") and len(v) &gt; 1 and v[-2] not in VOWELS:
            return v + "e"      # squabbl =&gt; squabble
        if v.endswith("f") and len(v) &gt; 2 and v[-2] in VOWELS and v[-3] not in VOWELS:
            return v + "e"      # chaf =&gt; chafed
        if v.endswith("e"):
            return v + "e"      # decre =&gt; decree
        if v.endswith(("th", "ang", "un", "cr", "vr", "rs", "ps", "tr")):
            return v + "e"
    return v

</t>
<t tx="karstenw.20230303132850.11">def find_lexeme(self, verb):
    """ For a regular verb (base form), returns the forms using a rule-based approach.
    """
    v = verb.lower()
    if len(v) &gt; 1 and v.endswith("e") and v[-2] not in VOWELS:
        # Verbs ending in a consonant followed by "e": dance, save, devote, evolve.
        return [v, v, v, v + "s", v, v[:-1] + "ing"] + [v + "d"] * 6
    if len(v) &gt; 1 and v.endswith("y") and v[-2] not in VOWELS:
        # Verbs ending in a consonant followed by "y": comply, copy, magnify.
        return [v, v, v, v[:-1] + "ies", v, v + "ing"] + [v[:-1] + "ied"] * 6
    if v.endswith(("ss", "sh", "ch", "x")):
        # Verbs ending in sibilants: kiss, bless, box, polish, preach.
        return [v, v, v, v + "es", v, v + "ing"] + [v + "ed"] * 6
    if v.endswith("ic"):
        # Verbs ending in -ic: panic, mimic.
        return [v, v, v, v + "es", v, v + "king"] + [v + "ked"] * 6
    if len(v) &gt; 1 and v[-1] not in VOWELS and v[-2] not in VOWELS:
        # Verbs ending in a consonant cluster: delight, clamp.
        return [v, v, v, v + "s", v, v + "ing"] + [v + "ed"] * 6
    if (len(v) &gt; 1 and v.endswith(("y", "w")) and v[-2] in VOWELS) \
    or (len(v) &gt; 2 and v[-1] not in VOWELS and v[-2] in VOWELS and v[-3] in VOWELS) \
    or (len(v) &gt; 3 and v[-1] not in VOWELS and v[-3] in VOWELS and v[-4] in VOWELS):
        # Verbs ending in a long vowel or diphthong followed by a consonant: paint, devour, play.
        return [v, v, v, v + "s", v, v + "ing"] + [v + "ed"] * 6
    if len(v) &gt; 2 and v[-1] not in VOWELS and v[-2] in VOWELS and v[-3] not in VOWELS:
        # Verbs ending in a short vowel followed by a consonant: chat, chop, or compel.
        return [v, v, v, v + "s", v, v + v[-1] + "ing"] + [v + v[-1] + "ed"] * 6
    return [v, v, v, v + "s", v, v + "ing"] + [v + "ed"] * 6

</t>
<t tx="karstenw.20230303132850.12">def _count_syllables(word):
    """ Returns the estimated number of syllables in the word by counting vowel-groups.
    """
    n = 0
    p = False # True if the previous character was a vowel.
    for ch in word.endswith("e") and word[:-1] or word:
        v = ch in VOWELS
        n += int(v and not p)
        p = v
    return n


</t>
<t tx="karstenw.20230303132850.13">def grade(adjective, suffix=COMPARATIVE):
    """ Returns the comparative or superlative form of the given adjective.
    """
    n = _count_syllables(adjective)
    if adjective in grade_irregular:
        # A number of adjectives inflect irregularly.
        return grade_irregular[adjective][suffix != COMPARATIVE]
    elif adjective in grade_uninflected:
        # A number of adjectives don't inflect at all.
        return "%s %s" % (suffix == COMPARATIVE and "more" or "most", adjective)
    elif n &lt;= 2 and adjective.endswith("e"):
        # With one syllable and ending with an e: larger, wiser.
        suffix = suffix.lstrip("e")
    elif n == 1 and len(adjective) &gt;= 3 \
     and adjective[-1] not in VOWELS and adjective[-2] in VOWELS and adjective[-3] not in VOWELS:
        # With one syllable ending with consonant-vowel-consonant: bigger, thinner.
        if not adjective.endswith(("w")): # Exceptions: lower, newer.
            suffix = adjective[-1] + suffix
    elif n == 1:
        # With one syllable ending with more consonants or vowels: briefer.
        pass
    elif n == 2 and adjective.endswith("y"):
        # With two syllables ending with a y: funnier, hairier.
        adjective = adjective[:-1] + "i"
    elif n == 2 and adjective[-2:] in ("er", "le", "ow"):
        # With two syllables and specific suffixes: gentler, narrower.
        pass
    else:
        # With three or more syllables: more generous, more important.
        return "%s %s" % (suffix == COMPARATIVE and "more" or "most", adjective)
    return adjective + suffix


</t>
<t tx="karstenw.20230303132850.14">def comparative(adjective):
    return grade(adjective, COMPARATIVE)


</t>
<t tx="karstenw.20230303132850.15">def superlative(adjective):
    return grade(adjective, SUPERLATIVE)

#### ATTRIBUTIVE &amp; PREDICATIVE #####################################################################


</t>
<t tx="karstenw.20230303132850.16">def attributive(adjective):
    return adjective


</t>
<t tx="karstenw.20230303132850.17">def predicative(adjective):
    return adjective
</t>
<t tx="karstenw.20230303132850.2">def definite_article(word):
    return "the"


</t>
<t tx="karstenw.20230303132850.3">def indefinite_article(word):
    """ Returns the indefinite article for a given word.
        For example: indefinite_article("university") =&gt; "a" university.
    """
    word = word.split(" ")[0]
    for rule, article in RE_ARTICLE:
        if rule.search(word) is not None:
            return article

DEFINITE, INDEFINITE = \
    "definite", "indefinite"


</t>
<t tx="karstenw.20230303132850.4">def article(word, function=INDEFINITE):
    """ Returns the indefinite (a or an) or definite (the) article for the given word.
    """
    return function == DEFINITE and definite_article(word) or indefinite_article(word)

_article = article


</t>
<t tx="karstenw.20230303132850.5">def referenced(word, article=INDEFINITE):
    """ Returns a string with the article + the word.
    """
    return "%s %s" % (_article(word, article), word)

#print referenced("hour")
#print referenced("FBI")
#print referenced("bear")
#print referenced("one-liner")
#print referenced("european")
#print referenced("university")
#print referenced("uterus")
#print referenced("owl")
#print referenced("yclept")
#print referenced("year")

#### PLURALIZE #####################################################################################
# Based on "An Algorithmic Approach to English Pluralization" by Damian Conway:
# http://www.csse.monash.edu.au/~damian/papers/HTML/Plurals.html

# Prepositions are used in forms like "mother-in-law" and "man at arms".
plural_prepositions = set((
    "about"  , "before" , "during", "of"   , "till" ,
    "above"  , "behind" , "except", "off"  , "to"   ,
    "across" , "below"  , "for"   , "on"   , "under",
    "after"  , "beneath", "from"  , "onto" , "until",
    "among"  , "beside" , "in"    , "out"  , "unto" ,
    "around" , "besides", "into"  , "over" , "upon" ,
    "at"     , "between", "near"  , "since", "with" ,
    "athwart", "betwixt",
               "beyond",
               "but",
               "by"))

# Inflection rules that are either:
# - general,
# - apply to a certain category of words,
# - apply to a certain category of words only in classical mode,
# - apply only in classical mode.
# Each rule is a (suffix, inflection, category, classic)-tuple.
plural_rules = [
       # 0) Indefinite articles and demonstratives.
    ((   r"^a$|^an$", "some"       , None, False),
     (     r"^this$", "these"      , None, False),
     (     r"^that$", "those"      , None, False),
     (      r"^any$", "all"        , None, False)
    ), # 1) Possessive adjectives.
    ((       r"^my$", "our"        , None, False),
     (     r"^your$", "your"       , None, False),
     (      r"^thy$", "your"       , None, False),
     (r"^her$|^his$", "their"      , None, False),
     (      r"^its$", "their"      , None, False),
     (    r"^their$", "their"      , None, False)
    ), # 2) Possessive pronouns.
    ((     r"^mine$", "ours"       , None, False),
     (    r"^yours$", "yours"      , None, False),
     (    r"^thine$", "yours"      , None, False),
     (r"^her$|^his$", "theirs"     , None, False),
     (      r"^its$", "theirs"     , None, False),
     (    r"^their$", "theirs"     , None, False)
    ), # 3) Personal pronouns.
    ((        r"^I$", "we"         , None, False),
     (       r"^me$", "us"         , None, False),
     (   r"^myself$", "ourselves"  , None, False),
     (      r"^you$", "you"        , None, False),
     (r"^thou$|^thee$", "ye"       , None, False),
     ( r"^yourself$", "yourself"   , None, False),
     (  r"^thyself$", "yourself"   , None, False),
     ( r"^she$|^he$", "they"       , None, False),
     (r"^it$|^they$", "they"       , None, False),
     (r"^her$|^him$", "them"       , None, False),
     (r"^it$|^them$", "them"       , None, False),
     (  r"^herself$", "themselves" , None, False),
     (  r"^himself$", "themselves" , None, False),
     (   r"^itself$", "themselves" , None, False),
     ( r"^themself$", "themselves" , None, False),
     (  r"^oneself$", "oneselves"  , None, False)
    ), # 4) Words that do not inflect.
    ((          r"$", ""  , "uninflected", False),
     (          r"$", ""  , "uncountable", False),
     (         r"s$", "s" , "s-singular" , False),
     (      r"fish$", "fish"       , None, False),
     (r"([- ])bass$", "\\1bass"    , None, False),
     (       r"ois$", "ois"        , None, False),
     (     r"sheep$", "sheep"      , None, False),
     (      r"deer$", "deer"       , None, False),
     (       r"pox$", "pox"        , None, False),
     (r"([A-Z].*)ese$", "\\1ese"   , None, False),
     (      r"itis$", "itis"       , None, False),
     (r"(fruct|gluc|galact|lact|ket|malt|rib|sacchar|cellul)ose$", "\\1ose", None, False)
    ), # 5) Irregular plural forms (e.g., mongoose, oxen).
    ((     r"atlas$", "atlantes"   , None, True ),
     (     r"atlas$", "atlases"    , None, False),
     (      r"beef$", "beeves"     , None, True ),
     (   r"brother$", "brethren"   , None, True ),
     (     r"child$", "children"   , None, False),
     (    r"corpus$", "corpora"    , None, True ),
     (    r"corpus$", "corpuses"   , None, False),
     (      r"^cow$", "kine"       , None, True ),
     ( r"ephemeris$", "ephemerides", None, False),
     (  r"ganglion$", "ganglia"    , None, True ),
     (     r"genie$", "genii"      , None, True ),
     (     r"genus$", "genera"     , None, False),
     (  r"graffito$", "graffiti"   , None, False),
     (      r"loaf$", "loaves"     , None, False),
     (     r"money$", "monies"     , None, True ),
     (  r"mongoose$", "mongooses"  , None, False),
     (    r"mythos$", "mythoi"     , None, False),
     (   r"octopus$", "octopodes"  , None, True ),
     (      r"opus$", "opera"      , None, True ),
     (      r"opus$", "opuses"     , None, False),
     (       r"^ox$", "oxen"       , None, False),
     (     r"penis$", "penes"      , None, True ),
     (     r"penis$", "penises"    , None, False),
     ( r"soliloquy$", "soliloquies", None, False),
     (    r"testis$", "testes"     , None, False),
     (    r"trilby$", "trilbys"    , None, False),
     (      r"turf$", "turves"     , None, True ),
     (     r"numen$", "numena"     , None, False),
     (   r"occiput$", "occipita"   , None, True )
    ), # 6) Irregular inflections for common suffixes (e.g., synopses, mice, men).
    ((       r"man$", "men"        , None, False),
     (    r"person$", "people"     , None, False),
     (r"([lm])ouse$", "\\1ice"     , None, False),
     (     r"tooth$", "teeth"      , None, False),
     (     r"goose$", "geese"      , None, False),
     (      r"foot$", "feet"       , None, False),
     (      r"zoon$", "zoa"        , None, False),
     ( r"([csx])is$", "\\1es"      , None, False)
    ), # 7) Fully assimilated classical inflections
       #    (e.g., vertebrae, codices).
    ((        r"ex$", "ices" , "ex-ices" , False),
     (        r"ex$", "ices" , "ex-ices*", True ), # * = classical mode
     (        r"um$", "a"    ,    "um-a" , False),
     (        r"um$", "a"    ,    "um-a*", True ),
     (        r"on$", "a"    ,    "on-a" , False),
     (         r"a$", "ae"   ,    "a-ae" , False),
     (         r"a$", "ae"   ,    "a-ae*", True )
    ), # 8) Classical variants of modern inflections
       #    (e.g., stigmata, soprani).
    ((      r"trix$", "trices"     , None, True),
     (       r"eau$", "eaux"       , None, True),
     (       r"ieu$", "ieu"        , None, True),
     ( r"([iay])nx$", "\\1nges"    , None, True),
     (        r"en$", "ina"  ,  "en-ina*", True),
     (         r"a$", "ata"  ,   "a-ata*", True),
     (        r"is$", "ides" , "is-ides*", True),
     (        r"us$", "i"    ,    "us-i*", True),
     (        r"us$", "us "  ,   "us-us*", True),
     (         r"o$", "i"    ,     "o-i*", True),
     (          r"$", "i"    ,      "-i*", True),
     (          r"$", "im"   ,     "-im*", True)
    ), # 9) -ch, -sh and -ss take -es in the plural
       #    (e.g., churches, classes).
    ((   r"([cs])h$", "\\1hes"     , None, False),
     (        r"ss$", "sses"       , None, False),
     (         r"x$", "xes"        , None, False)
    ), # 10) -f or -fe sometimes take -ves in the plural
       #     (e.g, lives, wolves).
    (( r"([aeo]l)f$", "\\1ves"     , None, False),
     ( r"([^d]ea)f$", "\\1ves"     , None, False),
     (       r"arf$", "arves"      , None, False),
     (r"([nlw]i)fe$", "\\1ves"     , None, False),
    ), # 11) -y takes -ys if preceded by a vowel, -ies otherwise
       #     (e.g., storeys, Marys, stories).
    ((r"([aeiou])y$", "\\1ys"      , None, False),
     (r"([A-Z].*)y$", "\\1ys"      , None, False),
     (         r"y$", "ies"        , None, False)
    ), # 12) -o sometimes takes -os, -oes otherwise.
       #     -o is preceded by a vowel takes -os
       #     (e.g., lassos, potatoes, bamboos).
    ((         r"o$", "os",        "o-os", False),
     (r"([aeiou])o$", "\\1os"      , None, False),
     (         r"o$", "oes"        , None, False)
    ), # 13) Miltary stuff
       #     (e.g., Major Generals).
    ((         r"l$", "ls", "general-generals", False),
    ), # 14) Assume that the plural takes -s
       #     (cats, programmes, ...).
    ((          r"$", "s"          , None, False),)
]

# For performance, compile the regular expressions once:
plural_rules = [[(re.compile(r[0]), r[1], r[2], r[3]) for r in grp] for grp in plural_rules]

# Suffix categories.
plural_categories = {
    "uninflected": [
        "bison"      , "debris"     , "headquarters" , "news"       , "swine"        ,
        "bream"      , "diabetes"   , "herpes"       , "pincers"    , "trout"        ,
        "breeches"   , "djinn"      , "high-jinks"   , "pliers"     , "tuna"         ,
        "britches"   , "eland"      , "homework"     , "proceedings", "whiting"      ,
        "carp"       , "elk"        , "innings"      , "rabies"     , "wildebeest"   ,
        "chassis"    , "flounder"   , "jackanapes"   , "salmon"     ,
        "clippers"   , "gallows"    , "mackerel"     , "scissors"   ,
        "cod"        , "graffiti"   , "measles"      , "series"     ,
        "contretemps",                "mews"         , "shears"     ,
        "corps"      ,                "mumps"        , "species"
        ],
    "uncountable": [
        "advice"     , "fruit"      , "ketchup"      , "meat"       , "sand"         ,
        "bread"      , "furniture"  , "knowledge"    , "mustard"    , "software"     ,
        "butter"     , "garbage"    , "love"         , "news"       , "understanding",
        "cheese"     , "gravel"     , "luggage"      , "progress"   , "water"        ,
        "electricity", "happiness"  , "mathematics"  , "research"   ,
        "equipment"  , "information", "mayonnaise"   , "rice"
        ],
    "s-singular": [
        "acropolis"  , "caddis"     , "dais"         , "glottis"    , "pathos"       ,
        "aegis"      , "cannabis"   , "digitalis"    , "ibis"       , "pelvis"       ,
        "alias"      , "canvas"     , "epidermis"    , "lens"       , "polis"        ,
        "asbestos"   , "chaos"      , "ethos"        , "mantis"     , "rhinoceros"   ,
        "bathos"     , "cosmos"     , "gas"          , "marquis"    , "sassafras"    ,
        "bias"       ,                "glottis"      , "metropolis" , "trellis"
        ],
    "ex-ices": [
        "codex"      , "murex"      , "silex"
        ],
    "ex-ices*": [
        "apex"       , "index"      , "pontifex"     , "vertex"     ,
        "cortex"     , "latex"      , "simplex"      , "vortex"
        ],
    "um-a": [
        "agendum"    , "candelabrum", "desideratum"  , "extremum"   , "stratum"      ,
        "bacterium"  , "datum"      , "erratum"      , "ovum"
        ],
    "um-a*": [
        "aquarium"   , "emporium"   , "maximum"      , "optimum"    , "stadium"      ,
        "compendium" , "enconium"   , "medium"       , "phylum"     , "trapezium"    ,
        "consortium" , "gymnasium"  , "memorandum"   , "quantum"    , "ultimatum"    ,
        "cranium"    , "honorarium" , "millenium"    , "rostrum"    , "vacuum"       ,
        "curriculum" , "interregnum", "minimum"      , "spectrum"   , "velum"        ,
        "dictum"     , "lustrum"    , "momentum"     , "speculum"
        ],
    "on-a": [
        "aphelion"   , "hyperbaton" , "perihelion"   ,
        "asyndeton"  , "noumenon"   , "phenomenon"   ,
        "criterion"  , "organon"    , "prolegomenon"
        ],
    "a-ae": [
        "alga"       , "alumna"     , "vertebra"
        ],
    "a-ae*": [
        "abscissa"   , "aurora"     , "hyperbola"    , "nebula"     ,
        "amoeba"     , "formula"    , "lacuna"       , "nova"       ,
        "antenna"    , "hydra"      , "medusa"       , "parabola"
        ],
    "en-ina*": [
        "foramen"    , "lumen"      , "stamen"
    ],
    "a-ata*": [
        "anathema"   , "dogma"      , "gumma"        , "miasma"     , "stigma"       ,
        "bema"       , "drama"      , "lemma"        , "schema"     , "stoma"        ,
        "carcinoma"  , "edema"      , "lymphoma"     , "oedema"     , "trauma"       ,
        "charisma"   , "enema"      , "magma"        , "sarcoma"    ,
        "diploma"    , "enigma"     , "melisma"      , "soma"       ,
        ],
    "is-ides*": [
        "clitoris"   , "iris"
        ],
    "us-i*": [
        "focus"      , "nimbus"     , "succubus"     ,
        "fungus"     , "nucleolus"  , "torus"        ,
        "genius"     , "radius"     , "umbilicus"    ,
        "incubus"    , "stylus"     , "uterus"
        ],
    "us-us*": [
        "apparatus"  , "hiatus"     , "plexus"       , "status" ,
        "cantus"     , "impetus"    , "prospectus"   ,
        "coitus"     , "nexus"      , "sinus"        ,
        ],
    "o-i*": [
        "alto"       , "canto"      , "crescendo"    , "soprano"    ,
        "basso"      , "contralto"  , "solo"         , "tempo"
        ],
    "-i*": [
        "afreet"     , "afrit"      , "efreet"
        ],
    "-im*": [
        "cherub"     , "goy"        , "seraph"
        ],
    "o-os": [
        "albino"     , "dynamo"     , "guano"        , "lumbago"    , "photo"        ,
        "archipelago", "embryo"     , "inferno"      , "magneto"    , "pro"          ,
        "armadillo"  , "fiasco"     , "jumbo"        , "manifesto"  , "quarto"       ,
        "commando"   , "generalissimo",                "medico"     , "rhino"        ,
        "ditto"      , "ghetto"     , "lingo"        , "octavo"     , "stylo"
        ],
    "general-generals": [
        "Adjutant"   , "Brigadier"  , "Lieutenant"   , "Major"      , "Quartermaster",
        "adjutant"   , "brigadier"  , "lieutenant"   , "major"      , "quartermaster"
        ]
}


</t>
<t tx="karstenw.20230303132850.6">def pluralize(word, pos=NOUN, custom={}, classical=True):
    """ Returns the plural of a given word, e.g., child =&gt; children.
        Handles nouns and adjectives, using classical inflection by default
        (i.e., where "matrix" pluralizes to "matrices" and not "matrixes").
        The custom dictionary is for user-defined replacements.
    """
    if word in custom:
        return custom[word]
    # Recurse genitives.
    # Remove the apostrophe and any trailing -s,
    # form the plural of the resultant noun, and then append an apostrophe (dog's =&gt; dogs').
    if word.endswith(("'", "'s")):
        w = word.rstrip("'s")
        w = pluralize(w, pos, custom, classical)
        if w.endswith("s"):
            return w + "'"
        else:
            return w + "'s"
    # Recurse compound words
    # (e.g., Postmasters General, mothers-in-law, Roman deities).
    w = word.replace("-", " ").split(" ")
    if len(w) &gt; 1:
        if w[1] == "general" or \
           w[1] == "General" and \
           w[0] not in plural_categories["general-generals"]:
            return word.replace(w[0], pluralize(w[0], pos, custom, classical))
        elif w[1] in plural_prepositions:
            return word.replace(w[0], pluralize(w[0], pos, custom, classical))
        else:
            return word.replace(w[-1], pluralize(w[-1], pos, custom, classical))
    # Only a very few number of adjectives inflect.
    n = range(len(plural_rules))
    if pos.startswith(ADJECTIVE):
        n = [0, 1]
    # Apply pluralization rules.
    for i in n:
        for suffix, inflection, category, classic in plural_rules[i]:
            # A general rule, or a classic rule in classical mode.
            if category is None:
                if not classic or (classic and classical):
                    if suffix.search(word) is not None:
                        return suffix.sub(inflection, word)
            # A rule pertaining to a specific category of words.
            if category is not None:
                if word in plural_categories[category] and (not classic or (classic and classical)):
                    if suffix.search(word) is not None:
                        return suffix.sub(inflection, word)
    return word

#print pluralize("part-of-speech")
#print pluralize("child")
#print pluralize("dog's")
#print pluralize("wolf")
#print pluralize("bear")
#print pluralize("kitchen knife")
#print pluralize("octopus", classical=True)
#print pluralize("matrix", classical=True)
#print pluralize("matrix", classical=False)
#print pluralize("my", pos=ADJECTIVE)

#### SINGULARIZE ###################################################################################
# Adapted from Bermi Ferrer's Inflector for Python:
# http://www.bermi.org/inflector/

# Copyright (c) 2006 Bermi Ferrer Martinez
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software to deal in this software without restriction, including
# without limitation the rights to use, copy, modify, merge, publish,
# distribute, sublicense, and/or sell copies of this software, and to permit
# persons to whom this software is furnished to do so, subject to the following
# condition:
#
# THIS SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THIS SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THIS SOFTWARE.

singular_rules = [
    (r'(?i)(.)ae$'            , '\\1a'    ),
    (r'(?i)(.)itis$'          , '\\1itis' ),
    (r'(?i)(.)eaux$'          , '\\1eau'  ),
    (r'(?i)(quiz)zes$'        , '\\1'     ),
    (r'(?i)(matr)ices$'       , '\\1ix'   ),
    (r'(?i)(ap|vert|ind)ices$', '\\1ex'   ),
    (r'(?i)^(ox)en'           , '\\1'     ),
    (r'(?i)(alias|status)es$' , '\\1'     ),
    (r'(?i)([octop|vir])i$'   , '\\1us'  ),
    (r'(?i)(cris|ax|test)es$' , '\\1is'   ),
    (r'(?i)(shoe)s$'          , '\\1'     ),
    (r'(?i)(o)es$'            , '\\1'     ),
    (r'(?i)(bus)es$'          , '\\1'     ),
    (r'(?i)([m|l])ice$'       , '\\1ouse' ),
    (r'(?i)(x|ch|ss|sh)es$'   , '\\1'     ),
    (r'(?i)(m)ovies$'         , '\\1ovie' ),
    (r'(?i)(.)ombies$'        , '\\1ombie'),
    (r'(?i)(s)eries$'         , '\\1eries'),
    (r'(?i)([^aeiouy]|qu)ies$', '\\1y'    ),
        # -f, -fe sometimes take -ves in the plural
        # (e.g., lives, wolves).
    (r"([aeo]l)ves$"          , "\\1f"    ),
    (r"([^d]ea)ves$"          , "\\1f"    ),
    (r"arves$"                , "arf"     ),
    (r"erves$"                , "erve"    ),
    (r"([nlw]i)ves$"          , "\\1fe"   ),
    (r'(?i)([lr])ves$'        , '\\1f'    ),
    (r"([aeo])ves$"           , "\\1ve"   ),
    (r'(?i)(sive)s$'          , '\\1'     ),
    (r'(?i)(tive)s$'          , '\\1'     ),
    (r'(?i)(hive)s$'          , '\\1'     ),
    (r'(?i)([^f])ves$'        , '\\1fe'   ),
    # -ses suffixes.
    (r'(?i)(^analy)ses$'      , '\\1sis'  ),
    (r'(?i)((a)naly|(b)a|(d)iagno|(p)arenthe|(p)rogno|(s)ynop|(t)he)ses$', '\\1\\2sis'),
    (r'(?i)(.)opses$'         , '\\1opsis'),
    (r'(?i)(.)yses$'          , '\\1ysis' ),
    (r'(?i)(h|d|r|o|n|b|cl|p)oses$', '\\1ose'),
    (r'(?i)(fruct|gluc|galact|lact|ket|malt|rib|sacchar|cellul)ose$', '\\1ose'),
    (r'(?i)(.)oses$'          , '\\1osis' ),
    # -a
    (r'(?i)([ti])a$'          , '\\1um'   ),
    (r'(?i)(n)ews$'           , '\\1ews'  ),
    (r'(?i)s$'                , ''        ),
]

# For performance, compile the regular expressions only once:
singular_rules = [(re.compile(r[0]), r[1]) for r in singular_rules]

singular_uninflected = set((
    "bison"      , "debris"   , "headquarters", "pincers"    , "trout"     ,
    "bream"      , "diabetes" , "herpes"      , "pliers"     , "tuna"      ,
    "breeches"   , "djinn"    , "high-jinks"  , "proceedings", "whiting"   ,
    "britches"   , "eland"    , "homework"    , "rabies"     , "wildebeest",
    "carp"       , "elk"      , "innings"     , "salmon"     ,
    "chassis"    , "flounder" , "jackanapes"  , "scissors"   ,
    "christmas"  , "gallows"  , "mackerel"    , "series"     ,
    "clippers"   , "georgia"  , "measles"     , "shears"     ,
    "cod"        , "graffiti" , "mews"        , "species"    ,
    "contretemps",              "mumps"       , "swine"      ,
    "corps"      ,              "news"        , "swiss"      ,
))
singular_uncountable = set((
    "advice"     , "equipment", "happiness"   , "luggage"    , "news"      , "software"     ,
    "bread"      , "fruit"    , "information" , "mathematics", "progress"  , "understanding",
    "butter"     , "furniture", "ketchup"     , "mayonnaise" , "research"  , "water"        ,
    "cheese"     , "garbage"  , "knowledge"   , "meat"       , "rice"      ,
    "electricity", "gravel"   , "love"        , "mustard"    , "sand"      ,
))
singular_ie = set((
    "alergie"    , "cutie"    , "hoagie"      , "newbie"     , "softie"    , "veggie"       ,
    "auntie"     , "doggie"   , "hottie"      , "nightie"    , "sortie"    , "weenie"       ,
    "beanie"     , "eyrie"    , "indie"       , "oldie"      , "stoolie"   , "yuppie"       ,
    "birdie"     , "freebie"  , "junkie"      , "^pie"       , "sweetie"   , "zombie"       ,
    "bogie"      , "goonie"   , "laddie"      , "pixie"      , "techie"    ,
    "bombie"     , "groupie"  , "laramie"     , "quickie"    , "^tie"      ,
    "collie"     , "hankie"   , "lingerie"    , "reverie"    , "toughie"   ,
    "cookie"     , "hippie"   , "meanie"      , "rookie"     , "valkyrie"  ,
))
singular_irregular = {
       "atlantes": "atlas",
        "atlases": "atlas",
           "axes": "axe",
         "beeves": "beef",
       "brethren": "brother",
       "children": "child",
        "corpora": "corpus",
       "corpuses": "corpus",
    "ephemerides": "ephemeris",
           "feet": "foot",
        "ganglia": "ganglion",
          "geese": "goose",
         "genera": "genus",
          "genii": "genie",
       "graffiti": "graffito",
         "helves": "helve",
           "kine": "cow",
         "leaves": "leaf",
         "loaves": "loaf",
            "men": "man",
      "mongooses": "mongoose",
         "monies": "money",
          "moves": "move",
         "mythoi": "mythos",
         "numena": "numen",
       "occipita": "occiput",
      "octopodes": "octopus",
          "opera": "opus",
         "opuses": "opus",
            "our": "my",
           "oxen": "ox",
          "penes": "penis",
        "penises": "penis",
         "people": "person",
          "sexes": "sex",
    "soliloquies": "soliloquy",
          "teeth": "tooth",
         "testes": "testis",
        "trilbys": "trilby",
         "turves": "turf",
            "zoa": "zoon",
}


</t>
<t tx="karstenw.20230303132850.7">def singularize(word, pos=NOUN, custom={}):
    """ Returns the singular of a given word.
    """
    if word in custom:
        return custom[word]
    # Recurse compound words (e.g. mothers-in-law).
    if "-" in word:
        w = word.split("-")
        if len(w) &gt; 1 and w[1] in plural_prepositions:
            return singularize(w[0], pos, custom) + "-" + "-".join(w[1:])
    # dogs' =&gt; dog's
    if word.endswith("'"):
        return singularize(word[:-1]) + "'s"
    w = word.lower()
    for x in singular_uninflected:
        if x.endswith(w):
            return word
    for x in singular_uncountable:
        if x.endswith(w):
            return word
    for x in singular_ie:
        if w.endswith(x + "s"):
            return w
    for x in singular_irregular:
        if w.endswith(x):
            return re.sub('(?i)' + x + '$', singular_irregular[x], word)
    for suffix, inflection in singular_rules:
        m = suffix.search(word)
        g = m and m.groups() or []
        if m:
            for k in range(len(g)):
                if g[k] is None:
                    inflection = inflection.replace('\\' + str(k + 1), '')
            return suffix.sub(inflection, word)
    return word

#### VERB CONJUGATION ##############################################################################


</t>
<t tx="karstenw.20230303132850.8">class Verbs(_Verbs):

    @others
verbs = Verbs()

conjugate, lemma, lexeme, tenses = \
    verbs.conjugate, verbs.lemma, verbs.lexeme, verbs.tenses

#print conjugate("imaginarify", "part", parse=True)
#print conjugate("imaginarify", "part", parse=False)

#### COMPARATIVE &amp; SUPERLATIVE #####################################################################

VOWELS = "aeiouy"

grade_irregular = {
       "bad": (  "worse", "worst"),
       "far": ("further", "farthest"),
      "good": ( "better", "best"),
      "hind": ( "hinder", "hindmost"),
       "ill": (  "worse", "worst"),
      "less": ( "lesser", "least"),
    "little": (   "less", "least"),
      "many": (   "more", "most"),
      "much": (   "more", "most"),
      "well": ( "better", "best")
}

grade_uninflected = ["giant", "glib", "hurt", "known", "madly"]

COMPARATIVE = "er"
SUPERLATIVE = "est"


</t>
<t tx="karstenw.20230303132850.9">def __init__(self):
    _Verbs.__init__(self, os.path.join(MODULE, "en-verbs.txt"),
        language = "en",
          format = [0, 1, 2, 3, 7, 8, 17, 18, 19, 23, 25, 24, 16, 9, 10, 11, 15, 33, 26, 27, 28, 32],
         default = {
             1: 0, 2: 0, 3: 0, 7: 0,  # present singular =&gt; infinitive ("I walk")
             4: 7, 5: 7, 6: 7,          # present plural
            17: 25, 18: 25, 19: 25, 23: 25, # past singular
            20: 23, 21: 23, 22: 23,         # past plural
             9: 16, 10: 16, 11: 16, 15: 16, # present singular negated
            12: 15, 13: 15, 14: 15,         # present plural negated
            26: 33, 27: 33, 28: 33,         # past singular negated
            29: 32, 30: 32, 31: 32, 32: 33  # past plural negated
        })

</t>
<t tx="karstenw.20230303132853.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
import re

from math import log, ceil

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

from pattern.text.en.inflect import pluralize, referenced

sys.path.pop(0)

####################################################################################################

NUMERALS = {
    "zero"  : 0, "ten"       : 10, "twenty"  : 20,
    "one"   : 1, "eleven"    : 11, "thirty"  : 30,
    "two"   : 2, "twelve"    : 12, "forty"   : 40,
    "three" : 3, "thirteen"  : 13, "fifty"   : 50,
    "four"  : 4, "fourteen"  : 14, "sixty"   : 60,
    "five"  : 5, "fifteen"   : 15, "seventy" : 70,
    "six"   : 6, "sixteen"   : 16, "eighty"  : 80,
    "seven" : 7, "seventeen" : 17, "ninety"  : 90,
    "eight" : 8, "eighteen"  : 18,
    "nine"  : 9, "nineteen"  : 19
}

NUMERALS_INVERSE = dict((i, w) for w, i in NUMERALS.items()) # 0 =&gt; "zero"
NUMERALS_VERBOSE = {
    "half"  : ( 1, 0.5),
    "dozen" : (12, 0.0),
    "score" : (20, 0.0)
}

ORDER = ["hundred", "thousand"] + [m + "illion" for m in ("m", "b", "tr",
    "quadr",
    "quint",
    "sext",
    "sept",
    "oct",
    "non",
    "dec",
    "undec",
    "duodec",
    "tredec",
    "quattuordec",
    "quindec",
    "sexdec",
    "septemdec",
    "octodec",
    "novemdec",
    "vigint"
)]

# {"hundred": 100, "thousand": 1000, ...}
O = {
    ORDER[0]: 100,
    ORDER[1]: 1000
}
for i, k in enumerate(ORDER[2:]):
    O[k] = 1000000 * 1000 ** i

ZERO, MINUS, RADIX, THOUSANDS, CONJUNCTION = \
    "zero", "minus", "point", ",", "and"


</t>
<t tx="karstenw.20230303132853.2">def zshift(s):
    """ Returns a (string, count)-tuple, with leading zeros strippped from the string and counted.
    """
    s = s.lstrip()
    i = 0
    while s.startswith((ZERO, "0")):
        s = re.sub(r"^(0|%s)\s*" % ZERO, "", s, 1)
        i = i + 1
    return s, i

#print zshift("zero one")  # ("one", 1)
#print zshift("0 0 seven") # ("seven", 2)

#--- STRING TO NUMBER ------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303132853.3">def number(s):
    """ Returns the given numeric string as a float or an int.
        If no number can be parsed from the string, returns 0.
        For example:
        number("five point two million") =&gt; 5200000
        number("seventy-five point two") =&gt; 75.2
        number("three thousand and one") =&gt; 3001
    """
    s = s.strip()
    s = s.lower()
    # Negative number.
    if s.startswith(MINUS):
        return -number(s.replace(MINUS, "", 1))
    # Strip commas and dashes ("seventy-five").
    # Split into integral and fractional part.
    s = s.replace("&amp;", " %s " % CONJUNCTION)
    s = s.replace(THOUSANDS, "")
    s = s.replace("-", " ")
    s = s.split(RADIX)
    # Process fractional part.
    # Extract all the leading zeros.
    if len(s) &gt; 1:
        f = " ".join(s[1:])      # zero point zero twelve =&gt; zero twelve
        f, z = zshift(f)              # zero twelve =&gt; (1, "twelve")
        f = float(number(f))          # "twelve" =&gt; 12.0
        f /= 10**(len(str(int(f))) + z) # 10**(len("12")+1) = 1000; 12.0 / 1000 =&gt; 0.012
    else:
        f = 0
    i = n = 0
    s = s[0].split()
    for j, x in enumerate(s):
        if x in NUMERALS:
            # Map words from the dictionary of numerals: "eleven" =&gt; 11.
            i += NUMERALS[x]
        elif x in NUMERALS_VERBOSE:
            # Map words from alternate numerals: "two dozen" =&gt; 2 * 12
            i = i * NUMERALS_VERBOSE[x][0] + NUMERALS_VERBOSE[x][1]
        elif x in O:
            # Map thousands from the dictionary of orders.
            # When a thousand is encountered, the subtotal is shifted to the total
            # and we start a new subtotal. An exception to this is when we
            # encouter two following thousands (e.g. two million vigintillion is one subtotal).
            i *= O[x]
            if j &lt; len(s) - 1 and s[j + 1] in O:
                continue
            if O[x] &gt; 100:
                n += i
                i = 0
        elif x == CONJUNCTION:
            pass
        else:
            # Words that are not in any dicionary may be numbers (e.g. "2.5" =&gt; 2.5).
            try:
                i += "." in x and float(x) or int(x)
            except:
                pass
    return n + i + f

#print number("five point two septillion")
#print number("seventy-five point two")
#print number("three thousand and one")
#print number("1.2 million point two")
#print number("nothing")

#--- NUMBER TO STRING ------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303132853.4">def numerals(n, round=2):
    """ Returns the given int or float as a string of numerals.
        By default, the fractional part is rounded to two decimals.
        For example:
        numerals(4011) =&gt; four thousand and eleven
        numerals(2.25) =&gt; two point twenty-five
        numerals(2.249) =&gt; two point twenty-five
        numerals(2.249, round=3) =&gt; two point two hundred and forty-nine
    """
    if isinstance(n, str):
        if n.isdigit():
            n = int(n)
        else:
            # If the float is given as a string, extract the length of the fractional part.
            if round is None:
                round = len(n.split(".")[1])
            n = float(n)
    # For negative numbers, simply prepend minus.
    if n &lt; 0:
        return "%s %s" % (MINUS, numerals(abs(n)))
    # Split the number into integral and fractional part.
    # Converting the integral part to a long ensures a better accuracy during the recursion.
    i = int(n // 1)
    f = n - i
    # The remainder, which we will stringify in recursion.
    r = 0
    if i in NUMERALS_INVERSE: # 11 =&gt; eleven
        # Map numbers from the dictionary to numerals: 11 =&gt; "eleven".
        s = NUMERALS_INVERSE[i]
    elif i &lt; 100:
        # Map tens + digits: 75 =&gt; 70+5 =&gt; "seventy-five".
        s = numerals((i // 10) * 10) + "-" + numerals(i % 10)
    elif i &lt; 1000:
        # Map hundreds: 500 =&gt; 5*100 =&gt; "five hundred".
        # Store the remainders (tens + digits).
        s = numerals(i // 100) + " " + ORDER[0]
        r = i % 100
    else:
        # Map thousands by extracting the order (thousand/million/billion/...).
        # Store and recurse the remainder.
        s = ""
        o, base = 1, 1000
        while i &gt; base:
            o += 1
            base *= 1000
        while o &gt; len(ORDER) - 1:
            s += " " + ORDER[-1] # This occurs for consecutive thousands: million vigintillion.
            o -= len(ORDER) - 1
        s = "%s %s%s" % (numerals(i // int(base / 1000)), (o &gt; 1 and ORDER[o - 1] or ""), s)
        r = i % (base / 1000)
    if f != 0:
        # Map the fractional part: "two point twenty-five" =&gt; 2.25.
        # We cast it to a string first to find all the leading zeros.
        # This actually seems more accurate than calculating the leading zeros,
        # see also: http://python.org/doc/2.5.1/tut/node16.html.
        # Some rounding occurs.
        f = ("%." + str(round is None and 2 or round) + "f") % f
        f = f.replace("0.", "", 1).rstrip("0")
        f, z = zshift(f)
        f = f and " %s%s %s" % (RADIX, " %s" % ZERO * z, numerals(int(f))) or ""
    else:
        f = ""
    if r == 0:
        return s + f
    elif r &gt;= 1000:
        # Separate hundreds and thousands with a comma: two million, three hundred thousand.
        return "%s%s %s" % (s, THOUSANDS, numerals(r) + f)
    elif r &lt;= 100:
        # Separate hundreds and tens with "and": two thousand three hundred and five.
        return "%s %s %s" % (s, CONJUNCTION, numerals(r) + f)
    else:
        return "%s %s" % (s, numerals(r) + f)

#--- APPROXIMATE -----------------------------------------------------------------------------------
# Based on the Ruby Linguistics module by Michael Granger:
# http://www.deveiate.org/projects/Linguistics/wiki/English

NONE      = "no"          #  0
PAIR      = "a pair of"   #  2
SEVERAL   = "several"     #  3-7
NUMBER    = "a number of" #  8-17
SCORE     = "a score of"  # 18-22
DOZENS    = "dozens of"   # 22-200
COUNTLESS = "countless"

quantify_custom_plurals = {}


</t>
<t tx="karstenw.20230303132853.5">def approximate(word, amount=1, plural={}):
    """ Returns an approximation of the number of given objects.
        Two objects are described as being "a pair",
        smaller than eight is "several",
        smaller than twenty is "a number of",
        smaller than two hundred are "dozens",
        anything bigger is described as being tens or hundreds of thousands or millions.
        For example: approximate("chicken", 100) =&gt; "dozens of chickens".
    """
    try:
        p = pluralize(word, custom=plural)
    except:
        raise TypeError("can't pluralize %s (not a string)" % word.__class__.__name__)
    # Anything up to 200.
    if amount == 0:
        return "%s %s" % (NONE, p)
    if amount == 1:
        return referenced(word) # "a" chicken, "an" elephant
    if amount == 2:
        return "%s %s" % (PAIR, p)
    if 3 &lt;= amount &lt; 8:
        return "%s %s" % (SEVERAL, p)
    if 8 &lt;= amount &lt; 18:
        return "%s %s" % (NUMBER, p)
    if 18 &lt;= amount &lt; 23:
        return "%s %s" % (SCORE, p)
    if 23 &lt;= amount &lt; 200:
        return "%s %s" % (DOZENS, p)
    if amount &gt; 10000000:
        return "%s %s" % (COUNTLESS, p)
    # Hundreds and thousands.
    thousands = int(log(amount, 10) / 3)
    hundreds = ceil(log(amount, 10) % 3) - 1
    h = hundreds == 2 and "hundreds of " or (hundreds == 1 and "tens of " or "")
    t = thousands &gt; 0 and pluralize(ORDER[thousands]) + " of " or ""
    return "%s%s%s" % (h, t, p)

#print approximate("chicken", 0)
#print approximate("chicken", 1)
#print approximate("chicken", 2)
#print approximate("chicken", 3)
#print approximate("chicken", 10)
#print approximate("chicken", 100)
#print approximate("chicken", 1000)
#print approximate("chicken", 10000)
#print approximate("chicken", 100000)
#print approximate("chicken", 1000000)
#print approximate("chicken", 10000000)
#print approximate("chicken", 100000000)
#print approximate("chicken", 10000000000)

#--- COUNT -----------------------------------------------------------------------------------------

# count(word, amount, plural={})
# count([word1, word2, ...], plural={})
# counr({word1:0, word2:0, ...}, plural={})


</t>
<t tx="karstenw.20230303132853.6">def count(*args, **kwargs):
    """ Returns an approximation of the entire set.
        Identical words are grouped and counted and then quantified with an approximation.
    """
    if len(args) == 2 and isinstance(args[0], str):
        return approximate(args[0], args[1], kwargs.get("plural", {}))
    if len(args) == 1 and isinstance(args[0], str) and "amount" in kwargs:
        return approximate(args[0], kwargs["amount"], kwargs.get("plural", {}))
    if len(args) == 1 and isinstance(args[0], dict):
        count = args[0]
    if len(args) == 1 and isinstance(args[0], (list, tuple)):
        # Keep a count of each item in the list.
        count = {}
        for word in args[0]:
            try:
                count.setdefault(word, 0)
                count[word] += 1
            except:
                raise TypeError("can't count %s (not a string)" % word.__class__.__name__)
    # Create an iterator of (count, item) tuples, sorted highest-first.
    s = [(count[word], word) for word in count]
    s = max([n for (n, w) in s]) &gt; 1 and reversed(sorted(s)) or s
    # Concatenate approximate quantities of each item,
    # starting with the one that has the highest occurence.
    phrase = []
    for i, (n, word) in enumerate(s):
        phrase.append(approximate(word, n, kwargs.get("plural", {})))
        phrase.append(i == len(count) - 2 and " and " or ", ")
    return "".join(phrase[:-1])

quantify = count

#print count(["goose", "goose", "duck", "chicken", "chicken", "chicken"])
#print count(["penguin", "polar bear"])
#print count(["whale"])

#--- REFLECT ---------------------------------------------------------------------------------------

readable_types = (
    ("^&lt;type '"        , ""),
    ("^&lt;class '(.*)'\&gt;", "\\1 class"),
    ("'&gt;"              , ""),
    ("pyobjc"          , "PyObjC"),
    ("objc_class"      , "Objective-C class"),
    ("objc"            , "Objective-C"),
    ("&lt;objective-c class  (.*) at [0-9][0-9|a-z]*&gt;" , "Objective-C \\1 class"),
    ("bool"            , "boolean"),
    ("int"             , "integer"),
    ("long"            , "long integer"),
    ("float"           , "float"),
    ("str"             , "string"),
    ("unicode"         , "string"),
    ("dict"            , "dictionary"),
    ("NoneType"        , "None type"),
    ("instancemethod"  , "instance method"),
    ("builtin_function_or_method" , "built-in function"),
    ("classobj"        , "class object"),
    ("\."              , " "),
    ("_"               , " ")
)


</t>
<t tx="karstenw.20230303132853.7">def reflect(object, quantify=True, replace=readable_types):
    """ Returns the type of each object in the given object.
        - For modules, this means classes and functions etc.
        - For list and tuples, means the type of each item in it.
        - For other objects, means the type of the object itself.
    """
    _type = lambda object: type(object).__name__
    types = []
    # Classes and modules with a __dict__ attribute listing methods, functions etc.
    if hasattr(object, "__dict__"):
        # Function and method objects.
        if _type(object) in ("function", "instancemethod"):
            types.append(_type(object))
        # Classes and modules.
        else:
            for v in object.__dict__.values():
                try:
                    types.append(str(v.__classname__))
                except:
                    # Not a class after all (some stuff like ufunc in Numeric).
                    types.append(_type(v))
    # Lists and tuples can consist of several types of objects.
    elif isinstance(object, (list, tuple, set)):
        types += [_type(x) for x in object]
    # Dictionaries have keys pointing to objects.
    elif isinstance(object, dict):
        types += [_type(k) for k in object]
        types += [_type(v) for v in object.values()]
    else:
        types.append(_type(object))
    # Clean up type strings.
    m = {}
    for i in range(len(types)):
        k = types[i]
        # Execute the regular expressions once only,
        # next time we'll have the conversion cached.
        if k not in m:
            for a, b in replace:
                types[i] = re.sub(a, b, types[i])
            m[k] = types[i]
        types[i] = m[k]
    if not quantify:
        if not isinstance(object, (list, tuple, set, dict)) and not hasattr(object, "__dict__"):
            return types[0]
        return types
    return count(types, plural={"built-in function" : "built-in functions"})

#print reflect("hello")
#print reflect(["hello", "goobye"])
#print reflect((1,2,3,4,5))
#print reflect({"name": "linguistics", "version": 1.0})
#print reflect(reflect)
#print reflect(__dict__)
#import Foundation; print reflect(Foundation)
#import Numeric; print reflect(Numeric)
</t>
<t tx="karstenw.20230303132915.1">#### PATTERN | EN | MOOD &amp; MODALITY ################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303132921.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

### LIST FUNCTIONS #################################################################################


</t>
<t tx="karstenw.20230303132921.10">def subjunctive(sentence, classical=True, **kwargs):
    """ The subjunctive mood is a classical mood used to express a wish, judgment or opinion.
        It is marked by the verb wish/were, or infinitive form of a verb
        preceded by an "it is"-statement:
        "It is recommended that he bring his own computer."
    """
    S = sentence
    if not (hasattr(S, "words") and hasattr(S, "parse_token")):
        raise TypeError("%s object is not a parsed Sentence" % repr(S.__class__.__name__))
    if question(S):
        return False
    for i, w in enumerate(S):
        b = False
        if w.type.startswith("VB"):
            if s(w).startswith("wish"):
                # "I wish I knew."
                return True
            if s(w) == "hope" and i &gt; 0 and s(S[i - 1]) in ("i", "we"):
                # "I hope ..."
                return True
            if s(w) == "were" and i &gt; 0 and (s(S[i - 1]) in ("i", "it", "he", "she") or S[i - 1].type == "NN"):
                # "It is as though she were here." =&gt; counterfactual.
                return True
            if s(w) in subjunctive1:
                # "I propose that you be on time."
                b = True
            elif s(w) == "is" and 0 &lt; i &lt; len(S) - 1 and s(S[i - 1]) == "it" \
             and s(S[i + 1]) in subjunctive2:
                # "It is important that you be there." =&gt; but you aren't (yet).
                b = True
            elif s(w) == "is" and 0 &lt; i &lt; len(S) - 3 and s(S[i - 1]) == "it" \
             and s(S[i + 2]) in ("good", "bad") and s(S[i + 3]) == "idea":
                # "It is a good idea that you be there."
                b = True
        if b:
            # With classical=False, "It is important that you are there." passes.
            # This is actually an informal error: it states a fact, not a wish.
            v = find(lambda w: w.type.startswith("VB"), S[i + 1:])
            if v and classical is True and v and v.type == "VB":
                return True
            if v and classical is False:
                return True
    return False

#from __init__ import parse, Sentence
#
#for str in (
#  "I wouldn't do that if I were you.", # True
#  "I wish I knew.",                    # True
#  "I propose that you be on time.",    # True
#  "It is a bad idea to be late.",      # True
#  "I will be dead."):                  # False, predictive
#    print str
#    print parse(str)
#    print subjunctive(Sentence(parse(str)))
#    print


</t>
<t tx="karstenw.20230303132921.11">def negated(sentence, negative=("not", "n't", "never")):
    if hasattr(sentence, "string"):
        # Sentence object =&gt; string.
        sentence = sentence.string
    S = " %s " % (sentence).strip(".?!").lower()
    for w in negative:
        if " %s " % w in S:
            return True
    return False


</t>
<t tx="karstenw.20230303132921.12">def mood(sentence, **kwargs):
    """ Returns IMPERATIVE (command), CONDITIONAL (possibility), SUBJUNCTIVE (wish) or INDICATIVE (fact).
    """
    if isinstance(sentence, str):
        try:
            # A Sentence is expected but a string given.
            # Attempt to parse the string on-the-fly.
            from pattern.en import parse, Sentence
            sentence = Sentence(parse(sentence))
        except ImportError:
            pass
    if imperative(sentence, **kwargs):
        return IMPERATIVE
    if conditional(sentence, **kwargs):
        return CONDITIONAL
    if subjunctive(sentence, **kwargs):
        return SUBJUNCTIVE
    else:
        return INDICATIVE

### MODALITY #######################################################################################
# Functions take Sentence objects, see pattern.text.tree.Sentence and pattern.text.parsetree().


</t>
<t tx="karstenw.20230303132921.13">def d(*args):
    return dict.fromkeys(args, True)

AUXILLARY = {
      "be": ["be", "am", "m", "are", "is", "being", "was", "were" "been"],
     "can": ["can", "ca", "could"],
    "dare": ["dare", "dares", "daring", "dared"],
      "do": ["do", "does", "doing", "did", "done"],
    "have": ["have", "ve", "has", "having", "had"],
     "may": ["may", "might"],
    "must": ["must"],
    "need": ["need", "needs", "needing", "needed"],
   "ought": ["ought"],
   "shall": ["shall", "sha"],
    "will": ["will", "ll", "wo", "willing", "would", "d"]
}

MODIFIERS = ("fully", "highly", "most", "much", "strongly", "very")

EPISTEMIC = "epistemic" # Expresses degree of possiblity.

# -1.00 = NEGATIVE
# -0.75 = NEGATIVE, with slight doubts
# -0.50 = NEGATIVE, with doubts
# -0.25 = NEUTRAL, slightly negative
# +0.00 = NEUTRAL
# +0.25 = NEUTRAL, slightly positive
# +0.50 = POSITIVE, with doubts
# +0.75 = POSITIVE, with slight doubts
# +1.00 = POSITIVE

epistemic_MD = { # would =&gt; could =&gt; can =&gt; should =&gt; shall =&gt; will =&gt; must
    -1.00: d(),
    -0.75: d(),
    -0.50: d("would"),
    -0.25: d("could", "dare", "might"),
     0.00: d("can", "ca", "may"),
    +0.25: d("ought", "should"),
    +0.50: d("shall", "sha"),
    +0.75: d("will", "'ll", "wo"),
    +1.00: d("have", "has", "must", "need"),
}

epistemic_VB = { # wish =&gt; feel =&gt; believe =&gt; seem =&gt; think =&gt; know =&gt; prove + THAT
    -1.00: d(),
    -0.75: d(),
    -0.50: d("dispute", "disputed", "doubt", "question"),
    -0.25: d("hope", "want", "wish"),
     0.00: d("guess", "imagine", "seek"),
    +0.25: d("appear", "bet", "feel", "hear", "rumor", "rumour", "say", "said", "seem", "seemed",
             "sense", "speculate", "suspect", "suppose", "wager"),
    +0.50: d("allude", "anticipate", "assume", "claim", "claimed", "believe", "believed",
             "conjecture", "consider", "considered", "decide", "expect", "find", "found",
             "hypothesize", "imply", "indicate", "infer", "postulate", "predict", "presume",
             "propose", "report", "reported", "suggest", "suggested", "tend",
             "think", "thought"),
    +0.75: d("know", "known", "look", "see", "show", "shown"),
    +1.00: d("certify", "demonstrate", "prove", "proven", "verify"),
}

epistemic_RB = { # unlikely =&gt; supposedly =&gt; maybe =&gt; probably =&gt; usually =&gt; clearly =&gt; definitely
    -1.00: d("impossibly"),
    -0.75: d("hardly"),
    -0.50: d("presumptively", "rarely", "scarcely", "seldomly", "uncertainly", "unlikely"),
    -0.25: d("almost", "allegedly", "debatably", "nearly", "presumably", "purportedly", "reportedly",
             "reputedly", "rumoredly", "rumouredly", "supposedly"),
     0.00: d("barely", "hypothetically", "maybe", "occasionally", "perhaps", "possibly", "putatively",
             "sometimes", "sporadically", "traditionally", "widely"),
    +0.25: d("admittedly", "apparently", "arguably", "believably", "conceivably", "feasibly", "fairly",
             "hopefully", "likely", "ostensibly", "potentially", "probably", "quite", "seemingly"),
    +0.50: d("commonly", "credibly", "defendably", "defensibly", "effectively", "frequently",
             "generally", "largely", "mostly", "normally", "noticeably", "often", "plausibly",
             "reasonably", "regularly", "relatively", "typically", "usually"),
    +0.75: d("assuredly", "certainly", "clearly", "doubtless", "evidently", "evitably", "manifestly",
             "necessarily", "nevertheless", "observably", "ostensively", "patently", "plainly",
             "positively", "really", "surely", "truly", "undoubtably", "undoubtedly", "verifiably"),
    +1.00: d("absolutely", "always", "definitely", "incontestably", "indisputably", "indubitably",
             "ineluctably", "inescapably", "inevitably", "invariably", "obviously", "unarguably",
             "unavoidably", "undeniably", "unquestionably")
}

epistemic_JJ = {
    -1.00: d("absurd", "prepostoreous", "ridiculous"),
    -0.75: d("inconceivable", "unthinkable"),
    -0.50: d("misleading", "scant", "unlikely", "unreliable"),
    -0.25: d("customer-centric", "doubtful", "ever", "ill-defined, ""inadequate", "late",
             "uncertain", "unclear", "unrealistic", "unspecified", "unsure", "wild"),
     0.00: d("dynamic", "possible", "unknown"),
    +0.25: d("according", "creative", "likely", "local", "innovative", "interesting",
             "potential", "probable", "several", "some", "talented", "viable"),
    +0.50: d("certain", "generally", "many", "notable", "numerous", "performance-oriented",
             "promising", "putative", "well-known"),
    +0.75: d("concrete", "credible", "famous", "important", "major", "necessary", "original",
             "positive", "significant", "real", "robust", "substantial", "sure"),
    +1.00: d("confirmed", "definite", "prime", "undisputable"),
}

epistemic_NN = {
    -1.00: d("fantasy", "fiction", "lie", "myth", "nonsense"),
    -0.75: d("controversy"),
    -0.50: d("criticism", "debate", "doubt"),
    -0.25: d("belief", "chance", "faith", "luck", "perception", "speculation"),
     0.00: d("challenge", "guess", "feeling", "hunch", "opinion", "possibility", "question"),
    +0.25: d("assumption", "expectation", "hypothesis", "notion", "others", "team"),
    +0.50: d("example", "proces", "theory"),
    +0.75: d("conclusion", "data", "evidence", "majority", "proof", "symptom", "symptoms"),
    +1.00: d("fact", "truth", "power"),
}

epistemic_CC_DT_IN = {
     0.00: d("either", "whether"),
    +0.25: d("however", "some"),
    +1.00: d("despite")
}

epistemic_PRP = {
    +0.25: d("I", "my"),
    +0.50: d("our"),
    +0.75: d("we")
}

epistemic_weaseling = {
    -0.75: d("popular belief"),
    -0.50: d("but that", "but this", "have sought", "might have", "seems to"),
    -0.25: d("may also", "may be", "may have", "may have been", "some have", "sort of"),
    +0.00: d("been argued", "believed to", "considered to", "claimed to", "is considered", "is possible",
             "overall solutions", "regarded as", "said to"),
    +0.25: d("a number of", "in some", "one of", "some of",
             "many modern", "many people", "most people", "some people", "some cases", "some studies",
             "scientists", "researchers"),
    +0.50: d("in several", "is likely", "many of", "many other", "of many", "of the most", "such as",
             "several reasons", "several studies", "several universities", "wide range"),
    +0.75: d("almost always", "and many", "and some", "around the world", "by many", "in many", "in order to",
             "most likely"),
    +1.00: d("i.e.", "'s most", "of course", "There are", "without doubt"),
}


</t>
<t tx="karstenw.20230303132921.14">def modality(sentence, type=EPISTEMIC):
    """ Returns the sentence's modality as a weight between -1.0 and +1.0.
        Currently, the only type implemented is EPISTEMIC.
        Epistemic modality is used to express possibility (i.e. how truthful is what is being said).
    """
    if isinstance(sentence, str):
        try:
            # A Sentence is expected but a string given.
            # Attempt to parse the string on-the-fly.
            from pattern.en import parse, Sentence
            sentence = Sentence(parse(sentence))
        except ImportError:
            pass
    S, n, m = sentence, 0.0, 0
    if not (hasattr(S, "words") and hasattr(S, "parse_token")):
        raise TypeError("%s object is not a parsed Sentence" % repr(S.__class__.__name__))
    if type == EPISTEMIC:
        r = S.string.rstrip(" .!")
        for k, v in epistemic_weaseling.items():
            for phrase in v:
                if phrase in r:
                    n += k
                    m += 2
        for i, w in enumerate(S.words):
            for type, dict, weight in (
              (  "MD", epistemic_MD, 4),
              (  "VB", epistemic_VB, 2),
              (  "RB", epistemic_RB, 2),
              (  "JJ", epistemic_JJ, 1),
              (  "NN", epistemic_NN, 1),
              (  "CC", epistemic_CC_DT_IN, 1),
              (  "DT", epistemic_CC_DT_IN, 1),
              (  "IN", epistemic_CC_DT_IN, 1),
              ("PRP" , epistemic_PRP, 1),
              ("PRP$", epistemic_PRP, 1),
              ( "WP" , epistemic_PRP, 1)):
                # "likely" =&gt; weight 1, "very likely" =&gt; weight 2
                if i &gt; 0 and s(S[i - 1]) in MODIFIERS:
                    weight += 1
                # likely" =&gt; score 0.25 (neutral inclining towards positive).
                if w.type and w.type.startswith(type):
                    for k, v in dict.items():
                        # Prefer lemmata.
                        if (w.lemma or s(w)) in v:
                            # Reverse score for negated terms.
                            if i &gt; 0 and s(S[i - 1]) in ("not", "n't", "never", "without"):
                                k = -k * 0.5
                            n += weight * k
                            m += weight
                            break
            # Numbers, citations, explanations make the sentence more factual.
            if w.type in ("CD", "\"", "'", ":", "("):
                n += 0.75
                m += 1
    if m == 0:
        return 1.0 # No modal verbs/adverbs used, so statement must be true.
    return max(-1.0, min(n / (m or 1), +1.0))


</t>
<t tx="karstenw.20230303132921.15">def uncertain(sentence, threshold=0.5):
    return modality(sentence) &lt;= threshold

#from __init__ import parse, Sentence
#
#for str in (
#  "I wish it would stop raining.",
#  "It will surely stop raining soon."):
#    print str
#    print parse(str)
#    print modality(Sentence(parse(str)))
#    print

#---------------------------------------------------------------------------------------------------

# Celle, A. (2009). Hearsay adverbs and modality, in: Modality in English, Mouton.
# Allegedly, presumably, purportedly, ... are in the negative range because
# they introduce a fictious point of view by referring to an unclear source.

#---------------------------------------------------------------------------------------------------

# Tseronis, A. (2009). Qualifying standpoints. LOT Dissertation Series: 233.
# Following adverbs are not epistemic but indicate the way in which things are said.
# 1) actually, admittedly, avowedly, basically, bluntly, briefly, broadly, candidly,
#    confidentially, factually, figuratively, frankly, generally, honestly, hypothetically,
#    in effect, in fact, in reality, indeed, literally, metaphorically, naturally,
#    of course, objectively, personally, really, roughly, seriously, simply, sincerely,
#    strictly, truly, truthfully.
# 2) bizarrely, commendably, conveniently, curiously, disappointingly, fortunately, funnily,
#    happily, hopefully, illogically, interestingly, ironically, justifiably, justly, luckily,
#    oddly, paradoxically, preferably, regretfully, regrettably, sadly, significantly,
#    strangely, surprisingly, tragically, unaccountably, unfortunately, unhappily unreasonably

#---------------------------------------------------------------------------------------------------

# The modality() function was tested with BioScope and Wikipedia training data from CoNLL2010 Shared Task 1.
# See for example Morante, R., Van Asch, V., Daelemans, W. (2010):
# Memory-Based Resolution of In-Sentence Scopes of Hedge Cues
# http://www.aclweb.org/anthology/W/W10/W10-3006.pdf
# Sentences in the training corpus are labelled as "certain" or "uncertain".
# For Wikipedia sentences, 2000 "certain" and 2000 "uncertain":
# modality(sentence) &gt; 0.5 =&gt; A 0.70 P 0.73 R 0.64 F1 0.68
</t>
<t tx="karstenw.20230303132921.2">def find(function, list):
    """ Returns the first item in the list for which function(item) is True, None otherwise.
    """
    for item in list:
        if function(item):
            return item

### MOOD ###########################################################################################
# Functions take Sentence objects, see pattern.text.tree.Sentence and pattern.text.parsetree().

INDICATIVE = "indicative"  # They went for a walk.
IMPERATIVE = "imperative"  # Let's go for a walk!
CONDITIONAL = "conditional" # It might be nice to go for a walk when it stops raining.
SUBJUNCTIVE = "subjunctive" # It would be nice to go for a walk sometime.


</t>
<t tx="karstenw.20230303132921.3">def s(word):
    return word.string.lower()


</t>
<t tx="karstenw.20230303132921.4">def join(words):
    return " ".join([w.string.lower() for w in words])


</t>
<t tx="karstenw.20230303132921.5">def question(sentence):
    return len(sentence) &gt; 0 and sentence[-1].string == "?"


</t>
<t tx="karstenw.20230303132921.6">def verb(word):
    return word.type.startswith(("VB", "MD")) and (word.chunk is None or word.chunk.type.endswith("VP"))


</t>
<t tx="karstenw.20230303132921.7">def verbs(sentence, i=0, j=None):
    return [w for w in sentence[i:j or len(sentence)] if verb(w)]


</t>
<t tx="karstenw.20230303132921.8">def imperative(sentence, **kwargs):
    """ The imperative mood is used to give orders, commands, warnings, instructions, 
        or to make requests (if used with "please").
        It is marked by the infinitive form of the verb, without "to":
        "For goodness sake, just stop it!"
    """
    S = sentence
    if not (hasattr(S, "words") and hasattr(S, "parse_token")):
        raise TypeError("%s object is not a parsed Sentence" % repr(S.__class__.__name__))
    if question(S):
        return False
    if S.subjects and s(S.subjects[0]) not in ("you", "yourself"):
        # The subject can only identify as "you" (2sg): "Control yourself!".
        return False
    r = s(S).rstrip(" .!")
    for cc in ("if", "assuming", "provided that", "given that"):
        # A conjunction can also indicate conditional mood.
        if cc + " " in r:
            return False
    for i, w in enumerate(S):
        if verb(w):
            if s(w) in ("do", "let") and w == verbs(S)[0]:
                # "Do your homework!"
                return True
            if s(w) in ("do", "let"):
                # "Let's not argue."
                continue
            if s(w) in ("would", "should", "'d", "could", "can", "may", "might"):
                # "You should leave." =&gt; conditional.
                return False
            if s(w) in ("will", "shall") and i &gt; 0 and s(S[i - 1]) == "you" and not verbs(S, 0, i):
                # "You will eat your dinner."
                continue
            if w.type == "VB" and (i == 0 or s(S[i - 1]) != "to"):
                # "Come here!"
                return True
            # Break on any other verb form.
            return False
    return False

#from __init__ import parse, Sentence
#
#for str in (
#  "Do your homework!",                   # True
#  "Do whatever you want.",               # True
#  "Do not listen to me.",                # True
#  "Do it if you think it is necessary.", # False
#  "Turn that off, will you.",            # True
#  "Let's help him.",                     # True
#  "Help me!",                            # True
#  "You will help me.",                   # True
#  "I hope you will help me.",            # False
#  "I can help you.",                     # False
#  "I can help you if you let me."):      # False
#    print str
#    print parse(str)
#    print imperative(Sentence(parse(str)))
#    print


</t>
<t tx="karstenw.20230303132921.9">def conditional(sentence, predictive=True, **kwargs):
    """ The conditional mood is used to talk about possible or imaginary situations.
        It is marked by the infinitive form of the verb, preceded by would/could/should:
        "we should be going", "we could have stayed longer".
        With predictive=False, sentences with will/shall need an explicit if/when/once-clause:
        - "I will help you" =&gt; predictive.
        - "I will help you if you pay me" =&gt; speculative.
        Sentences with can/may always need an explicit if-clause.
    """
    S = sentence
    if not (hasattr(S, "words") and hasattr(S, "parse_token")):
        raise TypeError("%s object is not a parsed Sentence" % repr(S.__class__.__name__))
    if question(S):
        return False
    i = find(lambda w: s(w) == "were", S)
    i = i and i.index or 0
    if i &gt; 0 and (s(S[i - 1]) in ("i", "it", "he", "she") or S[i - 1].type == "NN"):
        # "As if it were summer already." =&gt; subjunctive (wish).
        return False
    for i, w in enumerate(S):
        if w.type == "MD":
            if s(w) == "ought" and i &lt; len(S) and s(S[i + 1]) == "to":
                # "I ought to help you."
                return True
            if s(w) in ("would", "should", "'d", "could", "might"):
                # "I could help you."
                return True
            if s(w) in ("will", "shall", "'ll") and i &gt; 0 and s(S[i - 1]) == "you" and not verbs(S, 0, i):
                # "You will help me." =&gt; imperative.
                return False
            if s(w) in ("will", "shall", "'ll") and predictive:
                # "I will help you." =&gt; predictive.
                return True
            if s(w) in ("will", "shall", "'ll", "can", "may"):
                # "I will help you when I get back." =&gt; speculative.
                r = s(S).rstrip(" .!")
                for cc in ("if", "when", "once", "as soon as", "assuming", "provided that", "given that"):
                    if cc + " " in r:
                        return True
    return False

#from __init__ import parse, Sentence
#
#for str in (
#  "We ought to help him.",          # True
#  "We could help him.",             # True
#  "I will help you.",               # True
#  "You will help me.",              # False (imperative)
#  "I hope you will help me.",       # True (predictive)
#  "I can help you.",                # False
#  "I can help you if you let me."): # True
#    print str
#    print parse(str)
#    print conditional(Sentence(parse(str)))
#    print

subjunctive1 = [
    "advise", "ask", "command", "demand", "desire", "insist",
    "propose", "recommend", "request", "suggest", "urge"]
subjunctive2 = [
    "best", "crucial", "desirable", "essential", "imperative",
    "important", "recommended", "urgent", "vital"]

for w in list(subjunctive1): # Inflect.
    subjunctive1.append(w + "s")
    subjunctive1.append(w.rstrip("e") + "ed")


</t>
<t tx="karstenw.20230303132932.1"></t>
<t tx="karstenw.20230303132938.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230303132956.1">#### PATTERN | VECTOR | WORDLIST ###################################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
from io import open

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""


</t>
<t tx="karstenw.20230303132956.10">def __getitem__(self, i):
    self._load()
    return self._data[i]

</t>
<t tx="karstenw.20230303132956.11">def __setitem__(self, i, v):
    self._load()
    self._data[i] = v

</t>
<t tx="karstenw.20230303132956.12">def insert(self, i, v):
    self._load()
    self._data.insert(i, v)

</t>
<t tx="karstenw.20230303132956.13">def append(self, v):
    self._load()
    self._data.append(v)

</t>
<t tx="karstenw.20230303132956.14">def extend(self, v):
    self._load()
    self._data.extend(v)

</t>
<t tx="karstenw.20230303132956.2">class Wordlist(object):

    @others
ACADEMIC  = Wordlist("academic")  # English academic words.
BASIC     = Wordlist("basic")     # English basic words (850) that express 90% of concepts.
PROFANITY = Wordlist("profanity") # English swear words.
TIME      = Wordlist("time")      # English time and date words.
STOPWORDS = Wordlist("stopwords") # English stop words ("a", "the", ...).

# Note: if used for lookups, performance can be increased by using a dict:
# blacklist = dict.fromkeys(PROFANITY+TIME, True)
# for i in range(1000):
#    corpus.append(Document(src[i], exclude=blacklist))
</t>
<t tx="karstenw.20230303132956.3">def __init__(self, name, data=[]):
    """ Lazy read-only list of words.
    """
    self._name = name
    self._data = data

</t>
<t tx="karstenw.20230303132956.4">def _load(self):
    if not self._data:
        self._data = open(os.path.join(MODULE, self._name + ".txt")).read().split(", ")

</t>
<t tx="karstenw.20230303132956.5">def __repr__(self):
    self._load()
    return repr(self._data)

</t>
<t tx="karstenw.20230303132956.6">def __iter__(self):
    self._load()
    return iter(self._data)

</t>
<t tx="karstenw.20230303132956.7">def __len__(self):
    self._load()
    return len(self._data)

</t>
<t tx="karstenw.20230303132956.8">def __contains__(self, w):
    self._load()
    return w in self._data

</t>
<t tx="karstenw.20230303132956.9">def __add__(self, iterable):
    self._load()
    return Wordlist(None, data=sorted(self._data + list(iterable)))

</t>
<t tx="karstenw.20230303133026.1"></t>
<t tx="karstenw.20230303133031.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230303133045.1">#### PATTERN | WORDNET #########################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

################################################################################
# WordNet is a lexical database for English.
# It disambiguates word senses, e.g., "tree" in the sense of a plant
# or in the sense of a graph.
# It groups similar word senses into sets of synonyms called synsets,
# with a short description and semantic relations to other synsets:
# -  synonym = a word that is similar in meaning,
# - hypernym = a word with a broader meaning,       (tree =&gt; plant)
# -  hyponym = a word with a more specific meaning, (tree =&gt; oak)
# -  holonym = a word that is the whole of parts,   (tree =&gt; forest)
# -  meronym = a word that is a part of the whole,  (tree =&gt; trunk)
# -  antonym = a word that is opposite in meaning.

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
import glob

from io import open

from math import log

from pattern.text import lazydict

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

# Path to WordNet /dict folder.
#CORPUS = ""
#os.environ["WNHOME"] = os.path.join(MODULE, CORPUS)
#os.environ["WNSEARCHDIR"] = os.path.join(MODULE, CORPUS, "dict")

import nltk
from nltk.corpus import wordnet as wn
from nltk.corpus import sentiwordnet as swn
from nltk.corpus import wordnet_ic as wn_ic
from nltk.corpus.reader.wordnet import Synset as WordNetSynset

# Make sure the necessary corpora are downloaded to the local drive
for token in ("wordnet", "wordnet_ic", "sentiwordnet"):
    try:
        nltk.data.find("corpora/" + token)
    except LookupError:
        try:
            nltk.download(token, quiet = True, raise_on_error = True)
        except ValueError:
            # Sometimes there are problems with the default index.xml URL. Then we will try this...
            from nltk.downloader import Downloader as NLTKDownloader
            d = NLTKDownloader("http://nltk.github.com/nltk_data/")
            d.download(token, quiet = True, raise_on_error = True)

# Use the Brown corpus for calculating information content (IC)
brown_ic = wn_ic.ic('ic-brown.dat')
IC_CORPUS, IC_MAX = brown_ic, {}
for key in IC_CORPUS:
    IC_MAX[key] = max(IC_CORPUS[key].values())

# This will hold the WordNet version
VERSION = wn.get_version() or "3.0"

#---------------------------------------------------------------------------------------------------

DIACRITICS = {
    "a": ("á", "ä", "â", "à", "å"),
    "e": ("é", "ë", "ê", "è"),
    "i": ("í", "ï", "î", "ì"),
    "o": ("ó", "ö", "ô", "ò", "ō", "ø"),
    "u": ("ú", "ü", "û", "ù", "ů"),
    "y": ("ý", "ÿ", "ý"),
    "s": ("š",),
    "c": ("ç", "č"),
    "n": ("ñ",),
    "z": ("ž",)
}


</t>
<t tx="karstenw.20230303133045.10">def __getitem__(self, i):
    return self.synonyms[i]

</t>
<t tx="karstenw.20230303133045.11">def __eq__(self, synset):
    return isinstance(synset, Synset) and self.id == synset.id

</t>
<t tx="karstenw.20230303133045.12">def __ne__(self, synset):
    return not self.__eq__(synset)
__repr__ = lambda self: self._wnsynset.__repr__()

</t>
<t tx="karstenw.20230303133045.13">@property
def id(self):
    return self._wnsynset.offset()

</t>
<t tx="karstenw.20230303133045.14">@property
def pos(self):
    """ Yields the part-of-speech tag (NOUN, VERB, ADJECTIVE or ADVERB).
    """
    pos = self._wnsynset.pos()
    if pos == wn.NOUN:
        return NOUN
    if pos == wn.VERB:
        return VERB
    if pos == wn.ADJ or pos == wn.ADJ_SAT:
        return ADJECTIVE
    if pos == wn.ADV:
        return ADVERB

part_of_speech = tag = pos

</t>
<t tx="karstenw.20230303133045.15">@property
def synonyms(self):
    """ Yields a list of word forms (i.e. synonyms), for example:
        synsets("TV")[0].synonyms =&gt; ["television", "telecasting", "TV", "video"]
    """
    return [s for s in self._wnsynset.lemma_names()]

senses = synonyms # Backwards compatibility; senses = list of Synsets for a word.

</t>
<t tx="karstenw.20230303133045.16">@property
def gloss(self):
    """ Yields a descriptive string.
    """
    return self._wnsynset.definition()

</t>
<t tx="karstenw.20230303133045.17">@property
def lexname(self):
    """ Yields a category, e.g., noun.animal.
    """
    return self._wnsynset.lexname()

</t>
<t tx="karstenw.20230303133045.18">@property
def antonym(self):
    """ Yields the semantically opposite synset, for example:
        synsets("death")[0].antonym =&gt; Synset("birth").
    """
    p = [Synset(a.synset()) for l in self._wnsynset.lemmas() for a in l.antonyms()]
    return len(p) &gt; 0 and p or None

</t>
<t tx="karstenw.20230303133045.19">def meronyms(self):
    """ Yields a list of synsets that are semantic members/parts of this synset, for example:
        synsets("house")[0].meronyms() =&gt;
        [Synset("library"),
         Synset("loft"),
         Synset("porch")
        ]
    """
    p = self._wnsynset.member_meronyms()
    p += self._wnsynset.part_meronyms()
    return [Synset(p) for p in p]

</t>
<t tx="karstenw.20230303133045.2">def normalize(word):
    """ Normalizes the word for synsets() or Sentiwordnet[] by removing diacritics
        (PyWordNet does not take unicode) and replacing spaces with underscores.
    """
    if not isinstance(word, str):
        word = str(word)
    if not isinstance(word, str):
        try:
            word = word.encode("utf-8", "ignore")
        except:
            pass
    for k, v in DIACRITICS.items():
        for v in v:
            word = word.replace(v, k)

    # Replace spaces with underscores
    word = word.replace(" ", "_")

    return word

</t>
<t tx="karstenw.20230303133045.20">def holonyms(self):
    """ Yields a list of synsets of which this synset is a member/part, for example:
        synsets("tree")[0].holonyms() =&gt; Synset("forest").
    """
    p = self._wnsynset.member_holonyms()
    p += self._wnsynset.part_holonyms()
    return [Synset(p) for p in p]

</t>
<t tx="karstenw.20230303133045.21">def hyponyms(self, recursive=False, depth=None):
    """ Yields a list of semantically more specific synsets, for example:
        synsets("train")[0].hyponyms() =&gt;
        [Synset("boat train"),
         Synset("car train"),
         Synset("freight train"),
         Synset("hospital train"),
         Synset("mail train"),
         Synset("passenger train"),
         Synset("streamliner"),
         Synset("subway train")
        ]
    """
    p = [Synset(p) for p in self._wnsynset.hyponyms()]
    if depth is None and recursive is False:
        return p
    if depth == 0:
        return []
    if depth is not None:
        depth -= 1
    if depth is None or depth &gt; 0:
        [p.extend(s.hyponyms(True, depth)) for s in list(p)]
    return p

</t>
<t tx="karstenw.20230303133045.22">def hypernyms(self, recursive=False, depth=None):
    """ Yields a list of semantically broader synsets.
    """
    p = [Synset(p) for p in self._wnsynset.hypernyms()]
    if depth is None and recursive is False:
        return p
    if depth == 0:
        return []
    if depth is not None:
        depth -= 1
    if depth is None or depth &gt; 0:
        [p.extend(s.hypernyms(True, depth)) for s in list(p)]
    return p

</t>
<t tx="karstenw.20230303133045.23">@property
def hypernym(self):
    """ Yields the synset that is the semantic parent, for example:
        synsets("train")[0].hypernym =&gt; Synset("public transport").
    """
    p = self.hypernyms()
    return len(p) &gt; 0 and p[0] or None

</t>
<t tx="karstenw.20230303133045.24">def similar(self):
    """ Returns a list of similar synsets for adjectives and adverbs, for example:
        synsets("almigthy",JJ)[0].similar() =&gt; Synset("powerful").
    """
    # ALSO_SEE returns wn.Sense instead of wn.Synset in some cases:
    #s = lambda x: isinstance(x, wn.Sense) and x.synset or x
    p = [Synset(p) for p in self._wnsynset.similar_tos()]
    p += [Synset(p) for p in self._wnsynset.also_sees()]
    return p

</t>
<t tx="karstenw.20230303133045.25">def similarity(self, synset):
    """ Returns the semantic similarity of the given synsets (0.0-1.0).
        synsets("cat")[0].similarity(synsets("dog")[0]) =&gt; 0.86.
        synsets("cat")[0].similarity(synsets("box")[0]) =&gt; 0.17.
    """

    return self._wnsynset.lin_similarity(synset._wnsynset, IC_CORPUS)

</t>
<t tx="karstenw.20230303133045.26">@property
def ic(self):
    offset, pos = self.id, self.pos
    if pos in _pattern2wordnet:
        pos = _pattern2wordnet[pos]
    if pos in IC_CORPUS and offset in IC_CORPUS[pos]:
        return IC_CORPUS[pos][offset] / IC_MAX[pos]
    return None

</t>
<t tx="karstenw.20230303133045.27">@property
def weight(self):
    return (    sentiwordnet is not None
            and sentiwordnet.synset(self.id, self.pos)[:2]
            or  None )


</t>
<t tx="karstenw.20230303133045.28">def similarity(synset1, synset2):
    """ Returns the semantic similarity of the given synsets.
    """
    return synset1.similarity(synset2)


</t>
<t tx="karstenw.20230303133045.29">def ancestor(synset1, synset2):
    """ Returns the common ancestor of both synsets.
        For example synsets("cat")[0].ancestor(synsets("dog")[0]) =&gt; Synset("carnivore")
    """
    h1, h2 = synset1.hypernyms(recursive=True), synset2.hypernyms(recursive=True)
    for s in h1:
        if s in h2:
            return s

least_common_subsumer = lcs = ancestor

</t>
<t tx="karstenw.20230303133045.3">def synsets(word, pos=NOUN):
    """ Returns a list of Synset objects, one for each word sense.
        Each word can be understood in different "senses", 
        each of which is part of a set of synonyms (= Synset).
    """
    word, pos = normalize(word), pos.lower()
    # import pdb
    # pdb.set_trace()
    try:
        if pos.startswith(NOUN.lower()): # "NNS" or "nn" will also pass.
            w = wn.synsets(word, pos = wn.NOUN)
        elif pos.startswith(VERB.lower()):
            w = wn.synsets(word, pos = wn.VERB)
        elif pos.startswith(ADJECTIVE.lower()):
            w = wn.synsets(word, pos = wn.ADJ)
        elif pos.startswith(ADVERB.lower()):
            w = wn.synsets(word, pos = wn.ADV)
        else:
            raise TypeError("part of speech must be NOUN, VERB, ADJECTIVE or ADVERB, not %s" % repr(pos))
        return [Synset(synset) for synset in w]
    except KeyError:
        return []
    return []


</t>
<t tx="karstenw.20230303133045.30">def map32(id, pos=NOUN):
    """ Returns an (id, pos)-tuple with the WordNet2 synset id for the given WordNet3 synset id.
        Returns None if no id was found.
    """
    global _map32_cache
    if not _map32_cache:
        _map32_cache = open(os.path.join(MODULE, "dict", "index.32"), encoding="latin-1").readlines()
        _map32_cache = (x for x in _map32_cache if x[0] != ";") # comments
        _map32_cache = dict(x.strip().split(" ") for x in _map32_cache)
    k = pos in _map32_pos2 and pos or _map32_pos1.get(pos, "x")
    k += str(id).lstrip("0")
    k = _map32_cache.get(k, None)
    if k is not None:
        return int(k[1:]), _map32_pos2[k[0]]
    return None

</t>
<t tx="karstenw.20230303133045.31">class SentiWordNet(Sentiment):

    @others
if not hasattr(Sentiment, "PLACEHOLDER"):
    sentiwordnet = SentiWordNet()
else:
    sentiwordnet = None

# Backwards compatibility.
# Older code may be using pattern.en.wordnet.sentiment[w],
# which yields a (positive, negative, neutral)-tuple.


</t>
<t tx="karstenw.20230303133045.32">def __init__(self, path=None, language="en"):
    """ A sentiment lexicon with scores from SentiWordNet.
        The value for each word is a tuple with values for
        polarity (-1.0-1.0), subjectivity (0.0-1.0) and intensity (0.5-2.0).
    """
    Sentiment.__init__(self, path=path, language=language)

</t>
<t tx="karstenw.20230303133045.33">def load(self):
    pass

</t>
<t tx="karstenw.20230303133045.34">def synset(self, id, pos=ADJECTIVE):
    if pos in _pattern2wordnet:
        pos = _pattern2wordnet[pos]
    try:
        s = wn._synset_from_pos_and_offset(pos, id)
        lemma = s.lemma_names()[0]
        return self[lemma]
    except:
        pass

    return None

# Words are stored without diacritics,
# use wordnet.normalize(word).
</t>
<t tx="karstenw.20230303133045.35">def __getitem__(self, k):
    synsets = list(swn.senti_synsets(k))
    if synsets:
        p, n = synsets[0].pos_score(), synsets[0].neg_score()
        v = (float(p) - float(n), float(p) + float(n))
        return v
    else:
        return None

</t>
<t tx="karstenw.20230303133045.36">def assessments(self, words=[], negation=True):
    raise NotImplementedError

</t>
<t tx="karstenw.20230303133045.37">def __call__(self, s, negation=True):
    raise NotImplementedError

</t>
<t tx="karstenw.20230303133045.38">class sentiment(object):

    @others
sentiment = sentiment()

#print sentiwordnet["industry"] # (0.0, 0.0)
#print sentiwordnet["horrible"] # (-0.625, 0.625)
#print sentiwordnet.synset(synsets("horrible", pos="JJ")[0].id, pos="JJ")
#print synsets("horrible", pos="JJ")[0].weight
</t>
<t tx="karstenw.20230303133045.39">def load(self, **kwargs):
    sentiwordnet.load(**kwargs)

</t>
<t tx="karstenw.20230303133045.4">class _synset(lazydict):

    @others
</t>
<t tx="karstenw.20230303133045.40">def __getitem__(self, w):
    p, s = sentiwordnet.get(w, (0.0, 0.0))
    return p &lt; 0 and (0.0, -p, 1.0 - s) or (p, 0.0, 1.0 - s)

</t>
<t tx="karstenw.20230303133045.41">def __contains__(self, w):
    return w in sentiwordnet

</t>
<t tx="karstenw.20230303133045.5">def __getitem__(self, k):
    for pos in ("n", "v", "a", "r"):
        try:
            synset = wn._synset_from_pos_and_offset(pos, k)
        except:
            pass
        if synset:
            return synset
    return None


</t>
<t tx="karstenw.20230303133045.6">class Synset(object):

    @others
</t>
<t tx="karstenw.20230303133045.7">def __init__(self, synset):
    """ A set of synonyms that share a common meaning.
    """
    if isinstance(synset, WordNetSynset):
        self._wnsynset = synset
    elif isinstance(synset, Synset):
        self = self
    elif isinstance(synset, (tuple, int)):
        if isinstance(synset, int):
            synset = (synset, "NN")
        offset, pos = synset
        self._wnsynset = wn._synset_from_pos_and_offset(_pattern2wordnet[pos] if pos in _pattern2wordnet else pos, offset)
    else:
        raise NotImplementedError

    self._synset = _synset

</t>
<t tx="karstenw.20230303133045.8">def __iter__(self):
    for s in self.synonyms:
        yield s

</t>
<t tx="karstenw.20230303133045.9">def __len__(self):
    return len(self.synonyms)

</t>
<t tx="karstenw.20230303134347.1"></t>
<t tx="karstenw.20230303134359.1">#### PATTERN | ES ##################################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2012 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################
# Spanish linguistical tools using fast regular expressions.

@others
if __name__ == "__main__":
    commandline(parse)
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303134406.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

# Import parser base classes.
from pattern.text import (
    Lexicon, Model, Morphology, Context, Parser as _Parser, ngrams, pprint, commandline,
    PUNCTUATION
)
# Import parser universal tagset.
from pattern.text import (
    penntreebank2universal,
    PTB, PENN, UNIVERSAL,
    NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X
)
# Import parse tree base classes.
from pattern.text.tree import (
    Tree, Text, Sentence, Slice, Chunk, PNPChunk, Chink, Word, table,
    SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA, AND, OR
)
# Import spelling base class.
from pattern.text import (
    Spelling
)
# Import verb tenses.
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE, CONDITIONAL,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    INDICATIVE, IMPERATIVE, SUBJUNCTIVE,
    IMPERFECTIVE, PERFECTIVE, PROGRESSIVE,
    IMPERFECT, PRETERITE,
    PARTICIPLE, GERUND
)
# Import inflection functions.
from pattern.text.es.inflect import (
    article, referenced, DEFINITE, INDEFINITE,
    MASCULINE, MALE, FEMININE, FEMALE, NEUTER, NEUTRAL, PLURAL, M, F, N, PL,
    pluralize, singularize, NOUN, VERB, ADJECTIVE,
    verbs, conjugate, lemma, lexeme, tenses,
    predicative, attributive
)
# Import all submodules.
from pattern.text.es import inflect

sys.path.pop(0)

#--- SPANISH PARSER --------------------------------------------------------------------------------
# The Spanish parser (accuracy 92%) is based on the Spanish portion Wikicorpus v.1.0 (FDL license),
# using 1.5M words from the tagged sections 10000-15000.
# Samuel Reese, Gemma Boleda, Montse Cuadros, Lluís Padró, German Rigau.
# Wikicorpus: A Word-Sense Disambiguated Multilingual Wikipedia Corpus.
# Proceedings of 7th Language Resources and Evaluation Conference (LREC'10),
# La Valleta, Malta. May, 2010.
# http://www.lsi.upc.edu/~nlp/wikicorpus/

# The lexicon uses the Parole tagset:
# http://www.lsi.upc.edu/~nlp/SVMTool/parole.html
# http://nlp.lsi.upc.edu/freeling/doc/tagsets/tagset-es.html
PAROLE = "parole"
parole = {
    "AO": "JJ",   # primera
    "AQ": "JJ",   # absurdo
    "CC": "CC",   # e
    "CS": "IN",   # porque
    "DA": "DT",   # el
    "DD": "DT",   # ese
    "DI": "DT",   # mucha
    "DP": "PRP$", # mi, nuestra
    "DT": "DT",   # cuántos
    "Fa": ".",    # !
    "Fc": ",",    # ,
    "Fd": ":",    # :
    "Fe": "\"",   # "
    "Fg": ".",    # -
    "Fh": ".",    # /
    "Fi": ".",    # ?
    "Fp": ".",    # .
    "Fr": ".",    # &gt;&gt;
    "Fs": ".",    # ...
   "Fpa": "(",    # (
   "Fpt": ")",    # )
    "Fx": ".",    # ;
    "Fz": ".",    #
     "I": "UH",   # ehm
    "NC": "NN",   # islam
   "NCS": "NN",   # guitarra
   "NCP": "NNS",  # guitarras
    "NP": "NNP",  # Óscar
    "P0": "PRP",  # se
    "PD": "DT",   # ése
    "PI": "DT",   # uno
    "PP": "PRP",  # vos
    "PR": "WP$",  # qué
    "PT": "WP$",  # qué
    "PX": "PRP$", # mío
    "RG": "RB",   # tecnológicamente
    "RN": "RB",   # no
    "SP": "IN",   # por
   "VAG": "VBG",  # habiendo
   "VAI": "MD",   # había
   "VAN": "MD",   # haber
   "VAS": "MD",   # haya
   "VMG": "VBG",  # habiendo
   "VMI": "VB",   # habemos
   "VMM": "VB",   # compare
   "VMN": "VB",   # comparecer
   "VMP": "VBN",  # comparando
   "VMS": "VB",   # compararan
   "VSG": "VBG",  # comparando
   "VSI": "VB",   # será
   "VSN": "VB",   # ser
   "VSP": "VBN",  # sido
   "VSS": "VB",   # sea
     "W": "NN",   # septiembre
     "Z": "CD",   # 1,7
    "Zd": "CD",   # 1,7
    "Zm": "CD",   # £1,7
    "Zp": "CD",   # 1,7%
}


</t>
<t tx="karstenw.20230303134406.10">def parse(s, *args, **kwargs):
    """ Returns a tagged Unicode string.
    """
    return parser.parse(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303134406.11">def parsetree(s, *args, **kwargs):
    """ Returns a parsed Text from the given string.
    """
    return Text(parse(s, *args, **kwargs))


</t>
<t tx="karstenw.20230303134406.12">def tree(s, token=[WORD, POS, CHUNK, PNP, REL, LEMMA]):
    """ Returns a parsed Text from the given parsed string.
    """
    return Text(s, token)


</t>
<t tx="karstenw.20230303134406.13">def tag(s, tokenize=True, encoding="utf-8", **kwargs):
    """ Returns a list of (token, tag)-tuples from the given string.
    """
    tags = []
    for sentence in parse(s, tokenize, True, False, False, False, encoding, **kwargs).split():
        for token in sentence:
            tags.append((token[0], token[1]))
    return tags


</t>
<t tx="karstenw.20230303134406.14">def keywords(s, top=10, **kwargs):
    """ Returns a sorted list of keywords in the given string.
    """
    return parser.find_keywords(s, **dict({
        "frequency": parser.frequency,
              "top": top,
              "pos": ("NN",),
           "ignore": ("rt",)}, **kwargs))


</t>
<t tx="karstenw.20230303134406.15">def suggest(w):
    """ Returns a list of (word, confidence)-tuples of spelling corrections.
    """
    return spelling.suggest(w)

split = tree # Backwards compatibility.

#---------------------------------------------------------------------------------------------------
# python -m pattern.es xml -s "A quien se hace de miel las moscas le comen." -OTCL

</t>
<t tx="karstenw.20230303134406.2">def parole2penntreebank(token, tag):
    """ Converts a Parole tag to a Penn Treebank II tag.
        For example: importantísimo/AQ =&gt; importantísimo/ADJ
    """
    return (token, parole.get(tag, tag))


</t>
<t tx="karstenw.20230303134406.3">def parole2universal(token, tag):
    """ Converts a Parole tag to a universal tag.
        For example: importantísimo/AQ =&gt; importantísimo/ADJ
    """
    if tag == "CS":
        return (token, CONJ)
    if tag == "DP":
        return (token, DET)
    if tag in ("P0", "PD", "PI", "PP", "PR", "PT", "PX"):
        return (token, PRON)
    return penntreebank2universal(*parole2penntreebank(token, tag))

ABBREVIATIONS = set((
    "a.C.", "a.m.", "apdo.", "aprox.", "Av.", "Avda.", "c.c.", "D.", "Da.", "d.C.",
    "d.j.C.", "dna.", "Dr.", "Dra.", "esq.", "etc.", "Gob.", "h.", "m.n.", "no.",
    "núm.", "pág.", "P.D.", "P.S.", "p.ej.", "p.m.", "Profa.", "q.e.p.d.", "S.A.",
    "S.L.", "Sr.", "Sra.", "Srta.", "s.s.s.", "tel.", "Ud.", "Vd.", "Uds.", "Vds.",
    "v.", "vol.", "W.C."
))


</t>
<t tx="karstenw.20230303134406.4">def find_lemmata(tokens):
    """ Annotates the tokens with lemmata for plural nouns and conjugated verbs,
        where each token is a [word, part-of-speech] list.
    """
    for token in tokens:
        word, pos, lemma = token[0], token[1], token[0]
        if pos.startswith(("DT",)):
            lemma = singularize(word, pos="DT")
        if pos.startswith(("JJ",)):
            lemma = predicative(word)
        if pos == "NNS":
            lemma = singularize(word)
        if pos.startswith(("VB", "MD")):
            lemma = conjugate(word, INFINITIVE) or word
        token.append(lemma.lower())
    return tokens


</t>
<t tx="karstenw.20230303134406.5">class Parser(_Parser):

    @others
parser = Parser(
     lexicon = os.path.join(MODULE, "es-lexicon.txt"),
   frequency = os.path.join(MODULE, "es-frequency.txt"),
  morphology = os.path.join(MODULE, "es-morphology.txt"),
     context = os.path.join(MODULE, "es-context.txt"),
     default = ("NCS", "NP", "Z"),
    language = "es"
)

lexicon = parser.lexicon # Expose lexicon.

spelling = Spelling(
        path = os.path.join(MODULE, "es-spelling.txt")
)


</t>
<t tx="karstenw.20230303134406.6">def find_tokens(self, tokens, **kwargs):
    kwargs.setdefault("abbreviations", ABBREVIATIONS)
    kwargs.setdefault("replace", {})
    return _Parser.find_tokens(self, tokens, **kwargs)

</t>
<t tx="karstenw.20230303134406.7">def find_lemmata(self, tokens, **kwargs):
    return find_lemmata(tokens)

</t>
<t tx="karstenw.20230303134406.8">def find_tags(self, tokens, **kwargs):
    if kwargs.get("tagset") in (PENN, None):
        kwargs.setdefault("map", lambda token, tag: parole2penntreebank(token, tag))
    if kwargs.get("tagset") == UNIVERSAL:
        kwargs.setdefault("map", lambda token, tag: parole2universal(token, tag))
    if kwargs.get("tagset") is PAROLE:
        kwargs.setdefault("map", lambda token, tag: (token, tag))
    return _Parser.find_tags(self, tokens, **kwargs)

</t>
<t tx="karstenw.20230303134406.9">def tokenize(s, *args, **kwargs):
    """ Returns a list of sentences, where punctuation marks have been split from words.
    """
    return parser.find_tokens(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303134421.1">#### PATTERN | ES | PARSER COMMAND-LINE ############################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303134428.1">from __future__ import absolute_import

from .__init__ import commandline, parse
commandline(parse)
</t>
<t tx="karstenw.20230303134433.1">#### PATTERN | ES | INFLECT ########################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2012 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).

####################################################################################################
# Regular expressions-based rules for Spanish word inflection:
# - pluralization and singularization of nouns,
# - conjugation of verbs,
# - predicative adjectives.

# Accuracy:
# 78% for pluralize()
# 94% for singularize()
# 81% for Verbs.find_lemma() (0.55 regular 87% + 0.45 irregular 74%)
# 87% for Verbs.find_lexeme() (0.55 regular 99% + 0.45 irregular 72%)
# 93% for predicative()

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303134440.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
import re

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

from pattern.text import Verbs as _Verbs
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE, CONDITIONAL,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    INDICATIVE, IMPERATIVE, SUBJUNCTIVE,
    IMPERFECTIVE, PERFECTIVE, PROGRESSIVE,
    IMPERFECT, PRETERITE,
    PARTICIPLE, GERUND
)

sys.path.pop(0)

VERB, NOUN, ADJECTIVE, ADVERB = "VB", "NN", "JJ", "RB"

VOWELS = ("a", "e", "i", "o", "u")
re_vowel = re.compile(r"a|e|i|o|u", re.I)
is_vowel = lambda ch: ch in VOWELS


</t>
<t tx="karstenw.20230303134440.10">def __init__(self):
    _Verbs.__init__(self, os.path.join(MODULE, "es-verbs.txt"),
        language = "es",
         default = {},
          format = [
            0, 1, 2, 3, 4, 5, 6, 8,     # indicativo presente
            34, 35, 36, 37, 38, 39, 24, # indicativo pretérito
            17, 18, 19, 20, 21, 22,     # indicativo imperfecto
            40, 41, 42, 43, 44, 45,     # indicativo futuro
            46, 47, 48, 49, 50, 51,     # indicativo condicional
            52, 54,                     # imperativo afirmativo
            55, 56, 57, 58, 59, 60,     # subjuntivo presente
            67, 68, 69, 70, 71, 72      # subjuntivo imperfecto
        ])

</t>
<t tx="karstenw.20230303134440.11">def find_lemma(self, verb):
    """ Returns the base form of the given inflected verb, using a rule-based approach.
    """
    # Spanish has 12,000+ verbs, ending in -ar (85%), -er (8%), -ir (7%).
    # Over 65% of -ar verbs (6500+) have a regular inflection.
    v = verb.lower()
    # Probably ends in -ir if preceding vowel in stem is -i.
    er_ir = lambda b: (len(b) &gt; 2 and b[-2] == "i") and b + "ir" or b + "er"
    # Probably infinitive if ends in -ar, -er or -ir.
    if v.endswith(("ar", "er", "ir")):
        return v
    # Ruleset for irregular inflections adds 10% accuracy.
    for a, b in verb_irregular_inflections:
        if v.endswith(a):
            return v[:-len(a)] + b
    # reconozco =&gt; reconocer
    v = v.replace("zco", "ce")
    # reconozcamos =&gt; reconocer
    v = v.replace("zca", "ce")
    # reconozcáis =&gt; reconocer
    v = v.replace("zcá", "ce")
    # saldrár =&gt; saler
    if "ldr" in v:
        return v[:v.index("ldr") + 1] + "er"
    # compondrán =&gt; componer
    if "ndr" in v:
        return v[:v.index("ndr") + 1] + "er"
    # Many verbs end in -ar and have a regular inflection:
    for x in ((
      "ando", "ado", "ad",                                # participle
      "aré", "arás", "ará", "aremos", "aréis", "arán", # future
      "aría", "arías", "aríamos", "aríais", "arían",    # conditional
      "aba", "abas", "ábamos", "abais", "aban",         # past imperfective
      "é", "aste", "ó", "asteis", "aron",               # past perfective
      "ara", "aras", "áramos", "arais", "aran")):       # past subjunctive
        if v.endswith(x):
            return v[:-len(x)] + "ar"
    # Many verbs end in -er and have a regular inflection:
    for x in ((
      "iendo", "ido", "ed",                               # participle
      "eré", "erás", "erá", "eremos", "eréis", "erán", # future
      "ería", "erías", "eríamos", "eríais", "erían",    # conditional
      "ía", "ías", "íamos", "íais", "ían",              # past imperfective
      "í", "iste", "ió", "imos", "isteis", "ieron",        # past perfective
      "era", "eras", "éramos", "erais", "eran")):       # past subjunctive
        if v.endswith(x):
            return er_ir(v[:-len(x)])
    # Many verbs end in -ir and have a regular inflection:
    for x in ((
      "iré", "irás", "irá", "iremos", "iréis", "irán", # future
      "iría", "irías", "iríamos", "iríais", "irían")):  # past subjunctive
        if v.endswith(x):
            return v[:-len(x)] + "ir"
    # Present 1sg -o: yo hablo, como, vivo =&gt; hablar, comer, vivir.
    if v.endswith("o"):
        return v[:-1] + "ar"
    # Present 2sg, 3sg and 3pl: tú hablas.
    if v.endswith(("as", "a", "an")):
        return v.rstrip("sn")[:-1] + "ar"
    # Present 2sg, 3sg and 3pl: tú comes, tú vives.
    if v.endswith(("es", "e", "en")):
        return er_ir(v.rstrip("sn")[:-1])
    # Present 1pl and 2pl: nosotros hablamos.
    for i, x in enumerate((
      ("amos", "áis"),
      ("emos", "éis"),
      ("imos", "ís"))):
        for x in x:
            if v.endswith(x):
                return v[:-len(x)] + ("ar", "er", "ir")[i]
    return v

</t>
<t tx="karstenw.20230303134440.12">def find_lexeme(self, verb):
    """ For a regular verb (base form), returns the forms using a rule-based approach.
    """
    v = verb.lower()
    if v.endswith(("arse", "erse", "irse")):
        # Reflexive verbs: calmarse (calmar) =&gt; me calmo.
        b = v[:-4]
    else:
        b = v[:-2]
    if v.endswith("ar") or not v.endswith(("er", "ir")):
        # Regular inflection for verbs ending in -ar.
        return [v,
            b + "o", b + "as", b + "a", b + "amos", b + "áis", b + "an", b + "ando",
            b + "é", b + "aste", b + "ó", b + "amos", b + "asteis", b + "aron", b + "ado",
            b + "aba", b + "abas", b + "aba", b + "ábamos", b + "abais", b + "aban",
            v + "é", v + "ás", v + "á", v + "emos", v + "éis", v + "án",
            v + "ía", v + "ías", v + "ía", v + "íamos", v + "íais", v + "ían",
            b + "a", v[:-1] + "d",
            b + "e", b + "es", b + "e", b + "emos", b + "éis", b + "en",
            v + "a", v + "as", v + "a", b + "áramos", v + "ais", v + "an"]
    else:
        # Regular inflection for verbs ending in -er and -ir.
        p1, p2 = v.endswith("er") and ("e", "é") or ("i", "e")
        return [v,
            b + "o", b + "es", b + "e", b + p1 + "mos", b + p2 + "is", b + "en", b + "iendo",
            b + "í", b + "iste", b + "ió", b + "imos", b + "isteis", b + "ieron", b + "ido",
            b + "ía", b + "ías", b + "ía", b + "íamos", b + "íais", b + "ían",
            v + "é", v + "ás", v + "á", v + "emos", v + "éis", v + "án",
            v + "ía", v + "ías", v + "ía", v + "íamos", v + "íais", v + "ían",
            b + "a", v[:-1] + "d",
            b + "a", b + "as", b + "a", b + "amos", b + "áis", b + "an",
            b + "iera", b + "ieras", b + "iera", b + "iéramos", b + "ierais", b + "ieran"]

</t>
<t tx="karstenw.20230303134440.13">def attributive(adjective, gender=MALE):
    w = adjective.lower()
    # normal =&gt; normales
    if PLURAL in gender and not is_vowel(w[-1:]):
        return w + "es"
    # el chico inteligente =&gt; los chicos inteligentes
    if PLURAL in gender and w.endswith(("a", "e")):
        return w + "s"
    # el chico alto =&gt; los chicos altos
    if w.endswith("o"):
        if FEMININE in gender and PLURAL in gender:
            return w[:-1] + "as"
        if FEMININE in gender:
            return w[:-1] + "a"
        if PLURAL in gender:
            return w + "s"
    return w

#print(attributive("intelligente", gender=PLURAL)) # intelligentes
#print(attributive("alto", gender=MALE+PLURAL))    # altos
#print(attributive("alto", gender=FEMALE+PLURAL))  # altas
#print(attributive("normal", gender=MALE))         # normal
#print(attributive("normal", gender=FEMALE))       # normal
#print(attributive("normal", gender=PLURAL))       # normales


</t>
<t tx="karstenw.20230303134440.14">def predicative(adjective):
    """ Returns the predicative adjective (lowercase).
        In Spanish, the attributive form is always used for descriptive adjectives:
        "el chico alto" =&gt; masculine,
        "la chica alta" =&gt; feminine.
        The predicative is useful for lemmatization.
    """
    w = adjective.lower()
    # histéricos =&gt; histérico
    if w.endswith(("os", "as")):
        w = w[:-1]
    # histérico =&gt; histérico
    if w.endswith("o"):
        return w
    # histérica =&gt; histérico
    if w.endswith("a"):
        return w[:-1] + "o"
    # horribles =&gt; horrible, humorales =&gt; humoral
    if w.endswith("es"):
        if len(w) &gt;= 4 and not is_vowel(normalize(w[-3])) and not is_vowel(normalize(w[-4])):
            return w[:-1]
        return w[:-2]
    return w
</t>
<t tx="karstenw.20230303134440.2">def normalize(vowel):
    return {"á": "a", "é": "e", "í": "i", "ó": "o", "ú": "u"}.get(vowel, vowel)

#### ARTICLE #######################################################################################
# Spanish inflection of depends on gender and number.

# Inflection gender.
MASCULINE, FEMININE, NEUTER, PLURAL = \
    MALE, FEMALE, NEUTRAL, PLURAL = \
        M, F, N, PL = "m", "f", "n", "p"


</t>
<t tx="karstenw.20230303134440.3">def definite_article(word, gender=MALE):
    """ Returns the definite article (el/la/los/las) for a given word.
    """
    if MASCULINE in gender:
        return PLURAL in gender and "los" or "el"
    return PLURAL in gender and "las" or "la"


</t>
<t tx="karstenw.20230303134440.4">def indefinite_article(word, gender=MALE):
    """ Returns the indefinite article (un/una/unos/unas) for a given word.
    """
    if MASCULINE in gender:
        return PLURAL in gender and "unos" or "un"
    return PLURAL in gender and "unas" or "una"

DEFINITE = "definite"
INDEFINITE = "indefinite"


</t>
<t tx="karstenw.20230303134440.5">def article(word, function=INDEFINITE, gender=MALE):
    """ Returns the indefinite (un) or definite (el) article for the given word.
    """
    return function == DEFINITE \
       and definite_article(word, gender) \
        or indefinite_article(word, gender)
_article = article


</t>
<t tx="karstenw.20230303134440.6">def referenced(word, article=INDEFINITE, gender=MALE):
    """ Returns a string with the article + the word.
    """
    return "%s %s" % (_article(word, article, gender), word)

#### PLURALIZE #####################################################################################

plural_irregular = {
     "mamá": "mamás",
     "papá": "papás",
     "sofá": "sofás",
   "dominó": "dominós",
}


</t>
<t tx="karstenw.20230303134440.7">def pluralize(word, pos=NOUN, custom={}):
    """ Returns the plural of a given word.
        For example: gato =&gt; gatos.
        The custom dictionary is for user-defined replacements.
    """
    if word in custom:
        return custom[word]
    w = word.lower()
    # Article: masculine el =&gt; los, feminine la =&gt; las.
    if w == "el":
        return "los"
    if w == "la":
        return "las"
    # Irregular inflections.
    if w in plural_irregular:
        return plural_irregular[w]
    # Words endings that are unlikely to inflect.
    if w.endswith((
      "idad",
      "esis", "isis", "osis",
      "dica", "grafía", "logía")):
        return w
    # Words ending in a vowel get -s: gato =&gt; gatos.
    if w.endswith(VOWELS) or w.endswith("é"):
        return w + "s"
    # Words ending in a stressed vowel get -s: hindú =&gt; hindúes.
    if w.endswith(("á", "é", "í", "ó", "ú")):
        return w + "es"
    # Words ending in -és get -eses: holandés =&gt; holandeses.
    if w.endswith("és"):
        return w[:-2] + "eses"
    # Words ending in -s preceded by an unstressed vowel: gafas =&gt; gafas.
    if w.endswith("s") and len(w) &gt; 3 and is_vowel(w[-2]):
        return w
    # Words ending in -z get -ces: luz =&gt; luces
    if w.endswith("z"):
        return w[:-1] + "ces"
    # Words that change vowel stress: graduación =&gt; graduaciones.
    for a, b in (
      ("án", "anes"),
      ("én", "enes"),
      ("ín", "ines"),
      ("ón", "ones"),
      ("ún", "unes")):
        if w.endswith(a):
            return w[:-2] + b
    # Words ending in a consonant get -es.
    return w + "es"

#print(pluralize("libro"))  # libros
#print(pluralize("señor"))  # señores
#print(pluralize("ley"))    # leyes
#print(pluralize("mes"))    # meses
#print(pluralize("luz"))    # luces
#print(pluralize("inglés")) # ingleses
#print(pluralize("rubí"))   # rubíes
#print(pluralize("papá"))   # papás

#### SINGULARIZE ###################################################################################


</t>
<t tx="karstenw.20230303134440.8">def singularize(word, pos=NOUN, custom={}):
    if word in custom:
        return custom[word]
    w = word.lower()
    # los gatos =&gt; el gato
    if pos == "DT":
        if w in ("la", "las", "los"):
            return "el"
        if w in ("una", "unas", "unos"):
            return "un"
        return w
    # hombres =&gt; hombre
    if w.endswith("es") and w[:-2].endswith(("br", "i", "j", "t", "zn")):
        return w[:-1]
    # gestiones =&gt; gestión
    for a, b in (
      ("anes", "án"),
      ("enes", "én"),
      ("eses", "és"),
      ("ines", "ín"),
      ("ones", "ón"),
      ("unes", "ún")):
        if w.endswith(a):
            return w[:-4] + b
    # hipotesis =&gt; hipothesis
    if w.endswith(("esis", "isis", "osis")):
        return w
    # luces =&gt; luz
    if w.endswith("ces"):
        return w[:-3] + "z"
    # hospitales =&gt; hospital
    if w.endswith("es"):
        return w[:-2]
    # gatos =&gt; gato
    if w.endswith("s"):
        return w[:-1]
    return w

#### VERB CONJUGATION ##############################################################################

verb_irregular_inflections = [
    ( "yéramos", "ir"   ), ( "cisteis", "cer"   ), ( "tuviera", "tener"), ( "ndieron", "nder" ),
    ( "ndiendo", "nder" ), ( "tándose", "tarse" ), ( "ndieran", "nder" ), ( "ndieras", "nder" ),
    ( "izaréis", "izar" ), ( "disteis", "der"   ), ( "irtiera", "ertir"), ( "pusiera", "poner"),
    ( "endiste", "ender"), ( "laremos", "lar"   ), ( "ndíamos", "nder" ), ( "icaréis", "icar" ),
    ( "dábamos", "dar"  ), ( "intiera", "entir" ), ( "iquemos", "icar" ), ( "jéramos", "cir"  ),
    ( "dierais", "der"  ), ( "endiera", "ender" ), ( "iéndose", "erse" ), ( "jisteis", "cir"  ),
    ( "cierais", "cer"  ), ( "ecíamos", "ecer"  ), ( "áramos", "ar"    ), ( "ríamos", "r"     ),
    ( "éramos", "r"     ), ( "iríais", "ir"     ), (   "temos", "tar"  ), (   "steis", "r"    ),
    (   "ciera", "cer"  ), (   "erais", "r"     ), (   "timos", "tir"  ), (   "uemos", "ar"   ),
    (   "tiera", "tir"  ), (   "bimos", "bir"   ), (  "ciéis", "ciar"  ), (   "gimos", "gir"  ),
    (   "jiste", "cir"  ), (   "mimos", "mir"   ), (  "guéis", "gar"   ), (  "stéis", "star"  ),
    (   "jimos", "cir"  ), (  "inéis", "inar"   ), (   "jemos", "jar"  ), (   "tenga", "tener"),
    (  "quéis", "car"   ), (  "bíais", "bir"    ), (   "jeron", "cir"  ), (  "uíais", "uir"   ),
    (  "ntéis", "ntar"  ), (   "jeras", "cir"   ), (   "jeran", "cir"  ), (  "ducía", "ducir" ),
    (   "yendo", "ir"   ), (   "eemos", "ear"   ), (   "ierta", "ertir"), (   "ierte", "ertir"),
    (   "nemos", "nar"  ), (  "ngáis", "ner"    ), (   "liera", "ler"  ), (  "endió", "ender" ),
    (  "uyáis", "uir"   ), (   "memos", "mar"   ), (   "ciste", "cer"  ), (   "ujera", "ucir" ),
    (   "uimos", "uir"  ), (   "ienda", "ender" ), (  "lléis", "llar"  ), (   "iemos", "iar"  ),
    (   "iende", "ender"), (   "rimos", "rir"   ), (   "semos", "sar"  ), (  "itéis", "itar"  ),
    (  "gíais", "gir"   ), (  "ndáis", "nder"   ), (  "tíais", "tir"   ), (   "demos", "dar"  ),
    (   "lemos", "lar"  ), (   "ponga", "poner" ), (   "yamos", "ir"   ), (  "icéis", "izar"  ),
    (    "bais", "r"    ), (   "rías", "r"      ), (   "rían", "r"     ), (   "iría", "ir"    ),
    (    "eran", "r"    ), (    "eras", "r"     ), (   "irán", "ir"    ), (   "irás", "ir"    ),
    (    "ongo", "oner" ), (    "aiga", "aer"   ), (   "ímos", "ir"    ), (   "ibía", "ibir"  ),
    (    "diga", "decir"), (   "edía", "edir"   ), (    "orte", "ortar"), (   "guió", "guir"  ),
    (    "iega", "egar" ), (    "oren", "orar"  ), (    "ores", "orar" ), (   "léis", "lar"   ),
    (    "irme", "irmar"), (    "siga", "seguir"), (   "séis", "sar"   ), (   "stré", "strar" ),
    (    "cien", "ciar" ), (    "cies", "ciar"  ), (    "dujo", "ducir"), (    "eses", "esar" ),
    (    "esen", "esar" ), (    "coja", "coger" ), (    "lice", "lizar"), (   "tías", "tir"   ),
    (   "tían", "tir"   ), (    "pare", "parar" ), (    "gres", "grar" ), (    "gren", "grar" ),
    (    "tuvo", "tener"), (   "uían", "uir"    ), (   "uías", "uir"   ), (    "quen", "car"  ),
    (    "ques", "car"  ), (   "téis", "tar"    ), (    "iero", "erir" ), (    "iere", "erir" ),
    (    "uche", "uchar"), (    "tuve", "tener" ), (    "inen", "inar" ), (    "pire", "pirar"),
    (   "reía", "reir"  ), (    "uste", "ustar" ), (   "ibió", "ibir"  ), (    "duce", "ducir"),
    (    "icen", "izar" ), (    "ices", "izar"  ), (    "ines", "inar" ), (    "ires", "irar" ),
    (    "iren", "irar" ), (    "duje", "ducir" ), (    "ille", "illar"), (    "urre", "urrir"),
    (    "tido", "tir"  ), (   "ndió", "nder"   ), (    "uido", "uir"  ), (    "uces", "ucir" ),
    (    "ucen", "ucir" ), (   "iéis", "iar"    ), (   "eció", "ecer"  ), (   "jéis", "jar"   ),
    (    "erve", "ervar"), (    "uyas", "uir"   ), (    "uyan", "uir"  ), (    "tía", "tir"   ),
    (    "uía", "uir"   ), (     "aos", "arse"  ), (     "gue", "gar"  ), (    "qué", "car"   ),
    (     "que", "car"  ), (     "rse", "rse"   ), (     "ste", "r"    ), (     "era", "r"    ),
    (    "tió", "tir"   ), (     "ine", "inar"  ), (     "ré", "r"     ), (      "ya", "ir"   ),
    (      "ye", "ir"   ), (     "tí", "tir"    ), (     "cé", "zar"   ), (      "ie", "iar"  ),
    (      "id", "ir"   ), (     "ué", "ar"     ),
]


</t>
<t tx="karstenw.20230303134440.9">class Verbs(_Verbs):

    @others
verbs = Verbs()

conjugate, lemma, lexeme, tenses = \
    verbs.conjugate, verbs.lemma, verbs.lexeme, verbs.tenses

#### ATTRIBUTIVE &amp; PREDICATIVE #####################################################################


</t>
<t tx="karstenw.20230303135132.1"></t>
<t tx="karstenw.20230303135134.1">@language python
@tabwidth -4
@others
if __name__ == "__main__":
    commandline(parse)
</t>
<t tx="karstenw.20230303135140.1">#### PATTERN | FR ##################################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2013 University of Antwerp, Belgium
# Copyright (c) 2013 St. Lucas University College of Art &amp; Design, Antwerp.
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################
# French linguistical tools using fast regular expressions.

from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

# Import parser base classes.
from pattern.text import (
    Lexicon, Model, Morphology, Context, Parser as _Parser, ngrams, pprint, commandline,
    PUNCTUATION
)
# Import parser universal tagset.
from pattern.text import (
    penntreebank2universal as _penntreebank2universal,
    PTB, PENN, UNIVERSAL,
    NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X
)
# Import parse tree base classes.
from pattern.text.tree import (
    Tree, Text, Sentence, Slice, Chunk, PNPChunk, Chink, Word, table,
    SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA, AND, OR
)
# Import sentiment analysis base classes.
from pattern.text import (
    Sentiment as _Sentiment,
    NOUN, VERB, ADJECTIVE, ADVERB,
    MOOD, IRONY
)
# Import spelling base class.
from pattern.text import (
    Spelling
)
# Import verb tenses.
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    INDICATIVE, IMPERATIVE, SUBJUNCTIVE, CONDITIONAL,
    IMPERFECTIVE, PERFECTIVE, PROGRESSIVE,
    IMPERFECT, PRETERITE,
    PARTICIPLE, GERUND
)
# Import inflection functions.
from pattern.text.fr.inflect import (
    pluralize, singularize, NOUN, VERB, ADJECTIVE,
    verbs, conjugate, lemma, lexeme, tenses,
    predicative, attributive
)
# Import all submodules.
from pattern.text.fr import inflect

sys.path.pop(0)

#--- FRENCH PARSER ---------------------------------------------------------------------------------
# The French parser is based on Lefff (Lexique des Formes Fléchies du Français).
# Benoît Sagot, Lionel Clément, Érice Villemonte de la Clergerie, Pierre Boullier.
# The Lefff 2 syntactic lexicon for French: architecture, acquisition.
# http://alpage.inria.fr/~sagot/lefff-en.html

# For words in Lefff that can have different part-of-speech tags,
# we used Lexique to find the most frequent POS-tag:
# http://www.lexique.org/

_subordinating_conjunctions = set((
    "afin", "comme", "lorsque", "parce", "puisque", "quand", "que", "quoique", "si"
))


</t>
<t tx="karstenw.20230303135140.10">def tokenize(s, *args, **kwargs):
    """ Returns a list of sentences, where punctuation marks have been split from words.
    """
    return parser.find_tokens(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303135140.11">def parse(s, *args, **kwargs):
    """ Returns a tagged Unicode string.
    """
    return parser.parse(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303135140.12">def parsetree(s, *args, **kwargs):
    """ Returns a parsed Text from the given string.
    """
    return Text(parse(s, *args, **kwargs))


</t>
<t tx="karstenw.20230303135140.13">def tree(s, token=[WORD, POS, CHUNK, PNP, REL, LEMMA]):
    """ Returns a parsed Text from the given parsed string.
    """
    return Text(s, token)


</t>
<t tx="karstenw.20230303135140.14">def tag(s, tokenize=True, encoding="utf-8", **kwargs):
    """ Returns a list of (token, tag)-tuples from the given string.
    """
    tags = []
    for sentence in parse(s, tokenize, True, False, False, False, encoding, **kwargs).split():
        for token in sentence:
            tags.append((token[0], token[1]))
    return tags


</t>
<t tx="karstenw.20230303135140.15">def keywords(s, top=10, **kwargs):
    """ Returns a sorted list of keywords in the given string.
    """
    return parser.find_keywords(s, **dict({
        "frequency": parser.frequency,
              "top": top,
              "pos": ("NN",),
           "ignore": ("rt",)}, **kwargs))


</t>
<t tx="karstenw.20230303135140.16">def suggest(w):
    """ Returns a list of (word, confidence)-tuples of spelling corrections.
    """
    return spelling.suggest(w)


</t>
<t tx="karstenw.20230303135140.17">def polarity(s, **kwargs):
    """ Returns the sentence polarity (positive/negative) between -1.0 and 1.0.
    """
    return sentiment(s, **kwargs)[0]


</t>
<t tx="karstenw.20230303135140.18">def subjectivity(s, **kwargs):
    """ Returns the sentence subjectivity (objective/subjective) between 0.0 and 1.0.
    """
    return sentiment(s, **kwargs)[1]


</t>
<t tx="karstenw.20230303135140.19">def positive(s, threshold=0.1, **kwargs):
    """ Returns True if the given sentence has a positive sentiment (polarity &gt;= threshold).
    """
    return polarity(s, **kwargs) &gt;= threshold

split = tree # Backwards compatibility.

#---------------------------------------------------------------------------------------------------
# python -m pattern.fr xml -s "C'est l'exception qui confirme la règle." -OTCL

</t>
<t tx="karstenw.20230303135140.2">def penntreebank2universal(token, tag):
    """ Converts a Penn Treebank II tag to a universal tag.
        For example: comme/IN =&gt; comme/CONJ
    """
    if tag == "IN" and token.lower() in _subordinating_conjunctions:
        return CONJ
    return _penntreebank2universal(token, tag)

ABBREVIATIONS = set((
    "av.", "boul.", "C.-B.", "c.-à-d.", "ex.", "éd.", "fig.", "I.-P.-E.", "J.-C.",
    "Ltee.", "Ltée.", "M.", "Me.", "Mlle.", "Mlles.", "MM.", "N.-B.", "N.-É.", "p.",
    "S.B.E.", "Ste.", "T.-N.", "t.a.b."
))

# While contractions in English are optional,
# they are required in French:
replacements = {
       "l'": "l' ",  # le/la
       "c'": "c' ",  # ce
       "d'": "d' ",  # de
       "j'": "j' ",  # je
       "m'": "m' ",  # me
       "n'": "n' ",  # ne
      "qu'": "qu' ", # que
       "s'": "s' ",  # se
       "t'": "t' ",  # te
   "jusqu'": "jusqu' ",
  "lorsqu'": "lorsqu' ",
  "puisqu'": "puisqu' ",
    # Same rule for Unicode apostrophe, see also Parser.find_tokens():
    r"(l|c|d|j|m|n|qu|s|t|jusqu|lorsqu|puisqu)’": "\\1&amp;rsquo; "
}
replacements.update(((k.upper(), v.upper()) for k, v in list(replacements.items())))


</t>
<t tx="karstenw.20230303135140.3">def find_lemmata(tokens):
    """ Annotates the tokens with lemmata for plural nouns and conjugated verbs,
        where each token is a [word, part-of-speech] list.
    """
    for token in tokens:
        word, pos, lemma = token[0], token[1], token[0]
        if pos.startswith(("DT", "PR", "WP")):
            lemma = singularize(word, pos=pos)
        if pos.startswith(("RB", "IN")) and (word.endswith(("'", "’")) or word == "du"):
            lemma = singularize(word, pos=pos)
        if pos.startswith(("JJ",)):
            lemma = predicative(word)
        if pos == "NNS":
            lemma = singularize(word)
        if pos.startswith(("VB", "MD")):
            lemma = conjugate(word, INFINITIVE) or word
        token.append(lemma.lower())
    return tokens


</t>
<t tx="karstenw.20230303135140.4">class Parser(_Parser):

    @others
</t>
<t tx="karstenw.20230303135140.5">def find_tokens(self, tokens, **kwargs):
    kwargs.setdefault("abbreviations", ABBREVIATIONS)
    kwargs.setdefault("replace", replacements)
    s = _Parser.find_tokens(self, tokens, **kwargs)
    s = [s.replace("&amp;rsquo ;", "’") if isinstance(s, str) else s for s in s]
    return s

</t>
<t tx="karstenw.20230303135140.6">def find_lemmata(self, tokens, **kwargs):
    return find_lemmata(tokens)

</t>
<t tx="karstenw.20230303135140.7">def find_tags(self, tokens, **kwargs):
    if kwargs.get("tagset") in (PENN, None):
        kwargs.setdefault("map", lambda token, tag: (token, tag))
    if kwargs.get("tagset") == UNIVERSAL:
        kwargs.setdefault("map", lambda token, tag: penntreebank2universal(token, tag))
    return _Parser.find_tags(self, tokens, **kwargs)


</t>
<t tx="karstenw.20230303135140.8">class Sentiment(_Sentiment):

    @others
parser = Parser(
     lexicon = os.path.join(MODULE, "fr-lexicon.txt"),
   frequency = os.path.join(MODULE, "fr-frequency.txt"),
  morphology = os.path.join(MODULE, "fr-morphology.txt"),
     context = os.path.join(MODULE, "fr-context.txt"),
     default = ("NN", "NNP", "CD"),
    language = "fr"
)

lexicon = parser.lexicon # Expose lexicon.

sentiment = Sentiment(
        path = os.path.join(MODULE, "fr-sentiment.xml"),
      synset = None,
   negations = ("n'", "ne", "ni", "non", "pas", "rien", "sans", "aucun", "jamais"),
   modifiers = ("RB",),
   modifier = lambda w: w.endswith("ment"),
   tokenizer = parser.find_tokens,
    language = "fr"
)

spelling = Spelling(
        path = os.path.join(MODULE, "fr-spelling.txt")
)


</t>
<t tx="karstenw.20230303135140.9">def load(self, path=None):
    _Sentiment.load(self, path)
    # Map "précaire" to "precaire" (without diacritics, +1% accuracy).
    if not path:
        for w, pos in list(dict.items(self)):
            w0 = w
            if not w.endswith(("à", "è", "é", "ê", "ï")):
                w = w.replace("à", "a")
                w = w.replace("é", "e")
                w = w.replace("è", "e")
                w = w.replace("ê", "e")
                w = w.replace("ï", "i")
            if w != w0:
                for pos, (p, s, i) in pos.items():
                    self.annotate(w, pos, p, s, i)

</t>
<t tx="karstenw.20230303135327.1">#### PATTERN | FR | INFLECT ########################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2013 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).

####################################################################################################
# Regular expressions-based rules for French word inflection:
# - pluralization and singularization of nouns,
# - conjugation of verbs,
# - predicative and attributive of adjectives.

# Accuracy:
# 92% for pluralize()
# 93% for singularize()
# 80% for Verbs.find_lemma() (mixed regular/irregular)
# 86% for Verbs.find_lexeme() (mixed regular/irregular)
# 95% predicative() (measured on Lexique French morphology word forms)

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303135336.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
import re

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

from pattern.text import Verbs as _Verbs
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    INDICATIVE, IMPERATIVE, SUBJUNCTIVE, CONDITIONAL,
    IMPERFECTIVE, PERFECTIVE, PROGRESSIVE,
    IMPERFECT, PRETERITE,
    PARTICIPLE, GERUND
)

sys.path.pop(0)

VERB, NOUN, ADJECTIVE, ADVERB = "VB", "NN", "JJ", "RB"

VOWELS = ("a", "e", "i", "o", "u")
re_vowel = re.compile(r"a|e|i|o|u", re.I)
is_vowel = lambda ch: ch in VOWELS

#### PLURALIZE #####################################################################################

plural_irregular = {
       "bleu": "bleus",
       "pneu": "pneus",
    "travail": "travaux",
    "vitrail": "vitraux"
}


</t>
<t tx="karstenw.20230303135336.2">def pluralize(word, pos=NOUN, custom={}):
    """ Returns the plural of a given word.
        The custom dictionary is for user-defined replacements.
    """
    if word in custom:
        return custom[word]
    w = word.lower()
    if w in plural_irregular:
        return plural_irregular[w]
    if w.endswith(("ais", "ois")):
        return w + "es"
    if w.endswith(("s", "x")):
        return w
    if w.endswith("al"):
        return w[:-2] + "aux"
    if w.endswith(("au", "eu")):
        return w + "x"
    return w + "s"

#### SINGULARIZE ###################################################################################


</t>
<t tx="karstenw.20230303135336.3">def singularize(word, pos=NOUN, custom={}):
    if word in custom:
        return custom[word]
    w = word.lower()
    # Common articles, determiners, pronouns:
    if pos in ("DT", "PRP", "PRP$", "WP", "RB", "IN"):
        if w == "du":
            return "de"
        if w == "ces":
            return "ce"
        if w == "les":
            return "le"
        if w == "des":
            return "un"
        if w == "mes":
            return "mon"
        if w == "ses":
            return "son"
        if w == "tes":
            return "ton"
        if w == "nos":
            return "notre"
        if w == "vos":
            return "votre"
        if w.endswith(("'", "’")):
            return w[:-1] + "e"
    if w.endswith("nnes"):  # parisiennes =&gt; parisien
        return w[:-3]
    if w.endswith("ntes"):  # passantes =&gt; passant
        return w[:-2]
    if w.endswith("euses"): # danseuses =&gt; danseur
        return w[:-3] + "r"
    if w.endswith("s"):
        return w[:-1]
    if w.endswith(("aux", "eux", "oux")):
        return w[:-1]
    if w.endswith("ii"):
        return w[:-1] + "o"
    if w.endswith(("ia", "ma")):
        return w[:-1] + "um"
    if "-" in w:
        return singularize(w.split("-")[0]) + "-" + "-".join(w.split("-")[1:])
    return w

#### VERB CONJUGATION ##############################################################################

verb_inflections = [
    ("issaient", "ir"   ), ("eassions", "er"   ), ("dissions", "dre" ), ("çassions", "cer"  ),
    ( "eraient", "er"   ), ( "assions", "er"   ), ( "issions", "ir"  ), (  "iraient", "ir"   ),
    ( "isaient", "ire"  ), ( "geaient", "ger"  ), ( "eassent", "er"  ), (  "geasses", "ger"  ),
    ( "eassiez", "er"   ), ( "dissiez", "dre"  ), ( "dissent", "dre" ), (  "endrons", "endre"),
    ( "endriez", "endre"), ( "endrais", "endre"), (  "erions", "er"  ), (   "assent", "er"   ),
    (  "assiez", "er"   ), (  "raient", "re"   ), (  "issent", "ir"  ), (   "issiez", "ir"   ),
    (  "irions", "ir"   ), (  "issons", "ir"   ), (  "issant", "ir"  ), (   "issait", "ir"   ),
    (  "issais", "ir"   ), (   "aient", "er"   ), (  "èrent", "er"  ), (    "erait", "er"   ),
    (   "eront", "er"   ), (   "erons", "er"   ), (   "eriez", "er"  ), (    "erais", "er"   ),
    (   "asses", "er"   ), (   "rions", "re"   ), (   "isses", "ir"  ), (    "irent", "ir"   ),
    (   "irait", "ir"   ), (   "irons", "ir"   ), (   "iriez", "ir"  ), (    "irais", "ir"   ),
    (   "iront", "ir"   ), (   "issez", "ir"   ), (    "ions", "er"  ), (     "erez", "er"   ),
    (    "eras", "er"   ), (    "erai", "er"   ), (    "asse", "er"  ), (     "âtes", "er"   ),
    (    "âmes", "er"   ), (    "isse", "ir"   ), (    "îtes", "ir"  ), (     "îmes", "ir"   ),
    (    "irez", "ir"   ), (    "iras", "ir"   ), (    "irai", "ir"  ), (     "ront", "re"   ),
    (     "iez", "er"   ), (     "ent", "er"   ), (     "ais", "er"  ), (      "ons", "er"   ),
    (     "ait", "er"   ), (     "ant", "er"   ), (     "era", "er"  ), (      "ira", "ir"   ),
    (      "es", "er"   ), (      "ez", "er"   ), (      "as", "er"  ), (       "ai", "er"   ),
    (      "ât", "er"   ), (      "ds", "dre"  ), (      "is", "ir"  ), (       "it", "ir"   ),
    (      "ît", "ir"   ), (      "ïr", "ïr"   ), (      "nd", "ndre"), (       "nu", "nir"  ),
    (       "e", "er"   ), (       "é", "er"   ), (       "a", "er"  ), (        "t", "re"   ),
    (       "s", "re"   ), (       "i", "ir"   ), (       "û", "ir"  ), (        "u", "re"   ),
    (       "d", "dre"  )
]


</t>
<t tx="karstenw.20230303135336.4">class Verbs(_Verbs):

    @others
verbs = Verbs()

conjugate, lemma, lexeme, tenses = \
    verbs.conjugate, verbs.lemma, verbs.lexeme, verbs.tenses

#### ATTRIBUTIVE &amp; PREDICATIVE #####################################################################


</t>
<t tx="karstenw.20230303135336.5">def __init__(self):
    _Verbs.__init__(self, os.path.join(MODULE, "fr-verbs.txt"),
        language = "fr",
         default = {},
          format = [
            0, 1, 2, 3, 4, 5, 6, 8, 24, # indicatif présent
            34, 35, 36, 37, 38, 39,     # indicatif passé simple
            17, 18, 19, 20, 21, 22,     # indicatif imparfait
            40, 41, 42, 43, 44, 45,     # indicatif futur simple
            46, 47, 48, 49, 50, 51,     # conditionnel présent
            52, 53, 54,                 # impératif présent
            55, 56, 57, 58, 59, 60,     # subjonctif présent
            67, 68, 69, 70, 71, 72      # subjonctif imparfait
        ])

</t>
<t tx="karstenw.20230303135336.6">def find_lemma(self, verb):
    """ Returns the base form of the given inflected verb, using a rule-based approach.
    """
    # French has 20,000+ verbs, ending in -er (majority), -ir, -re.
    v = verb.lower()
    if v.endswith(("er", "ir", "re")):
        return v
    for a, b in verb_inflections:
        if v.endswith(a):
            return v[:-len(a)] + b
    return v

</t>
<t tx="karstenw.20230303135336.7">def find_lexeme(self, verb):
    """ For a regular verb (base form), returns the forms using a rule-based approach.
    """
    v = verb.lower()
    b = v[:-2]
    if v.endswith("ir") and not \
       v.endswith(("couvrir", "cueillir", "découvrir", "offrir", "ouvrir", "souffrir")):
        # Regular inflection for verbs ending in -ir.
        # Some -ir verbs drop the last letter of the stem: dormir =&gt; je dors (not: je dormis).
        if v.endswith(("dormir", "mentir", "partir", "sentir", "servir", "sortir")):
            b0 = b[:-1]
        else:
            b0 = b + "i"
        return [v,
            b0 + "s", b0 + "s", b0 + "t", b + "issons", b + "issez", b + "issent", b + "issant", b + "i",
            b + "is", b + "is", b + "it", b + "îmes", b + "îtes", b + "irent",
            b + "issais", b + "issais", b + "issait", b + "issions", b + "issiez", b + "issaient",
            v + "ai", v + "as", v + "a", v + "ons", v + "ez", v + "ont",
            v + "ais", v + "ais", v + "ait", v + "ions", v + "iez", v + "aient",
            b + "is", b + "issons", b + "issez",
            b + "isse", b + "isses", b + "isse", b + "issions", b + "issiez", b + "issent",
            b + "isse", b + "isses", b + "ît", b + "issions", b + "issiez", b + "issent"
        ]
    elif v.endswith("re"):
        # Regular inflection for verbs ending in -re.
        # Verbs ending in -attre and -ettre drop the -t in the singular form.
        if v.endswith(("ttre")):
            b0 = b1 = b[:-1]
        else:
            b0 = b1 = b
        # Verbs ending in -aindre, -eindre and -oindre drop the -d.
        if v.endswith("indre"):
            b0, b1 = b[:-1], b[:-2] + "gn"
        # Verbs ending in -prendre drop the -d in the plural form.
        if v.endswith("prendre"):
            b0, b1 = b, b[:-1]
        return [v,
            b0 + "s", b0 + "s", b0 + "", b1 + "ons", b1 + "ez", b1 + "ent", b1 + "ant", b + "u",
            b + "is", b + "is", b + "it", b1 + "îmes", b1 + "îtes", b1 + "irent",
            b + "ais", b + "ais", b + "ait", b1 + "ions", b1 + "iez", b1 + "aient",
            b + "rai", b + "ras", b + "ra", b + "rons", b + "rez", b + "ront",
            b + "ais", b + "ais", b + "ait", b1 + "ions", b1 + "iez", b1 + "aient",
            b0 + "s", b1 + "ons", b1 + "ez",
            b + "e", b + "es", b + "e", b1 + "ions", b1 + "iez", b1 + "ent",
            b + "isse", b + "isses", b + "ît", b1 + "issions", b1 + "issiez", b1 + "issent"
        ]
    else:
        # Regular inflection for verbs ending in -er.
        # If the stem ends in -g, use -ge before hard vowels -a and -o: manger =&gt; mangeons.
        # If the stem ends in -c, use -ç before hard vowels -a and -o: lancer =&gt; lançons.
        e = v.endswith("ger") and "e" or ""
        c = v.endswith("cer") and b[:-1] + "ç" or b
        return [v,
            b + "e", b + "es", b + "e", c + e + "ons", b + "ez", b + "ent", c + e + "ant", b + "é",
            c + e + "ai", c + e + "as", c + e + "a", c + e + "âmes", c + e + "âtes", b + "èrent",
            c + e + "ais", c + e + "ais", c + e + "ait", b + "ions", b + "iez", c + e + "aient",
            v + "ai", v + "as", v + "a", v + "ons", v + "ez", v + "ont",
            v + "ais", v + "ais", v + "ait", v + "ions", v + "iez", v + "aient",
            b + "e", c + e + "ons", b + "ez",
            b + "e", b + "es", b + "e", b + "ions", b + "iez", b + "ent",
            c + e + "asse", c + e + "asses", c + e + "ât", c + e + "assions", c + e + "assiez", c + e + "assent"
        ]

</t>
<t tx="karstenw.20230303135336.8">def attributive(adjective):
    """ For a predicative adjective, returns the attributive form.
    """
    # Must deal with feminine and plural.
    raise NotImplementedError


</t>
<t tx="karstenw.20230303135336.9">def predicative(adjective):
    """ Returns the predicative adjective (lowercase): belles =&gt; beau.
    """
    w = adjective.lower()
    if w.endswith(("ais", "ois")):
        return w
    if w.endswith(("és", "ée", "ées")):
        return w.rstrip("es")
    if w.endswith(("que", "ques")):
        return w.rstrip("s")
    if w.endswith(("nts", "nte", "ntes")):
        return w.rstrip("es")
    if w.endswith("eaux"):
        return w.rstrip("x")
    if w.endswith(("aux", "ale", "ales")):
        return w.rstrip("uxles") + "l"
    if w.endswith(("rteuse", "rteuses", "ailleuse")):
        return w.rstrip("es") + "r"
    if w.endswith(("euse", "euses")):
        return w.rstrip("es") + "x"
    if w.endswith(("els", "elle", "elles")):
        return w.rstrip("les") + "el"
    if w.endswith(("ifs", "ive", "ives")):
        return w.rstrip("es")[:-2] + "if"
    if w.endswith(("is", "ie", "ies")):
        return w.rstrip("es")
    if w.endswith(("enne", "ennes")):
        return w.rstrip("nes") + "en"
    if w.endswith(("onne", "onnes")):
        return w.rstrip("nes") + "n"
    if w.endswith(("igne", "ignes", "ingue", "ingues")):
        return w.rstrip("s")
    if w.endswith(("ène", "ènes")):
        return w.rstrip("s")
    if w.endswith(("ns", "ne", "nes")):
        return w.rstrip("es")
    if w.endswith(("ite", "ites")):
        return w.rstrip("es")
    if w.endswith(("is", "ise", "ises")):
        return w.rstrip("es") + "s"
    if w.endswith(("rice", "rices")):
        return w.rstrip("rices") + "eur"
    if w.endswith(("iers", "ière", "ières")):
        return w.rstrip("es")[:-3] + "ier"
    if w.endswith(("ette", "ettes")):
        return w.rstrip("tes") + "et"
    if w.endswith(("rds", "rde", "rdes")):
        return w.rstrip("es")
    if w.endswith(("nds", "nde", "ndes")):
        return w.rstrip("es")
    if w.endswith(("us", "ue", "ues")):
        return w.rstrip("es")
    return w.rstrip("s")
</t>
<t tx="karstenw.20230303135415.1"></t>
<t tx="karstenw.20230303135420.1">#### PATTERN | IT ##################################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2013 University of Antwerp, Belgium
# Copyright (c) 2013 St. Lucas University College of Art &amp; Design, Antwerp.
# Author: Tom De Smedt &lt;tom@organisms.be&gt;, Fabio Marfia &lt;marfia@elet.polimi.it&gt;
# License: BSD (see LICENSE.txt for details).

####################################################################################################
# Italian linguistical tools using fast regular expressions.

@others
if __name__ == "__main__":
    commandline(parse)
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303135427.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

# Import parser base classes.
from pattern.text import (
    Lexicon, Model, Morphology, Context, Parser as _Parser, ngrams, pprint, commandline,
    PUNCTUATION
)
# Import parser universal tagset.
from pattern.text import (
    penntreebank2universal as _penntreebank2universal,
    PTB, PENN, UNIVERSAL,
    NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X
)
# Import parse tree base classes.
from pattern.text.tree import (
    Tree, Text, Sentence, Slice, Chunk, PNPChunk, Chink, Word, table,
    SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA, AND, OR
)
# Import sentiment analysis base classes.
from pattern.text import (
    Sentiment as _Sentiment,
    NOUN, VERB, ADJECTIVE, ADVERB,
    MOOD, IRONY
)
# Import spelling base class.
from pattern.text import (
    Spelling
)
# Import verb tenses.
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE, CONDITIONAL,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    INDICATIVE, IMPERATIVE, SUBJUNCTIVE,
    IMPERFECTIVE, PERFECTIVE, PROGRESSIVE,
    IMPERFECT, PRETERITE,
    PARTICIPLE, GERUND
)
# Import inflection functions.
from pattern.text.it.inflect import (
    article, referenced, DEFINITE, INDEFINITE,
    pluralize, singularize, NOUN, VERB, ADJECTIVE,
    verbs, conjugate, lemma, lexeme, tenses,
    predicative, attributive,
    gender, MASCULINE, MALE, FEMININE, FEMALE, NEUTER, NEUTRAL, PLURAL, M, F, N, PL
)
# Import all submodules.
from pattern.text.it import inflect

sys.path.pop(0)

#--- PARSER ----------------------------------------------------------------------------------------

_subordinating_conjunctions = set((
    "che"   , "perché", "sebbene",
    "come"  , "poiché", "senza",
    "se"    , "perciò", "salvo",
    "mentre", "finché", "dopo",
    "quando", "benché"
))


</t>
<t tx="karstenw.20230303135427.10">def tokenize(s, *args, **kwargs):
    """ Returns a list of sentences, where punctuation marks have been split from words.
    """
    return parser.find_tokens(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303135427.11">def parse(s, *args, **kwargs):
    """ Returns a tagged Unicode string.
    """
    return parser.parse(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303135427.12">def parsetree(s, *args, **kwargs):
    """ Returns a parsed Text from the given string.
    """
    return Text(parse(s, *args, **kwargs))


</t>
<t tx="karstenw.20230303135427.13">def tree(s, token=[WORD, POS, CHUNK, PNP, REL, LEMMA]):
    """ Returns a parsed Text from the given parsed string.
    """
    return Text(s, token)


</t>
<t tx="karstenw.20230303135427.14">def tag(s, tokenize=True, encoding="utf-8", **kwargs):
    """ Returns a list of (token, tag)-tuples from the given string.
    """
    tags = []
    for sentence in parse(s, tokenize, True, False, False, False, encoding, **kwargs).split():
        for token in sentence:
            tags.append((token[0], token[1]))
    return tags


</t>
<t tx="karstenw.20230303135427.15">def keywords(s, top=10, **kwargs):
    """ Returns a sorted list of keywords in the given string.
    """
    return parser.find_keywords(s, **dict({
        "frequency": parser.frequency,
              "top": top,
              "pos": ("NN",),
           "ignore": ("rt",)}, **kwargs))


</t>
<t tx="karstenw.20230303135427.16">def suggest(w):
    """ Returns a list of (word, confidence)-tuples of spelling corrections.
    """
    return spelling.suggest(w)


</t>
<t tx="karstenw.20230303135427.17">def polarity(s, **kwargs):
    """ Returns the sentence polarity (positive/negative) between -1.0 and 1.0.
    """
    return sentiment(s, **kwargs)[0]


</t>
<t tx="karstenw.20230303135427.18">def subjectivity(s, **kwargs):
    """ Returns the sentence subjectivity (objective/subjective) between 0.0 and 1.0.
    """
    return sentiment(s, **kwargs)[1]


</t>
<t tx="karstenw.20230303135427.19">def positive(s, threshold=0.1, **kwargs):
    """ Returns True if the given sentence has a positive sentiment (polarity &gt;= threshold).
    """
    return polarity(s, **kwargs) &gt;= threshold

split = tree # Backwards compatibility.

#---------------------------------------------------------------------------------------------------
# python -m pattern.it xml -s "Il gatto nero faceva le fusa." -OTCL

</t>
<t tx="karstenw.20230303135427.2">def penntreebank2universal(token, tag):
    """ Converts a Penn Treebank II tag to a universal tag.
        For example: che/IN =&gt; che/CONJ
    """
    if tag == "IN" and token.lower() in _subordinating_conjunctions:
        return CONJ
    return _penntreebank2universal(token, tag)

ABBREVIATIONS = [
    "a.C.", "all.", "apr.", "art.", "artt.", "b.c.", "c.a.", "cfr.", "c.d.",
    "c.m.", "C.V.", "d.C.", "Dott.", "ecc.", "egr.", "e.v.", "fam.", "giu.",
    "Ing.", "L.", "n.", "op.", "orch.", "p.es.", "Prof.", "prof.", "ql.co.",
    "secc.", "sig.", "s.l.m.", "s.r.l.", "Spett.", "S.P.Q.C.", "v.c."
]

replacements = (
    "a", "co", "all", "anch", "nient", "cinquant",
    "b", "de", "dev", "bell", "quell", "diciott",
    "c", "gl", "don", "cent", "quest", "occupo",
    "d", "po", "dov", "dall", "trent", "sessant",
    "l", "un", "nel", "dell", "tropp",
    "m",              "king",
    "n",              "nell",
    "r",              "sant",
    "s",              "sott",
                      "sull",
                      "tant",
                      "tutt",
                      "vent")

replacements += tuple(k.capitalize() for k in replacements)
replacements = dict((k + "'", k + "' ") for k in replacements)


</t>
<t tx="karstenw.20230303135427.3">def find_lemmata(tokens):
    """ Annotates the tokens with lemmata for plural nouns and conjugated verbs,
        where each token is a [word, part-of-speech] list.
    """
    for token in tokens:
        word, pos, lemma = token[0], token[1], token[0]
        if pos.startswith(("DT",)):
            lemma = singularize(word, pos="DT")
        if pos.startswith("JJ"):
            lemma = predicative(word)
        if pos == "NNS":
            lemma = singularize(word)
        if pos.startswith(("VB", "MD")):
            lemma = conjugate(word, INFINITIVE) or word
        token.append(lemma.lower())
    return tokens


</t>
<t tx="karstenw.20230303135427.4">class Parser(_Parser):

    @others
</t>
<t tx="karstenw.20230303135427.5">def find_tokens(self, tokens, **kwargs):
    kwargs.setdefault("abbreviations", ABBREVIATIONS)
    kwargs.setdefault("replace", replacements)
    #return _Parser.find_tokens(self, tokens, **kwargs)

    s = _Parser.find_tokens(self, tokens, **kwargs)
    s = [s.replace(" &amp;contraction ;", "'").replace("XXX -", "-") for s in s]
    return s

</t>
<t tx="karstenw.20230303135427.6">def find_lemmata(self, tokens, **kwargs):
    return find_lemmata(tokens)

</t>
<t tx="karstenw.20230303135427.7">def find_tags(self, tokens, **kwargs):
    if kwargs.get("tagset") in (PENN, None):
        kwargs.setdefault("map", lambda token, tag: (token, tag))
    if kwargs.get("tagset") == UNIVERSAL:
        kwargs.setdefault("map", lambda token, tag: penntreebank2universal(token, tag))
    return _Parser.find_tags(self, tokens, **kwargs)


</t>
<t tx="karstenw.20230303135427.8">class Sentiment(_Sentiment):

    @others
parser = Parser(
     lexicon = os.path.join(MODULE, "it-lexicon.txt"),
   frequency = os.path.join(MODULE, "it-frequency.txt"),
  morphology = os.path.join(MODULE, "it-morphology.txt"),
     context = os.path.join(MODULE, "it-context.txt"),
     default = ("NN", "NNP", "CD"),
    language = "it"
)

lexicon = parser.lexicon # Expose lexicon.

sentiment = Sentiment(
        path = os.path.join(MODULE, "it-sentiment.xml"),
      synset = None,
   negations = ("mai", "no", "non"),
   modifiers = ("RB",),
   modifier = lambda w: w.endswith(("mente")),
   tokenizer = parser.find_tokens,
    language = "it"
)

spelling = Spelling(
        path = os.path.join(MODULE, "it-spelling.txt")
)


</t>
<t tx="karstenw.20230303135427.9">def load(self, path=None):
    _Sentiment.load(self, path)

</t>
<t tx="karstenw.20230303135430.1">#### PATTERN | IT | INFLECT ########################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2013 University of Antwerp, Belgium
# Copyright (c) 2013 St. Lucas University College of Art &amp; Design, Antwerp.
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).

####################################################################################################
# Regular expressions-based rules for Italian word inflection:
# - pluralization and singularization of nouns,
# - conjugation of verbs,
# - predicative adjectives.

# Accuracy:
# 92% for gender()
# 93% for pluralize()
# 84% for singularize()
# 82% for Verbs.find_lemma()
# 90% for Verbs.find_lexeme()
# 88% for predicative()

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303135437.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
import re

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

# Import Verbs base class and verb tenses.
from pattern.text import Verbs as _Verbs
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE, CONDITIONAL,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    INDICATIVE, IMPERATIVE, SUBJUNCTIVE,
    IMPERFECTIVE, PERFECTIVE, PROGRESSIVE,
    IMPERFECT, PRETERITE,
    PARTICIPLE, GERUND
)

sys.path.pop(0)

VERB, NOUN, ADJECTIVE, ADVERB = "VB", "NN", "JJ", "RB"

VOWELS = "aeiouy"
re_vowel = re.compile(r"a|e|i|o|u|y", re.I)
is_vowel = lambda ch: ch in VOWELS

#### ARTICLE #######################################################################################

# Inflection gender.
MASCULINE, FEMININE, NEUTER, PLURAL = \
    MALE, FEMALE, NEUTRAL, PLURAL = \
        M, F, N, PL = "m", "f", "n", "p"

# Word starts with z or s + consonant?
zs = lambda w: w and (w[:1] == "z" or (w[:1] == "s" and not is_vowel(w[1:2])))


</t>
<t tx="karstenw.20230303135437.10">def __init__(self):
    _Verbs.__init__(self, os.path.join(MODULE, "it-verbs.txt"),
        language = "it",
         default = {},
          format = [
            0, 1, 2, 3, 4, 5, 6, 8,     # indicativo presente
            34, 35, 36, 37, 38, 39, 24, # indicativo passato remoto
            17, 18, 19, 20, 21, 22,     # indicativo imperfetto
            40, 41, 42, 43, 44, 45,     # indicativo futuro semplice
            46, 47, 48, 49, 50, 51,     # condizionale presente
                52, 521, 53, 54, 541,    # imperativo
            55, 56, 57, 58, 59, 60,     # congiuntivo presente
            67, 68, 69, 70, 71, 72      # congiontive imperfetto
        ])

</t>
<t tx="karstenw.20230303135437.11">def find_lemma(self, verb):
    """ Returns the base form of the given inflected verb, using a rule-based approach.
    """
    v = verb.lower()
    # Probably infinitive if ends in -are, -ere, -ire or reflexive -rsi.
    if v.endswith(("are", "ere", "ire", "rsi")):
        return v
    # Ruleset adds 3% accuracy.
    for a, b in verb_majority_vote:
        if v.endswith(a):
            return v[:-len(a)] + b
    v = v.replace("cha", "ca")
    v = v.replace("che", "ce")
    v = v.replace("gha", "ga")
    v = v.replace("ghe", "ge")
    v = v.replace("ghi", "gi")
    v = v.replace("gge", "ggie")
    # Many verbs end in -ire and have a regular inflection:
    for x in ((
      "irò", "irai", "irà", "iremo", "irete", "iranno",           # future
      "irei", "iresti", "irebbe", "iremmo", "ireste", "irebbero", # conditional
      "ascano",                                                   # subjunctive I
      "issi", "isse", "issimo", "iste", "issero",                 # subjunctive II
      "ivo", "ivi", "iva", "ivamo", "ivate", "ivano",             # past imperfective
      "isti", "immo", "iste", "irono", "ito",                     # past perfective
      "isco", "isci", "isce", "ite", "iscono", "indo")):          # present
        if v.endswith(x):
            return v[:-len(x)] + "ire"
    # Many verbs end in -are and have a regular inflection:
    for x in ((
      "erò", "erai", "erà", "eremo", "erete", "eranno",           # future
      "erei", "eresti", "erebbe", "eremmo", "ereste", "erebbero", # conditional
      "iamo", "iate", "ino",                                      # subjunctive I
      "assi", "asse", "assimo", "aste", "assero",                 # subjunctive II
      "avo", "avi", "ava", "avamo", "avate", "avano",             # past imperfective
      "ai", "asti", "ò", "ammo", "aste", "arono", "ato",          # past perfective
      "iamo", "ate", "ano", "ando")):                             # present
        if v.endswith(x):
            return v[:-len(x)] + "are"
    # Many verbs end in -ere and have a regular inflection:
    for x in ((
      "essi", "esse", "essimo", "este", "essero",                 # subjunctive II
      "evo", "evi", "eva", "evamo", "evate", "evano",             # past imperfective
      "ei", "esti", "è", "emmo", "este", "erono", "eto",          # past perfective
      "ete", "ono", "endo")):                                     # present
        if v.endswith(x):
            return v[:-len(x)] + "ere"
    if v.endswith("à"):
        return v[:-1] + "e"
    if v.endswith("ì"):
        return v[:-1] + "ire"
    if v.endswith("e"):
        return v[:-1] + "ere"
    if v.endswith(("a", "i", "o")):
        return v[:-1] + "are"
    return v

</t>
<t tx="karstenw.20230303135437.12">def find_lexeme(self, verb):
    """ For a regular verb (base form), returns the forms using a rule-based approach.
    """
    v = verb.lower()
    v = re.sub(r"rci$", "re", v)
    v = re.sub(r"rsi$", "re", v)
    v = re.sub(r"rre$", "re", v)
    b = v[:-3]
    if verb.endswith(("care", "gare")):
        b += "h"   # moltiplicare =&gt; tu moltiplichi
    if verb.endswith(("ciare", "giare")):
        b = b[:-1] # cominciare =&gt; tu cominci
    if v.endswith("are"):
        # -are = 1st conjugation
        a1, a2, a3, a4, a5, a6, a7 = "a", "a", "ò", "a", "i", "e", "a"
    elif v.endswith("ere"):
        # -ere = 2nd conjugation
        a1, a2, a3, a4, a5, a6, a7 = "e", "o", "è", "i", "a", "e", "e"
    elif v.endswith("ire"):
        # -ire = 3rd conjugation
        a1, a2, a3, a4, a5, a6, a7 = "i", "o", "i", "i", "a", "i", "e"
    else:
        # -orre, -urre = use 2nd conjugation
        a1, a2, a3, a4, a5, a6, a7 = "e", "o", "è", "i", "a", "e", "e"
    if verb.lower().endswith("ire"):
        # –ire verbs can add -isc between the root and declination.
        isc = "isc"
    else:
        isc = ""
    v = [verb.lower(),
        b + isc + "o", b + isc + "i", b + isc + a7, b + "iamo", b + a1 + "te", b + isc + a2 + "no", b + a1 + "ndo",
        b + a1 + "i", b + a1 + "sti", b + a3, b + a1 + "mmo", b + a1 + "ste", b + a1 + "rono", b + a1 + "to",
        b + a1 + "vo", b + a1 + "vi", b + a1 + "va", b + a1 + "vamo", b + a1 + "vate", b + a1 + "vano",
        b + a6 + "rò", b + a6 + "rai", b + a6 + "rà", b + a6 + "remo", b + a6 + "rete", b + a6 + "ranno",
        b + a6 + "rei", b + a6 + "resti", b + a6 + "rebbe", b + a6 + "remmo", b + a6 + "reste", b + a6 + "rebbero",
        b + isc + a4, b + isc + a5, b + "iamo", b + a1 + "te", b + isc + a5 + "no",
        b + isc + a5, b + isc + a5, b + isc + a5, b + "iamo", b + "iate", b + isc + a5 + "no",
        b + a1 + "ssi", b + a1 + "ssi", b + a1 + "sse", b + a1 + "ssimo", b + a1 + "ste", b + a1 + "ssero"
    ]
    for i, x in enumerate(v):
        x = x.replace("ii" , "i")
        x = x.replace("cha", "ca")
        x = x.replace("gha", "ga")
        x = x.replace("gga", "ggia")
        x = x.replace("cho", "co")
        x = x.replace("chò", "cò")
        v[i] = x
    return v

</t>
<t tx="karstenw.20230303135437.13">def attributive(adjective):
    """ For a predicative adjective, returns the attributive form.
    """
    # Must deal with feminine and plural.
    raise NotImplementedError


</t>
<t tx="karstenw.20230303135437.14">def predicative(adjective):
    """ Returns the predicative adjective.
    """
    w = adjective.lower()
    if w in adjective_predicative:
        return adjective_predicative[w]
    if w.endswith("ari"):
        return w + "o"
    if w.endswith(("ali", "ili", "esi", "nti", "ori")):
        return w[:-1] + "e"
    if w.endswith("isti"):
        return w[:-1] + "a"
    if w.endswith(("che", "ghe")):
        return w[:-2] + "a"
    if w.endswith(("chi", "ghi")):
        return w[:-2] + "o"
    if w.endswith("i"):
        return w[:-1] + "o"
    if w.endswith("e"):
        return w[:-1] + "a"
    return adjective
</t>
<t tx="karstenw.20230303135437.2">def definite_article(word, gender=MALE):
    """ Returns the definite article for a given word.
    """
    if PLURAL in gender and MALE in gender and (is_vowel(word[:1]) or zs(word)):
        return "gli"
    if PLURAL not in gender and word and is_vowel(word[:1]):
        return "l'"
    if PLURAL not in gender and MALE in gender and zs(word):
        return "lo"
    if MALE in gender:
        return PLURAL in gender and "i" or "il"
    if FEMALE in gender:
        return PLURAL in gender and "le" or "la"
    return "il"


</t>
<t tx="karstenw.20230303135437.3">def indefinite_article(word, gender=MALE):
    """ Returns the indefinite article for a given word.
    """
    if MALE in gender and zs(word):
        return PLURAL in gender and "degli" or "uno"
    if MALE in gender:
        return PLURAL in gender and "dei" or "un"
    if FEMALE in gender and is_vowel(word[:1]):
        return PLURAL in gender and "delle" or "un'"
    if FEMALE in gender:
        return PLURAL in gender and "delle" or "una"
    return "un"

DEFINITE, INDEFINITE = \
    "definite", "indefinite"


</t>
<t tx="karstenw.20230303135437.4">def article(word, function=INDEFINITE, gender=MALE):
    """ Returns the indefinite or definite article for the given word.
    """
    return function == DEFINITE \
       and definite_article(word, gender) \
        or indefinite_article(word, gender)

_article = article


</t>
<t tx="karstenw.20230303135437.5">def referenced(word, article=INDEFINITE, gender=MALE):
    """ Returns a string with the article + the word.
    """
    s = "%s&amp;space;%s" % (_article(word, article, gender), word)
    s = s.replace("'&amp;space;", "'")
    s = s.replace("&amp;space;", " ")
    return s

#### GENDER #########################################################################################


</t>
<t tx="karstenw.20230303135437.6">def gender(word):
    """ Returns the gender for the given word, either:
        MALE, FEMALE, (MALE, FEMALE), (MALE, PLURAL) or (FEMALE, PLURAL).
    """
    w = word.lower()
    # Adjectives ending in -e: cruciale, difficile, ...
    if w.endswith(("ale", "ile", "ese", "nte")):
        return (MALE, FEMALE)
    # Most nouns ending in -a (-e) are feminine, -o (-i) masculine:
    if w.endswith(("ore", "ista", "mma")):
        return MALE
    if w.endswith(("a", "tà", "tù", "ione", "rice")):
        return FEMALE
    if w.endswith(("e", "oni")):
        return (FEMALE, PLURAL)
    if w.endswith("i"):
        return (MALE, PLURAL)
    if w.endswith("o"):
        return MALE
    return MALE

#### PLURALIZE ######################################################################################

plural_co_chi = set((
    "abbaco", "baco", "cuoco", "fungo", "rammarico", "strascio", "valico" # ...
))

plural_go_ghi = set((
    "albergo", "catalogo", "chirurgo", "dialogo", "manico", "monologo", "stomaco" # ...
))

plural_irregular = {
    "braccio": "braccia", # bracci (arms of a lamp or cross)
    "budello": "budelli", # budella (intestines)
    "camicia": "camicie",
        "bue": "buoi",
        "dio": "dei",
       "dito": "dita",
     "doccia": "docce",
     "inizio": "inizi",
     "labbro": "labbra", # labbri (borders)
       "mano": "mani",
    "negozio": "negozi",
       "osso": "ossa", # ossi (dog bones)
       "uomo": "uomini",
       "uovo": "uova"
}


</t>
<t tx="karstenw.20230303135437.7">def pluralize(word, pos=NOUN, custom={}):
    """ Returns the plural of a given word.
    """
    if word in custom:
        return custom[word]
    w = word.lower()
    if len(w) &lt; 3:
        return w
    if w in plural_irregular:
        return plural_irregular[w]
    # provincia =&gt; province (but: socia =&gt; socie)
    if w.endswith(("cia", "gia")) and len(w) &gt; 4 and not is_vowel(w[-4]):
        return w[:-2] + "e"
    # amica =&gt; amiche
    if w.endswith(("ca", "ga")):
        return w[:-2] + "he"
    # studentessa =&gt; studentesse
    if w.endswith("a"):
        return w[:-1] + "e"
    # studente =&gt; studenti
    if w.endswith("e"):
        return w[:-1] + "i"
    # viaggio =&gt; viaggi (but: leggìo =&gt; leggìi)
    if w.endswith("io"):
        return w[:-2] + "i"
    # abbaco =&gt; abbachi
    if w in plural_co_chi:
        return w[:-2] + "chi"
    # albergo =&gt; alberghi
    if w in plural_co_chi:
        return w[:-2] + "ghi"
    # amico =&gt; amici
    if w.endswith("o"):
        return w[:-1] + "i"
    return w

#### SINGULARIZE ###################################################################################

singular_majority_vote = [
    ("tenti", "tente"), ("anti", "ante"), ( "oni", "one" ), ( "nti", "nto" ),
    (  "ali", "ale"  ), ( "ici", "ico" ), ( "nze", "nza" ), ( "ori", "ore" ),
    (  "che", "ca"   ), ( "ati", "ato" ), ( "ari", "ario"), ( "tti", "tto" ),
    (  "eri", "ero"  ), ( "chi", "co"  ), ( "ani", "ano" ), ( "ure", "ura" ),
    (  "ità", "ità"  ), ( "ivi", "ivo" ), ( "ini", "ino" ), ( "iti", "ito" ),
    (  "emi", "ema"  ), ( "ili", "ile" ), ( "oli", "olo" ), ( "esi", "ese" ),
    (  "ate", "ata"  ), ( "ssi", "sso" ), ( "rie", "ria" ), ( "ine", "ina" ),
    (  "lli", "llo"  ), ( "ggi", "ggio"), ( "tri", "tro" ), ( "imi", "imo" )
]

singular_irregular = dict((v, k) for k, v in plural_irregular.items())


</t>
<t tx="karstenw.20230303135437.8">def singularize(word, pos=NOUN, custom={}):
    """ Returns the singular of a given word.
    """
    if word in custom:
        return custom[word]
    w = word.lower()
    # il gatti =&gt; il gatto
    if pos == "DT":
        if w in ("i", "gli"):
            return "il"
        if w == "el":
            return "la"
        return w
    if len(w) &lt; 3:
        return w
    if w in singular_irregular:
        return singular_irregular[w]
    # Ruleset adds 16% accuracy.
    for a, b in singular_majority_vote:
        if w.endswith(a):
            return w[:-len(a)] + b
    # Probably an adjective ending in -e: cruciale, difficile, ...
    if w.endswith(("ali", "ari", "ili", "esi", "nti")):
        return w[:-1] + "e"
    # realisti =&gt; realista
    if w.endswith("isti"):
        return w[:-1] + "a"
    # amiche =&gt; amica
    if w.endswith(("che", "ghe")):
        return w[:-2] + "a"
    # alberghi =&gt; albergo
    if w.endswith(("chi", "ghi")):
        return w[:-2] + "o"
    # problemi =&gt; problema
    if w.endswith("emi"):
        return w[:-1] + "a"
    # case =&gt; case
    if w.endswith("e"):
        return w[:-1] + "a"
    # Ambigious: both -o and -a pluralize to -i.
    if w.endswith("i"):
        return w[:-1] + "o"
    return w

#### VERB CONJUGATION ##############################################################################
# The verb table was trained on Wiktionary and contains the top 1,250 frequent verbs.

verb_majority_vote = [
    ("iresti", "ire" ), ("ireste", "ire" ), ("iremmo", "ire" ), ("irebbe", "ire" ),
    ("iranno", "ire" ), ( "ssero", "re"  ), ( "ssimo", "re"  ), ( "ivate", "ire" ),
    ( "ivamo", "ire" ), ( "irete", "ire" ), ( "iremo", "ire" ), ( "irono", "ire" ),
    ( "scano", "re"  ), ( "hiamo", "are" ), ( "scono", "re"  ), ( "hiate", "are" ),
    (  "vano", "re"  ), (  "vate", "re"  ), (  "vamo", "re"  ), (  "simo", "e"   ),
    (  "rono", "re"  ), (  "isse", "ire" ), (  "isti", "ire" ), (  "tino", "tare"),
    (  "tato", "tare"), (  "irai", "ire" ), (  "tavo", "tare"), (  "tavi", "tare"),
    (  "tava", "tare"), (  "tate", "tare"), (  "iste", "ire" ), (  "irei", "ire" ),
    (  "immo", "ire" ), (  "rerò", "rare"), (  "rerà", "rare"), (  "iavo", "iare"),
    (  "iavi", "iare"), (  "iava", "iare"), (  "iato", "iare"), (  "iare", "iare"),
    (  "hino", "are" ), (   "ssi", "re"  ), (   "sse", "re"  ), (   "ndo", "re"  ),
    (   "irò", "ire" ), (   "tai", "tare"), (   "ite", "ire" ), (   "irà", "ire" ),
    (   "sco", "re"  ), (   "sca", "re"  ), (   "iai", "iare"), (    "ii", "ire" ),
    (    "hi", "are" )
]


</t>
<t tx="karstenw.20230303135437.9">class Verbs(_Verbs):

    @others
verbs = Verbs()

conjugate, lemma, lexeme, tenses = \
    verbs.conjugate, verbs.lemma, verbs.lexeme, verbs.tenses

#### ATTRIBUTIVE &amp; PREDICATIVE #####################################################################

adjective_predicative = {
       "bei": "bello",
       "bel": "bello",
     "bell'": "bello",
     "begli": "bello",
      "buon": "buono",
     "buon'": "buona",
      "gran": "grande",
    "grand'": "grande",
    "grandi": "grande",
       "san": "santo",
     "sant'": "santa"
}


</t>
<t tx="karstenw.20230303135501.1">#### PATTERN | FR | PARSER COMMAND-LINE ############################################################
# Copyright (c) 2013 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303135509.1">from __future__ import absolute_import

from .__init__ import parse, commandline
commandline(parse)
</t>
<t tx="karstenw.20230303135514.1">#### PATTERN | IT | PARSER COMMAND-LINE ############################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303135522.1">from __future__ import absolute_import

from .__init__ import parse, commandline
commandline(parse)
</t>
<t tx="karstenw.20230303135616.1"></t>
<t tx="karstenw.20230303135620.1">#### PATTERN | NL ##################################################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################
# Dutch linguistical tools using fast regular expressions.

@others
if __name__ == "__main__":
    commandline(parse)
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303135631.1">#### PATTERN | NL | PARSER COMMAND-LINE ############################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303135643.1">#### PATTERN | NL | INFLECT ########################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).

####################################################################################################
# Regular expressions-based rules for Dutch word inflection:
# - pluralization and singularization of nouns,
# - conjugation of verbs,
# - predicative and attributive of adjectives.

# Accuracy (measured on CELEX Dutch morphology word forms):
# 79% for pluralize()
# 91% for singularize()
# 90% for Verbs.find_lemma()
# 88% for Verbs.find_lexeme()
# 99% for predicative()
# 99% for attributive()

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303135652.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
import re

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

# Import parser base classes.
from pattern.text import (
    Lexicon, Model, Morphology, Context, Parser as _Parser, ngrams, pprint, commandline,
    PUNCTUATION
)
# Import parser universal tagset.
from pattern.text import (
    penntreebank2universal,
    PTB, PENN, UNIVERSAL,
    NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X
)
# Import parse tree base classes.
from pattern.text.tree import (
    Tree, Text, Sentence, Slice, Chunk, PNPChunk, Chink, Word, table,
    SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA, AND, OR
)
# Import sentiment analysis base classes.
from pattern.text import (
    Sentiment as _Sentiment,
    NOUN, VERB, ADJECTIVE, ADVERB,
    MOOD, IRONY
)
# Import spelling base class.
from pattern.text import (
    Spelling
)
# Import verb tenses.
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    PROGRESSIVE,
    PARTICIPLE
)
# Import inflection functions.
from pattern.text.nl.inflect import (
    pluralize, singularize, NOUN, VERB, ADJECTIVE,
    verbs, conjugate, lemma, lexeme, tenses,
    predicative, attributive
)
# Import all submodules.
from pattern.text.nl import inflect

sys.path.pop(0)

#--- DUTCH PARSER ----------------------------------------------------------------------------------
# The Dutch parser (accuracy 92%) is based on Jeroen Geertzen's language model:
# Brill-NL, http://cosmion.net/jeroen/software/brill_pos/

# The lexicon uses the WOTAN tagset:
# http://lands.let.ru.nl/literature/hvh.1999.2.ps
WOTAN = "wotan"
wotan = {
    "Adj(": (("vergr", "JJR"), ("overtr", "JJS"), ("", "JJ")),
    "Adv(": (("deel", "RP"), ("", "RB")),
    "Art(": (("", "DT"),),
   "Conj(": (("", "CC"),),
     "Int": (("", "UH"),),
    "Misc": (("symb", "SYM"), ("vreemd", "FW")),
      "N(": (("eigen,ev", "NNP"), ("eigen,mv", "NNPS"), ("ev", "NN"), ("mv", "NNS")),
    "Num(": (("", "CD"),),
   "Prep(": (("inf", "TO"), ("", "IN")),
   "Pron(": (("bez", "PRP$"), ("", "PRP")),
   "Punc(": (("komma", ","), ("open", "("), ("sluit", ")"), ("schuin", "CC"), ("", ".")),
      "V(": (("hulp", "MD"), ("ott,3", "VBZ"), ("ott", "VBP"), ("ovt", "VBD"),
             ("verl", "VBN"), ("teg", "VBG"), ("", "VB"))
}


</t>
<t tx="karstenw.20230303135652.10">def load(self, path=None):
    _Sentiment.load(self, path)
    # Map "verschrikkelijk" to adverbial "verschrikkelijke" (+1%)
    if not path:
        for w, pos in list(dict.items(self)):
            if "JJ" in pos:
                p, s, i = pos["JJ"]
                self.annotate(attributive(w), "JJ", p, s, i)

</t>
<t tx="karstenw.20230303135652.11">def tokenize(s, *args, **kwargs):
    """ Returns a list of sentences, where punctuation marks have been split from words.
    """
    return parser.find_tokens(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303135652.12">def parse(s, *args, **kwargs):
    """ Returns a tagged Unicode string.
    """
    return parser.parse(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303135652.13">def parsetree(s, *args, **kwargs):
    """ Returns a parsed Text from the given string.
    """
    return Text(parse(s, *args, **kwargs))


</t>
<t tx="karstenw.20230303135652.14">def tree(s, token=[WORD, POS, CHUNK, PNP, REL, LEMMA]):
    """ Returns a parsed Text from the given parsed string.
    """
    return Text(s, token)


</t>
<t tx="karstenw.20230303135652.15">def tag(s, tokenize=True, encoding="utf-8", **kwargs):
    """ Returns a list of (token, tag)-tuples from the given string.
    """
    tags = []
    for sentence in parse(s, tokenize, True, False, False, False, encoding, **kwargs).split():
        for token in sentence:
            tags.append((token[0], token[1]))
    return tags


</t>
<t tx="karstenw.20230303135652.16">def keywords(s, top=10, **kwargs):
    """ Returns a sorted list of keywords in the given string.
    """
    return parser.find_keywords(s, **dict({
        "frequency": parser.frequency,
              "top": top,
              "pos": ("NN",),
           "ignore": ("rt", "mensen")}, **kwargs))


</t>
<t tx="karstenw.20230303135652.17">def suggest(w):
    """ Returns a list of (word, confidence)-tuples of spelling corrections.
    """
    return spelling.suggest(w)


</t>
<t tx="karstenw.20230303135652.18">def polarity(s, **kwargs):
    """ Returns the sentence polarity (positive/negative) between -1.0 and 1.0.
    """
    return sentiment(s, **kwargs)[0]


</t>
<t tx="karstenw.20230303135652.19">def subjectivity(s, **kwargs):
    """ Returns the sentence subjectivity (objective/subjective) between 0.0 and 1.0.
    """
    return sentiment(s, **kwargs)[1]


</t>
<t tx="karstenw.20230303135652.2">def wotan2penntreebank(token, tag):
    """ Converts a WOTAN tag to a Penn Treebank II tag.
        For example: bokkenrijders/N(soort,mv,neut) =&gt; bokkenrijders/NNS
    """
    for k, v in wotan.items():
        if tag.startswith(k):
            for a, b in v:
                if a in tag:
                    return (token, b)
    return (token, tag)


</t>
<t tx="karstenw.20230303135652.20">def positive(s, threshold=0.1, **kwargs):
    """ Returns True if the given sentence has a positive sentiment (polarity &gt;= threshold).
    """
    return polarity(s, **kwargs) &gt;= threshold

split = tree # Backwards compatibility.

#---------------------------------------------------------------------------------------------------
# python -m pattern.nl xml -s "De kat wil wel vis eten maar geen poot nat maken." -OTCL

</t>
<t tx="karstenw.20230303135652.3">def wotan2universal(token, tag):
    """ Converts a WOTAN tag to a universal tag.
        For example: bokkenrijders/N(soort,mv,neut) =&gt; bokkenrijders/NOUN
    """
    if tag.startswith("Adv"):
        return (token, ADV)
    return penntreebank2universal(*wotan2penntreebank(token, tag))

ABBREVIATIONS = set((
    "a.d.h.v.", "afb.", "a.u.b.", "bv.", "b.v.", "bijv.", "blz.", "ca.", "cfr.", "dhr.", "dr.",
    "d.m.v.", "d.w.z.", "e.a.", "e.d.", "e.g.", "enz.", "etc.", "e.v.", "evt.", "fig.", "i.e.",
    "i.h.b.", "ir.", "i.p.v.", "i.s.m.", "m.a.w.", "max.", "m.b.t.", "m.b.v.", "mevr.", "min.",
    "n.a.v.", "nl.", "n.o.t.k.", "n.t.b.", "n.v.t.", "o.a.", "ong.", "pag.", "ref.", "t.a.v.",
    "tel.", "zgn."
))


</t>
<t tx="karstenw.20230303135652.4">def find_lemmata(tokens):
    """ Annotates the tokens with lemmata for plural nouns and conjugated verbs,
        where each token is a [word, part-of-speech] list.
    """
    for token in tokens:
        word, pos, lemma = token[0], token[1], token[0]
        if pos.startswith("JJ") and word.endswith("e"):
            lemma = predicative(word)
        if pos == "NNS":
            lemma = singularize(word)
        if pos.startswith(("VB", "MD")):
            lemma = conjugate(word, INFINITIVE) or word
        token.append(lemma.lower())
    return tokens


</t>
<t tx="karstenw.20230303135652.5">class Parser(_Parser):

    @others
</t>
<t tx="karstenw.20230303135652.6">def find_tokens(self, tokens, **kwargs):
    # 's in Dutch preceded by a vowel indicates plural ("auto's"): don't replace.
    kwargs.setdefault("abbreviations", ABBREVIATIONS)
    kwargs.setdefault("replace", {"'n": " 'n"})
    s = _Parser.find_tokens(self, tokens, **kwargs)
    s = [re.sub(r"' s (ochtends|morgens|middags|avonds)", "'s \\1", s) for s in s]
    return s

</t>
<t tx="karstenw.20230303135652.7">def find_lemmata(self, tokens, **kwargs):
    return find_lemmata(tokens)

</t>
<t tx="karstenw.20230303135652.8">def find_tags(self, tokens, **kwargs):
    if kwargs.get("tagset") in (PENN, None):
        kwargs.setdefault("map", lambda token, tag: wotan2penntreebank(token, tag))
    if kwargs.get("tagset") == UNIVERSAL:
        kwargs.setdefault("map", lambda token, tag: wotan2universal(token, tag))
    if kwargs.get("tagset") is WOTAN:
        kwargs.setdefault("map", lambda token, tag: (token, tag))
    return _Parser.find_tags(self, tokens, **kwargs)


</t>
<t tx="karstenw.20230303135652.9">class Sentiment(_Sentiment):

    @others
parser = Parser(
     lexicon = os.path.join(MODULE, "nl-lexicon.txt"),
   frequency = os.path.join(MODULE, "nl-frequency.txt"),
  morphology = os.path.join(MODULE, "nl-morphology.txt"),
     context = os.path.join(MODULE, "nl-context.txt"),
     default = ("N(soort,ev,neut)", "N(eigen,ev)", "Num()"),
    language = "nl"
)

lexicon = parser.lexicon # Expose lexicon.

sentiment = Sentiment(
        path = os.path.join(MODULE, "nl-sentiment.xml"),
      synset = "cornetto_id",
   negations = ("geen", "gene", "ni", "niet", "nooit"),
   modifiers = ("JJ", "RB",),
   modifier = lambda w: w.endswith(("ig", "isch", "lijk")),
   tokenizer = parser.find_tokens,
    language = "nl"
)

spelling = Spelling(
        path = os.path.join(MODULE, "nl-spelling.txt")
)


</t>
<t tx="karstenw.20230303135655.1">from __future__ import absolute_import

from .__init__ import commandline, parse
commandline(parse)
</t>
<t tx="karstenw.20230303135658.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
import re

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

from pattern.text import Verbs as _Verbs
from pattern.text import (
    INFINITIVE, PRESENT, PAST, FUTURE,
    FIRST, SECOND, THIRD,
    SINGULAR, PLURAL, SG, PL,
    PROGRESSIVE,
    PARTICIPLE
)

sys.path.pop(0)

VERB, NOUN, ADJECTIVE, ADVERB = "VB", "NN", "JJ", "RB"

VOWELS = ("a", "e", "i", "o", "u")
re_vowel = re.compile(r"a|e|i|o|u|y", re.I)
is_vowel = lambda ch: ch in VOWELS

#### PLURALIZE ######################################################################################

plural_irregular_en = set(("dag", "dak", "dal", "pad", "vat", "weg"))
plural_irregular_een = set(("fee", "genie", "idee", "orgie", "ree"))
plural_irregular_eren = set(("blad", "ei", "gelid", "gemoed", "kalf", "kind", "lied", "rad", "rund"))
plural_irregular_deren = set(("hoen", "been"))

plural_irregular = {
     "centrum": "centra",
    "escargot": "escargots",
      "gedrag": "gedragingen",
       "gelid": "gelederen",
       "kaars": "kaarsen",
       "kleed": "kleren",
         "koe": "koeien",
         "lam": "lammeren",
      "museum": "museums",
        "stad": "steden",
       "stoel": "stoelen",
         "vlo": "vlooien"
}


</t>
<t tx="karstenw.20230303135658.10">def predicative(adjective):
    """ Returns the predicative adjective (lowercase).
        In Dutch, the attributive form preceding a noun is common:
        "rake opmerking" =&gt; "raak", "straffe uitspraak" =&gt; "straf", "dwaze blik" =&gt; "dwaas".
    """
    w = adjective.lower()
    if w in adjective_predicative:
        return adjective_predicative[w]
    if w.endswith("ste"):
        return w[:-1]
    if w.endswith("ere"):
        return w[:-1]
    if w.endswith("bele"):
        return w[:-1]
    if w.endswith("le") and len(w) &gt; 2 and is_vowel(w[-3]) and not w.endswith(("eule", "oele")):
        return w[:-2] + w[-3] + "l"
    if w.endswith("ve") and len(w) &gt; 2 and is_vowel(w[-3]) and not w.endswith(("euve", "oeve", "ieve")):
        return w[:-2] + w[-3] + "f"
    if w.endswith("ze") and len(w) &gt; 2 and is_vowel(w[-3]) and not w.endswith(("euze", "oeze", "ieze")):
        return w[:-2] + w[-3] + "s"
    if w.endswith("ve"):
        return w[:-2] + "f"
    if w.endswith("ze"):
        return w[:-2] + "s"
    if w.endswith("e") and len(w) &gt; 2:
        if not is_vowel(w[-2]) and w[-2] == w[-3]:
            return w[:-2]
        if len(w) &gt; 3 and not is_vowel(w[-2]) and is_vowel(w[-3]) and w[-3] != "i" and not is_vowel(w[-4]):
            return w[:-2] + w[-3] + w[-2]
        return w[:-1]
    return w
</t>
<t tx="karstenw.20230303135658.2">def pluralize(word, pos=NOUN, custom={}):
    """ Returns the plural of a given word.
        For example: stad =&gt; steden.
        The custom dictionary is for user-defined replacements.
    """
    if word in custom.keys():
        return custom[word]
    w = word.lower()
    if pos == NOUN:
        if w in plural_irregular_en:    # dag =&gt; dagen
            return w + "en"
        if w in plural_irregular_een:   # fee =&gt; feeën
            return w + "ën"
        if w in plural_irregular_eren:  # blad =&gt; bladeren
            return w + "eren"
        if w in plural_irregular_deren: # been =&gt; beenderen
            return w + "deren"
        if w in plural_irregular:
            return plural_irregular[w]
        # Words ending in -icus get -ici: academicus =&gt; academici
        if w.endswith("icus"):
            return w[:-2] + "i"
        # Words ending in -s usually get -sen: les =&gt; lessen.
        if w.endswith(("es", "as", "nis", "ris", "vis")):
            return w + "sen"
        # Words ending in -s usually get -zen: huis =&gt; huizen.
        if w.endswith("s") and not w.endswith(("us", "ts", "mens")):
            return w[:-1] + "zen"
        # Words ending in -f usually get -ven: brief =&gt; brieven.
        if w.endswith("f"):
            return w[:-1] + "ven"
        # Words ending in -um get -ums: museum =&gt; museums.
        if w.endswith("um"):
            return w + "s"
        # Words ending in unstressed -ee or -ie get -ën: bacterie =&gt; bacteriën
        if w.endswith("ie"):
            return w + "s"
        if w.endswith(("ee", "ie")):
            return w[:-1] + "ën"
        # Words ending in -heid get -heden: mogelijkheid =&gt; mogelijkheden
        if w.endswith("heid"):
            return w[:-4] + "heden"
        # Words ending in -e -el -em -en -er -ie get -s: broer =&gt; broers.
        if w.endswith(("é", "e", "el", "em", "en", "er", "eu", "ie", "ue", "ui", "eau", "ah")):
            return w + "s"
        # Words ending in a vowel get 's: auto =&gt; auto's.
        if w.endswith(VOWELS) or w.endswith("y") and not w.endswith("e"):
            return w + "'s"
        # Words ending in -or always get -en: motor =&gt; motoren.
        if w.endswith("or"):
            return w + "en"
        # Words ending in -ij get -en: boerderij =&gt; boerderijen.
        if w.endswith("ij"):
            return w + "en"
        # Words ending in two consonants get -en: hand =&gt; handen.
        if len(w) &gt; 1 and not is_vowel(w[-1]) and not is_vowel(w[-2]):
            return w + "en"
        # Words ending in one consonant with a short sound: fles =&gt; flessen.
        if len(w) &gt; 2 and not is_vowel(w[-1]) and not is_vowel(w[-3]):
            return w + w[-1] + "en"
        # Words ending in one consonant with a long sound: raam =&gt; ramen.
        if len(w) &gt; 2 and not is_vowel(w[-1]) and w[-2] == w[-3]:
            return w[:-2] + w[-1] + "en"
        return w + "en"
    return w

#### SINGULARIZE ###################################################################################

singular_irregular = dict((v, k) for k, v in plural_irregular.items())


</t>
<t tx="karstenw.20230303135658.3">def singularize(word, pos=NOUN, custom={}):
    if word in custom.keys():
        return custom[word]
    w = word.lower()
    if pos == NOUN and w in singular_irregular:
        return singular_irregular[w]
    if pos == NOUN and w.endswith(("ën", "en", "s", "i")):
        # auto's =&gt; auto
        if w.endswith("'s"):
            return w[:-2]
        # broers =&gt; broer
        if w.endswith("s"):
            return w[:-1]
        # academici =&gt; academicus
        if w.endswith("ici"):
            return w[:-1] + "us"
        # feeën =&gt; fee
        if w.endswith("ën") and w[:-2] in plural_irregular_een:
            return w[:-2]
        # bacteriën =&gt; bacterie
        if w.endswith("ën"):
            return w[:-2] + "e"
        # mogelijkheden =&gt; mogelijkheid
        if w.endswith("heden"):
            return w[:-5] + "heid"
        # artikelen =&gt; artikel
        if w.endswith("elen") and not w.endswith("delen"):
            return w[:-2]
        # chinezen =&gt; chinees
        if w.endswith("ezen"):
            return w[:-4] + "ees"
        # neven =&gt; neef
        if w.endswith("even") and len(w) &gt; 4 and not is_vowel(w[-5]):
            return w[:-4] + "eef"
        if w.endswith("en"):
            w = w[:-2]
            # ogen =&gt; oog
            if w in ("og", "om", "ur"):
                return w[:-1] + w[-2] + w[-1]
            # hoenderen =&gt; hoen
            if w.endswith("der") and w[:-3] in plural_irregular_deren:
                return w[:-3]
            # eieren =&gt; ei
            if w.endswith("er") and w[:-2] in plural_irregular_eren:
                return w[:-2]
            # dagen =&gt; dag (not daag)
            if w in plural_irregular_en:
                return w
            # huizen =&gt; huis
            if w.endswith("z"):
                return w[:-1] + "s"
            # brieven =&gt; brief
            if w.endswith("v"):
                return w[:-1] + "f"
             # motoren =&gt; motor
            if w.endswith("or"):
                return w
            # flessen =&gt; fles
            if len(w) &gt; 1 and not is_vowel(w[-1]) and w[-1] == w[-2]:
                return w[:-1]
            # baarden =&gt; baard
            if len(w) &gt; 1 and not is_vowel(w[-1]) and not is_vowel(w[-2]):
                return w
            # boerderijen =&gt; boerderij
            if w.endswith("ij"):
                return w
            # idealen =&gt; ideaal
            if w.endswith(("eal", "ean", "eol", "ial", "ian", "iat", "iol")):
                return w[:-1] + w[-2] + w[-1]
            # ramen =&gt; raam
            if len(w) &gt; 2 and not is_vowel(w[-1]) and is_vowel(w[-2]) and not is_vowel(w[-3]):
                return w[:-1] + w[-2] + w[-1]
            return w
    return w

#### VERB CONJUGATION ##############################################################################


</t>
<t tx="karstenw.20230303135658.4">class Verbs(_Verbs):

    @others
verbs = Verbs()

conjugate, lemma, lexeme, tenses = \
    verbs.conjugate, verbs.lemma, verbs.lexeme, verbs.tenses

#### ATTRIBUTIVE &amp; PREDICATIVE #####################################################################

adjective_attributive = {
     "civiel": "civiele",
    "complex": "complexe",
      "enkel": "enkele",
       "grof": "grove",
       "half": "halve",
     "luttel": "luttele",
     "mobiel": "mobiele",
     "parijs": "parijse",
        "ruw": "ruwe",
     "simpel": "simpele",
    "stabiel": "stabiele",
    "steriel": "steriele",
    "subtiel": "subtiele",
       "teer": "tere"
}


</t>
<t tx="karstenw.20230303135658.5">def __init__(self):
    _Verbs.__init__(self, os.path.join(MODULE, "nl-verbs.txt"),
        language = "nl",
          format = [0, 1, 2, 3, 7, 8, 17, 18, 19, 23, 25, 24, 16, 9, 10, 11, 15, 33, 26, 27, 28, 32],
         default = {
             1: 0, 2: 0, 3: 0, 7: 0,  # present singular
             4: 7, 5: 7, 6: 7,          # present plural
            17: 25, 18: 25, 19: 25, 23: 25, # past singular
            20: 23, 21: 23, 22: 23,         # past plural
             9: 16, 10: 16, 11: 16, 15: 16, # present singular negated
            12: 15, 13: 15, 14: 15,         # present plural negated
            26: 33, 27: 33, 28: 33,         # past singular negated
            29: 32, 30: 32, 31: 32, 32: 33  # past plural negated
        })

</t>
<t tx="karstenw.20230303135658.6">def load(self):
    _Verbs.load(self)
    self._inverse["was"]   = "zijn" # Instead of "wassen".
    self._inverse["waren"] = "zijn"
    self._inverse["zagen"] = "zien"
    self._inverse["wist"]  = "weten"
    self._inverse["zou"]   = "zullen"

</t>
<t tx="karstenw.20230303135658.7">def find_lemma(self, verb):
    """ Returns the base form of the given inflected verb, using a rule-based approach.
        This is problematic if a verb ending in -e is given in the past tense or gerund.
    """
    v = verb.lower()
    # Common prefixes: op-bouwen and ver-bouwen inflect like bouwen.
    for prefix in ("aan", "be", "her", "in", "mee", "ont", "op", "over", "uit", "ver"):
        if v.startswith(prefix) and v[len(prefix):] in self.inflections:
            return prefix + self.inflections[v[len(prefix):]]
    # Present participle -end: hengelend, knippend.
    if v.endswith("end"):
        b = v[:-3]
    # Past singular -de or -te: hengelde, knipte.
    elif v.endswith(("de", "det", "te", "tet")):
        b = v[:-2]
    # Past plural -den or -ten: hengelden, knipten.
    elif v.endswith(("chten"),):
        b = v[:-2]
    elif v.endswith(("den", "ten")) and len(v) &gt; 3 and is_vowel(v[-4]):
        b = v[:-2]
    elif v.endswith(("den", "ten")):
        b = v[:-3]
    # Past participle ge- and -d or -t: gehengeld, geknipt.
    elif v.endswith(("d", "t")) and v.startswith("ge"):
        b = v[2:-1]
    # Present 2nd or 3rd singular: wordt, denkt, snakt, wacht.
    elif v.endswith(("cht"),):
        b = v
    elif v.endswith(("dt", "bt", "gt", "kt", "mt", "pt", "wt", "xt", "aait", "ooit")):
        b = v[:-1]
    elif v.endswith("t") and len(v) &gt; 2 and not is_vowel(v[-2]):
        b = v[:-1]
    elif v.endswith("en") and len(v) &gt; 3:
        return v
    else:
        b = v
    # hengel =&gt; hengelen (and not hengellen)
    if len(b) &gt; 2 and b.endswith(("el", "nder", "om", "tter")) and not is_vowel(b[-3]):
        pass
    # Long vowel followed by -f or -s: geef =&gt; geven.
    elif len(b) &gt; 2 and not is_vowel(b[-1]) and is_vowel(b[-2]) and is_vowel(b[-3])\
      or b.endswith(("ijf", "erf"),):
        if b.endswith("f"):
            b = b[:-1] + "v"
        if b.endswith("s"):
            b = b[:-1] + "z"
        if b[-2] == b[-3]:
            b = b[:-2] + b[-1]
    # Short vowel followed by consonant: snak =&gt; snakken.
    elif len(b) &gt; 1 and not is_vowel(b[-1]) and is_vowel(b[-2]) and not b.endswith(("er", "ig")):
        b = b + b[-1]
    b = b + "en"
    b = b.replace("vven", "ven") # omgevven =&gt; omgeven
    b = b.replace("zzen", "zen") # genezzen =&gt; genezen
    b = b.replace("aen", "aan")  # doorgaen =&gt; doorgaan
    return b

</t>
<t tx="karstenw.20230303135658.8">def find_lexeme(self, verb):
    """ For a regular verb (base form), returns the forms using a rule-based approach.
    """
    v = verb.lower()
    # Stem = infinitive minus -en.
    b = b0 = re.sub("en$", "", v)
    # zweven =&gt; zweef, graven =&gt; graaf
    if b.endswith("v"):
        b = b[:-1] + "f"
    if b.endswith("z"):
        b = b[:-1] + "s"
    # Vowels with a long sound are doubled, we need to guess how it sounds:
    if len(b) &gt; 2 and not is_vowel(b[-1]) and is_vowel(b[-2]) and not is_vowel(b[-3]):
        if not v.endswith(("elen", "deren", "keren", "nderen", "tteren")):
            b = b[:-1] + b[-2] + b[-1]
    # pakk =&gt; pak
    if len(b) &gt; 1 and not is_vowel(b[-1]) and b[-1] == b[-2]:
        b = b[:-1]
    # Present tense gets -t:
    sg = not b.endswith("t") and b + "t" or b
    # Past tense ending in a consonant in "xtc-koffieshop" gets -t, otherwise -d:
    dt = b0 and b0[-1] in "xtckfshp" and "t" or (not b.endswith("d") and "d" or "")
    # Past tense -e and handle common irregular inflections:
    p = b + dt + "e"
    for suffix, irregular in (("erfde", "ierf"), ("ijfde", "eef"), ("ingde", "ong"), ("inkte", "onk")):
        if p.endswith(suffix):
            p = p[:-len(suffix)] + irregular; break
    # Past participle: ge-:
    pp = re.sub("tt$", "t", "ge" + b + dt)
    pp = pp.startswith(("geop", "gein", "geaf")) and pp[2:4] + "ge" + pp[4:] or pp # geopstart =&gt; opgestart
    pp = pp.startswith(("gever", "gebe", "gege")) and pp[2:] or pp
    return [v, b, sg, sg, v, b0 + "end", p, p, p, b + dt + "en", p, pp]

</t>
<t tx="karstenw.20230303135658.9">def attributive(adjective):
    """ For a predicative adjective, returns the attributive form (lowercase).
        In Dutch, the attributive is formed with -e: "fel" =&gt; "felle kritiek".
    """
    w = adjective.lower()
    if w in adjective_attributive:
        return adjective_attributive[w]
    if w.endswith("e"):
        return w
    if w.endswith(("er", "st")) and len(w) &gt; 4:
        return w + "e"
    if w.endswith("ees"):
        return w[:-2] + w[-1] + "e"
    if w.endswith("el") and len(w) &gt; 2 and not is_vowel(w[-3]):
        return w + "e"
    if w.endswith("ig"):
        return w + "e"
    if len(w) &gt; 2 and (not is_vowel(w[-1]) and is_vowel(w[-2]) and is_vowel(w[-3]) or w[:-1].endswith("ij")):
        if w.endswith("f"):
            w = w[:-1] + "v"
        if w.endswith("s"):
            w = w[:-1] + "z"
        if w[-2] == w[-3]:
            w = w[:-2] + w[-1]
    elif len(w) &gt; 1 and is_vowel(w[-2]) and w.endswith(tuple("bdfgklmnprst")):
        w = w + w[-1]
    return w + "e"

adjective_predicative = dict((v, k) for k, v in adjective_attributive.items())
adjective_predicative.update({
          "moe": "moe",
        "taboe": "taboe",
    "voldoende": "voldoende"
})


</t>
<t tx="karstenw.20230303135727.1"></t>
<t tx="karstenw.20230303135735.1">#### PATTERN | RU ##################################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################
# English linguistical tools using fast regular expressions.

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303135744.1">#### PATTERN | RU | PARSER COMMAND-LINE ############################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303135800.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

sys.path.insert(0, os.path.join(MODULE, "..", "..", "..", ".."))

# Import parser base classes.
from pattern.text import (
    Lexicon, Model, Morphology, Context, Parser as _Parser, ngrams, pprint, commandline,
    PUNCTUATION
)
# Import parser universal tagset.
from pattern.text import (
    penntreebank2universal,
    PTB, PENN, UNIVERSAL,
    NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X
)
# Import parse tree base classes.
from pattern.text.tree import (
    Tree, Text, Sentence, Slice, Chunk, PNPChunk, Chink, Word, table,
    SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA, AND, OR
)

# Import spelling base class.
from pattern.text import (
    Spelling
)

sys.path.pop(0)

#--- Russian PARSER --------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303135800.2">class Parser(_Parser):

    @others
parser = Parser(
    lexicon=os.path.join(MODULE, "ru-lexicon.txt"),  # A dict of known words =&gt; most frequent tag.
    frequency=os.path.join(MODULE, "ru-frequency.txt"),  # A dict of word frequency.
    model=os.path.join(MODULE, "ru-model.slp"),  # A SLP classifier trained on WSJ (01-07).
    #morphology=os.path.join(MODULE, "en-morphology.txt"),  # A set of suffix rules
    #context=os.path.join(MODULE, "en-context.txt"),  # A set of contextual rules.
    #entities=os.path.join(MODULE, "en-entities.txt"),  # A dict of named entities: John = NNP-PERS.
    #default=("NN", "NNP", "CD"),
    language="ru"
)


spelling = Spelling(
    path=os.path.join(MODULE, "ru-spelling.txt"),
    alphabet='CYRILLIC'
)


</t>
<t tx="karstenw.20230303135800.3">def find_tags(self, tokens, **kwargs):
    if kwargs.get("tagset") in (PENN, None):
        kwargs.setdefault("map", lambda token, tag: (token, tag))
    if kwargs.get("tagset") == UNIVERSAL:
        kwargs.setdefault("map", lambda token, tag: penntreebank2universal(token, tag))
    return _Parser.find_tags(self, tokens, **kwargs)

</t>
<t tx="karstenw.20230303135800.4">def tokenize(s, *args, **kwargs):
    """ Returns a list of sentences, where punctuation marks have been split from words.
    """
    return parser.find_tokens(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303135800.5">def parse(s, *args, **kwargs):
    """ Returns a tagged Unicode string.
    """
    return parser.parse(s, *args, **kwargs)


</t>
<t tx="karstenw.20230303135800.6">def parsetree(s, *args, **kwargs):
    """ Returns a parsed Text from the given string.
    """
    return Text(parse(s, *args, **kwargs))


</t>
<t tx="karstenw.20230303135800.7">def suggest(w):
    """ Returns a list of (word, confidence)-tuples of spelling corrections.
    """
    return spelling.suggest(w)
</t>
<t tx="karstenw.20230303135803.1">from __future__ import absolute_import

from .__init__ import parse, commandline
commandline(parse)
</t>
<t tx="karstenw.20230303135940.1"></t>
<t tx="karstenw.20230303140000.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140017.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140022.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140025.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140030.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140035.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140045.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140056.1">@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140103.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140111.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140120.1">@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140126.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140131.1"># -*- coding: utf-8 -*-

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140139.1"># -*- coding: utf-8 -*-
# These tests require a working internet connection.

@others
if __name__ == "__main__":

    result = unittest.TextTestRunner(verbosity=1).run(suite())
    sys.exit(not result.wasSuccessful())
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140159.1">from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range, next

from io import open

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import datetime
import codecs
import random
import unittest

from pattern import db

# To test MySQL, you need MySQLdb and a username + password with rights to create a database.
HOST, PORT, USERNAME, PASSWORD = \
    "localhost", 3306, "root", ""

DB_MYSQL = DB_SQLITE = None


</t>
<t tx="karstenw.20230303140159.10">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140159.100">def test_pprint(self):
    pass

</t>
<t tx="karstenw.20230303140159.101">def suite(**kwargs):

    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUnicode))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestEntities))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestDate))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUtilityFunctions))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSchema))

    # MySQL
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestMySQLDatabase))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestMySQLTable))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestMySQLQuery))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestMySQLView))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestDeleteMySQLDatabase))

    # SQLite
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSQLiteDatabase))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSQLiteTable))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSQLiteQuery))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSQLiteView))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestDeleteSQLiteDatabase))

    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestCSV))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestDatasheet))
    return suite

</t>
<t tx="karstenw.20230303140159.11">def test_encode_entities(self):
    # Assert HTML entity encoder (e.g., "&amp;" =&gt; "&amp;&amp;amp;")
    for a, b in (
      ("&amp;#201;", "&amp;#201;"),
      ("&amp;", "&amp;amp;"),
      ("&lt;", "&amp;lt;"),
      ("&gt;", "&amp;gt;"),
      ('"', "&amp;quot;"),
      ("'", "&amp;#39;")):
        self.assertEqual(db.encode_entities(a), b)
    print("pattern.db.encode_entities()")

</t>
<t tx="karstenw.20230303140159.12">def test_decode_entities(self):
    # Assert HMTL entity decoder (e.g., "&amp;amp;" =&gt; "&amp;")
    for a, b in (
      ("&amp;#38;", "&amp;"),
      ("&amp;amp;", "&amp;"),
      ("&amp;#x0026;", "&amp;"),
      ("&amp;#160;", "\xa0"),
      ("&amp;foo;", "&amp;foo;")):
        self.assertEqual(db.decode_entities(a), b)
    print("pattern.db.decode_entities()")

</t>
<t tx="karstenw.20230303140159.13">class TestDate(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.14">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140159.15">def test_date(self):
    # Assert string input and default date formats.
    for s in (
      "2010-09-21 09:27:01",
      "2010-09-21T09:27:01Z",
      "2010-09-21T09:27:01+0000",
      "2010-09-21 09:27",
      "2010-09-21",
      "21/09/2010",
      "21 September 2010",
      "September 21 2010",
      "September 21, 2010",
      1285054021):
        v = db.date(s)
        self.assertEqual(v.format, "%Y-%m-%d %H:%M:%S")
        self.assertEqual(v.year, 2010)
        self.assertEqual(v.month, 9)
        self.assertEqual(v.day, 21)
    # Assert NOW.
    for v in (db.date(), db.date(db.NOW)):
        self.assertEqual(v.year, datetime.datetime.now().year)
        self.assertEqual(v.month, datetime.datetime.now().month)
        self.assertEqual(v.day, datetime.datetime.now().day)
    self.assertEqual(db.date().year, db.YEAR)
    # Assert integer input.
    v1 = db.date(2010, 9, 21, format=db.DEFAULT_DATE_FORMAT)
    v2 = db.date(2010, 9, 21, 9, 27, 1, 0, db.DEFAULT_DATE_FORMAT)
    v3 = db.date(2010, 9, 21, hour=9, minute=27, second=1, format=db.DEFAULT_DATE_FORMAT)
    self.assertEqual(str(v1), "2010-09-21 00:00:00")
    self.assertEqual(str(v2), "2010-09-21 09:27:01")
    self.assertEqual(str(v3), "2010-09-21 09:27:01")
    # Assert week and weekday input
    v4 = db.date(2014, week=1, weekday=1, hour=12, format=db.DEFAULT_DATE_FORMAT)
    self.assertEqual(str(v4), "2013-12-30 12:00:00")
    # Assert Date input.
    v5 = db.date(db.date(2014, 1, 1))
    self.assertEqual(str(v5), "2014-01-01 00:00:00")
    # Assert timestamp input.
    v6 = db.date(db.date(2014, 1, 1).timestamp)
    self.assertEqual(str(v5), "2014-01-01 00:00:00")
    # Assert DateError for other input.
    self.assertRaises(db.DateError, db.date, None)
    print("pattern.db.date()")

</t>
<t tx="karstenw.20230303140159.16">def test_format(self):
    # Assert custom input formats.
    v = db.date("2010-09", "%Y-%m")
    self.assertEqual(str(v), "2010-09-01 00:00:00")
    self.assertEqual(v.year, 2010)
    # Assert custom output formats.
    v = db.date("2010-09", "%Y-%m", format="%Y-%m")
    self.assertEqual(v.format, "%Y-%m")
    self.assertEqual(str(v), "2010-09")
    self.assertEqual(v.year, 2010)
    # Assert strftime() for date &lt; 1900.
    v = db.date(1707, 4, 15)
    self.assertEqual(str(v), "1707-04-15 00:00:00")
    self.assertRaises(ValueError, lambda: v.timestamp)
    print("pattern.db.Date.__str__()")

</t>
<t tx="karstenw.20230303140159.17">def test_timestamp(self):
    # Assert Date.timestamp.
    v = db.date(2010, 9, 21, format=db.DEFAULT_DATE_FORMAT)
    self.assertEqual(v.timestamp, 1285020000)
    print("pattern.db.Date.timestamp")

</t>
<t tx="karstenw.20230303140159.18">def test_time(self):
    # Assert Date + time().
    v = db.date("2010-09-21 9:27:00")
    v = v - db.time(days=1, hours=1, minutes=1, seconds=1)
    self.assertEqual(str(v), "2010-09-20 08:25:59")
    # Assert Date + time(years, months)
    v = db.date(2014, 1, 31)
    v = v + db.time(years=1, months=1)
    self.assertEqual(str(v), "2015-02-28 00:00:00")
    print("pattern.db.time()")

</t>
<t tx="karstenw.20230303140159.19">class TestUtilityFunctions(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.2">def create_db_mysql():

    global DB_MYSQL

    # Make sure the database handle is setup and connected
    if not DB_MYSQL or not DB_MYSQL._connection:
        DB_MYSQL = db.Database(
            type = db.MYSQL,
            name = "pattern_unittest_db",
            host = HOST,
            port = PORT,
        username = USERNAME,
        password = PASSWORD)

    # Drop all tables first
    for table in list(DB_MYSQL.tables):
        DB_MYSQL.drop(table)

    return DB_MYSQL


</t>
<t tx="karstenw.20230303140159.20">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140159.21">def test_encryption(self):
    # Assert string password encryption.
    v1 = "test"
    v2 = db.encrypt_string(v1, key="1234")
    v3 = db.decrypt_string(v2, key="1234")
    self.assertTrue(v2 != "test")
    self.assertTrue(v3 == "test")
    print("pattern.db.encrypt_string()")
    print("pattern.db.decrypt_string()")

</t>
<t tx="karstenw.20230303140159.22">def test_json(self):
    # Assert JSON input and output.
    v1 = ["a,b", 1, 1.0, True, False, None, [1, 2], {"a:b": 1.2, "a,b": True, "a": [1, {"2": 3}], "1": "None"}]
    v2 = db.json.dumps(v1)
    v3 = db.json.loads(v2)
    self.assertEqual(v1, v3)
    print("pattern.db.json.dumps()")
    print("pattern.db.json.loads()")

</t>
<t tx="karstenw.20230303140159.23">def test_order(self):
    # Assert a list of indices in the order as when the given list is sorted.
    v = [3, 1, 2]
    self.assertEqual(db.order(v), [1, 2, 0])
    self.assertEqual(db.order(v, reverse=True), [0, 2, 1])
    self.assertEqual(db.order(v, cmp=lambda a, b: a - b), [1, 2, 0])
    self.assertEqual(db.order(v, key=lambda i: i), [1, 2, 0])
    print("pattern.db.order()")

</t>
<t tx="karstenw.20230303140159.24">def test_avg(self):
    # Assert (1+2+3+4) / 4 = 2.5.
    self.assertEqual(db.avg([1, 2, 3, 4]), 2.5)
    print("pattern.db.avg()")

</t>
<t tx="karstenw.20230303140159.25">def test_variance(self):
    # Assert 2.5.
    self.assertEqual(db.variance([1, 2, 3, 4, 5]), 2.5)
    print("pattern.db.variance()")

</t>
<t tx="karstenw.20230303140159.26">def test_stdev(self):
    # Assert 2.429.
    self.assertAlmostEqual(db.stdev([1, 5, 6, 7, 6, 8]), 2.429, places=3)
    print("pattern.db.stdev()")

</t>
<t tx="karstenw.20230303140159.27">def test_sqlite_functions(self):
    # Assert year(), month(), day(), ..., first(), last() and group_concat() for SQLite.
    v = "1707-04-15 01:02:03"
    self.assertEqual(db.sqlite_year(v), 1707)
    self.assertEqual(db.sqlite_month(v), 4)
    self.assertEqual(db.sqlite_day(v), 15)
    self.assertEqual(db.sqlite_hour(v), 1)
    self.assertEqual(db.sqlite_minute(v), 2)
    self.assertEqual(db.sqlite_second(v), 3)
    # Aggregate functions.
    for f, a, b in (
      (db.sqlite_first, [1, 2, 3], 1),
      (db.sqlite_last, [1, 2, 3], 3),
      (db.sqlite_group_concat, [1, 2, 3], "1,2,3")):
        f = f()
        for x in a:
            f.step(x)
        self.assertEqual(f.finalize(), b)
    print("pattern.db.sqlite_year()")
    print("pattern.db.sqlite_month()")
    print("pattern.db.sqlite_day()")
    print("pattern.db.sqlite_hour()")
    print("pattern.db.sqlite_minute()")
    print("pattern.db.sqlite_second()")
    print("pattern.db.sqlite_first()")
    print("pattern.db.sqlite_last()")
    print("pattern.db.sqlite_group_concat()")

</t>
<t tx="karstenw.20230303140159.28">class _TestDatabase(object):

    @others
</t>
<t tx="karstenw.20230303140159.29">def setUp(self):

    # Delete all tables first
    for table in list(self.db):
        self.db.drop(table)

</t>
<t tx="karstenw.20230303140159.3">def create_db_sqlite():

    global DB_SQLITE

    # Make sure the database handle is setup and connected
    if not DB_SQLITE or not DB_SQLITE._connection:
        DB_SQLITE = db.Database(
            type = db.SQLITE,
            name = "pattern_unittest_db",
            host = HOST,
            port = PORT,
        username = USERNAME,
        password = PASSWORD)

    # Drop all tables first
    for table in list(DB_MYSQL.tables):
        DB_SQLITE.drop(table)

    return DB_SQLITE

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.30">def tearDown(self):
    for table in list(self.db):
        self.db.drop(table)

</t>
<t tx="karstenw.20230303140159.31">def test_escape(self):
    # Assert str, unicode, int, long, float, bool and None field values.
    for v, s in (
      (   "a", "'a'"),
      (     1, "1"),
      (int(1), "1"),
      (   1.0, "1.0"),
      (  True, "1"),
      ( False, "0"),
      (  None, "null")):
        self.assertEqual(db._escape(v), s)
    # Assert date.
    v = db.date("1707-04-15")
    self.assertEqual(db._escape(v), "'1707-04-15 00:00:00'")
    # Assert current date.
    v = "current_timestamp"
    self.assertEqual(db._escape(v), "current_timestamp")
    # Assert subquery.
    v = self.db.create("dummy", fields=[db.pk()])
    v = v.query()
    self.assertEqual(db._escape(v), "(select dummy.* from `dummy`)")
    # Assert MySQL and SQLite quotes.
    if self.db.type == db.MYSQL:
        self.assertEqual(self.db.escape("'"), "'\\''")
    if self.db.type == db.SQLITE:
        self.assertEqual(self.db.escape("'"), "''''")
    print("pattern.db._escape()")

</t>
<t tx="karstenw.20230303140159.32">def test_database(self):
    # Assert Database properties.
    self.assertTrue(self.db.type == self.type)
    self.assertTrue(self.db.name == "pattern_unittest_db")
    self.assertTrue(self.db.host == HOST)
    self.assertTrue(self.db.port == PORT)
    self.assertTrue(self.db.username == USERNAME)
    self.assertTrue(self.db.password == PASSWORD)
    self.assertTrue(self.db.tables == {})
    self.assertTrue(self.db.relations == [])
    self.assertTrue(self.db.connected)
    self.db.disconnect()
    self.assertTrue(self.db.connected == False)
    self.assertTrue(self.db.connection is None)
    self.db.connect()
    print("pattern.db.Database(type=%s)" % self.type.upper())

</t>
<t tx="karstenw.20230303140159.33">def test_create_table(self):
    # Assert Database.create() new table.
    v = self.db.create("products", fields=[
        db.primary_key("pid"),
        db.field("name", db.STRING, index=True, optional=False),
        db.field("price", db.FLOAT)
    ])
    # Assert that the last query executed is stored.
    if self.db.type == db.SQLITE:
        self.assertEqual(self.db.query, "pragma table_info(`products`);")
    if self.db.type == db.MYSQL:
        self.assertEqual(self.db.query, "show columns from `products`;")
    # Assert new Table exists in Database.tables.
    self.assertTrue(isinstance(v, db.Table))
    self.assertTrue(len(self.db) == 1)
    self.assertTrue(v.pk == "pid")
    self.assertTrue(v.fields == ["pid", "name", "price"])
    self.assertTrue(self.db[v.name] == v)
    self.assertTrue(self.db.tables[v.name] == v)
    self.assertTrue(getattr(self.db, v.name) == v)
    # Assert Database._field_SQL subroutine for Database.create().
    for field, sql1, sql2 in (
      (db.primary_key("pid"),
       ("`pid` integer not null primary key auto_increment", None),
       ("`pid` integer not null primary key autoincrement", None)),
      (db.field("name", db.STRING, index=True, optional=False),
       ("`name` varchar(100) not null", "create index `products_name` on `products` (`name`);"),
       ("`name` varchar(100) not null", "create index `products_name` on `products` (`name`);")),
      (db.field("price", db.INTEGER),
       ("`price` integer null", None),
       ("`price` integer null", None))):
        if self.db.type == db.MYSQL:
            self.assertEqual(self.db._field_SQL(self.db["products"].name, field), sql1)
        if self.db.type == db.SQLITE:
            self.assertEqual(self.db._field_SQL(self.db["products"].name, field), sql2)
    # Assert TableError if table already exists.
    self.assertRaises(db.TableError, self.db.create, "products")
    # Assert remove table.
    self.db.drop("products")
    self.assertTrue(len(self.db) == 0)
    print("pattern.db.Database.create()")


</t>
<t tx="karstenw.20230303140159.34">class TestDeleteMySQLDatabase(unittest.TestCase):
    @others
</t>
<t tx="karstenw.20230303140159.35">def runTest(self):
    create_db_mysql()._delete()


</t>
<t tx="karstenw.20230303140159.36">class TestDeleteSQLiteDatabase(unittest.TestCase):
    @others
</t>
<t tx="karstenw.20230303140159.37">def runTest(self):
    create_db_sqlite()._delete()


</t>
<t tx="karstenw.20230303140159.38">class TestMySQLDatabase(unittest.TestCase, _TestDatabase):
    @others
</t>
<t tx="karstenw.20230303140159.39">def setUp(self):
    self.db, self.type = create_db_mysql(), db.MYSQL
    _TestDatabase.setUp(self)


</t>
<t tx="karstenw.20230303140159.4">class TestUnicode(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.40">class TestSQLiteDatabase(unittest.TestCase, _TestDatabase):
    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.41">def setUp(self):
    self.db, self.type = create_db_sqlite(), db.SQLITE
    _TestDatabase.setUp(self)

</t>
<t tx="karstenw.20230303140159.42">class TestSchema(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.43">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140159.44">def test_string(self):
    # Assert callable String.
    v1 = db._String()
    v2 = db._String()(0)
    v3 = db._String()(200)
    v4 = db._String()(300)
    self.assertEqual(v1, "string")
    self.assertEqual(v2, "varchar(1)")
    self.assertEqual(v3, "varchar(200)")
    self.assertEqual(v4, "varchar(255)")

</t>
<t tx="karstenw.20230303140159.45">def test_field(self):
    # Assert field() return value with different optional parameters.
    #                                                         NAME     TYPE            DEFAULT INDEX      OPTIONAL
    for kwargs, f in (
      (dict(name="id",    type=db.INT),                      ("id",    "integer",      None,   False,     True)),
      (dict(name="id",    type=db.INT,    index=db.PRIMARY), ("id",    "integer",      None,   "primary", True)),
      (dict(name="id",    type=db.INT,    index=db.UNIQUE),  ("id",    "integer",      None,   "unique",  True)),
      (dict(name="id",    type=db.INT,    index="0"),        ("id",    "integer",      None,   False,     True)),
      (dict(name="id",    type=db.INT,    index="1"),        ("id",    "integer",      None,   True,      True)),
      (dict(name="id",    type=db.INT,    index=True),       ("id",    "integer",      None,   True,      True)),
      (dict(name="id",    type=db.INT,    default=0),        ("id",    "integer",      0,      False,     True)),
      (dict(name="name",  type=db.STRING),                   ("name",  "varchar(100)", None,   False,     True)),
      (dict(name="name",  type=db.STRING, optional=False),   ("name",  "varchar(100)", None,   False,     False)),
      (dict(name="name",  type=db.STRING, optional="0"),     ("name",  "varchar(100)", None,   False,     False)),
      (dict(name="name",  type=db.STRING(50)),               ("name",  "varchar(50)",  None,   False,     True)),
      (dict(name="price", type=db.FLOAT,  default=0),        ("price", "real",         0,      False,     True)),
      (dict(name="show",  type=db.BOOL),                     ("show",  "tinyint(1)",   None,   False,     True)),
      (dict(name="show",  type=db.BOOL,   default=True),     ("show",  "tinyint(1)",   True,   False,     True)),
      (dict(name="show",  type=db.BOOL,   default=False),    ("show",  "tinyint(1)",   False,  False,     True)),
      (dict(name="date",  type=db.DATE),                     ("date",  "timestamp",    "now",  False,     True)),
      (dict(name="date",  type=db.DATE,   default=db.NOW),   ("date",  "timestamp",    "now",  False,     True)),
      (dict(name="date",  type=db.DATE,   default="1999-12-31 23:59:59"),
                                                             ("date", "timestamp", "1999-12-31 23:59:59", False, True))):
        self.assertEqual(db.field(**kwargs), f)
    # Assert primary_key() return value.
    self.assertTrue(db.primary_key() == db.pk() == ("id", "integer", None, "primary", False))
    print("pattern.db.field()")

</t>
<t tx="karstenw.20230303140159.46">def test_schema(self):
    now1 = "current_timestamp"
    now2 = "'CURRENT_TIMESTAMP'"
    # Assert Schema (= table schema in a uniform way across database engines).
    #   NAME    TYPE            DEFAULT INDEX  OPTIONAL
    for args, v in (
      (("id", "integer", None, "pri", False), ("id", db.INT, None, db.PRIMARY, False, None)),
      (("id", "integer", None, "uni", False), ("id", db.INT, None, db.UNIQUE, False, None)),
      (("id", "int", None, "yes", True), ("id", db.INT, None, True, True, None)),
      (("id", "real", None, "mul", True), ("id", db.FLOAT, None, True, True, None)),
      (("id", "real", None, "1", True), ("id", db.FLOAT, None, True, True, None)),
      (("id", "double", None, "0", True), ("id", db.FLOAT, None, False, True, None)),
      (("id", "double", 0, False, False), ("id", db.FLOAT, 0, False, False, None)),
      (("text", "varchar(10)", "?", False, True), ("text", db.STRING, "?", False, True, 10)),
      (("text", "char(20)", "", False, True), ("text", db.STRING, None, False, True, 20)),
      (("text", "text", None, False, True), ("text", db.TEXT, None, False, True, None)),
      (("text", "blob", None, False, True), ("text", db.BLOB, None, False, True, None)),
      (("show", "tinyint(1)", None, False, True), ("show", db.BOOL, None, False, True, None)),
      (("date", "timestamp", None, False, True), ("date", db.DATE, None, False, True, None)),
      (("date", "timestamp", now1, False, True), ("date", db.DATE, db.NOW, False, True, None)),
      (("date", "time", now2, False, "YES"), ("date", db.DATE, db.NOW, False, True, None))):
        s = db.Schema(*args)
        self.assertEqual(s.name, v[0])
        self.assertEqual(s.type, v[1])
        self.assertEqual(s.default, v[2])
        self.assertEqual(s.index, v[3])
        self.assertEqual(s.optional, v[4])
        self.assertEqual(s.length, v[5])
    print("pattern.db.Schema()")

</t>
<t tx="karstenw.20230303140159.47">class _TestTable(object):

    @others
</t>
<t tx="karstenw.20230303140159.48">def setUp(self):

    # Delete all tables first
    for table in list(self.db):
        self.db.drop(table)

    # Create test tables.
    self.db.create("persons", fields=[
        db.primary_key("id"),
        db.field("name", db.STRING)
    ])
    self.db.create("products", fields=[
        db.primary_key("id"),
        db.field("name", db.STRING),
        db.field("price", db.FLOAT, default=0.0)
    ])
    self.db.create("orders", fields=[
        db.primary_key("id"),
        db.field("person", db.INTEGER, index=True),
        db.field("product", db.INTEGER, index=True),
    ])

</t>
<t tx="karstenw.20230303140159.49">def tearDown(self):

    # Drop test tables.
    for table in list(self.db):
        self.db.drop(table)

</t>
<t tx="karstenw.20230303140159.5">def setUp(self):
    # Test data with different (or wrong) encodings.
    self.strings = (
        "ünîcøde",
        "ünîcøde".encode("utf-16"),
        "ünîcøde".encode("latin-1"),
        "ünîcøde".encode("windows-1252"),
        "ünîcøde",
        "אוניקאָד"
    )

</t>
<t tx="karstenw.20230303140159.50">def test_table(self):
    # Assert Table properties.
    v = self.db.persons
    self.assertTrue(v.db == self.db)
    self.assertTrue(v.pk == "id")
    self.assertTrue(v.fields == ["id", "name"])
    self.assertTrue(v.name == "persons")
    self.assertTrue(v.abs("name") == "persons.name")
    self.assertTrue(v.rows() == [])
    self.assertTrue(v.schema["id"].type == db.INTEGER)
    self.assertTrue(v.schema["id"].index == db.PRIMARY)
    print("pattern.db.Table")

</t>
<t tx="karstenw.20230303140159.51">def test_rename(self):
    # Assert ALTER TABLE when name changes.
    v = self.db.persons
    v.name = "clients"
    self.assertEqual(self.db.query, "alter table `persons` rename to `clients`;")
    self.assertEqual(self.db.tables.get("clients"), v)
    print("pattern.db.Table.name")

</t>
<t tx="karstenw.20230303140159.52">def test_fields(self):
    # Assert ALTER TABLE when column is inserted.
    v = self.db.products
    v.fields.append(db.field("description", db.TEXT))
    self.assertEqual(v.fields, ["id", "name", "price", "description"])
    print("pattern.db.Table.fields")

</t>
<t tx="karstenw.20230303140159.53">def test_insert_update_delete(self):
    # Assert Table.insert().
    v1 = self.db.persons.insert(name="Kurt Gödel")
    v2 = self.db.products.insert(name="pizza", price=10.0)
    v3 = self.db.products.insert({"name": "garlic bread", "price": 3.0})
    v4 = self.db.orders.insert(person=v1, product=v3)
    self.assertEqual(v1, 1)
    self.assertEqual(v2, 1)
    self.assertEqual(v3, 2)
    self.assertEqual(v4, 1)
    self.assertEqual(self.db.persons.rows(), [(1, "Kurt Gödel")])
    self.assertEqual(self.db.products.rows(), [(1, "pizza", 10.0), (2, "garlic bread", 3.0)])
    self.assertEqual(self.db.orders.rows(), [(1, 1, 2)])
    self.assertEqual(self.db.orders.count(), 1)
    self.assertEqual(self.db.products.xml.replace(' extra="auto_increment"', ""),
        '&lt;?xml version="1.0" encoding="utf-8"?&gt;\n'
        '&lt;table name="products" fields="id, name, price" count="2"&gt;\n'
        '\t&lt;schema&gt;\n'
        '\t\t&lt;field name="id" type="integer" index="primary" optional="no" /&gt;\n'
        '\t\t&lt;field name="name" type="string" length="100" /&gt;\n'
        '\t\t&lt;field name="price" type="float" default="0.0" /&gt;\n'
        '\t&lt;/schema&gt;\n'
        '\t&lt;rows&gt;\n'
        '\t\t&lt;row id="1" name="pizza" price="10.0" /&gt;\n'
        '\t\t&lt;row id="2" name="garlic bread" price="3.0" /&gt;\n'
        '\t&lt;/rows&gt;\n'
        '&lt;/table&gt;'
    )
    # Assert transactions with commit=False.
    if self.db.type == db.SQLITE:
        self.db.orders.insert(person=v1, product=v2, commit=False)
        self.db.rollback()
        self.assertEqual(len(self.db.orders), 1)
    self.db.orders.insert(person=v1, product=v2, commit=False)
    # Assert Table.update().
    self.db.products.update(2, price=4.0)
    self.db.products.update(2, {"price": 4.5})
    self.db.products.update(db.all(db.filter("name", "pi*")), name="deeppan pizza")
    self.assertEqual(self.db.products.rows(), [(1, "deeppan pizza", 10.0), (2, "garlic bread", 4.5)])
    # Assert Table.delete().
    self.db.products.delete(db.all(db.filter("name", "deeppan*")))
    self.db.products.delete(db.ALL)
    self.db.orders.delete(1)
    self.assertEqual(len(self.db.products), 0)
    self.assertEqual(len(self.db.orders), 1)
    print("pattern.db.Table.insert()")
    print("pattern.db.Table.update()")
    print("pattern.db.Table.delete()")

</t>
<t tx="karstenw.20230303140159.54">def test_filter(self):
    # Assert Table.filter().
    self.db.persons.insert(name="Kurt Gödel")
    self.db.persons.insert(name="M. C. Escher")
    self.db.persons.insert(name="Johann Sebastian Bach")
    f = self.db.persons.filter
    self.assertEqual(f(("name",), id=1), [("Kurt Gödel",)])
    self.assertEqual(f(db.ALL, id=(1, 2)), [(1, "Kurt Gödel"), (2, "M. C. Escher")])
    self.assertEqual(f({"id": (1, 2)}), [(1, "Kurt Gödel"), (2, "M. C. Escher")])
    self.assertEqual(f("id", name="Johan*"), [(3,)])
    self.assertEqual(f("id", name=("J*", "K*")), [(1,), (3,)])
    print("pattern.db.Table.filter()")

</t>
<t tx="karstenw.20230303140159.55">def test_search(self):
    # Assert Table.search =&gt; Query object.
    v = self.db.persons.search()
    self.assertTrue(isinstance(v, db.Query))
    self.assertTrue(v.table == self.db.persons)

</t>
<t tx="karstenw.20230303140159.56">def test_datasheet(self):
    # Assert Table.datasheet() =&gt; Datasheet object.
    v = self.db.persons.datasheet()
    self.assertTrue(isinstance(v, db.Datasheet))
    self.assertTrue(v.fields[0] == ("id", db.INTEGER))
    print("pattern.db.Table.datasheet()")


</t>
<t tx="karstenw.20230303140159.57">class TestMySQLTable(unittest.TestCase, _TestTable):
    @others
</t>
<t tx="karstenw.20230303140159.58">def setUp(self):
    self.db = create_db_mysql()
    _TestTable.setUp(self)


</t>
<t tx="karstenw.20230303140159.59">class TestSQLiteTable(unittest.TestCase, _TestTable):
    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.6">def test_decode_utf8(self):
    # Assert unicode.
    for s in self.strings:
        self.assertTrue(isinstance(db.decode_utf8(s), str))
    print("pattern.db.decode_utf8()")

</t>
<t tx="karstenw.20230303140159.60">def setUp(self):
    self.db = DB_SQLITE
    _TestTable.setUp(self)

</t>
<t tx="karstenw.20230303140159.61">class _TestQuery(object):

    @others
</t>
<t tx="karstenw.20230303140159.62">def setUp(self):

    # Delete all tables first
    for table in list(self.db):
        self.db.drop(table)

    # Create test tables.
    self.db.create("persons", fields=[
        db.primary_key("id"),
        db.field("name", db.STRING),
        db.field("age", db.INTEGER),
        db.field("gender", db.INTEGER)
    ])
    self.db.create("gender", fields=[
        db.primary_key("id"),
        db.field("name", db.STRING)
    ])
    # Create test data.
    self.db.persons.insert(name="john", age="30", gender=2)
    self.db.persons.insert(name="jack", age="20", gender=2)
    self.db.persons.insert(name="jane", age="30", gender=1)
    self.db.gender.insert(name="female")
    self.db.gender.insert(name="male")

</t>
<t tx="karstenw.20230303140159.63">def tearDown(self):
    # Drop test tables.
    for table in list(self.db):
        self.db.drop(table)

</t>
<t tx="karstenw.20230303140159.64">def _query(self, *args, **kwargs):
    """ Returns a pattern.db.Query object on a mock Table and Database.
    """
    class Database(object):
        escape, relations = lambda self, v: db._escape(v), []

    class Table(object):
        name, fields, db = "persons", ["id", "name", "age", "sex"], Database()
    return db.Query(Table(), *args, **kwargs)

</t>
<t tx="karstenw.20230303140159.65">def test_abs(self):
    # Assert absolute fieldname for trivial cases.
    self.assertEqual(db.abs("persons", "name"), "persons.name")
    self.assertEqual(db.abs("persons", ("id", "name")), ["persons.id", "persons.name"])
    # Assert absolute fieldname with SQL functions (e.g., avg(product.price)).
    for f in db.sql_functions.split("|"):
        self.assertEqual(db.abs("persons", "%s(name)" % f), "%s(persons.name)" % f)
    print("pattern.db.abs()")

</t>
<t tx="karstenw.20230303140159.66">def test_cmp(self):
    # Assert WHERE-clause from cmp() function.
    q = self.db.persons.search(fields=["name"])
    self.assertTrue(isinstance(q, db.Query))
    for args, sql in (
      (("name", "Kurt%",     db.LIKE),    "name like 'Kurt%'"),
      (("name", "Kurt*",     "="),        "name like 'Kurt%'"),
      (("name", "*Gödel",    "=="),       "name like '%Gödel'"),
      (("name", "Kurt*",     "!="),       "name not like 'Kurt%'"),
      (("name", "Kurt*",     "&lt;&gt;"),       "name not like 'Kurt%'"),
      (("name", "Gödel",     "i="),       "name like 'Gödel'"),     # case-insensitive search
      (("id",   (1, 2),      db.IN),      "id in (1,2)"),
      (("id",   (1, 2),      "="),        "id in (1,2)"),
      (("id",   (1, 2),      "=="),       "id in (1,2)"),
      (("id",   (1, 2),      "!="),       "id not in (1,2)"),
      (("id",   (1, 2),      "&lt;&gt;"),       "id not in (1,2)"),
      (("id",   (1, 3),      db.BETWEEN), "id between 1 and 3"),
      (("id",   (1, 3),      ":"),        "id between 1 and 3"),
      (("name", ("G", "K*"), "="),        "(name='G' or name like 'K%')"),
      (("name", None,        "="),        "name is null"),
      (("name", None,        "=="),       "name is null"),
      (("name", None,        "!="),       "name is not null"),
      (("name", None,        "&lt;&gt;"),       "name is not null"),
      (("name", q,           "="),        "name in (select persons.name from `persons`)"),
      (("name", q,           "=="),       "name in (select persons.name from `persons`)"),
      (("name", q,           "!="),       "name not in (select persons.name from `persons`)"),
      (("name", q,           "&lt;&gt;"),       "name not in (select persons.name from `persons`)"),
      (("name", "Gödel",     "="),        "name='Gödel'"),
      (("id",   1,           "&gt;"),        "id&gt;1")):
        self.assertEqual(db.cmp(*args), sql)
    print("pattern.db.cmp()")

</t>
<t tx="karstenw.20230303140159.67">def test_filterchain(self):
    # Assert WHERE with AND/OR combinations from FilterChain object().
    yesterday = db.date()
    yesterday -= db.time(days=1)
    f1 = db.FilterChain(("name", "garlic bread"))
    f2 = db.FilterChain(("name", "pizza"), ("price", 10, "&lt;"), operator=db.AND)
    f3 = db.FilterChain(f1, f2, operator=db.OR)
    f4 = db.FilterChain(f3, ("date", yesterday, "&gt;"), operator=db.AND)
    self.assertEqual(f1.SQL(), "name='garlic bread'")
    self.assertEqual(f2.SQL(), "name='pizza' and price&lt;10")
    self.assertEqual(f3.SQL(), "(name='garlic bread') or (name='pizza' and price&lt;10)")
    self.assertEqual(f4.SQL(), "((name='garlic bread') or (name='pizza' and price&lt;10)) and date&gt;'%s'" % yesterday)
    # Assert subquery in filter chain.
    q = self._query(fields=["name"])
    f = db.any(("name", "Gödel"), ("name", q))
    self.assertEqual(f.SQL(), "name='Gödel' or name in (select persons.name from `persons`)")
    print("pattern.db.FilterChain")

</t>
<t tx="karstenw.20230303140159.68">def test_query(self):
    # Assert table query results from Table.search().
    for kwargs, sql, rows in (
      (dict(fields=db.ALL),
        "select persons.* from `persons`;",
        [(1, "john", 30, 2),
         (2, "jack", 20, 2),
         (3, "jane", 30, 1)]),
      (dict(fields=db.ALL, range=(0, 2)),
        "select persons.* from `persons` limit 0, 2;",
        [(1, "john", 30, 2),
         (2, "jack", 20, 2)]),
      (dict(fields=db.ALL, filters=[("age", 30, "&lt;")]),
        "select persons.* from `persons` where persons.age&lt;30;",
        [(2, "jack", 20, 2)]),
      (dict(fields=db.ALL, filters=db.any(("age", 30, "&lt;"), ("name", "john"))),
        "select persons.* from `persons` where persons.age&lt;30 or persons.name='john';",
        [(1, "john", 30, 2),
         (2, "jack", 20, 2)]),
      (dict(fields=["name", "gender.name"], relations=[db.relation("gender", "id", "gender")]),
        "select persons.name, gender.name from `persons` left join `gender` on persons.gender=gender.id;",
        [("john", "male"),
         ("jack", "male"),
         ("jane", "female")]),
      (dict(fields=["name", "age"], sort="name"),
        "select persons.name, persons.age from `persons` order by persons.name asc;",
        [("jack", 20),
         ("jane", 30),
         ("john", 30)]),
      (dict(fields=["name", "age"], sort=1, order=db.DESCENDING),
        "select persons.name, persons.age from `persons` order by persons.name desc;",
        [("john", 30),
         ("jane", 30),
         ("jack", 20)]),
      (dict(fields=["age", "name"], sort=["age", "name"], order=[db.ASCENDING, db.DESCENDING]),
        "select persons.age, persons.name from `persons` order by persons.age asc, persons.name desc;",
        [(20, "jack"),
         (30, "john"),
         (30, "jane")]),
      (dict(fields=["age", "name"], group="age", function=db.CONCATENATE),
        "select persons.age, group_concat(persons.name) from `persons` group by persons.age;",
        [(20, "jack"),
         (30, "john,jane")]),
      (dict(fields=["id", "name", "age"], group="age", function=[db.COUNT, db.CONCATENATE]),
        "select count(persons.id), group_concat(persons.name), persons.age from `persons` group by persons.age;",
        [(1, "jack", 20),
         (2, "john,jane", 30)])):
        v = self.db.persons.search(**kwargs)
        v.xml
        self.assertEqual(v.SQL(), sql)
        self.assertEqual(v.rows(), rows)
    # Assert Database.link() permanent relations.
    v = self.db.persons.search(fields=["name", "gender.name"])
    v.aliases["gender.name"] = "gender"
    self.db.link("persons", "gender", "gender", "id", join=db.LEFT)
    self.assertEqual(v.SQL(),
        "select persons.name, gender.name as gender from `persons` left join `gender` on persons.gender=gender.id;")
    self.assertEqual(v.rows(),
        [('john', 'male'),
         ('jack', 'male'),
         ('jane', 'female')])
    print("pattern.db.Table.search()")
    print("pattern.db.Table.Query")

</t>
<t tx="karstenw.20230303140159.69">def test_xml(self):
    # Assert Query.xml dump.
    v = self.db.persons.search(fields=["name", "gender.name"])
    v.aliases["gender.name"] = "gender"
    self.db.link("persons", "gender", "gender", "id", join=db.LEFT)
    self.assertEqual(v.xml,
        '&lt;?xml version="1.0" encoding="utf-8"?&gt;\n'
        '&lt;query table="persons" fields="name, gender" count="3"&gt;\n'
        '\t&lt;schema&gt;\n'
        '\t\t&lt;field name="name" type="string" length="100" /&gt;\n'
        '\t\t&lt;field name="gender" type="string" length="100" /&gt;\n'
        '\t&lt;/schema&gt;\n'
        '\t&lt;rows&gt;\n'
        '\t\t&lt;row name="john" gender="male" /&gt;\n'
        '\t\t&lt;row name="jack" gender="male" /&gt;\n'
        '\t\t&lt;row name="jane" gender="female" /&gt;\n'
        '\t&lt;/rows&gt;\n'
        '&lt;/query&gt;'
    )
    # Assert Database.create() from XML.
    self.assertRaises(db.TableError, self.db.create, v.xml) # table 'persons' already exists
    self.db.create(v.xml, name="persons2")
    self.assertTrue("persons2" in self.db)
    self.assertTrue(self.db.persons2.fields == ["name", "gender"])
    self.assertTrue(len(self.db.persons2) == 3)
    print("pattern.db.Query.xml")


</t>
<t tx="karstenw.20230303140159.7">def test_encode_utf8(self):
    # Assert Python bytestring.
    for s in self.strings:
        self.assertTrue(isinstance(db.encode_utf8(s), bytes))
    print("pattern.db.encode_utf8()")

</t>
<t tx="karstenw.20230303140159.70">class TestMySQLQuery(unittest.TestCase, _TestQuery):
    @others
</t>
<t tx="karstenw.20230303140159.71">def setUp(self):
    self.db = create_db_mysql()
    _TestQuery.setUp(self)


</t>
<t tx="karstenw.20230303140159.72">class TestSQLiteQuery(unittest.TestCase, _TestQuery):
    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.73">def setUp(self):
    self.db = create_db_sqlite()
    _TestQuery.setUp(self)

</t>
<t tx="karstenw.20230303140159.74">class _TestView(object):

    @others
</t>
<t tx="karstenw.20230303140159.75">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140159.76">def tearDown(self):
    # Drop test tables.
    for table in list(self.db):
        self.db.drop(table)

</t>
<t tx="karstenw.20230303140159.77">def test_view(self):

    class Products(db.View):
        def __init__(self, database):
            db.View.__init__(self, database, "products", schema=[
                db.pk(),
                db.field("name", db.STRING),
                db.field("price", db.FLOAT)
            ])
            self.setup()
            self.table.insert(name="pizza", price=15.0)

        def render(self, query, **kwargs):
            q = self.table.search(fields=["name", "price"], filters=[("name", "*%s*" % query)])
            s = []
            for row in q.rows():
                s.append("&lt;tr&gt;%s&lt;/tr&gt;" % "".join(
                    ["&lt;td class=\"%s\"&gt;%s&lt;/td&gt;" % f for f in zip(q.fields, row)]))
            return "&lt;table&gt;" + "".join(s) + "&lt;/table&gt;"

    # Assert View with automatic Table creation.
    v = Products(self.db)
    self.assertEqual(v.render("iz"),
        "&lt;table&gt;"
        "&lt;tr&gt;"
        "&lt;td class=\"name\"&gt;pizza&lt;/td&gt;"
        "&lt;td class=\"price\"&gt;15.0&lt;/td&gt;"
        "&lt;/tr&gt;"
        "&lt;/table&gt;"
    )
    print("pattern.db.View")


</t>
<t tx="karstenw.20230303140159.78">class TestMySQLView(unittest.TestCase, _TestView):
    @others
</t>
<t tx="karstenw.20230303140159.79">def setUp(self):
    self.db = create_db_mysql()
    _TestView.setUp(self)


</t>
<t tx="karstenw.20230303140159.8">def test_string(self):
    # Assert string() with default for "" and None.
    for v, s in ((True, "True"), (1, "1"), (1.0, "1.0"), ("", "????"), (None, "????")):
        self.assertEqual(db.string(v, default="????"), s)
    print("pattern.db.string()")

</t>
<t tx="karstenw.20230303140159.80">class TestSQLiteView(unittest.TestCase, _TestView):
    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.81">def setUp(self):
    self.db = create_db_sqlite()
    _TestView.setUp(self)

</t>
<t tx="karstenw.20230303140159.82">class TestCSV(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.83">def setUp(self):
    # Create test table.
    self.csv = db.CSV(
        rows=[
            ["Schrödinger", "cat", True, 3, db.date(2009, 11, 3)],
            ["Hofstadter", "labrador", True, 5, db.date(2007, 8, 4)]
        ],
        fields=[
            ["name", db.STRING],
            ["type", db.STRING],
            ["tail", db.BOOLEAN],
            ["age", db.INTEGER],
            ["date", db.DATE],
        ])

</t>
<t tx="karstenw.20230303140159.84">def test_csv_header(self):
    # Assert field headers parser.
    v1 = db.csv_header_encode("age", db.INTEGER)
    v2 = db.csv_header_decode("age (INTEGER)")
    self.assertEqual(v1, "age (INTEGER)")
    self.assertEqual(v2, ("age", db.INTEGER))
    print("pattern.db.csv_header_encode()")
    print("pattern.db.csv_header_decode()")

</t>
<t tx="karstenw.20230303140159.85">def test_csv(self):
    # Assert saving and loading data (field types are preserved).
    v = self.csv
    v.save("test.csv", headers=True)
    v = db.CSV.load("test.csv", headers=True)
    self.assertTrue(isinstance(v, list))
    self.assertTrue(v.headers[0] == ("name", db.STRING))
    self.assertTrue(v[0] == ["Schrödinger", "cat", True, 3, db.date(2009, 11, 3)])
    os.unlink("test.csv")
    print("pattern.db.CSV")
    print("pattern.db.CSV.save()")
    print("pattern.db.CSV.load()")

</t>
<t tx="karstenw.20230303140159.86">def test_file(self):
    # Assert CSV file contents.
    v = self.csv
    v.save("test.csv", headers=True)
    v = open("test.csv", "rb").read()
    v = db.decode_utf8(v.lstrip(codecs.BOM_UTF8))
    v = v.replace("\r\n", "\n")
    self.assertEqual(v,
        '"name (STRING)","type (STRING)","tail (BOOLEAN)","age (INTEGER)","date (DATE)"\n'
        '"Schrödinger","cat","True","3","2009-11-03 00:00:00"\n'
        '"Hofstadter","labrador","True","5","2007-08-04 00:00:00"'
    )
    os.unlink("test.csv")

</t>
<t tx="karstenw.20230303140159.87">class TestDatasheet(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.88">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140159.89">def test_rows(self):
    # Assert Datasheet.rows DatasheetRows object.
    v = db.Datasheet(rows=[[1, 2], [3, 4]])
    v.rows += [5, 6]
    v.rows[0] = [0, 0]
    v.rows.swap(0, 1)
    v.rows.insert(1, [1, 1])
    v.rows.pop(1)
    self.assertTrue(isinstance(v.rows, db.DatasheetRows))
    self.assertEqual(v.rows, [[3, 4], [0, 0], [5, 6]])
    self.assertEqual(v.rows[0], [3, 4])
    self.assertEqual(v.rows[-1], [5, 6])
    self.assertEqual(v.rows.count([3, 4]), 1)
    self.assertEqual(v.rows.index([3, 4]), 0)
    self.assertEqual(sorted(v.rows, reverse=True), [[5, 6], [3, 4], [0, 0]])
    self.assertRaises(AttributeError, v._set_rows, [])
    # Assert default for new rows with missing columns.
    v.rows.extend([[7], [9]], default=0)
    self.assertEqual(v.rows, [[3, 4], [0, 0], [5, 6], [7, 0], [9, 0]])
    print("pattern.db.Datasheet.rows")

</t>
<t tx="karstenw.20230303140159.9">class TestEntities(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140159.90">def test_columns(self):
    # Assert Datasheet.columns DatasheetColumns object.
    v = db.Datasheet(rows=[[1, 3], [2, 4]])
    v.columns += [5, 6]
    v.columns[0] = [0, 0]
    v.columns.swap(0, 1)
    v.columns.insert(1, [1, 1])
    v.columns.pop(1)
    self.assertTrue(isinstance(v.columns, db.DatasheetColumns))
    self.assertEqual(v.columns, [[3, 4], [0, 0], [5, 6]])
    self.assertEqual(v.columns[0], [3, 4])
    self.assertEqual(v.columns[-1], [5, 6])
    self.assertEqual(v.columns.count([3, 4]), 1)
    self.assertEqual(v.columns.index([3, 4]), 0)
    self.assertEqual(sorted(v.columns, reverse=True), [[5, 6], [3, 4], [0, 0]])
    self.assertRaises(AttributeError, v._set_columns, [])
    # Assert default for new columns with missing rows.
    v.columns.extend([[7], [9]], default=0)
    self.assertEqual(v.columns, [[3, 4], [0, 0], [5, 6], [7, 0], [9, 0]])
    print("pattern.db.Datasheet.columns")

</t>
<t tx="karstenw.20230303140159.91">def test_column(self):
    # Assert DatasheetColumn object.
    # It has a reference to the parent Datasheet, as long as it is not deleted from the datasheet.
    v = db.Datasheet(rows=[[1, 3], [2, 4]])
    column = v.columns[0]
    column.insert(1, 0, default=None)
    self.assertEqual(v, [[1, 3], [0, None], [2, 4]])
    del v.columns[0]
    self.assertTrue(column._datasheet, None)
    print("pattern.db.DatasheetColumn")

</t>
<t tx="karstenw.20230303140159.92">def test_fields(self):
    # Assert Datasheet with incomplete headers.
    v = db.Datasheet(rows=[["Schrödinger", "cat"]], fields=[("name", db.STRING)])
    self.assertEqual(v.fields, [("name", db.STRING)])
    # Assert (None, None) for missing headers.
    v.columns.swap(0, 1)
    self.assertEqual(v.fields, [(None, None), ("name", db.STRING)])
    v.columns[0] = ["dog"]
    self.assertEqual(v.fields, [(None, None), ("name", db.STRING)])
    # Assert removing a column removes the header.
    v.columns.pop(0)
    self.assertEqual(v.fields, [("name", db.STRING)])
    # Assert new columns with header description.
    v.columns.append(["cat"])
    v.columns.append([3], field=("age", db.INTEGER))
    self.assertEqual(v.fields, [("name", db.STRING), (None, None), ("age", db.INTEGER)])
    # Assert column by name.
    self.assertEqual(v.name, ["Schrödinger"])
    print("pattern.db.Datasheet.fields")

</t>
<t tx="karstenw.20230303140159.93">def test_group(self):
    # Assert Datasheet.group().
    v1 = db.Datasheet(rows=[[1, 2, "a"], [1, 3, "b"], [1, 4, "c"], [0, 0, "d"]])
    v2 = v1.group(0)
    v3 = v1.group(0, function=db.LAST)
    v4 = v1.group(0, function=(db.FIRST, db.COUNT, db.CONCATENATE))
    v5 = v1.group(0, function=db.CONCATENATE, key=lambda j: j &gt; 0)
    self.assertEqual(v2, [[1, 2, "a"], [0, 0, "d"]])
    self.assertEqual(v3, [[1, 4, "c"], [0, 0, "d"]])
    self.assertEqual(v4, [[1, 3, "a,b,c"], [0, 1, "d"]])
    self.assertEqual(v5, [[True, "2,3,4", "a,b,c"], [False, "0", "d"]])
    print("pattern.db.Datasheet.group()")

</t>
<t tx="karstenw.20230303140159.94">def test_slice(self):
    # Assert Datasheet slices.
    v = db.Datasheet([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    v = v.copy()
    self.assertEqual(v.slice(0, 1, 3, 2), [[2, 3], [5, 6], [8, 9]])
    self.assertEqual(v[2], [7, 8, 9])
    self.assertEqual(v[2, 2], 9)
    self.assertEqual(v[2, 1:], [8, 9])
    self.assertEqual(v[0:2], [[1, 2, 3], [4, 5, 6]])
    self.assertEqual(v[0:2, 1], [2, 5])
    self.assertEqual(v[0:2, 0:2], [[1, 2], [4, 5]])
    # Assert new Datasheet for i:j slices.
    self.assertTrue(isinstance(v[0:2], db.Datasheet))
    self.assertTrue(isinstance(v[0:2, 0:2], db.Datasheet))
    print("pattern.db.Datasheet.slice()")

</t>
<t tx="karstenw.20230303140159.95">def test_copy(self):
    # Assert Datasheet.copy().
    v = db.Datasheet([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    self.assertTrue(v.copy(), [[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    self.assertTrue(v.copy(rows=[0]), [[1, 2, 3]])
    self.assertTrue(v.copy(rows=[0], columns=[0]), [[1]])
    self.assertTrue(v.copy(columns=[0]), [[1], [4], [7]])
    print("pattern.db.Datasheet.copy()")

</t>
<t tx="karstenw.20230303140159.96">def test_map(self):
    # Assert Datasheet.map() (in-place).
    v = db.Datasheet(rows=[[1, 2], [3, 4]])
    v.map(lambda x: x + 1)
    self.assertEqual(v, [[2, 3], [4, 5]])
    print("pattern.db.Datasheet.map()")

</t>
<t tx="karstenw.20230303140159.97">def test_json(self):
    # Assert JSON output.
    v = db.Datasheet(rows=[["Schrödinger", 3], ["Hofstadter", 5]])
    self.assertEqual(v.json, '[["Schrödinger", 3], ["Hofstadter", 5]]')
    # Assert JSON output with headers.
    v = db.Datasheet(rows=[["Schrödinger", 3], ["Hofstadter", 5]],
                   fields=[("name", db.STRING), ("age", db.INT)])
    random.seed(0)
    w = db.json.loads(v.json)
    self.assertTrue({"age": 3, "name": "Schrödinger"} in w)
    self.assertTrue({"age": 5, "name": "Hofstadter"} in w)
    print("pattern.db.Datasheet.json")

</t>
<t tx="karstenw.20230303140159.98">def test_flip(self):
    # Assert flip matrix.
    v = db.flip(db.Datasheet([[1, 2], [3, 4]]))
    self.assertEqual(v, [[1, 3], [2, 4]])
    print("pattern.db.flip()")

</t>
<t tx="karstenw.20230303140159.99">def test_truncate(self):
    # Assert string truncate().
    v1 = "a" * 50
    v2 = "a" * 150
    v3 = "aaa " * 50
    self.assertEqual(db.truncate(v1), (v1, ""))
    self.assertEqual(db.truncate(v2), ("a" * 99 + "-", "a" * 51))
    self.assertEqual(db.truncate(v3), (("aaa " * 25).strip(), "aaa " * 25))
    print("pattern.db.truncate()")

</t>
<t tx="karstenw.20230303140202.1">from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
import subprocess

from pattern import de

from io import open

try:
    PATH = os.path.dirname(os.path.realpath(__file__))
except:
    PATH = ""

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140202.10">def test_find_lexeme(self):
    # Assert the accuracy of the verb conjugation algorithm.
    i, n = 0, 0
    for v, lexeme1 in de.inflect.verbs.infinitives.items():
        lexeme2 = de.inflect.verbs.find_lexeme(v)
        for j in range(len(lexeme2)):
            if lexeme1[j] == "":
                continue
            if lexeme1[j] == lexeme2[j]:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.86)
    print("pattern.de.inflect.verbs.find_lexeme()")

</t>
<t tx="karstenw.20230303140202.11">def test_conjugate(self):
    # Assert different tenses with different conjugations.
    for (v1, v2, tense) in (
      ("sein",  "sein",     de.INFINITIVE),
      ("sein",  "bin",     (de.PRESENT, 1, de.SINGULAR)),
      ("sein",  "bist",    (de.PRESENT, 2, de.SINGULAR)),
      ("sein",  "ist",     (de.PRESENT, 3, de.SINGULAR)),
      ("sein",  "sind",    (de.PRESENT, 1, de.PLURAL)),
      ("sein",  "seid",    (de.PRESENT, 2, de.PLURAL)),
      ("sein",  "sind",    (de.PRESENT, 3, de.PLURAL)),
      ("sein",  "seiend",  (de.PRESENT + de.PARTICIPLE)),
      ("sein",  "war",     (de.PAST, 1, de.SINGULAR)),
      ("sein",  "warst",   (de.PAST, 2, de.SINGULAR)),
      ("sein",  "war",     (de.PAST, 3, de.SINGULAR)),
      ("sein",  "waren",   (de.PAST, 1, de.PLURAL)),
      ("sein",  "wart",    (de.PAST, 2, de.PLURAL)),
      ("sein",  "waren",   (de.PAST, 3, de.PLURAL)),
      ("sein",  "gewesen", (de.PAST + de.PARTICIPLE)),
      ("sein",  "sei",     (de.PRESENT, 2, de.SINGULAR, de.IMPERATIVE)),
      ("sein",  "seien",   (de.PRESENT, 1, de.PLURAL, de.IMPERATIVE)),
      ("sein",  "seid",    (de.PRESENT, 2, de.PLURAL, de.IMPERATIVE)),
      ("sein", "sei",     (de.PRESENT, 1, de.SINGULAR, de.SUBJUNCTIVE)),
      ("sein", "seiest",  (de.PRESENT, 2, de.SINGULAR, de.SUBJUNCTIVE)),
      ("sein", "sei",     (de.PRESENT, 3, de.SINGULAR, de.SUBJUNCTIVE)),
      ("sein", "seien",   (de.PRESENT, 1, de.PLURAL, de.SUBJUNCTIVE)),
      ("sein", "seiet",   (de.PRESENT, 2, de.PLURAL, de.SUBJUNCTIVE)),
      ("sein", "seien",   (de.PRESENT, 3, de.PLURAL, de.SUBJUNCTIVE)),
      ("sein", "wäre",    (de.PAST, 1, de.SINGULAR, de.SUBJUNCTIVE)),
      ("sein", "wärest",  (de.PAST, 2, de.SINGULAR, de.SUBJUNCTIVE)),
      ("sein", "wäre",    (de.PAST, 3, de.SINGULAR, de.SUBJUNCTIVE)),
      ("sein", "wären",   (de.PAST, 1, de.PLURAL, de.SUBJUNCTIVE)),
      ("sein", "wäret",   (de.PAST, 2, de.PLURAL, de.SUBJUNCTIVE)),
      ("sein", "wären",   (de.PAST, 3, de.PLURAL, de.SUBJUNCTIVE))):
        self.assertEqual(de.conjugate(v1, tense), v2)
    print("pattern.de.conjugate()")

</t>
<t tx="karstenw.20230303140202.12">def test_lexeme(self):
    # Assert all inflections of "sein".
    v = de.lexeme("sein")
    self.assertEqual(v, [
        "sein", "bin", "bist", "ist", "sind", "seid", "seiend",
        "war", "warst", "waren", "wart", "gewesen",
        "sei", "seien", "seiest", "seiet",
        "wäre", "wärest", "wären", "wäret"
    ])
    print("pattern.de.inflect.lexeme()")

</t>
<t tx="karstenw.20230303140202.13">def test_tenses(self):
    # Assert tense recognition.
    self.assertTrue((de.PRESENT, 3, de.SG) in de.tenses("ist"))
    self.assertTrue("2sg" in de.tenses("bist"))
    print("pattern.de.tenses()")

</t>
<t tx="karstenw.20230303140202.14">class TestParser(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140202.15">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140202.16">def test_find_lemmata(self):
    # Assert lemmata for nouns, adjectives and verbs.
    v = de.parser.find_lemmata([["Ich", "PRP"], ["sage", "VB"], ["schöne", "JJ"], ["Dinge", "NNS"]])
    self.assertEqual(v, [
        ["Ich", "PRP", "ich"],
        ["sage", "VB", "sagen"],
        ["schöne", "JJ", "schön"],
        ["Dinge", "NNS", "ding"]])
    print("pattern.de.parser.find_lemmata()")

</t>
<t tx="karstenw.20230303140202.17">def test_parse(self):
    # Assert parsed output with Penn Treebank II tags (slash-formatted).
    # 1) "der große Hund" is a noun phrase, "auf der Matte" is a prepositional noun phrase.
    v = de.parser.parse("Der große Hund sitzt auf der Matte.")
    self.assertEqual(v,
        "Der/DT/B-NP/O große/JJ/I-NP/O Hund/NN/I-NP/O " + \
        "sitzt/VB/B-VP/O " + \
        "auf/IN/B-PP/B-PNP der/DT/B-NP/I-PNP Matte/NN/I-NP/I-PNP ././O/O"
    )
    # 2) "große" and "sitzt" lemmata are "groß" and "sitzen".
    # Note how articles are problematic ("der" can be male subject but also plural possessive).
    v = de.parser.parse("Der große Hund sitzt auf der Matte.", lemmata=True)
    self.assertEqual(v,
        "Der/DT/B-NP/O/der große/JJ/I-NP/O/groß Hund/NN/I-NP/O/hund " + \
        "sitzt/VB/B-VP/O/sitzen " + \
        "auf/IN/B-PP/B-PNP/auf der/DT/B-NP/I-PNP/der Matte/NN/I-NP/I-PNP/matte ././O/O/."
    )
    # 3) Assert the accuracy of the German tagger.
    i, n = 0, 0
    for sentence in open(os.path.join(PATH, "corpora", "tagged-de-tiger.txt")).readlines():
        sentence = sentence.strip()
        s1 = [w.split("/") for w in sentence.split(" ")]
        s1 = [de.stts2penntreebank(w, pos) for w, pos in s1]
        s2 = [[w for w, pos in s1]]
        s2 = de.parse(s2, tokenize=False)
        s2 = [w.split("/") for w in s2.split(" ")]
        for j in range(len(s1)):
            if s1[j][1] == s2[j][1]:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.844)
    print("pattern.de.parse()")

</t>
<t tx="karstenw.20230303140202.18">def test_tag(self):
    # Assert [("der", "DT"), ("grosse", "JJ"), ("Hund", "NN")].
    v = de.tag("der grosse Hund")
    self.assertEqual(v, [("der", "DT"), ("grosse", "JJ"), ("Hund", "NN")])
    print("pattern.de.tag()")

</t>
<t tx="karstenw.20230303140202.19">def test_command_line(self):
    # Assert parsed output from the command-line (example from the documentation).
    p = ["python", "-m", "pattern.de", "-s", "Der grosse Hund.", "-OTCRL"]
    p = subprocess.Popen(p, stdout=subprocess.PIPE)
    p.wait()
    v = p.stdout.read().decode('utf-8')
    v = v.strip()
    self.assertEqual(v, "Der/DT/B-NP/O/O/der grosse/JJ/I-NP/O/O/gross Hund/NN/I-NP/O/O/hund ././O/O/O/.")
    print("python -m pattern.de")

</t>
<t tx="karstenw.20230303140202.2">class TestInflection(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140202.20">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestInflection))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestParser))
    return suite

</t>
<t tx="karstenw.20230303140202.3">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140202.4">def test_gender(self):
    # Assert der Hund =&gt; MASCULINE
    # Assert die Studentin =&gt; FEMININE
    # Assert das Auto =&gt; NEUTRAL
    self.assertEqual(de.gender("Hund"), de.MASCULINE)
    self.assertEqual(de.gender("Studentin"), de.FEMININE)
    self.assertEqual(de.gender("Auto"), de.NEUTRAL)

</t>
<t tx="karstenw.20230303140202.5">def test_pluralize(self):
    # Assert the accuracy of the pluralization algorithm.
    from pattern.db import Datasheet
    i, n = 0, 0
    for tag, sg, pl in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-de-celex.csv")):
        if tag == "n":
            if de.pluralize(sg) == pl:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.69)
    print("pattern.de.pluralize()")

</t>
<t tx="karstenw.20230303140202.6">def test_singularize(self):
    # Assert the accuracy of the singularization algorithm.
    from pattern.db import Datasheet
    i, n = 0, 0
    for tag, sg, pl in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-de-celex.csv")):
        if tag == "n":
            if de.singularize(pl) == sg:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.82)
    print("pattern.de.singularize()")

</t>
<t tx="karstenw.20230303140202.7">def test_attributive(self):
    # Assert "groß" =&gt; "großer" (masculine, nominative), and others.
    for lemma, inflected, gender, role, article in (
      ("groß", "großer", de.MALE,    de.SUBJECT,  None),
      ("groß", "großen", de.MALE,    de.OBJECT,   None),
      ("groß", "großem", de.MALE,    de.INDIRECT, None),
      ("groß", "großen", de.MALE,    de.PROPERTY, None),
      ("groß", "große",  de.FEMALE,  de.SUBJECT,  None),
      ("groß", "große",  de.FEMALE,  de.OBJECT,   None),
      ("groß", "großer", de.FEMALE,  de.INDIRECT, None),
      ("groß", "großes", de.NEUTRAL, de.SUBJECT,  None),
      ("groß", "großes", de.NEUTRAL, de.OBJECT,   None),
      ("groß", "großen", de.MALE,    de.PROPERTY, "mein"),
      ("groß", "großen", de.FEMALE,  de.PROPERTY, "jeder"),
      ("groß", "großen", de.FEMALE,  de.PROPERTY, "mein"),
      ("groß", "großen", de.PLURAL,  de.INDIRECT, "jede"),
      ("groß", "großen", de.PLURAL,  de.PROPERTY, "jeder")):
        v = de.attributive(lemma, gender, role, article)
        self.assertEqual(v, inflected)
    print("pattern.de.attributive()")

</t>
<t tx="karstenw.20230303140202.8">def test_predicative(self):
    # Assert the accuracy of the predicative algorithm ("großer" =&gt; "groß").
    from pattern.db import Datasheet
    i, n = 0, 0
    for tag, pred, attr in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-de-celex.csv")):
        if tag == "a":
            if de.predicative(attr) == pred:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.98)
    print("pattern.de.predicative()")

</t>
<t tx="karstenw.20230303140202.9">def test_find_lemma(self):
    # Assert the accuracy of the verb lemmatization algorithm.
    # Note: the accuracy is higher (88%) when measured on CELEX word forms
    # (presumably because de.inflect.verbs has high percentage irregular verbs).
    i, n = 0, 0
    for v1, v2 in de.inflect.verbs.inflections.items():
        if de.inflect.verbs.find_lemma(v1) == v2:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.86)
    print("pattern.de.inflect.verbs.find_lemma()")

</t>
<t tx="karstenw.20230303140204.1">from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
import random
import subprocess

from pattern import text
from pattern import en

from io import open

try:
    PATH = os.path.dirname(os.path.realpath(__file__))
except:
    PATH = ""

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140204.10">def test_lemma(self):
    # Assert the infinitive of "weren't".
    v = en.inflect.lemma("weren't")
    self.assertEqual(v, "be")
    print("pattern.en.inflect.lemma()")

</t>
<t tx="karstenw.20230303140204.11">def test_lexeme(self):
    # Assert all inflections of "be".
    v = en.inflect.lexeme("be")
    self.assertEqual(v, [
        "be", "am", "are", "is", "being",
        "was", "were", "been",
        "am not", "aren't", "isn't", "wasn't", "weren't"
    ])
    v = en.inflect.lexeme("imaginerify")
    self.assertEqual(v, [
        "imaginerify", "imaginerifies", "imaginerifying", "imaginerified"
    ])
    print("pattern.en.inflect.lexeme()")

</t>
<t tx="karstenw.20230303140204.12">def test_tenses(self):
    # Assert tense recognition.
    self.assertTrue((en.inflect.PRESENT, 1, en.inflect.SINGULAR) in en.inflect.tenses("am"))
    self.assertTrue("1sg" in en.inflect.tenses("am"))
    self.assertTrue("1sg" in en.inflect.tenses("will"))
    self.assertTrue("2sg-" in en.inflect.tenses("won't"))
    self.assertTrue("part" in en.inflect.tenses("imaginarifying"))
    print("pattern.en.inflect.tenses()")

</t>
<t tx="karstenw.20230303140204.13">def test_comparative(self):
    # Assert "nice" =&gt; "nicer".
    self.assertEqual(en.inflect.comparative("nice"), "nicer")
    print("pattern.en.inflect.comparative()")

</t>
<t tx="karstenw.20230303140204.14">def test_superlative(self):
    # Assert "nice" =&gt; "nicest"
    self.assertEqual(en.inflect.superlative("nice"), "nicest")
    # Assert "important" =&gt; "most important"
    self.assertEqual(en.inflect.superlative("important"), "most important")
    print("pattern.en.inflect.superlative()")

</t>
<t tx="karstenw.20230303140204.15">class TestQuantification(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140204.16">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140204.17">def test_extract_leading_zeros(self):
    # Assert "zero zero one" =&gt; ("one", 2).
    from pattern.text.en.inflect_quantify import zshift
    v = zshift("zero zero one")
    self.assertEqual(v, ("one", 2))
    v = zshift("0 0 one")
    self.assertEqual(v, ("one", 2))
    print("pattern.en.quantify._extract_leading_zeros()")

</t>
<t tx="karstenw.20230303140204.18">def test_numerals(self):
    # Assert number to numerals.
    for x, s in (
      (    1.5, "one point five"),
      (     15, "fifteen"),
      (    150, "one hundred and fifty"),
      (    151, "one hundred and fifty-one"),
      (   1510, "one thousand five hundred and ten"),
      (  15101, "fifteen thousand one hundred and one"),
      ( 150101, "one hundred and fifty thousand one hundred and one"),
      (1500101, "one million, five hundred thousand one hundred and one")):
        self.assertEqual(en.numerals(x), s)
    print("pattern.en.numerals()")

</t>
<t tx="karstenw.20230303140204.19">def test_number(self):
    # Assert numeric string = actual number (after rounding).
    for i in range(100):
        x = random.random()
        y = en.number(en.numerals(x, round=10))
        self.assertAlmostEqual(x, y, places=10)
    print("pattern.en.number()")

</t>
<t tx="karstenw.20230303140204.2">class TestInflection(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140204.20">def test_quantify(self):
    # Assert quantification algorithm.
    for a, s in (
      (   2 * ["carrot"], "a pair of carrots"),
      (   4 * ["carrot"], "several carrots"),
      (   9 * ["carrot"], "a number of carrots"),
      (  19 * ["carrot"], "a score of carrots"),
      (  23 * ["carrot"], "dozens of carrots"),
      ( 201 * ["carrot"], "hundreds of carrots"),
      (1001 * ["carrot"], "thousands of carrots"),
      ({"carrot": 4, "parrot": 2}, "several carrots and a pair of parrots")):
        self.assertEqual(en.quantify(a), s)
    print("pattern.en.quantify()")

</t>
<t tx="karstenw.20230303140204.21">def test_reflect(self):
    self.assertEqual(en.reflect(""), "a string")
    self.assertEqual(en.reflect(["", "", ""]), "several strings")
    self.assertEqual(en.reflect(en.reflect), "a function")
    print("pattern.en.reflect()")

</t>
<t tx="karstenw.20230303140204.22">class TestSpelling(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140204.23">def test_spelling(self):
    # Assert case-sensitivity + numbers.
    for a, b in (
      (   ".", "."   ),
      (   "?", "?"   ),
      (   "!", "!"   ),
      (   "I", "I"   ),
      (   "a", "a"   ),
      (  "42", "42"  ),
      ("3.14", "3.14"),
      ( "The", "The" ),
      ( "the", "the" )):
        self.assertEqual(en.suggest(a)[0][0], b)
    # Assert spelling suggestion accuracy.
    # Note: simply training on more text will not improve accuracy.
    i = j = 0.0
    from pattern.db import Datasheet
    for correct, wrong in Datasheet.load(os.path.join(PATH, "corpora", "spelling-birkbeck.csv")):
        for w in wrong.split(" "):
            if en.suggest(w)[0][0] == correct:
                i += 1
            else:
                j += 1
    self.assertTrue(i / (i + j) &gt; 0.70)
    print("pattern.en.suggest()")

</t>
<t tx="karstenw.20230303140204.24">class TestParser(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140204.25">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140204.26">def test_tokenize(self):
    # Assert list with two sentences.
    # The tokenizer should at least handle common abbreviations and punctuation.
    v = en.tokenize("The cat is eating (e.g., a fish). Yum!")
    self.assertEqual(v, ["The cat is eating ( e.g. , a fish ) .", "Yum !"])
    print("pattern.en.tokenize()")

</t>
<t tx="karstenw.20230303140204.27">def _test_morphological_rules(self, function=en.parser.morphology.apply):
    """ For each word in WordNet that is not in Brill's lexicon,
        test if the given tagger((word, "NN")) yields an improved (word, tag).
        Returns the relative scores for nouns, verbs, adjectives and adverbs.
    """
    scores = []
    for tag, lexicon in (
      ("NN", en.wordnet.NOUNS),
      ("VB", en.wordnet.VERBS),
      ("JJ", en.wordnet.ADJECTIVES),
      ("RB", en.wordnet.ADVERBS)):
        i, n = 0, 0
        for word in lexicon():
            word = word.replace("_", " ")
            if word not in en.lexicon:
                if function([word, "NN"])[1].startswith(tag):
                    i += 1
                n += 1
        scores.append(float(i) / n)
    return scores

</t>
<t tx="karstenw.20230303140204.28">def test_default_suffix_rules(self):
    # Assert part-of-speech tag for unknown tokens.
    for a, b in (
      (["eating",  "NN"], ["eating",  "VBG"]),
      (["tigers",  "NN"], ["tigers",  "NNS"]),
      (["really",  "NN"], ["really",  "RB"]),
      (["foolish", "NN"], ["foolish", "JJ"])):
        self.assertEqual(text._suffix_rules(a), b)
    # Test with words in WordNet that are not in Brill's lexicon.
    # Given are the scores for detection of nouns, verbs, adjectives and adverbs.
    # The baseline should increase (not decrease) when the algorithm is modified.
    v = self._test_morphological_rules(function=text._suffix_rules)
    self.assertTrue(v[0] &gt; 0.91) # NN
    self.assertTrue(v[1] &gt; 0.23) # VB
    self.assertTrue(v[2] &gt; 0.38) # JJ
    self.assertTrue(v[3] &gt; 0.60) # RB
    print("pattern.text._suffix_rules()")

</t>
<t tx="karstenw.20230303140204.29">def test_apply_morphological_rules(self):
    # Assert part-of-speech tag for unknown tokens (Brill's lexical rules).
    v = self._test_morphological_rules(function=en.parser.morphology.apply)
    self.assertTrue(v[0] &gt; 0.85) # NN
    self.assertTrue(v[1] &gt; 0.19) # VB
    self.assertTrue(v[2] &gt; 0.65) # JJ
    self.assertTrue(v[3] &gt; 0.59) # RB
    print("pattern.en.parser.morphology.apply()")

</t>
<t tx="karstenw.20230303140204.3">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140204.30">def test_apply_context_rules(self):
    # Assert part-of-speech tags based on word context.
    for a, b in (                                                                 # Rule:
      ([["", "JJ"], ["", "JJ"], ["", ","]], [["", "JJ"], ["", "NN"], ["", ","]]), # SURROUNDTAG
      ([["", "NNP"], ["", "RB"]],           [["", "NNP"], ["", "NNP"]]),          # PREVTAG
      ([["", "NN"], ["", "PRP$"]],          [["", "VB"], ["", "PRP$"]]),          # NEXTTAG
      ([["phone", ""], ["", "VBZ"]],        [["phone", ""], ["", "NNS"]]),        # PREVWD
      ([["", "VB"], ["countries", ""]],     [["", "JJ"], ["countries", ""]]),     # NEXTWD
      ([["close", "VB"], ["to", ""]],       [["close", "RB"], ["to", ""]]),       # RBIGRAM
      ([["very", ""], ["much", "JJ"]],      [["very", ""], ["much", "RB"]]),      # LBIGRAM
      ([["such", "JJ"], ["as", "DT"]],      [["such", "JJ"], ["as", "IN"]]),      # WDNEXTWD
      ([["be", "VB"]],                      [["be", "VB"]])):                     # CURWD
        self.assertEqual(en.parser.context.apply(a), b)
    print("pattern.en.parser.context.apply()")

</t>
<t tx="karstenw.20230303140204.31">def test_find_tags(self):
    # Assert part-of-speech-tag annotation.
    v = en.parser.find_tags(["black", "cat"])
    self.assertEqual(v, [["black", "JJ"], ["cat", "NN"]])
    self.assertEqual(en.parser.find_tags(["felix"])[0][1], "NN")
    self.assertEqual(en.parser.find_tags(["Felix"])[0][1], "NNP")
    print("pattern.en.parser.find_tags()")

</t>
<t tx="karstenw.20230303140204.32">def test_find_chunks(self):
    # Assert chunk tag annotation.
    v = en.parser.find_chunks([["black", "JJ"], ["cat", "NN"]])
    self.assertEqual(v, [["black", "JJ", "B-NP", "O"], ["cat", "NN", "I-NP", "O"]])
    # Assert the accuracy of the chunker.
    # For example, in "The very black cat must be really meowing really loud in the yard.":
    # - "The very black" (NP)
    # - "must be really meowing" (VP)
    # - "really loud" (ADJP)
    # - "in" (PP)
    # - "the yard" (NP)
    v = en.parser.find_chunks([
        ["", "DT"], ["", "RB"], ["", "JJ"], ["", "NN"],
        ["", "MD"], ["", "RB"], ["", "VBZ"], ["", "VBG"],
        ["", "RB"], ["", "JJ"],
        ["", "IN"],
        ["", "CD"], ["", "NNS"]
    ])
    self.assertEqual(v, [
        ["", "DT", "B-NP", "O"], ["", "RB", "I-NP", "O"], ["", "JJ", "I-NP", "O"], ["", "NN", "I-NP", "O"],
        ["", "MD", "B-VP", "O"], ["", "RB", "I-VP", "O"], ["", "VBZ", "I-VP", "O"], ["", "VBG", "I-VP", "O"],
        ["", "RB", "B-ADJP", "O"], ["", "JJ", "I-ADJP", "O"],
        ["", "IN", "B-PP", "B-PNP"],
        ["", "CD", "B-NP", "I-PNP"], ["", "NNS", "I-NP", "I-PNP"]])
    # Assert commas inside chunks.
    # - "the big, black cat"
    v = en.parser.find_chunks([
        ["", "DT"], ["", "JJ"], ["", ","], ["", "JJ"], ["", "NN"]
    ])
    self.assertEqual(v, [
        ["", "DT", "B-NP", "O"],
        ["", "JJ", "I-NP", "O"],
        ["", ",", "I-NP", "O"],
        ["", "JJ", "I-NP", "O"],
        ["", "NN", "I-NP", "O"]
    ])
    # - "big, black and furry"
    v = en.parser.find_chunks([
        ["", "JJ"], ["", ","], ["", "JJ"], ["", "CC"], ["", "JJ"]
    ])
    self.assertEqual(v, [
        ["", "JJ", "B-ADJP", "O"],
        ["", ",", "I-ADJP", "O"],
        ["", "JJ", "I-ADJP", "O"],
        ["", "CC", "I-ADJP", "O"],
        ["", "JJ", "I-ADJP", "O"]
    ])
    # - big, and very black (= two chunks "big" and "very black")
    v = en.parser.find_chunks([
        ["", "JJ"], ["", ","], ["", "CC"], ["", "RB"], ["", "JJ"]
    ])
    self.assertEqual(v, [
        ["", "JJ", "B-ADJP", "O"],
        ["", ",", "O", "O"],
        ["", "CC", "O", "O"],
        ["", "RB", "B-ADJP", "O"],
        ["", "JJ", "I-ADJP", "O"]
    ])
    # Assert cases for which we have written special rules.
    # - "perhaps you" (ADVP + NP)
    v = en.parser.find_chunks([["", "RB"], ["", "PRP"]])
    self.assertEqual(v, [["", "RB", "B-ADVP", "O"], ["", "PRP", "B-NP", "O"]])
    # - "very nice cats" (NP)
    v = en.parser.find_chunks([["", "RB"], ["", "JJ"], ["", "PRP"]])
    self.assertEqual(v, [["", "RB", "B-NP", "O"], ["", "JJ", "I-NP", "O"], ["", "PRP", "I-NP", "O"]])
    print("pattern.en.parser.find_chunks()")

</t>
<t tx="karstenw.20230303140204.33">def test_find_labels(self):
    # Assert relation tag annotation (SBJ/OBJ).
    v = en.parser.find_labels([
        ["", "", "NP"], ["", "", "NP"],
        ["", "", "VP"], ["", "", "VP"],
        ["", "", "NP"]])
    self.assertEqual(v, [
        ["", "", "NP", "NP-SBJ-1"], ["", "", "NP", "NP-SBJ-1"],
        ["", "", "VP", "VP-1"], ["", "", "VP", "VP-1"],
        ["", "", "NP", "NP-OBJ-1"]])
    print("pattern.en.parser.find_labels()")

</t>
<t tx="karstenw.20230303140204.34">def test_find_prepositions(self):
    # Assert preposition tag annotation (PP + NP).
    v = en.parser.find_prepositions([
        ["", "", "NP"],
        ["", "", "VP"],
        ["", "", "PP"],
        ["", "", "NP"],
        ["", "", "NP"], ])
    self.assertEqual(v, [
        ["", "", "NP", "O"],
        ["", "", "VP", "O"],
        ["", "", "PP", "B-PNP"],
        ["", "", "NP", "I-PNP"],
        ["", "", "NP", "I-PNP"]])
    # Assert PNP's with consecutive PP's.
    v = en.parse("The cat was looking at me from up on the roof with interest.", prepositions=True)
    self.assertEqual(v,
        "The/DT/B-NP/O cat/NN/I-NP/O " \
        "was/VBD/B-VP/O looking/VBG/I-VP/O " \
        "at/IN/B-PP/B-PNP me/PRP/B-NP/I-PNP " \
        "from/IN/B-PP/B-PNP up/IN/I-PP/I-PNP on/IN/I-PP/I-PNP the/DT/B-NP/I-PNP roof/NN/I-NP/I-PNP " \
        "with/IN/B-PP/B-PNP interest/NN/B-NP/I-PNP " \
        "././O/O"
    )
    print("pattern.en.parser.find_prepositions()")

</t>
<t tx="karstenw.20230303140204.35">def test_find_lemmata(self):
    # Assert lemmata for nouns and verbs.
    v = en.parser.find_lemmata([["cats", "NNS"], ["wearing", "VBG"], ["hats", "NNS"]])
    self.assertEqual(v, [
        ["cats", "NNS", "cat"],
        ["wearing", "VBG", "wear"],
        ["hats", "NNS", "hat"]])
    print("pattern.en.parser.find_lemmata()")

</t>
<t tx="karstenw.20230303140204.36">def test_named_entity_recognition(self):
    # Assert named entities.
    v = en.parser.parse("Arnold Schwarzenegger is cool.", chunks=False)
    self.assertEqual(v,
        "Arnold/NNP-PERS Schwarzenegger/NNP-PERS is/VBZ cool/JJ ./."
    )
    print("pattern.en.parser.entities.apply()")

</t>
<t tx="karstenw.20230303140204.37">def test_parse(self):
    # Assert parsed output with Penn Treebank II tags (slash-formatted).
    # 1) "the black cat" is a noun phrase, "on the mat" is a prepositional noun phrase.
    v = en.parser.parse("The black cat sat on the mat.")
    self.assertEqual(v,
        "The/DT/B-NP/O black/JJ/I-NP/O cat/NN/I-NP/O " + \
        "sat/VBD/B-VP/O " + \
        "on/IN/B-PP/B-PNP the/DT/B-NP/I-PNP mat/NN/I-NP/I-PNP ././O/O"
    )
    # 2) "the black cat" is the subject, "a fish" is the object.
    v = en.parser.parse("The black cat is eating a fish.", relations=True)
    self.assertEqual(v,
        "The/DT/B-NP/O/NP-SBJ-1 black/JJ/I-NP/O/NP-SBJ-1 cat/NN/I-NP/O/NP-SBJ-1 " + \
        "is/VBZ/B-VP/O/VP-1 eating/VBG/I-VP/O/VP-1 " + \
        "a/DT/B-NP/O/NP-OBJ-1 fish/NN/I-NP/O/NP-OBJ-1 ././O/O/O"
    )
    # 3) "chasing" and "mice" lemmata are "chase" and "mouse".
    v = en.parser.parse("The black cat is chasing mice.", lemmata=True)
    self.assertEqual(v,
        "The/DT/B-NP/O/the black/JJ/I-NP/O/black cat/NN/I-NP/O/cat " + \
        "is/VBZ/B-VP/O/be chasing/VBG/I-VP/O/chase " + \
        "mice/NNS/B-NP/O/mouse ././O/O/."
    )
    # 4) Assert str.
    self.assertTrue(isinstance(v, str))
    # 5) Assert str for faulty input (bytestring with unicode characters).
    self.assertTrue(isinstance(en.parse("ø ü"), str))
    self.assertTrue(isinstance(en.parse("ø ü", tokenize=True, tags=False, chunks=False), str))
    self.assertTrue(isinstance(en.parse("ø ü", tokenize=False, tags=False, chunks=False), str))
    self.assertTrue(isinstance(en.parse("o u", encoding="ascii"), str))
    # 6) Assert optional parameters (i.e., setting all to False).
    self.assertEqual(en.parse("ø ü.", tokenize=True, tags=False, chunks=False), "ø ü .")
    self.assertEqual(en.parse("ø ü.", tokenize=False, tags=False, chunks=False), "ø ü.")
    # 7) Assert the accuracy of the English tagger.
    i, n = 0, 0
    for corpus, a in (("tagged-en-wsj.txt", (0.968, 0.945)), ("tagged-en-oanc.txt", (0.929, 0.932))):
        for sentence in open(os.path.join(PATH, "corpora", corpus)).readlines():
            sentence = sentence.strip()
            s1 = [w.split("/") for w in sentence.split(" ")]
            s2 = [[w for w, pos in s1]]
            s2 = en.parse(s2, tokenize=False)
            s2 = [w.split("/") for w in s2.split(" ")]
            for j in range(len(s1)):
                if s1[j][1] == s2[j][1].split("-")[0]:
                    i += 1
                n += 1
        #print(corpus, float(i) / n)
        self.assertTrue(float(i) / n &gt; (en.parser.model and a[0] or a[1]))
    print("pattern.en.parse()")

</t>
<t tx="karstenw.20230303140204.38">def test_tagged_string(self):
    # Assert splitable TaggedString with language and tags properties.
    v = en.parser.parse("The black cat sat on the mat.", relations=True, lemmata=True)
    self.assertEqual(v.language, "en")
    self.assertEqual(v.tags,
        ["word", "part-of-speech", "chunk", "preposition", "relation", "lemma"])
    self.assertEqual(v.split(text.TOKENS)[0][0],
        ["The", "DT", "B-NP", "O", "NP-SBJ-1", "the"])
    print("pattern.en.parse().split()")

</t>
<t tx="karstenw.20230303140204.39">def test_parsetree(self):
    # Assert parsetree(s) == Text.
    v = en.parsetree("The cat purs.")
    self.assertTrue(isinstance(v, en.Text))
    print("pattern.en.parsetree()")

</t>
<t tx="karstenw.20230303140204.4">def test_indefinite_article(self):
    # Assert "a" or "an".
    for article, word in (
     ("an", "hour"),
     ("an", "FBI"),
      ("a", "bear"),
      ("a", "one-liner"),
      ("a", "European"),
      ("a", "university"),
      ("a", "uterus"),
     ("an", "owl"),
     ("an", "yclept"),
      ("a", "year")):
        self.assertEqual(en.article(word, function=en.INDEFINITE), article)
    self.assertEqual(en.inflect.article("heir", function=en.DEFINITE), "the")
    self.assertEqual(en.inflect.referenced("ewe"), "a ewe")
    print("pattern.en.inflect.article()")

</t>
<t tx="karstenw.20230303140204.40">def test_split(self):
    # Assert split(parse(s)) == Text.
    v = en.split(en.parse("The cat purs."))
    self.assertTrue(isinstance(v, en.Text))
    print("pattern.en.split()")

</t>
<t tx="karstenw.20230303140204.41">def test_tag(self):
    # Assert [("black", "JJ"), ("cats", "NNS")].
    v = en.tag("black cats")
    self.assertEqual(v, [("black", "JJ"), ("cats", "NNS")])
    v = en.tag("")
    self.assertEqual(v, [])
    print("pattern.en.tag()")

</t>
<t tx="karstenw.20230303140204.42">def test_ngrams(self):
    # Assert n-grams with and without punctuation marks / sentence marks.
    s = "The cat is napping."
    v1 = en.ngrams(s, n=2)
    v2 = en.ngrams(s, n=3, punctuation=en.PUNCTUATION.strip("."))
    self.assertEqual(v1, [("The", "cat"), ("cat", "is"), ("is", "napping")])
    self.assertEqual(v2, [("The", "cat", "is"), ("cat", "is", "napping"), ("is", "napping", ".")])
    s = "The cat purrs. The dog barks."
    v1 = en.ngrams(s, n=2)
    v2 = en.ngrams(s, n=2, continuous=True)
    self.assertEqual(v1, [("The", "cat"), ("cat", "purrs"), ("The", "dog"), ("dog", "barks")])
    self.assertEqual(v2, [("The", "cat"), ("cat", "purrs"), ("purrs", "The"), ("The", "dog"), ("dog", "barks")])
    print("pattern.en.ngrams()")

</t>
<t tx="karstenw.20230303140204.43">def test_command_line(self):
    # Assert parsed output from the command-line (example from the documentation).
    p = ["python", "-m", "pattern.en", "-s", "Nice cat.", "-OTCRL"]
    p = subprocess.Popen(p, stdout=subprocess.PIPE)
    p.wait()
    v = p.stdout.read().decode('utf-8')
    v = v.strip()
    self.assertEqual(v, "Nice/JJ/B-NP/O/O/nice cat/NN/I-NP/O/O/cat ././O/O/O/.")
    print("python -m pattern.en")

</t>
<t tx="karstenw.20230303140204.44">class TestParseTree(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140204.45">def setUp(self):
    # Parse sentences to test on.
    # Creating a Text creates Sentence, Chunk, PNP and Word.
    # Creating a Sentence tests Sentence.append() and Sentence.parse_token().
    self.text = "I'm eating pizza with a fork. What a tasty pizza!"
    self.text = en.Text(en.parse(self.text, relations=True, lemmata=True))

</t>
<t tx="karstenw.20230303140204.46">def test_copy(self):
    # Assert deepcopy of Text, Sentence, Chunk, PNP and Word.
    self.text = self.text.copy()
    print("pattern.en.Text.copy()")

</t>
<t tx="karstenw.20230303140204.47">def test_xml(self):
    # Assert XML export and import.
    self.text = en.Text.from_xml(self.text.xml)
    print("pattern.en.Text.xml")
    print("pattern.en.Text.from_xml()")

</t>
<t tx="karstenw.20230303140204.48">def test_text(self):
    # Assert Text.
    self.assertEqual(self.text.sentences[0].string, "I 'm eating pizza with a fork .")
    self.assertEqual(self.text.sentences[1].string, "What a tasty pizza !")
    print("pattern.en.Text")

</t>
<t tx="karstenw.20230303140204.49">def test_sentence(self):
    # Assert Sentence.
    v = self.text[0]
    self.assertTrue(v.start == 0)
    self.assertTrue(v.stop == 8)
    self.assertTrue(v.string == "I 'm eating pizza with a fork .")
    self.assertTrue(v.subjects == [self.text[0].chunks[0]])
    self.assertTrue(v.verbs == [self.text[0].chunks[1]])
    self.assertTrue(v.objects == [self.text[0].chunks[2]])
    self.assertTrue(v.nouns == [self.text[0].words[3], self.text[0].words[6]])
    # Sentence.string must be unicode.
    self.assertTrue(isinstance(v.string, str))
    self.assertTrue(isinstance(str(v), str))
    print("pattern.en.Sentence")

</t>
<t tx="karstenw.20230303140204.5">def test_pluralize(self):
    # Assert "octopodes" for classical plural of "octopus".
    # Assert "octopuses" for modern plural.
    self.assertEqual("octopodes", en.inflect.pluralize("octopus", classical=True))
    self.assertEqual("octopuses", en.inflect.pluralize("octopus", classical=False))
    # Assert the accuracy of the pluralization algorithm.
    from pattern.db import Datasheet
    i, n = 0, 0
    for sg, pl in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-en-celex.csv")):
        if en.inflect.pluralize(sg) == pl:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.95)
    print("pattern.en.inflect.pluralize()")

</t>
<t tx="karstenw.20230303140204.50">def test_sentence_constituents(self):
    # Assert in-order list of Chunk, PNP and Word.
    v = self.text[0].constituents(pnp=True)
    self.assertEqual(v, [
        self.text[0].chunks[0],
        self.text[0].chunks[1],
        self.text[0].chunks[2],
        self.text[0].pnp[0],
        self.text[0].words[7],
    ])
    print("pattern.en.Sentence.constituents()")

</t>
<t tx="karstenw.20230303140204.51">def test_slice(self):
    # Assert sentence slice.
    v = self.text[0].slice(start=4, stop=6)
    self.assertTrue(v.parent == self.text[0])
    self.assertTrue(v.string == "with a")
    # Assert sentence slice tag integrity.
    self.assertTrue(v.words[0].type == "IN")
    self.assertTrue(v.words[1].chunk is None)
    print("pattern.en.Slice")

</t>
<t tx="karstenw.20230303140204.52">def test_chunk(self):
    # Assert chunk with multiple words ("a fork").
    v = self.text[0].chunks[4]
    self.assertTrue(v.start == 5)
    self.assertTrue(v.stop == 7)
    self.assertTrue(v.string == "a fork")
    self.assertTrue(v.lemmata == ["a", "fork"])
    self.assertTrue(v.words == [self.text[0].words[5], self.text[0].words[6]])
    self.assertTrue(v.head == self.text[0].words[6])
    self.assertTrue(v.type == "NP")
    self.assertTrue(v.role is None)
    self.assertTrue(v.pnp is not None)
    # Assert chunk that is subject/object of the sentence ("pizza").
    v = self.text[0].chunks[2]
    self.assertTrue(v.role == "OBJ")
    self.assertTrue(v.relation == 1)
    self.assertTrue(v.related == [self.text[0].chunks[0], self.text[0].chunks[1]])
    self.assertTrue(v.subject == self.text[0].chunks[0])
    self.assertTrue(v.verb == self.text[0].chunks[1])
    self.assertTrue(v.object is None)
    # Assert chunk traversal.
    self.assertEqual(v.nearest("VP"), self.text[0].chunks[1])
    self.assertEqual(v.previous(), self.text[0].chunks[1])
    self.assertEqual(v.next(), self.text[0].chunks[3])
    print("pattern.en.Chunk")

</t>
<t tx="karstenw.20230303140204.53">def test_chunk_conjunctions(self):
    # Assert list of conjunct/disjunct chunks ("black cat" AND "white cat").
    v = en.Sentence(en.parse("black cat and white cat"))
    self.assertEqual(v.chunk[0].conjunctions, [(v.chunk[1], en.AND)])
    print("pattern.en.Chunk.conjunctions()")

</t>
<t tx="karstenw.20230303140204.54">def test_chunk_modifiers(self):
    # Assert list of nearby adjectives and adverbs with no role, for VP.
    v = en.Sentence(en.parse("Perhaps you should go."))
    self.assertEqual(v.chunk[2].modifiers, [v.chunk[0]]) # should &lt;=&gt; perhaps
    print("pattern.en.Chunk.modifiers")

</t>
<t tx="karstenw.20230303140204.55">def test_pnp(self):
    # Assert PNP chunk ("with a fork").
    v = self.text[0].pnp[0]
    self.assertTrue(v.string == "with a fork")
    self.assertTrue(v.chunks == [self.text[0].chunks[3], self.text[0].chunks[4]])
    self.assertTrue(v.pp == self.text[0].chunks[3])
    print("pattern.en.PNP")

</t>
<t tx="karstenw.20230303140204.56">def test_word(self):
    # Assert word tags ("fork" =&gt; NN).
    v = self.text[0].words[6]
    self.assertTrue(v.index == 6)
    self.assertTrue(v.string == "fork")
    self.assertTrue(v.lemma == "fork")
    self.assertTrue(v.type == "NN")
    self.assertTrue(v.chunk == self.text[0].chunks[4])
    self.assertTrue(v.pnp is not None)
    for i, tags in enumerate([
      ["I", "PRP", "B-NP", "O", "NP-SBJ-1", "i"],
      ["'m", "VBP", "B-VP", "O", "VP-1", "be"],
      ["eating", "VBG", "I-VP", "O", "VP-1", "eat"],
      ["pizza", "NN", "B-NP", "O", "NP-OBJ-1", "pizza"],
      ["with", "IN", "B-PP", "B-PNP", "O", "with"],
      ["a", "DT", "B-NP", "I-PNP", "O", "a"],
      ["fork", "NN", "I-NP", "I-PNP", "O", "fork"],
      [".", ".", "O", "O", "O", "."]]):
        self.assertEqual(self.text[0].words[i].tags, tags)
    print("pattern.en.Word")

</t>
<t tx="karstenw.20230303140204.57">def test_word_custom_tags(self):
    # Assert word custom tags ("word/part-of-speech/.../some-custom-tag").
    s = en.Sentence("onion/NN/FOOD", token=[en.WORD, en.POS, "semantic_type"])
    v = s.words[0]
    self.assertEqual(v.semantic_type, "FOOD")
    self.assertEqual(v.custom_tags["semantic_type"], "FOOD")
    self.assertEqual(v.copy().custom_tags["semantic_type"], "FOOD")
    # Assert addition of new custom tags.
    v.custom_tags["taste"] = "pungent"
    self.assertEqual(s.token, [en.WORD, en.POS, "semantic_type", "taste"])
    print("pattern.en.Word.custom_tags")

</t>
<t tx="karstenw.20230303140204.58">def test_find(self):
    # Assert first item for which given function is True.
    v = text.tree.find(lambda x: x &gt; 10, [1, 2, 3, 11, 12])
    self.assertEqual(v, 11)
    print("pattern.text.tree.find()")

</t>
<t tx="karstenw.20230303140204.59">def test_zip(self):
    # Assert list of zipped tuples, using default to balance uneven lists.
    v = text.tree.zip([1, 2, 3], [4, 5, 6, 7], default=0)
    self.assertEqual(v, [(1, 4), (2, 5), (3, 6), (0, 7)])
    print("pattern.text.tree.zip()")

</t>
<t tx="karstenw.20230303140204.6">def test_singularize(self):
    # Assert the accuracy of the singularization algorithm.
    from pattern.db import Datasheet
    i, n = 0, 0
    for sg, pl in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-en-celex.csv")):
        if en.inflect.singularize(pl) == sg:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.95)
    print("pattern.en.inflect.singularize()")

</t>
<t tx="karstenw.20230303140204.60">def test_unzip(self):
    v = text.tree.unzip(1, [(1, 4), (2, 5), (3, 6)])
    self.assertEqual(v, [4, 5, 6])
    print("pattern.text.tree.unzip()")

</t>
<t tx="karstenw.20230303140204.61">def test_unique(self):
    # Assert list copy with unique items.
    v = text.tree.unique([1, 1, 1])
    self.assertEqual(len(v), 1)
    self.assertEqual(v[0], 1)
    print("pattern.text.tree.unique()")

</t>
<t tx="karstenw.20230303140204.62">def test_map(self):
    # Assert dynamic Map().
    v = text.tree.Map(lambda x: x + 1, [1, 2, 3])
    self.assertEqual(list(v), [2, 3, 4])
    self.assertEqual(v.items[0], 1)
    print("pattern.text.tree.Map()")

</t>
<t tx="karstenw.20230303140204.63">class TestModality(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140204.64">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140204.65">def test_imperative(self):
    # Assert True for sentences that are orders, commands, warnings.
    from pattern.text.en.modality import imperative
    for b, s in (
      (True, "Do your homework!"),
      (True, "Do not listen to me."),
      (True, "Turn that off, will you."),
      (True, "Let's help him."),
      (True, "Help me!"),
      (True, "You will help me."),
      (False, "Do it if you think it is necessary."),
      (False, "I hope you will help me."),
      (False, "I can help you."),
      (False, "I can help you if you let me.")):
        self.assertEqual(imperative(en.Sentence(en.parse(s))), b)
    print("pattern.en.modality.imperative()")

</t>
<t tx="karstenw.20230303140204.66">def test_conditional(self):
    # Assert True for sentences that contain possible or imaginary situations.
    from pattern.text.en.modality import conditional
    for b, s in (
      (True, "We ought to help him."),
      (True, "We could help him."),
      (True, "I will help you."),
      (True, "I hope you will help me."),
      (True, "I can help you if you let me."),
      (False, "You will help me."),
      (False, "I can help you.")):
        self.assertEqual(conditional(en.Sentence(en.parse(s))), b)
    # Assert predictive mood.
    s = "I will help you."
    v = conditional(en.Sentence(en.parse(s)), predictive=False)
    self.assertEqual(v, False)
    # Assert speculative mood.
    s = "I will help you if you pay me."
    v = conditional(en.Sentence(en.parse(s)), predictive=False)
    self.assertEqual(v, True)
    print("pattern.en.modality.conditional()")

</t>
<t tx="karstenw.20230303140204.67">def test_subjunctive(self):
    # Assert True for sentences that contain wishes, judgments or opinions.
    from pattern.text.en.modality import subjunctive
    for b, s in (
      (True, "I wouldn't do that if I were you."),
      (True, "I wish I knew."),
      (True, "I propose that you be on time."),
      (True, "It is a bad idea to be late."),
      (False, "I will be late.")):
        self.assertEqual(subjunctive(en.Sentence(en.parse(s))), b)
    print("pattern.en.modality.subjunctive()")

</t>
<t tx="karstenw.20230303140204.68">def test_negated(self):
    # Assert True for sentences that contain "not", "n't" or "never".
    for b, s in (
      (True, "Not true?"),
      (True, "Never true."),
      (True, "Isn't true."),):
        self.assertEqual(en.negated(en.Sentence(en.parse(s))), b)
    print("pattern.en.negated()")

</t>
<t tx="karstenw.20230303140204.69">def test_mood(self):
    # Assert imperative mood.
    v = en.mood(en.Sentence(en.parse("Do your homework!")))
    self.assertEqual(v, en.IMPERATIVE)
    # Assert conditional mood.
    v = en.mood(en.Sentence(en.parse("We ought to help him.")))
    self.assertEqual(v, en.CONDITIONAL)
    # Assert subjunctive mood.
    v = en.mood(en.Sentence(en.parse("I wouldn't do that if I were you.")))
    self.assertEqual(v, en.SUBJUNCTIVE)
    # Assert indicative mood.
    v = en.mood(en.Sentence(en.parse("The weather is nice today.")))
    self.assertEqual(v, en.INDICATIVE)
    print("pattern.en.mood()")

</t>
<t tx="karstenw.20230303140204.7">def test_find_lemma(self):
    # Assert the accuracy of the verb lemmatization algorithm.
    # Note: the accuracy is higher (95%) when measured on CELEX word forms
    # (probably because en.verbs has high percentage irregular verbs).
    i, n = 0, 0
    for v1, v2 in en.inflect.verbs.inflections.items():
        if en.inflect.verbs.find_lemma(v1) == v2:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.90)
    print("pattern.en.inflect.verbs.find_lemma()")

</t>
<t tx="karstenw.20230303140204.70">def test_modality(self):
    # Assert -1.0 =&gt; +1.0 representing the degree of certainty.
    v = en.modality(en.Sentence(en.parse("I wish it would stop raining.")))
    self.assertTrue(v &lt; 0)
    v = en.modality(en.Sentence(en.parse("It will surely stop raining soon.")))
    self.assertTrue(v &gt; 0)
    # Assert the accuracy of the modality algorithm.
    # Given are the scores for the CoNLL-2010 Shared Task 1 Wikipedia uncertainty data:
    # http://www.inf.u-szeged.hu/rgai/conll2010st/tasks.html#task1
    # The baseline should increase (not decrease) when the algorithm is modified.
    from pattern.db import Datasheet
    from pattern.metrics import test
    sentences = []
    for certain, sentence in Datasheet.load(os.path.join(PATH, "corpora", "uncertainty-conll2010.csv")):
        sentence = en.parse(sentence, chunks=False, light=True)
        sentence = en.Sentence(sentence)
        sentences.append((sentence, int(certain) &gt; 0))
    A, P, R, F = test(lambda sentence: en.modality(sentence) &gt; 0.5, sentences)
    #print(A, P, R, F)
    self.assertTrue(A &gt; 0.69)
    self.assertTrue(P &gt; 0.72)
    self.assertTrue(R &gt; 0.63)
    self.assertTrue(F &gt; 0.68)
    print("pattern.en.modality()")

</t>
<t tx="karstenw.20230303140204.71">class TestSentiment(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140204.72">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140204.73">def test_sentiment_avg(self):
    # Assert 2.5.
    from pattern.text import avg
    v = avg([1, 2, 3, 4])
    self.assertEqual(v, 2.5)
    print("pattern.text.avg")

</t>
<t tx="karstenw.20230303140204.74">def test_sentiment(self):
    # Assert &lt; 0 for negative adjectives and &gt; 0 for positive adjectives.
    self.assertTrue(en.sentiment("wonderful")[0] &gt; 0)
    self.assertTrue(en.sentiment("horrible")[0] &lt; 0)
    self.assertTrue(en.sentiment(en.wordnet.synsets("horrible", pos="JJ")[0])[0] &lt; 0)
    self.assertTrue(en.sentiment(en.Text(en.parse("A bad book. Really horrible.")))[0] &lt; 0)
    # Assert that :) and :( are recognized.
    self.assertTrue(en.sentiment(":)")[0] &gt; 0)
    self.assertTrue(en.sentiment(":(")[0] &lt; 0)
    # Assert the accuracy of the sentiment analysis (for the positive class).
    # Given are the scores for Pang &amp; Lee's polarity dataset v2.0:
    # http://www.cs.cornell.edu/people/pabo/movie-review-data/
    # The baseline should increase (not decrease) when the algorithm is modified.
    from pattern.db import Datasheet
    from pattern.metrics import test
    reviews = []
    for score, review in Datasheet.load(os.path.join(PATH, "corpora", "polarity-en-pang&amp;lee1.csv")):
        reviews.append((review, int(score) &gt; 0))
    from time import time
    t = time()
    A, P, R, F = test(lambda review: en.positive(review), reviews)
    #print(A, P, R, F)
    self.assertTrue(A &gt; 0.752)
    self.assertTrue(P &gt; 0.772)
    self.assertTrue(R &gt; 0.715)
    self.assertTrue(F &gt; 0.743)
    # Assert the accuracy of the sentiment analysis on short text (for the positive class).
    # Given are the scores for Pang &amp; Lee's sentence polarity dataset v1.0:
    # http://www.cs.cornell.edu/people/pabo/movie-review-data/
    reviews = []
    for score, review in Datasheet.load(os.path.join(PATH, "corpora", "polarity-en-pang&amp;lee2.csv")):
        reviews.append((review, int(score) &gt; 0))
    A, P, R, F = test(lambda review: en.positive(review), reviews)
    #print(A, P, R, F)
    self.assertTrue(A &gt; 0.654)
    self.assertTrue(P &gt; 0.660)
    self.assertTrue(R &gt; 0.636)
    self.assertTrue(F &gt; 0.648)
    print("pattern.en.sentiment()")

</t>
<t tx="karstenw.20230303140204.75">def test_sentiment_twitter(self):
    sanders = os.path.join(PATH, "corpora", "polarity-en-sanders.csv")
    if os.path.exists(sanders):
        # Assert the accuracy of the sentiment analysis on tweets.
        # Given are the scores for Sanders Twitter Sentiment Corpus:
        # http://www.sananalytics.com/lab/twitter-sentiment/
        # Positive + neutral is taken as polarity &gt;= 0.0,
        # Negative is taken as polarity &lt; 0.0.
        # Since there are a lot of neutral cases,
        # and the algorithm predicts 0.0 by default (i.e., majority class) the results are good.
        # Distinguishing negative from neutral from positive is a much harder task
        from pattern.db import Datasheet
        from pattern.metrics import test
        reviews = []
        for i, id, date, tweet, polarity, topic in Datasheet.load(sanders):
            if polarity != "irrelevant":
                reviews.append((tweet, polarity in ("positive", "neutral")))
        A, P, R, F = test(lambda review: en.positive(review, threshold=0.0), reviews)
        #print(A, P, R, F)
        self.assertTrue(A &gt; 0.824)
        self.assertTrue(P &gt; 0.879)
        self.assertTrue(R &gt; 0.911)
        self.assertTrue(F &gt; 0.895)

</t>
<t tx="karstenw.20230303140204.76">def test_sentiment_assessment(self):
    # Assert that en.sentiment() has a fine-grained "assessments" property.
    v = en.sentiment("A warm and pleasant day.").assessments
    self.assertTrue(v[1][0][0] == "pleasant")
    self.assertTrue(v[1][1] &gt; 0)
    print("pattern.en.sentiment().assessments")

</t>
<t tx="karstenw.20230303140204.77">def test_polarity(self):
    # Assert that en.polarity() yields en.sentiment()[0].
    s = "A great day!"
    self.assertTrue(en.polarity(s) == en.sentiment(s)[0])
    print("pattern.en.polarity()")

</t>
<t tx="karstenw.20230303140204.78">def test_subjectivity(self):
    # Assert that en.subjectivity() yields en.sentiment()[1].
    s = "A great day!"
    self.assertTrue(en.subjectivity(s) == en.sentiment(s)[1])
    print("pattern.en.subjectivity()")

</t>
<t tx="karstenw.20230303140204.79">def test_positive(self):
    # Assert that en.positive() yields polarity &gt;= 0.1.
    s = "A great day!"
    self.assertTrue(en.positive(s))
    print("pattern.en.subjectivity()")

</t>
<t tx="karstenw.20230303140204.8">def test_find_lexeme(self):
    # Assert the accuracy of the verb conjugation algorithm.
    i, n = 0, 0
    for v, lexeme1 in en.inflect.verbs.infinitives.items():
        lexeme2 = en.inflect.verbs.find_lexeme(v)
        for j in range(len(lexeme2)):
            if lexeme1[j] == lexeme2[j] or \
               lexeme1[j] == "" and \
               lexeme1[j &gt; 5 and 10 or 0] == lexeme2[j]:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.90)
    print("pattern.en.inflect.verbs.find_lexeme()")

</t>
<t tx="karstenw.20230303140204.80">def test_sentiwordnet(self):
    # Assert &lt; 0 for negative words and &gt; 0 for positive words.
    try:
        from pattern.text.en.wordnet import SentiWordNet
        lexicon = SentiWordNet()
        lexicon.load()
    except ImportError as e:
        # SentiWordNet data file is not installed in default location, stop test.
        print(e)
        return
    self.assertTrue(lexicon["wonderful"][0] &gt; 0)
    self.assertTrue(lexicon["horrible"][0] &lt; 0)
    print("pattern.en.sentiment.SentiWordNet")

</t>
<t tx="karstenw.20230303140204.81">class TestWordNet(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140204.82">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140204.83">def test_normalize(self):
    # Assert normalization of simple diacritics (WordNet does not store diacritics).
    self.assertEqual(en.wordnet.normalize("cliché"), "cliche")
    self.assertEqual(en.wordnet.normalize("façade"), "facade")
    print("pattern.en.wordnet.normalize()")

</t>
<t tx="karstenw.20230303140204.84">def test_version(self):
    print("WordNet " + en.wordnet.VERSION)

</t>
<t tx="karstenw.20230303140204.85">def test_synsets(self):
    # Assert synsets by part-of-speech.
    for word, pos in (
         ("cat", en.wordnet.NOUN),
        ("purr", en.wordnet.VERB),
        ("nice", en.wordnet.ADJECTIVE),
      ("nicely", en.wordnet.ADVERB),
         ("cat", "nn"),
         ("cat", "NNS")):
        self.assertTrue(en.wordnet.synsets(word, pos) != [])
    # Assert TypeError when part-of-speech is not NOUN, VERB, ADJECTIVE or ADVERB.
    self.assertRaises(TypeError, en.wordnet.synsets, "cat", "unknown_pos")
    print("pattern.en.wordnet.synsets()")

</t>
<t tx="karstenw.20230303140204.86">def test_synset(self):
    v = en.wordnet.synsets("puma")[0]
    # Assert Synset(id).
    self.assertEqual(v, en.wordnet.Synset(v.id))
    self.assertEqual(v.pos, en.wordnet.NOUN)
    self.assertAlmostEqual(v.ic, 0.0, places=1)
    self.assertTrue("cougar" in v.synonyms) # ["cougar", "puma", "catamount", ...]
    self.assertTrue("feline" in v.gloss)    # "large American feline resembling a lion"
    # Assert WordNet relations.
    s = en.wordnet.synsets
    v = s("tree")[0]
    self.assertTrue(v.hypernym          in v.hypernyms())
    self.assertTrue(s("woody plant")[0] in v.hypernyms())
    self.assertTrue(s("entity")[0]      in v.hypernyms(recursive=True))
    self.assertTrue(s("beech")[0]       in v.hyponyms())
    self.assertTrue(s("red beech")[0]   in v.hyponyms(recursive=True))
    self.assertTrue(s("trunk")[0]       in v.meronyms())
    self.assertTrue(s("forest")[0]      in v.holonyms())
    # Assert Lin-similarity.
    self.assertTrue(
        v.similarity(s("flower")[0]) &gt;
        v.similarity(s("teapot")[0]))
    print("pattern.en.wordnet.Synset")

</t>
<t tx="karstenw.20230303140204.87">def test_ancenstor(self):
    # Assert least-common-subsumer algorithm.
    v1 = en.wordnet.synsets("cat")[0]
    v2 = en.wordnet.synsets("dog")[0]
    self.assertTrue(en.wordnet.ancestor(v1, v2) == en.wordnet.synsets("carnivore")[0])
    print("pattern.en.wordnet.ancestor()")

</t>
<t tx="karstenw.20230303140204.88">def test_map32(self):
    # Assert sense mapping from WN 3.0 to 2.1.
    self.assertEqual(en.wordnet.map32(18850, "JJ"), (19556, "JJ"))
    self.assertEqual(en.wordnet.map32(1382437, "VB"), (1370230, "VB"))
    print("pattern.en.wordnet.map32")

</t>
<t tx="karstenw.20230303140204.89">def test_sentiwordnet(self):
    # Assert SentiWordNet is loaded correctly.
    if en.wordnet.sentiwordnet is None:
        return
    try:
        en.wordnet.sentiwordnet.load()
    except ImportError:
        return
    v = en.wordnet.synsets("anguish")[0]
    self.assertEqual(v.weight, (-0.625, 0.625))
    v = en.wordnet.synsets("enzymology")[0]
    self.assertEqual(v.weight, (0.125, 0.125))
    print("pattern.en.wordnet.sentiwordnet")

</t>
<t tx="karstenw.20230303140204.9">def test_conjugate(self):
    # Assert different tenses with different conjugations.
    for (v1, v2, tense) in (
      ("be",   "be",       en.INFINITIVE),
      ("be",   "am",      (en.PRESENT, 1, en.SINGULAR)),
      ("be",   "are",     (en.PRESENT, 2, en.SINGULAR)),
      ("be",   "is",      (en.PRESENT, 3, en.SINGULAR)),
      ("be",   "are",     (en.PRESENT, 0, en.PLURAL)),
      ("be",   "being",   (en.PRESENT + en.PARTICIPLE,)),
      ("be",   "was",     (en.PAST, 1, en.SINGULAR)),
      ("be",   "were",    (en.PAST, 2, en.SINGULAR)),
      ("be",   "was",     (en.PAST, 3, en.SINGULAR)),
      ("be",   "were",    (en.PAST, 0, en.PLURAL)),
      ("be",   "were",    (en.PAST, 0, None)),
      ("be",   "been",    (en.PAST + en.PARTICIPLE,)),
      ("be",   "am",      "1sg"),
      ("be",   "are",     "2sg"),
      ("be",   "is",      "3sg"),
      ("be",   "are",     "1pl"),
      ("be",   "are",     "2pl"),
      ("be",   "are",     "3pl"),
      ("be",   "are",     "pl"),
      ("be",   "being",   "part"),
      ("be",   "was",     "1sgp"),
      ("be",   "were",    "2sgp"),
      ("be",   "was",     "3sgp"),
      ("be",   "were",    "1ppl"),
      ("be",   "were",    "2ppl"),
      ("be",   "were",    "3ppl"),
      ("be",   "were",    "p"),
      ("be",   "were",    "ppl"),
      ("be",   "been",    "ppart"),
      ("be",   "am not",  "1sg-"),
      ("be",   "aren't",  "2sg-"),
      ("be",   "isn't",   "3sg-"),
      ("be",   "aren't",  "1pl-"),
      ("be",   "aren't",  "2pl-"),
      ("be",   "aren't",  "3pl-"),
      ("be",   "aren't",  "pl-"),
      ("be",   "wasn't",  "1sgp-"),
      ("be",   "weren't", "2sgp-"),
      ("be",   "wasn't",  "3sgp-"),
      ("be",   "weren't", "1ppl-"),
      ("be",   "weren't", "2ppl-"),
      ("be",   "weren't", "3ppl-"),
      ("be",   "weren't", "ppl-"),
      ("had",  "have",    "inf"),
      ("had",  "have",    "1sg"),
      ("had",  "have",    "2sg"),
      ("had",  "has",     "3sg"),
      ("had",  "have",    "pl"),
      ("had",  "having",  "part"),
      ("has",  "had",     "1sgp"),
      ("has",  "had",     "2sgp"),
      ("has",  "had",     "3sgp"),
      ("has",  "had",     "ppl"),
      ("has",  "had",     "p"),
      ("has",  "had",     "ppart"),
      ("will", "will",    "1sg"),
      ("will", "will",    "2sg"),
      ("will", "will",    "3sg"),
      ("will", "will",    "1pl"),
      ("imaginerify", "imaginerifying", "part"),
      ("imaginerify", "imaginerified", "3sgp"),
      ("imaginerify", None, "1sg-")):
        self.assertEqual(en.inflect.conjugate(v1, tense), v2)
    print("pattern.en.inflect.conjugate()")

</t>
<t tx="karstenw.20230303140204.90">class TestWordlists(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140204.91">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140204.92">def test_wordlist(self):
    # Assert lazy loading Wordlist.
    v = en.wordlist.STOPWORDS
    self.assertTrue("the" in v)
    # Assert Wordlist to dict.
    v = dict.fromkeys(en.wordlist.STOPWORDS, True)
    self.assertTrue("the" in v)
    # Assert new Wordlist by adding other Wordlists.
    v = en.wordlist.STOPWORDS + en.wordlist.ACADEMIC
    self.assertTrue("the" in v)
    self.assertTrue("dr." in v)
    print("pattern.en.wordlist.Wordlist")

</t>
<t tx="karstenw.20230303140204.93">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestInflection))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestQuantification))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSpelling))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestParser))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestParseTree))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestModality))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSentiment))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestWordNet))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestWordlists))
    return suite

</t>
<t tx="karstenw.20230303140206.1">from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
import subprocess

from pattern import es

from io import open

try:
    PATH = os.path.dirname(os.path.realpath(__file__))
except:
    PATH = ""

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140206.10">def test_conjugate(self):
    # Assert different tenses with different conjugations.
    for (v1, v2, tense) in (
      ("ser", "ser",        es.INFINITIVE),
      ("ser", "soy",       (es.PRESENT, 1, es.SINGULAR)),
      ("ser", "eres",      (es.PRESENT, 2, es.SINGULAR)),
      ("ser", "es",        (es.PRESENT, 3, es.SINGULAR)),
      ("ser", "somos",     (es.PRESENT, 1, es.PLURAL)),
      ("ser", "sois",      (es.PRESENT, 2, es.PLURAL)),
      ("ser", "son",       (es.PRESENT, 3, es.PLURAL)),
      ("ser", "siendo",    (es.PRESENT + es.PARTICIPLE)),
      ("ser", "sido",      (es.PAST + es.PARTICIPLE)),
      ("ser", "era",       (es.IMPERFECT, 1, es.SINGULAR)),
      ("ser", "eras",      (es.IMPERFECT, 2, es.SINGULAR)),
      ("ser", "era",       (es.IMPERFECT, 3, es.SINGULAR)),
      ("ser", "éramos",    (es.IMPERFECT, 1, es.PLURAL)),
      ("ser", "erais",     (es.IMPERFECT, 2, es.PLURAL)),
      ("ser", "eran",      (es.IMPERFECT, 3, es.PLURAL)),
      ("ser", "fui",       (es.PRETERITE, 1, es.SINGULAR)),
      ("ser", "fuiste",    (es.PRETERITE, 2, es.SINGULAR)),
      ("ser", "fue",       (es.PRETERITE, 3, es.SINGULAR)),
      ("ser", "fuimos",    (es.PRETERITE, 1, es.PLURAL)),
      ("ser", "fuisteis",  (es.PRETERITE, 2, es.PLURAL)),
      ("ser", "fueron",    (es.PRETERITE, 3, es.PLURAL)),
      ("ser", "sería",     (es.CONDITIONAL, 1, es.SINGULAR)),
      ("ser", "serías",    (es.CONDITIONAL, 2, es.SINGULAR)),
      ("ser", "sería",     (es.CONDITIONAL, 3, es.SINGULAR)),
      ("ser", "seríamos",  (es.CONDITIONAL, 1, es.PLURAL)),
      ("ser", "seríais",   (es.CONDITIONAL, 2, es.PLURAL)),
      ("ser", "serían",    (es.CONDITIONAL, 3, es.PLURAL)),
      ("ser", "seré",      (es.FUTURE, 1, es.SINGULAR)),
      ("ser", "serás",     (es.FUTURE, 2, es.SINGULAR)),
      ("ser", "será",      (es.FUTURE, 3, es.SINGULAR)),
      ("ser", "seremos",   (es.FUTURE, 1, es.PLURAL)),
      ("ser", "seréis",    (es.FUTURE, 2, es.PLURAL)),
      ("ser", "serán",     (es.FUTURE, 3, es.PLURAL)),
      ("ser", "sé",        (es.PRESENT, 2, es.SINGULAR, es.IMPERATIVE)),
      ("ser", "sed",       (es.PRESENT, 2, es.PLURAL, es.IMPERATIVE)),
      ("ser", "sea",      (es.PRESENT, 1, es.SINGULAR, es.SUBJUNCTIVE)),
      ("ser", "seas",     (es.PRESENT, 2, es.SINGULAR, es.SUBJUNCTIVE)),
      ("ser", "sea",      (es.PRESENT, 3, es.SINGULAR, es.SUBJUNCTIVE)),
      ("ser", "seamos",   (es.PRESENT, 1, es.PLURAL, es.SUBJUNCTIVE)),
      ("ser", "seáis",    (es.PRESENT, 2, es.PLURAL, es.SUBJUNCTIVE)),
      ("ser", "sean",     (es.PRESENT, 3, es.PLURAL, es.SUBJUNCTIVE)),
      ("ser", "fuera",    (es.PAST, 1, es.SINGULAR, es.SUBJUNCTIVE)),
      ("ser", "fueras",   (es.PAST, 2, es.SINGULAR, es.SUBJUNCTIVE)),
      ("ser", "fuera",    (es.PAST, 3, es.SINGULAR, es.SUBJUNCTIVE)),
      ("ser", "fuéramos", (es.PAST, 1, es.PLURAL, es.SUBJUNCTIVE)),
      ("ser", "fuerais",  (es.PAST, 2, es.PLURAL, es.SUBJUNCTIVE)),
      ("ser", "fueran",   (es.PAST, 3, es.PLURAL, es.SUBJUNCTIVE))):
        self.assertEqual(es.conjugate(v1, tense), v2)
    print("pattern.es.conjugate()")

</t>
<t tx="karstenw.20230303140206.11">def test_lexeme(self):
    # Assert all inflections of "ser".
    v = es.lexeme("ser")
    self.assertEqual(v, [
        'ser', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'siendo',
        'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'sido',
        'era', 'eras', 'éramos', 'erais', 'eran',
        'seré', 'serás', 'será', 'seremos', 'seréis', 'serán',
        'sería', 'serías', 'seríamos', 'seríais', 'serían',
        'sé', 'sed',
        'sea', 'seas', 'seamos', 'seáis', 'sean',
        'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran'
    ])
    print("pattern.es.inflect.lexeme()")

</t>
<t tx="karstenw.20230303140206.12">def test_tenses(self):
    # Assert tense recognition.
    self.assertTrue((es.PRESENT, 3, es.SG) in es.tenses("es"))
    self.assertTrue("2sg" in es.tenses("eres"))
    # The CONDITIONAL is sometimes described as a mood,
    # and sometimes as a tense of the indicative mood (e.g., in Spanish):
    t1 = (es.CONDITIONAL, 1, es.SG)
    t2 = (es.PRESENT, 1, es.SG, es.CONDITIONAL)
    self.assertTrue("1sg-&gt;" in es.tenses("sería"))
    self.assertTrue(t1 in es.tenses("sería"))
    self.assertTrue(t2 in es.tenses("sería"))
    self.assertTrue(t1 in es.tenses(es.conjugate("ser", mood=es.INDICATIVE, tense=es.CONDITIONAL)))
    self.assertTrue(t2 in es.tenses(es.conjugate("ser", mood=es.CONDITIONAL)))
    print("pattern.es.tenses()")

</t>
<t tx="karstenw.20230303140206.13">class TestParser(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140206.14">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140206.15">def test_find_lemmata(self):
    # Assert lemmata for nouns, adjectives, verbs and determiners.
    v = es.parser.find_lemmata([
        ["Los", "DT"], ["gatos", "NNS"], ["negros", "JJ"], ["se", "PRP"], ["sentó", "VB"],
        ["en", "IN"], ["la", "DT"], ["alfombra", "NN"]])
    self.assertEqual(v, [
        ["Los", "DT", "el"],
        ["gatos", "NNS", "gato"],
        ["negros", "JJ", "negro"],
        ["se", "PRP", "se"],
        ["sentó", "VB", "sentar"],
        ["en", "IN", "en"],
        ["la", "DT", "el"],
        ["alfombra", "NN", "alfombra"]])
    print("pattern.es.parser.find_lemmata()")

</t>
<t tx="karstenw.20230303140206.16">def test_parse(self):
    # Assert parsed output with Penn Treebank II tags (slash-formatted).
    # "el gato negro" is a noun phrase, "en la alfombra" is a prepositional noun phrase.
    v = es.parser.parse("El gato negro se sentó en la alfombra.")
    self.assertEqual(v, # XXX - shouldn't "se" be part of the verb phrase?
        "El/DT/B-NP/O gato/NN/I-NP/O negro/JJ/I-NP/O " + \
        "se/PRP/B-NP/O sentó/VB/B-VP/O " + \
        "en/IN/B-PP/B-PNP la/DT/B-NP/I-PNP alfombra/NN/I-NP/I-PNP ././O/O"
    )
    # Assert the accuracy of the Spanish tagger.
    i, n = 0, 0
    for sentence in open(os.path.join(PATH, "corpora", "tagged-es-wikicorpus.txt")).readlines():
        sentence = sentence.strip()
        s1 = [w.split("/") for w in sentence.split(" ")]
        s2 = [[w for w, pos in s1]]
        s2 = es.parse(s2, tokenize=False, tagset=es.PAROLE)
        s2 = [w.split("/") for w in s2.split(" ")]
        for j in range(len(s1)):
            if s1[j][1] == s2[j][1]:
                i += 1
            n += 1
    #print(float(i) / n)
    self.assertTrue(float(i) / n &gt; 0.92)
    print("pattern.es.parser.parse()")

</t>
<t tx="karstenw.20230303140206.17">def test_tag(self):
    # Assert [("el", "DT"), ("gato", "NN"), ("negro", "JJ")].
    v = es.tag("el gato negro")
    self.assertEqual(v, [("el", "DT"), ("gato", "NN"), ("negro", "JJ")])
    print("pattern.es.tag()")

</t>
<t tx="karstenw.20230303140206.18">def test_command_line(self):
    # Assert parsed output from the command-line (example from the documentation).
    p = ["python", "-m", "pattern.es", "-s", "El gato negro.", "-OTCRL"]
    p = subprocess.Popen(p, stdout=subprocess.PIPE)
    p.wait()
    v = p.stdout.read().decode('utf-8')
    v = v.strip()
    self.assertEqual(v, "El/DT/B-NP/O/O/el gato/NN/I-NP/O/O/gato negro/JJ/I-NP/O/O/negro ././O/O/O/.")
    print("python -m pattern.es")

</t>
<t tx="karstenw.20230303140206.19">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestInflection))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestParser))
    return suite

</t>
<t tx="karstenw.20230303140206.2">class TestInflection(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140206.3">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140206.4">def test_pluralize(self):
    # Assert the accuracy of the pluralization algorithm.
    from pattern.db import Datasheet
    test = {}
    for w, lemma, tag, f in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-es-davies.csv")):
        if tag == "n":
            test.setdefault(lemma, []).append(w)
    i, n = 0, 0
    for sg, pl in test.items():
        pl = sorted(pl, key=len, reverse=True)[0]
        if es.pluralize(sg) == pl:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.77)
    print("pattern.es.pluralize()")

</t>
<t tx="karstenw.20230303140206.5">def test_singularize(self):
    # Assert the accuracy of the singularization algorithm.
    from pattern.db import Datasheet
    test = {}
    for w, lemma, tag, f in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-es-davies.csv")):
        if tag == "n":
            test.setdefault(lemma, []).append(w)
    i, n = 0, 0
    for sg, pl in test.items():
        pl = sorted(pl, key=len, reverse=True)[0]
        if es.singularize(pl) == sg:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.93)
    print("pattern.es.singularize()")

</t>
<t tx="karstenw.20230303140206.6">def test_attributive(self):
    # Assert "alto" =&gt; "altos" (masculine, plural), and others.
    for lemma, inflected, gender in (
      ("alto",  "alto",   es.MALE + es.SINGULAR),
      ("alto",  "altos",  es.MALE + es.PLURAL),
      ("alto",  "alta",   es.FEMALE + es.SINGULAR),
      ("alto",  "altas",  es.FEMALE + es.PLURAL),
      ("verde", "verdes", es.MALE + es.PLURAL),
      ("verde", "verdes", es.FEMALE + es.PLURAL)):
        v = es.attributive(lemma, gender)
        self.assertEqual(v, inflected)
    print("pattern.es.attributive()")

</t>
<t tx="karstenw.20230303140206.7">def test_predicative(self):
    # Assert the accuracy of the predicative algorithm ("horribles" =&gt; "horrible").
    from pattern.db import Datasheet
    test = {}
    for w, lemma, tag, f in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-es-davies.csv")):
        if tag == "j":
            test.setdefault(lemma, []).append(w)
    i, n = 0, 0
    for pred, attr in test.items():
        attr = sorted(attr, key=len, reverse=True)[0]
        if es.predicative(attr) == pred:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.92)
    print("pattern.es.predicative()")

</t>
<t tx="karstenw.20230303140206.8">def test_find_lemma(self):
    # Assert the accuracy of the verb lemmatization algorithm.
    i, n = 0, 0
    for v1, v2 in es.inflect.verbs.inflections.items():
        if es.inflect.verbs.find_lemma(v1) == v2:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.80)
    print("pattern.es.inflect.verbs.find_lemma()")

</t>
<t tx="karstenw.20230303140206.9">def test_find_lexeme(self):
    # Assert the accuracy of the verb conjugation algorithm.
    i, n = 0, 0
    for v, lexeme1 in es.inflect.verbs.infinitives.items():
        lexeme2 = es.inflect.verbs.find_lexeme(v)
        for j in range(len(lexeme2)):
            if lexeme1[j] == lexeme2[j]:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.85)
    print("pattern.es.inflect.verbs.find_lexeme()")

</t>
<t tx="karstenw.20230303140209.1">from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
import subprocess

from pattern import fr

from io import open

try:
    PATH = os.path.dirname(os.path.realpath(__file__))
except:
    PATH = ""

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140209.10">class TestParser(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140209.11">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140209.12">def test_find_prepositions(self):
    v = fr.parser.parse("Parce que c'est comme ça.")

</t>
<t tx="karstenw.20230303140209.13">def test_find_lemmata(self):
    # Assert lemmata for nouns, adjectives, verbs and determiners.
    v = fr.parser.find_lemmata([
        ["Les", "DT"], ["chats", "NNS"], ["noirs", "JJ"], ["s'", "PRP"], ["étaient", "VB"], ["assis", "VB"],
        ["sur", "IN"], ["le", "DT"], ["tapis", "NN"]])
    self.assertEqual(v, [
        ["Les", "DT", "le"],
        ["chats", "NNS", "chat"],
        ["noirs", "JJ", "noir"],
        ["s'", "PRP", "se"],
        ["étaient", "VB", "être"],
        ["assis", "VB", "asseoir"],
        ["sur", "IN", "sur"],
        ["le", "DT", "le"],
        ["tapis", "NN", "tapis"]])
    print("pattern.fr.parser.find_lemmata()")

</t>
<t tx="karstenw.20230303140209.14">def test_parse(self):
    # Assert parsed output with Penn Treebank II tags (slash-formatted).
    # "le chat noir" is a noun phrase, "sur le tapis" is a prepositional noun phrase.
    v = fr.parser.parse("Le chat noir s'était assis sur le tapis.")
    self.assertEqual(v,
        "Le/DT/B-NP/O chat/NN/I-NP/O noir/JJ/I-NP/O " + \
        "s'/PRP/B-NP/O était/VB/B-VP/O assis/VBN/I-VP/O " + \
        "sur/IN/B-PP/B-PNP le/DT/B-NP/I-PNP tapis/NN/I-NP/I-PNP ././O/O"
    )
    # Assert the accuracy of the French tagger.
    f = fr.penntreebank2universal
    i, n = 0, 0
    for sentence in open(os.path.join(PATH, "corpora", "tagged-fr-wikinews.txt")).readlines():
        sentence = sentence.strip()
        s1 = [w.split("/") for w in sentence.split(" ")]
        s2 = [[w for w, pos in s1]]
        s2 = fr.parse(s2, tokenize=False)
        s2 = [w.split("/") for w in s2.split(" ")]
        for j in range(len(s1)):
            if f(*s1[j][:2])[1] == f(*s2[j][:2])[1]:
                i += 1
            n += 1
    #print(float(i) / n)
    self.assertTrue(float(i) / n &gt; 0.85)
    print("pattern.fr.parser.parse()")

</t>
<t tx="karstenw.20230303140209.15">def test_tag(self):
    # Assert [("le", "DT"), ("chat", "NN"), ("noir", "JJ")].
    v = fr.tag("le chat noir")
    self.assertEqual(v, [("le", "DT"), ("chat", "NN"), ("noir", "JJ")])
    print("pattern.fr.tag()")

</t>
<t tx="karstenw.20230303140209.16">def test_command_line(self):
    # Assert parsed output from the command-line (example from the documentation).
    p = ["python", "-m", "pattern.fr", "-s", "Le chat noir.", "-OTCRL"]
    p = subprocess.Popen(p, stdout=subprocess.PIPE)
    p.wait()
    v = p.stdout.read().decode('utf-8')
    v = v.strip()
    self.assertEqual(v, "Le/DT/B-NP/O/O/le chat/NN/I-NP/O/O/chat noir/JJ/I-NP/O/O/noir ././O/O/O/.")
    print("python -m pattern.fr")

</t>
<t tx="karstenw.20230303140209.17">class TestSentiment(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140209.18">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140209.19">def test_sentiment(self):
    # Assert &lt; 0 for negative adjectives and &gt; 0 for positive adjectives.
    self.assertTrue(fr.sentiment("fabuleux")[0] &gt; 0)
    self.assertTrue(fr.sentiment("terrible")[0] &lt; 0)
    # Assert the accuracy of the sentiment analysis.
    # Given are the scores for 1,500 book reviews.
    # The baseline should increase (not decrease) when the algorithm is modified.
    from pattern.db import Datasheet
    from pattern.metrics import test
    reviews = []
    for review, score in Datasheet.load(os.path.join(PATH, "corpora", "polarity-fr-amazon.csv")):
        reviews.append((review, int(score) &gt; 0))
    A, P, R, F = test(lambda review: fr.positive(review), reviews)
    #print(A, P, R, F)
    self.assertTrue(A &gt; 0.751)
    self.assertTrue(P &gt; 0.765)
    self.assertTrue(R &gt; 0.725)
    self.assertTrue(F &gt; 0.744)
    print("pattern.fr.sentiment()")

</t>
<t tx="karstenw.20230303140209.2">class TestInflection(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140209.20">def test_tokenizer(self):
    # Assert that french sentiment() uses French tokenizer. ("t'aime" =&gt; "t' aime").
    v1 = fr.sentiment("je t'aime")
    v2 = fr.sentiment("je ne t'aime pas")
    self.assertTrue(v1[0] &gt; 0)
    self.assertTrue(v2[0] &lt; 0)
    self.assertTrue(v1.assessments[0][0] == ["aime"])
    self.assertTrue(v2.assessments[0][0] == ["ne", "aime"])

</t>
<t tx="karstenw.20230303140209.21">def suite():
    suite = unittest.TestSuite()
    #suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestInflection))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestParser))
    #suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSentiment))
    return suite

</t>
<t tx="karstenw.20230303140209.3">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140209.4">def test_predicative(self):
    # Assert the accuracy of the predicative algorithm ("belles" =&gt; "bea").
    from pattern.db import Datasheet
    i, n = 0, 0
    for pred, attr, tag in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-fr-lexique.csv")):
        if tag == "a":
            if fr.predicative(attr) == pred:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.95)
    print("pattern.fr.predicative()")

</t>
<t tx="karstenw.20230303140209.5">def test_find_lemma(self):
    # Assert the accuracy of the verb lemmatization algorithm.
    i, n = 0, 0
    for v1, v2 in fr.inflect.verbs.inflections.items():
        if fr.inflect.verbs.find_lemma(v1) == v2:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.80)
    print("pattern.fr.inflect.verbs.find_lemma()")

</t>
<t tx="karstenw.20230303140209.6">def test_find_lexeme(self):
    # Assert the accuracy of the verb conjugation algorithm.
    i, n = 0, 0
    for v, lexeme1 in fr.inflect.verbs.infinitives.items():
        lexeme2 = fr.inflect.verbs.find_lexeme(v)
        for j in range(len(lexeme2)):
            if lexeme1[j] == lexeme2[j]:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.85)
    print("pattern.fr.inflect.verbs.find_lexeme()")

</t>
<t tx="karstenw.20230303140209.7">def test_conjugate(self):
    # Assert different tenses with different conjugations.
    for (v1, v2, tense) in (
      ("être", "être",      fr.INFINITIVE),
      ("être", "suis",     (fr.PRESENT, 1, fr.SINGULAR)),
      ("être", "es",       (fr.PRESENT, 2, fr.SINGULAR)),
      ("être", "est",      (fr.PRESENT, 3, fr.SINGULAR)),
      ("être", "sommes",   (fr.PRESENT, 1, fr.PLURAL)),
      ("être", "êtes",     (fr.PRESENT, 2, fr.PLURAL)),
      ("être", "sont",     (fr.PRESENT, 3, fr.PLURAL)),
      ("être", "étant",    (fr.PRESENT + fr.PARTICIPLE)),
      ("être", "été",      (fr.PAST + fr.PARTICIPLE)),
      ("être", "étais",    (fr.IMPERFECT, 1, fr.SINGULAR)),
      ("être", "étais",    (fr.IMPERFECT, 2, fr.SINGULAR)),
      ("être", "était",    (fr.IMPERFECT, 3, fr.SINGULAR)),
      ("être", "étions",   (fr.IMPERFECT, 1, fr.PLURAL)),
      ("être", "étiez",    (fr.IMPERFECT, 2, fr.PLURAL)),
      ("être", "étaient",  (fr.IMPERFECT, 3, fr.PLURAL)),
      ("être", "fus",      (fr.PRETERITE, 1, fr.SINGULAR)),
      ("être", "fus",      (fr.PRETERITE, 2, fr.SINGULAR)),
      ("être", "fut",      (fr.PRETERITE, 3, fr.SINGULAR)),
      ("être", "fûmes",    (fr.PRETERITE, 1, fr.PLURAL)),
      ("être", "fûtes",    (fr.PRETERITE, 2, fr.PLURAL)),
      ("être", "furent",   (fr.PRETERITE, 3, fr.PLURAL)),
      ("être", "serais",   (fr.CONDITIONAL, 1, fr.SINGULAR)),
      ("être", "serais",   (fr.CONDITIONAL, 2, fr.SINGULAR)),
      ("être", "serait",   (fr.CONDITIONAL, 3, fr.SINGULAR)),
      ("être", "serions",  (fr.CONDITIONAL, 1, fr.PLURAL)),
      ("être", "seriez",   (fr.CONDITIONAL, 2, fr.PLURAL)),
      ("être", "seraient", (fr.CONDITIONAL, 3, fr.PLURAL)),
      ("être", "serai",    (fr.FUTURE, 1, fr.SINGULAR)),
      ("être", "seras",    (fr.FUTURE, 2, fr.SINGULAR)),
      ("être", "sera",     (fr.FUTURE, 3, fr.SINGULAR)),
      ("être", "serons",   (fr.FUTURE, 1, fr.PLURAL)),
      ("être", "serez",    (fr.FUTURE, 2, fr.PLURAL)),
      ("être", "seront",   (fr.FUTURE, 3, fr.PLURAL)),
      ("être", "sois",     (fr.PRESENT, 2, fr.SINGULAR, fr.IMPERATIVE)),
      ("être", "soyons",   (fr.PRESENT, 1, fr.PLURAL, fr.IMPERATIVE)),
      ("être", "soyez",    (fr.PRESENT, 2, fr.PLURAL, fr.IMPERATIVE)),
      ("être", "sois",     (fr.PRESENT, 1, fr.SINGULAR, fr.SUBJUNCTIVE)),
      ("être", "sois",     (fr.PRESENT, 2, fr.SINGULAR, fr.SUBJUNCTIVE)),
      ("être", "soit",     (fr.PRESENT, 3, fr.SINGULAR, fr.SUBJUNCTIVE)),
      ("être", "soyons",   (fr.PRESENT, 1, fr.PLURAL, fr.SUBJUNCTIVE)),
      ("être", "soyez",    (fr.PRESENT, 2, fr.PLURAL, fr.SUBJUNCTIVE)),
      ("être", "soient",   (fr.PRESENT, 3, fr.PLURAL, fr.SUBJUNCTIVE)),
      ("être", "fusse",    (fr.PAST, 1, fr.SINGULAR, fr.SUBJUNCTIVE)),
      ("être", "fusses",   (fr.PAST, 2, fr.SINGULAR, fr.SUBJUNCTIVE)),
      ("être", "fût",      (fr.PAST, 3, fr.SINGULAR, fr.SUBJUNCTIVE)),
      ("être", "fussions", (fr.PAST, 1, fr.PLURAL, fr.SUBJUNCTIVE)),
      ("être", "fussiez",  (fr.PAST, 2, fr.PLURAL, fr.SUBJUNCTIVE)),
      ("être", "fussent",  (fr.PAST, 3, fr.PLURAL, fr.SUBJUNCTIVE))):
        self.assertEqual(fr.conjugate(v1, tense), v2)
    print("pattern.fr.conjugate()")

</t>
<t tx="karstenw.20230303140209.8">def test_lexeme(self):
    # Assert all inflections of "être".
    v = fr.lexeme("être")
    self.assertEqual(v, [
        "être", "suis", "es", "est", "sommes", "êtes", "sont", "étant", "été",
        "fus", "fut", "fûmes", "fûtes", "furent",
        "étais", "était", "étions", "étiez", "étaient",
        "serai", "seras", "sera", "serons", "serez", "seront",
        "serais", "serait", "serions", "seriez", "seraient",
        "sois", "soyons", "soyez", "soit", "soient",
        "fusse", "fusses", "fût", "fussions", "fussiez", "fussent"
    ])
    print("pattern.fr.inflect.lexeme()")

</t>
<t tx="karstenw.20230303140209.9">def test_tenses(self):
    # Assert tense recognition.
    self.assertTrue((fr.PRESENT, 3, fr.SG) in fr.tenses("est"))
    self.assertTrue("2sg" in fr.tenses("es"))
    print("pattern.fr.tenses()")

</t>
<t tx="karstenw.20230303140211.1">from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
import subprocess

from pattern import it

from io import open

try:
    PATH = os.path.dirname(os.path.realpath(__file__))
except:
    PATH = ""

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140211.10">def test_find_lexeme(self):
    # Assert the accuracy of the verb conjugation algorithm.
    i, n = 0, 0
    for v, lexeme1 in it.inflect.verbs.infinitives.items():
        lexeme2 = it.inflect.verbs.find_lexeme(v)
        for j in range(len(lexeme2)):
            if lexeme1[j] == lexeme2[j]:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.89)
    print("pattern.it.inflect.verbs.find_lexeme()")

</t>
<t tx="karstenw.20230303140211.11">def test_conjugate(self):
    # Assert different tenses with different conjugations.
    for (v1, v2, tense) in (
      ("essere", "essere",     it.INFINITIVE),
      ("essere", "sono",      (it.PRESENT, 1, it.SINGULAR)),
      ("essere", "sei",       (it.PRESENT, 2, it.SINGULAR)),
      ("essere", "è",         (it.PRESENT, 3, it.SINGULAR)),
      ("essere", "siamo",     (it.PRESENT, 1, it.PLURAL)),
      ("essere", "siete",     (it.PRESENT, 2, it.PLURAL)),
      ("essere", "sono",      (it.PRESENT, 3, it.PLURAL)),
      ("essere", "essendo",   (it.PRESENT + it.PARTICIPLE)),
      ("essere", "stato",     (it.PAST + it.PARTICIPLE)),
      ("essere", "ero",       (it.IMPERFECT, 1, it.SINGULAR)),
      ("essere", "eri",       (it.IMPERFECT, 2, it.SINGULAR)),
      ("essere", "era",       (it.IMPERFECT, 3, it.SINGULAR)),
      ("essere", "eravamo",   (it.IMPERFECT, 1, it.PLURAL)),
      ("essere", "eravate",   (it.IMPERFECT, 2, it.PLURAL)),
      ("essere", "erano",     (it.IMPERFECT, 3, it.PLURAL)),
      ("essere", "fui",       (it.PRETERITE, 1, it.SINGULAR)),
      ("essere", "fosti",     (it.PRETERITE, 2, it.SINGULAR)),
      ("essere", "fu",        (it.PRETERITE, 3, it.SINGULAR)),
      ("essere", "fummo",     (it.PRETERITE, 1, it.PLURAL)),
      ("essere", "foste",     (it.PRETERITE, 2, it.PLURAL)),
      ("essere", "furono",    (it.PRETERITE, 3, it.PLURAL)),
      ("essere", "sarei",     (it.CONDITIONAL, 1, it.SINGULAR)),
      ("essere", "saresti",   (it.CONDITIONAL, 2, it.SINGULAR)),
      ("essere", "sarebbe",   (it.CONDITIONAL, 3, it.SINGULAR)),
      ("essere", "saremmo",   (it.CONDITIONAL, 1, it.PLURAL)),
      ("essere", "sareste",   (it.CONDITIONAL, 2, it.PLURAL)),
      ("essere", "sarebbero", (it.CONDITIONAL, 3, it.PLURAL)),
      ("essere", "sarò",      (it.FUTURE, 1, it.SINGULAR)),
      ("essere", "sarai",     (it.FUTURE, 2, it.SINGULAR)),
      ("essere", "sarà",      (it.FUTURE, 3, it.SINGULAR)),
      ("essere", "saremo",    (it.FUTURE, 1, it.PLURAL)),
      ("essere", "sarete",    (it.FUTURE, 2, it.PLURAL)),
      ("essere", "saranno",   (it.FUTURE, 3, it.PLURAL)),
      ("essere", "sii",       (it.PRESENT, 2, it.SINGULAR, it.IMPERATIVE)),
      ("essere", "sia",       (it.PRESENT, 3, it.SINGULAR, it.IMPERATIVE)),
      ("essere", "siamo",     (it.PRESENT, 1, it.PLURAL, it.IMPERATIVE)),
      ("essere", "siate",     (it.PRESENT, 2, it.PLURAL, it.IMPERATIVE)),
      ("essere", "siano",     (it.PRESENT, 3, it.PLURAL, it.IMPERATIVE)),
      ("essere", "sia",       (it.PRESENT, 1, it.SINGULAR, it.SUBJUNCTIVE)),
      ("essere", "sia",       (it.PRESENT, 2, it.SINGULAR, it.SUBJUNCTIVE)),
      ("essere", "sia",       (it.PRESENT, 3, it.SINGULAR, it.SUBJUNCTIVE)),
      ("essere", "siamo",     (it.PRESENT, 1, it.PLURAL, it.SUBJUNCTIVE)),
      ("essere", "siate",     (it.PRESENT, 2, it.PLURAL, it.SUBJUNCTIVE)),
      ("essere", "siano",     (it.PRESENT, 3, it.PLURAL, it.SUBJUNCTIVE)),
      ("essere", "fossi",     (it.PAST, 1, it.SINGULAR, it.SUBJUNCTIVE)),
      ("essere", "fossi",     (it.PAST, 2, it.SINGULAR, it.SUBJUNCTIVE)),
      ("essere", "fosse",     (it.PAST, 3, it.SINGULAR, it.SUBJUNCTIVE)),
      ("essere", "fossimo",   (it.PAST, 1, it.PLURAL, it.SUBJUNCTIVE)),
      ("essere", "foste",     (it.PAST, 2, it.PLURAL, it.SUBJUNCTIVE)),
      ("essere", "fossero",   (it.PAST, 3, it.PLURAL, it.SUBJUNCTIVE))):
        self.assertEqual(it.conjugate(v1, tense), v2)
    print("pattern.it.conjugate()")

</t>
<t tx="karstenw.20230303140211.12">def test_lexeme(self):
    # Assert all inflections of "essere".
    v = it.lexeme("essere")
    self.assertEqual(v, [
        'essere', 'sono', 'sei', 'è', 'siamo', 'siete', 'essendo',
        'fui', 'fosti', 'fu', 'fummo', 'foste', 'furono', 'stato',
        'ero', 'eri', 'era', 'eravamo', 'eravate', 'erano',
        'sarò', 'sarai', 'sarà', 'saremo', 'sarete', 'saranno',
        'sarei', 'saresti', 'sarebbe', 'saremmo', 'sareste', 'sarebbero',
        'sii', 'sia', 'siate', 'siano',
        'fossi', 'fosse', 'fossimo', 'fossero'
    ])
    print("pattern.it.inflect.lexeme()")

</t>
<t tx="karstenw.20230303140211.13">def test_tenses(self):
    # Assert tense recognition.
    self.assertTrue((it.PRESENT, 3, it.SG) in it.tenses("è"))
    self.assertTrue("2sg" in it.tenses("sei"))
    print("pattern.it.tenses()")

</t>
<t tx="karstenw.20230303140211.14">class TestParser(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140211.15">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140211.16">def test_find_lemmata(self):
    # Assert lemmata for nouns, adjectives, verbs and determiners.
    v = it.parser.find_lemmata([
        ["I", "DT"], ["gatti", "NNS"], ["neri", "JJ"],
        ["seduti", "VB"], ["sul", "IN"], ["tatami", "NN"]])
    self.assertEqual(v, [
        ["I", "DT", "il"],
        ["gatti", "NNS", "gatto"],
        ["neri", "JJ", "nero"],
        ["seduti", "VB", "sedutare"],
        ["sul", "IN", "sul"],
        ["tatami", "NN", "tatami"]])
    print("pattern.it.parser.find_lemmata()")

</t>
<t tx="karstenw.20230303140211.17">def test_parse(self):
    # Assert parsed output with Penn Treebank II tags (slash-formatted).
    # "il gatto nero" is a noun phrase, "sulla stuoia" is a prepositional noun phrase.
    v = it.parser.parse("Il gatto nero seduto sulla stuoia.")
    self.assertEqual(v,
        "Il/DT/B-NP/O gatto/NN/I-NP/O nero/JJ/I-NP/O " +
        "seduto/VB/B-VP/O " + \
        "sulla/IN/B-PP/B-PNP stuoia/NN/B-NP/I-PNP ././O/O"
    )
    # Assert the accuracy of the Italian tagger.
    i, n = 0, 0
    for sentence in open(os.path.join(PATH, "corpora", "tagged-it-wacky.txt")).readlines():
        sentence = sentence.strip()
        s1 = [w.split("/") for w in sentence.split(" ")]
        s2 = [[w for w, pos in s1]]
        s2 = it.parse(s2, tokenize=False)
        s2 = [w.split("/") for w in s2.split(" ")]
        for j in range(len(s1)):
            t1 = s1[j][1]
            t2 = s2[j][1]
            # WaCKy test set tags plural nouns as "NN", pattern.it as "NNS".
            # Some punctuation marks are also tagged differently,
            # but these are not necessarily errors.
            if t1 == t2 or (t1 == "NN" and t2.startswith("NN")) or s1[j][0] in "\":;)-":
                i += 1
            n += 1
    #print(float(i) / n)
    self.assertTrue(float(i) / n &gt; 0.92)
    print("pattern.it.parser.parse()")

</t>
<t tx="karstenw.20230303140211.18">def test_tag(self):
    # Assert [("il", "DT"), ("gatto", "NN"), ("nero", "JJ")].
    v = it.tag("il gatto nero")
    self.assertEqual(v, [("il", "DT"), ("gatto", "NN"), ("nero", "JJ")])
    print("pattern.it.tag()")

</t>
<t tx="karstenw.20230303140211.19">def test_command_line(self):
    # Assert parsed output from the command-line (example from the documentation).
    p = ["python", "-m", "pattern.it", "-s", "Il gatto nero.", "-OTCRL"]
    p = subprocess.Popen(p, stdout=subprocess.PIPE)
    p.wait()
    v = p.stdout.read().decode('utf-8')
    v = v.strip()
    self.assertEqual(v, "Il/DT/B-NP/O/O/il gatto/NN/I-NP/O/O/gatto nero/JJ/I-NP/O/O/nero ././O/O/O/.")
    print("python -m pattern.it")

</t>
<t tx="karstenw.20230303140211.2">class TestInflection(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140211.20">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestInflection))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestParser))
    return suite

</t>
<t tx="karstenw.20230303140211.3">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140211.4">def test_article(self):
    # Assert definite and indefinite article inflection.
    for a, n, g in (
      ("il" , "giorno"      , it.M),
      ("l'" , "altro giorno", it.M),
      ("lo" , "zio"         , it.M),
      ("l'" , "amica"       , it.F),
      ("la" , "nouva amica" , it.F),
      ("i"  , "giapponesi"  , it.M + it.PL),
      ("gli", "italiani"    , it.M + it.PL),
      ("gli", "zii"         , it.M + it.PL),
      ("le" , "zie"         , it.F + it.PL)):
        v = it.article(n, "definite", gender=g)
        self.assertEqual(a, v)
    for a, n, g in (
      ("uno", "zio"  , it.M),
      ("una", "zia"  , it.F),
      ("un" , "amico", it.M),
      ("un'", "amica", it.F)):
        v = it.article(n, "indefinite", gender=g)
        self.assertEqual(a, v)
    v = it.referenced("amica", gender="f")
    self.assertEqual(v, "un'amica")
    print("pattern.it.article()")
    print("pattern.it.referenced()")

</t>
<t tx="karstenw.20230303140211.5">def test_gender(self):
    # Assert the accuracy of the gender disambiguation algorithm.
    from pattern.db import Datasheet
    i, n = 0, 0
    for pos, sg, pl, mf in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-it-wiktionary.csv")):
        g = it.gender(sg)
        if mf in g and it.PLURAL not in g:
            i += 1
        g = it.gender(pl)
        if mf in g and it.PLURAL in g:
            i += 1
        n += 2
    self.assertTrue(float(i) / n &gt; 0.92)
    print("pattern.it.gender()")

</t>
<t tx="karstenw.20230303140211.6">def test_pluralize(self):
    # Assert the accuracy of the pluralization algorithm.
    from pattern.db import Datasheet
    i, n = 0, 0
    for pos, sg, pl, mf in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-it-wiktionary.csv")):
        if it.pluralize(sg) == pl:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.93)
    print("pattern.it.pluralize()")

</t>
<t tx="karstenw.20230303140211.7">def test_singularize(self):
    # Assert the accuracy of the singularization algorithm.
    from pattern.db import Datasheet
    i, n = 0, 0
    for pos, sg, pl, mf in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-it-wiktionary.csv")):
        if it.singularize(pl) == sg:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.84)
    print("pattern.it.singularize()")

</t>
<t tx="karstenw.20230303140211.8">def test_predicative(self):
    # Assert the accuracy of the predicative algorithm ("cruciali" =&gt; "cruciale").

    from pattern.db import Datasheet
    i, n = 0, 0
    for pos, sg, pl, mf in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-it-wiktionary.csv")):
        if pos != "j":
            continue
        if it.predicative(pl) == sg:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.87)
    print("pattern.it.predicative()")

</t>
<t tx="karstenw.20230303140211.9">def test_find_lemma(self):
    # Assert the accuracy of the verb lemmatization algorithm.
    i, n = 0, 0
    r = 0
    for v1, v2 in it.inflect.verbs.inflections.items():
        if it.inflect.verbs.find_lemma(v1) == v2:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.81)
    print("pattern.it.inflect.verbs.find_lemma()")

</t>
<t tx="karstenw.20230303140212.1">from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
import subprocess

from pattern import nl

from io import open

try:
    PATH = os.path.dirname(os.path.realpath(__file__))
except:
    PATH = ""

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140212.10">def test_conjugate(self):
    # Assert different tenses with different conjugations.
    for (v1, v2, tense) in (
      ("zijn",  "zijn",     nl.INFINITIVE),
      ("zijn",  "ben",     (nl.PRESENT, 1, nl.SINGULAR)),
      ("zijn",  "bent",    (nl.PRESENT, 2, nl.SINGULAR)),
      ("zijn",  "is",      (nl.PRESENT, 3, nl.SINGULAR)),
      ("zijn",  "zijn",    (nl.PRESENT, 0, nl.PLURAL)),
      ("zijn",  "zijnd",   (nl.PRESENT + nl.PARTICIPLE,)),
      ("zijn",  "was",     (nl.PAST, 1, nl.SINGULAR)),
      ("zijn",  "was",     (nl.PAST, 2, nl.SINGULAR)),
      ("zijn",  "was",     (nl.PAST, 3, nl.SINGULAR)),
      ("zijn",  "waren",   (nl.PAST, 0, nl.PLURAL)),
      ("zijn",  "was",     (nl.PAST, 0, None)),
      ("zijn",  "geweest", (nl.PAST + nl.PARTICIPLE,)),
      ("had",   "hebben",   "inf"),
      ("had",   "heb",      "1sg"),
      ("had",   "hebt",     "2sg"),
      ("had",   "heeft",    "3sg"),
      ("had",   "hebben",   "pl"),
      ("had",   "hebbend",  "part"),
      ("heeft", "had",      "1sgp"),
      ("heeft", "had",      "2sgp"),
      ("heeft", "had",      "3sgp"),
      ("heeft", "hadden",   "ppl"),
      ("heeft", "had",      "p"),
      ("heeft", "gehad",    "ppart"),
      ("smsen", "smste",    "3sgp")):
        self.assertEqual(nl.conjugate(v1, tense), v2)
    print("pattern.nl.conjugate()")

</t>
<t tx="karstenw.20230303140212.11">def test_lexeme(self):
    # Assert all inflections of "zijn".
    v = nl.lexeme("zijn")
    self.assertEqual(v, [
        "zijn", "ben", "bent", "is", "zijnd", "waren", "was", "geweest"
    ])
    print("pattern.nl.inflect.lexeme()")

</t>
<t tx="karstenw.20230303140212.12">def test_tenses(self):
    # Assert tense recognition.
    self.assertTrue((nl.PRESENT, 3, "sg") in nl.tenses("is"))
    self.assertTrue("3sg" in nl.tenses("is"))
    print("pattern.nl.tenses()")

</t>
<t tx="karstenw.20230303140212.13">class TestParser(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140212.14">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140212.15">def test_wotan2penntreebank(self):
    # Assert tag translation.
    for penntreebank, wotan in (
      ("NNP",  "N(eigen,ev,neut)"),
      ("NNPS", "N(eigen,mv,neut)"),
      ("NN",   "N(soort,ev,neut)"),
      ("NNS",  "N(soort,mv,neut)"),
      ("VBZ",  "V(refl,ott,3,ev)"),
      ("VBP",  "V(intrans,ott,1_of_2_of_3,mv)"),
      ("VBD",  "V(trans,ovt,1_of_2_of_3,mv)"),
      ("VBN",  "V(trans,verl_dw,onverv)"),
      ("VBG",  "V(intrans,teg_dw,onverv)"),
      ("VB",   "V(intrans,inf)"),
      ("MD",   "V(hulp_of_kopp,ott,3,ev)"),
      ("JJ",   "Adj(attr,stell,onverv)"),
      ("JJR",  "Adj(adv,vergr,onverv)"),
      ("JJS",  "Adj(attr,overtr,verv_neut)"),
      ("RP",   "Adv(deel_v)"),
      ("RB",   "Adv(gew,geen_func,stell,onverv)"),
      ("DT",   "Art(bep,zijd_of_mv,neut)"),
      ("CC",   "Conj(neven)"),
      ("CD",   "Num(hoofd,bep,zelfst,onverv)"),
      ("TO",   "Prep(voor_inf)"),
      ("IN",   "Prep(voor)"),
      ("PRP",  "Pron(onbep,neut,attr)"),
      ("PRP$", "Pron(bez,2,ev,neut,attr)"),
      (",",    "Punc(komma)"),
      ("(",    "Punc(haak_open)"),
      (")",    "Punc(haak_sluit)"),
      (".",    "Punc(punt)"),
      ("UH",   "Int"),
      ("SYM",  "Misc(symbool)")):
        self.assertEqual(nl.wotan2penntreebank("", wotan)[1], penntreebank)
    print("pattern.nl.wotan2penntreebank()")

</t>
<t tx="karstenw.20230303140212.16">def test_find_lemmata(self):
    # Assert lemmata for nouns and verbs.
    v = nl.parser.find_lemmata([["katten", "NNS"], ["droegen", "VBD"], ["hoeden", "NNS"]])
    self.assertEqual(v, [
        ["katten", "NNS", "kat"],
        ["droegen", "VBD", "dragen"],
        ["hoeden", "NNS", "hoed"]])
    print("pattern.nl.parser.find_lemmata()")

</t>
<t tx="karstenw.20230303140212.17">def test_parse(self):
    # Assert parsed output with Penn Treebank II tags (slash-formatted).
    # 1) "de zwarte kat" is a noun phrase, "op de mat" is a prepositional noun phrase.
    v = nl.parser.parse("De zwarte kat zat op de mat.")
    self.assertEqual(v,
        "De/DT/B-NP/O zwarte/JJ/I-NP/O kat/NN/I-NP/O " + \
        "zat/VBD/B-VP/O " + \
        "op/IN/B-PP/B-PNP de/DT/B-NP/I-PNP mat/NN/I-NP/I-PNP ././O/O"
    )
    # 2) "jaagt" and "vogels" lemmata are "jagen" and "vogel".
    v = nl.parser.parse("De zwarte kat jaagt op vogels.", lemmata=True)
    self.assertEqual(v,
        "De/DT/B-NP/O/de zwarte/JJ/I-NP/O/zwart kat/NN/I-NP/O/kat " + \
        "jaagt/VBZ/B-VP/O/jagen " + \
        "op/IN/B-PP/B-PNP/op vogels/NNS/B-NP/I-PNP/vogel ././O/O/."
    )
    # Assert the accuracy of the Dutch tagger.
    i, n = 0, 0
    for sentence in open(os.path.join(PATH, "corpora", "tagged-nl-twnc.txt")).readlines():
        sentence = sentence.strip()
        s1 = [w.split("/") for w in sentence.split(" ")]
        s1 = [nl.wotan2penntreebank(w, tag) for w, tag in s1]
        s2 = [[w for w, pos in s1]]
        s2 = nl.parse(s2, tokenize=False)
        s2 = [w.split("/") for w in s2.split(" ")]
        for j in range(len(s1)):
            if s1[j][1] == s2[j][1]:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.90)
    print("pattern.nl.parser.parse()")

</t>
<t tx="karstenw.20230303140212.2">class TestInflection(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140212.3">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140212.4">def test_pluralize(self):
    # Assert "auto's" as plural of "auto".
    self.assertEqual("auto's", nl.inflect.pluralize("auto"))
    # Assert the accuracy of the pluralization algorithm.
    from pattern.db import Datasheet
    i, n = 0, 0
    for pred, attr, sg, pl in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-nl-celex.csv")):
        if nl.pluralize(sg) == pl:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.74)
    print("pattern.nl.pluralize()")

</t>
<t tx="karstenw.20230303140212.5">def test_singularize(self):
    # Assert the accuracy of the singularization algorithm.
    from pattern.db import Datasheet
    i, n = 0, 0
    for pred, attr, sg, pl in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-nl-celex.csv")):
        if nl.singularize(pl) == sg:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.88)
    print("pattern.nl.singularize()")

</t>
<t tx="karstenw.20230303140212.6">def test_attributive(self):
    # Assert the accuracy of the attributive algorithm ("fel" =&gt; "felle").
    from pattern.db import Datasheet
    i, n = 0, 0
    for pred, attr, sg, pl in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-nl-celex.csv")):
        if nl.attributive(pred) == attr:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.96)
    print("pattern.nl.attributive()")

</t>
<t tx="karstenw.20230303140212.7">def test_predicative(self):
    # Assert the accuracy of the predicative algorithm ("felle" =&gt; "fel").
    from pattern.db import Datasheet
    i, n = 0, 0
    for pred, attr, sg, pl in Datasheet.load(os.path.join(PATH, "corpora", "wordforms-nl-celex.csv")):
        if nl.predicative(attr) == pred:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.96)
    print("pattern.nl.predicative()")

</t>
<t tx="karstenw.20230303140212.8">def test_find_lemma(self):
    # Assert the accuracy of the verb lemmatization algorithm.
    # Note: the accuracy is higher (90%) when measured on CELEX word forms
    # (presumably because nl.inflect.verbs has high percentage irregular verbs).
    i, n = 0, 0
    for v1, v2 in nl.inflect.verbs.inflections.items():
        if nl.inflect.verbs.find_lemma(v1) == v2:
            i += 1
        n += 1
    self.assertTrue(float(i) / n &gt; 0.83)
    print("pattern.nl.inflect.verbs.find_lemma()")

</t>
<t tx="karstenw.20230303140212.9">def test_find_lexeme(self):
    # Assert the accuracy of the verb conjugation algorithm.
    i, n = 0, 0
    for v, lexeme1 in nl.inflect.verbs.infinitives.items():
        lexeme2 = nl.inflect.verbs.find_lexeme(v)
        for j in range(len(lexeme2)):
            if lexeme1[j] == lexeme2[j] or \
               lexeme1[j] == "" and \
               lexeme1[j &gt; 5 and 10 or 0] == lexeme2[j]:
                i += 1
            n += 1
    self.assertTrue(float(i) / n &gt; 0.79)
    print("pattern.nl.inflect.verbs.find_lexeme()")

</t>
<t tx="karstenw.20230303140213.1">def test_tag(self):
    # Assert [("zwarte", "JJ"), ("panters", "NNS")].
    v = nl.tag("zwarte panters")
    self.assertEqual(v, [("zwarte", "JJ"), ("panters", "NNS")])
    print("pattern.nl.tag()")

</t>
<t tx="karstenw.20230303140213.2">def test_command_line(self):
    # Assert parsed output from the command-line (example from the documentation).
    p = ["python", "-m", "pattern.nl", "-s", "Leuke kat.", "-OTCRL"]
    p = subprocess.Popen(p, stdout=subprocess.PIPE)
    p.wait()
    v = p.stdout.read().decode('utf-8')
    v = v.strip()
    self.assertEqual(v, "Leuke/JJ/B-NP/O/O/leuk kat/NN/I-NP/O/O/kat ././O/O/O/.")
    print("python -m pattern.nl")

</t>
<t tx="karstenw.20230303140213.3">class TestSentiment(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140213.4">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140213.5">def test_sentiment(self):
    # Assert &lt; 0 for negative adjectives and &gt; 0 for positive adjectives.
    self.assertTrue(nl.sentiment("geweldig")[0] &gt; 0)
    self.assertTrue(nl.sentiment("verschrikkelijk")[0] &lt; 0)
    # Assert the accuracy of the sentiment analysis.
    # Given are the scores for 3,000 book reviews.
    # The baseline should increase (not decrease) when the algorithm is modified.
    from pattern.db import Datasheet
    from pattern.metrics import test
    reviews = []
    for score, review in Datasheet.load(os.path.join(PATH, "corpora", "polarity-nl-bol.com.csv")):
        reviews.append((review, int(score) &gt; 0))
    A, P, R, F = test(lambda review: nl.positive(review), reviews)
    #print(A, P, R, F)
    self.assertTrue(A &gt; 0.808)
    self.assertTrue(P &gt; 0.780)
    self.assertTrue(R &gt; 0.860)
    self.assertTrue(F &gt; 0.818)
    print("pattern.nl.sentiment()")

</t>
<t tx="karstenw.20230303140213.6">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestInflection))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestParser))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSentiment))
    return suite

</t>
<t tx="karstenw.20230303140214.1">from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
import random
import subprocess

from pattern import text
from pattern import ru

from io import open

try:
    PATH = os.path.dirname(os.path.realpath(__file__))
except:
    PATH = ""

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140214.2">class TestSpelling(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------

</t>
<t tx="karstenw.20230303140214.3">def test_spelling(self):
    i = j = 0.0
    from pattern.db import Datasheet
    for correct, wrong in Datasheet.load(os.path.join(PATH, "corpora", "spelling-ru.csv")):
        for w in wrong.split(" "):
            suggested = ru.suggest(w)
            if suggested[0][0] == correct:
                i += 1
            else:
                j += 1
    self.assertTrue(i / (i + j) &gt; 0.65)
    print("pattern.ru.suggest()")

</t>
<t tx="karstenw.20230303140214.4">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSpelling))
    return suite

</t>
<t tx="karstenw.20230303140216.1">from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
import time
import math

from types import GeneratorType

from pattern import metrics

try:
    PATH = os.path.dirname(os.path.realpath(__file__))
except:
    PATH = ""

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140216.10">def test_agreement(self):
    # Assert 0.210 (example from http://en.wikipedia.org/wiki/Fleiss'_kappa).
    m = [[0, 0, 0, 0, 14],
          [0, 2, 6, 4, 2 ],
          [0, 0, 3, 5, 6 ],
          [0, 3, 9, 2, 0 ],
          [2, 2, 8, 1, 1 ],
          [7, 7, 0, 0, 0 ],
          [3, 2, 6, 3, 0 ],
          [2, 5, 3, 2, 2 ],
          [6, 5, 2, 1, 0 ],
          [0, 2, 2, 3, 7 ]]
    v = metrics.agreement(m)
    self.assertAlmostEqual(v, 0.210, places=3)
    print("pattern.metrics.agreement()")


</t>
<t tx="karstenw.20230303140216.11">class TestTextMetrics(unittest.TestCase):

    @others
</t>
<t tx="karstenw.20230303140216.12">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140216.13">def test_levenshtein(self):
    # Assert 0 (identical strings).
    v = metrics.levenshtein("gallahad", "gallahad")
    self.assertEqual(v, 0)
    # Assert 3 (1 insert, 1 delete, 1 replace).
    v = metrics.levenshtein("gallahad", "_g_llaha")
    self.assertEqual(v, 3)
    print("pattern.metrics.levenshtein()")

</t>
<t tx="karstenw.20230303140216.14">def test_levenshtein_similarity(self):
    # Assert 1.0 (identical strings).
    v = metrics.levenshtein_similarity("gallahad", "gallahad")
    self.assertEqual(v, 1.0)
    # Assert 0.75 (2 out of 8 characters differ).
    v = metrics.levenshtein_similarity("gallahad", "g_ll_had")
    self.assertEqual(v, 0.75)
    print("pattern.metrics.levenshtein_similarity()")

</t>
<t tx="karstenw.20230303140216.15">def test_dice_coefficient(self):
    # Assert 1.0 (identical strings).
    v = metrics.dice_coefficient("gallahad", "gallahad")
    self.assertEqual(v, 1.0)
    # Assert 0.25 (example from http://en.wikipedia.org/wiki/Dice_coefficient).
    v = metrics.dice_coefficient("night", "nacht")
    self.assertEqual(v, 0.25)
    print("pattern.metrics.dice_coefficient()")

</t>
<t tx="karstenw.20230303140216.16">def test_similarity(self):
    self.assertEqual(
        metrics.levenshtein_similarity("night", "nacht"),
        metrics.similarity("night", "nacht", metrics.LEVENSHTEIN))
    self.assertEqual(
        metrics.dice_coefficient("night", "nacht"),
        metrics.similarity("night", "nacht", metrics.DICE))
    print("pattern.metrics.similarity()")

</t>
<t tx="karstenw.20230303140216.17">def test_readability(self):
    # Assert that technical jargon is in the "difficult" range (&lt; 0.30).
    s = "The Australian platypus is seemingly a hybrid of a mammal and reptilian creature."
    v = metrics.readability(s)
    self.assertTrue(v &lt; 0.30)
    # Assert that Dr. Seuss is in the "easy" range (&gt; 0.70).
    s = "'I know some good games we could play,' said the cat. " + \
        "'I know some new tricks,' said the cat in the hat. " + \
        "'A lot of good tricks. I will show them to you.' " + \
        "'Your mother will not mind at all if I do.'"
    v = metrics.readability(s)
    self.assertTrue(v &gt; 0.70)
    print("pattern.metrics.readability()")

</t>
<t tx="karstenw.20230303140216.18">def test_intertextuality(self):
    # Evaluate accuracy for plagiarism detection.
    from pattern.db import Datasheet
    data = Datasheet.load(os.path.join(PATH, "corpora", "plagiarism-clough&amp;stevenson.csv"))
    data = [((txt, src), int(plagiarism) &gt; 0) for txt, src, plagiarism in data]

    def plagiarism(txt, src):
        return metrics.intertextuality([txt, src], n=3)[0, 1] &gt; 0.05
    A, P, R, F = metrics.test(lambda x: plagiarism(*x), data)
    self.assertTrue(P &gt; 0.96)
    self.assertTrue(R &gt; 0.94)
    print("pattern.metrics.intertextuality()")

</t>
<t tx="karstenw.20230303140216.19">def test_ttr(self):
    # Assert type-token ratio: words = 7, unique words = 6.
    s = "The black cat \n sat on the mat."
    v = metrics.ttr(s)
    self.assertAlmostEqual(v, 0.86, places=2)
    print("pattern.metrics.ttr()")

</t>
<t tx="karstenw.20230303140216.2">class TestProfiling(unittest.TestCase):

    @others
</t>
<t tx="karstenw.20230303140216.20">def test_suffixes(self):
    # Assert base =&gt; inflected and reversed inflected =&gt; base suffixes.
    s = [("beau", "beaux"), ("jeune", "jeunes"), ("hautain", "hautaines")]
    v = metrics.suffixes(s, n=3)
    self.assertEqual(v, [
        (2, "nes", [("ne", 0.5), ("n", 0.5)]),
        (1, "aux", [("au", 1.0)])])
    v = metrics.suffixes(s, n=2, reverse=False)
    self.assertEqual(v, [
        (1, "ne", [("nes", 1.0)]),
        (1, "in", [("ines", 1.0)]),
        (1, "au", [("aux", 1.0)])])
    print("pattern.metrics.suffixes()")

</t>
<t tx="karstenw.20230303140216.21">def test_isplit(self):
    # Assert string.split() iterator.
    v = metrics.isplit("test\nisplit")
    self.assertTrue(isinstance(v, GeneratorType))
    self.assertEqual(list(v), ["test", "isplit"])
    print("pattern.metrics.isplit()")

</t>
<t tx="karstenw.20230303140216.22">def test_cooccurrence(self):
    s = "The black cat sat on the mat."
    v = metrics.cooccurrence(s, window=(-1, 1),
            term1 = lambda w: w in ("cat",),
        normalize = lambda w: w.lower().strip(".:;,!?()[]'\""))
    self.assertEqual(sorted(v.keys()), ["cat"])
    self.assertEqual(sorted(v["cat"].keys()), ["black", "cat", "sat"])
    self.assertEqual(sorted(v["cat"].values()), [1, 1, 1])
    s = [("The", "DT"), ("black", "JJ"), ("cat", "NN"), ("sat", "VB"), ("on", "IN"), ("the", "DT"), ("mat", "NN")]
    v = metrics.co_occurrence(s, window=(-2, -1),
         term1 = lambda token: token[1].startswith("NN"),
         term2 = lambda token: token[1].startswith("JJ"))
    self.assertEqual(v, {("cat", "NN"): {("black", "JJ"): 1}})
    print("pattern.metrics.cooccurrence()")


</t>
<t tx="karstenw.20230303140216.23">class TestInterpolation(unittest.TestCase):

    @others
</t>
<t tx="karstenw.20230303140216.24">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140216.25">def test_lerp(self):
    # Assert linear interpolation.
    v = metrics.lerp(100, 200, 0.5)
    self.assertEqual(v, 150.0)
    print("pattern.metrics.lerp()")

</t>
<t tx="karstenw.20230303140216.26">def test_smoothstep(self):
    # Assert cubic interpolation.
    v1 = metrics.smoothstep(0.0, 1.0, 0.5)
    v2 = metrics.smoothstep(0.0, 1.0, 0.9)
    v3 = metrics.smoothstep(0.0, 1.0, 0.1)
    self.assertEqual(v1, 0.5)
    self.assertTrue(v2 &gt; 0.9)
    self.assertTrue(v3 &lt; 0.1)
    print("pattern.metrics.smoothstep()")

</t>
<t tx="karstenw.20230303140216.27">def test_smoothrange(self):
    # Assert nice ranges for line charts.
    v = list(metrics.smoothrange(0.0, 1.0))
    [self.assertAlmostEqual(x, y, places=1) for x, y in zip(v,
        [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])]
    v = list(metrics.smoothrange(-2, 2))
    [self.assertAlmostEqual(x, y, places=1) for x, y in zip(v,
        [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0])]
    v = list(metrics.smoothrange(1, 13))
    [self.assertAlmostEqual(x, y, places=1) for x, y in zip(v,
        [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0])]
    print("pattern.metrics.smoothrange()")


</t>
<t tx="karstenw.20230303140216.28">class TestStatistics(unittest.TestCase):

    @others
</t>
<t tx="karstenw.20230303140216.29">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140216.3">def setUp(self):
    # Test set for accuracy, precision and recall:
    self.documents = (
        (None, True),
        (None, True),
        (None, False)
    )

</t>
<t tx="karstenw.20230303140216.30">def test_mean(self):
    # Assert (1+2+3+4) / 4 = 2.5.
    v = metrics.mean([1, 2, 3, 4])
    self.assertEqual(v, 2.5)
    print("pattern.metrics.mean()")

</t>
<t tx="karstenw.20230303140216.31">def test_median(self):
    # Assert 2.5 (between 2 and 3).
    v = metrics.median([1, 2, 3, 4])
    self.assertEqual(v, 2.5)
    # Assert 3 (middle of list).
    v = metrics.median([1, 2, 3, 4, 5])
    self.assertEqual(v, 3)
    # Assert that empty list raises ValueError.
    self.assertRaises(ValueError, metrics.median, [])
    print("pattern.metrics.median()")

</t>
<t tx="karstenw.20230303140216.32">def test_variance(self):
    # Assert 2.5.
    v = metrics.variance([1, 2, 3, 4, 5], sample=True)
    self.assertEqual(v, 2.5)
    # Assert 2.0 (population variance).
    v = metrics.variance([1, 2, 3, 4, 5], sample=False)
    self.assertEqual(v, 2.0)
    print("pattern.metrics.variance()")

</t>
<t tx="karstenw.20230303140216.33">def test_standard_deviation(self):
    # Assert 2.429 (sample).
    v = metrics.standard_deviation([1, 5, 6, 7, 6, 8], sample=True)
    self.assertAlmostEqual(v, 2.429, places=3)
    # Assert 2.217 (population).
    v = metrics.standard_deviation([1, 5, 6, 7, 6, 8], sample=False)
    self.assertAlmostEqual(v, 2.217, places=3)
    print("pattern.metrics.standard_deviation()")

</t>
<t tx="karstenw.20230303140216.34">def test_histogram(self):
    # Assert 1 bin.
    v = metrics.histogram([1, 2, 3, 4], k=0)
    self.assertTrue(len(v) == 1)
    # Assert 4 bins, each with one value, each with midpoint == value.
    v = metrics.histogram([1, 2, 3, 4], k=4, range=(0.5, 4.5))
    for i, ((start, stop), v) in enumerate(sorted(v.items())):
        self.assertTrue(i + 1 == v[0])
        self.assertAlmostEqual(start + (stop - start) / 2, i + 1, places=3)
    # Assert 2 bins, one with all the low numbers, one with the high number.
    v = metrics.histogram([1, 2, 3, 4, 100], k=2)
    v = sorted(v.values(), key=lambda item: len(item))
    self.assertTrue(v[0] == [100])
    self.assertTrue(v[1] == [1, 2, 3, 4])
    print("pattern.metrics.histogram()")

</t>
<t tx="karstenw.20230303140216.35">def test_moment(self):
    # Assert 0.0 (1st central moment = 0.0).
    v = metrics.moment([1, 2, 3, 4, 5], n=1)
    self.assertEqual(v, 0.0)
    # Assert 2.0 (2nd central moment = population variance).
    v = metrics.moment([1, 2, 3, 4, 5], n=2)
    self.assertEqual(v, 2.0)
    print("pattern.metrics.moment()")

</t>
<t tx="karstenw.20230303140216.36">def test_skewness(self):
    # Assert &lt; 0.0 (few low values).
    v = metrics.skewness([1, 100, 101, 102, 103])
    self.assertTrue(v &lt; 0.0)
    # Assert &gt; 0.0 (few high values).
    v = metrics.skewness([1, 2, 3, 4, 100])
    self.assertTrue(v &gt; 0.0)
    # Assert 0.0 (evenly distributed).
    v = metrics.skewness([1, 2, 3, 4])
    self.assertTrue(v == 0.0)
    print("pattern.metrics.skewness()")

</t>
<t tx="karstenw.20230303140216.37">def test_kurtosis(self):
    # Assert -1.2 for the uniform distribution.
    a = 1
    b = 1000
    v = metrics.kurtosis([float(i - a) / (b - a) for i in range(a, b)])
    self.assertAlmostEqual(v, -1.2, places=3)
    print("pattern.metrics.kurtosis()")

</t>
<t tx="karstenw.20230303140216.38">def test_quantile(self):
    # Assert 2.5 (quantile with p=0.5 == median).
    v = metrics.quantile([1, 2, 3, 4], p=0.5, a=1, b=-1, c=0, d=1)
    self.assertEqual(v, 2.5)
    # Assert 3.0 (discontinuous sample).
    v = metrics.quantile([1, 2, 3, 4], p=0.5, a=0.5, b=0, c=1, d=0)
    self.assertEqual(v, 3.0)
    return "pattern.metrics.quantile()"

</t>
<t tx="karstenw.20230303140216.39">def test_boxplot(self):
    # Different a,b,c,d quantile parameters produce different results.
    # By approximation, assert (53, 79.5, 84.5, 92, 98).
    a = [79, 53, 82, 91, 87, 98, 80, 93]
    v = metrics.boxplot(a)
    self.assertEqual(v[0], min(a))
    self.assertTrue(abs(v[1] - 79.5) &lt;= 0.5)
    self.assertTrue(abs(v[2] - metrics.median(a)) &lt;= 0.5)
    self.assertTrue(abs(v[3] - 92.0) &lt;= 0.5)
    self.assertEqual(v[4], max(a))
    print("pattern.metrics.boxplot()")


</t>
<t tx="karstenw.20230303140216.4">def test_duration(self):
    # Assert 0.1 or slightly higher.
    v = metrics.duration(time.sleep, 0.1)
    self.assertTrue(v &gt; 0.1)
    print("pattern.metrics.duration()")

</t>
<t tx="karstenw.20230303140216.40">class TestStatisticalTests(unittest.TestCase):

    @others
</t>
<t tx="karstenw.20230303140216.41">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140216.42">def test_fisher_test(self):
    # Assert Fisher exact test significance.
    v = metrics.fisher_exact_test(a=1, b=9, c=11, d=3)
    self.assertAlmostEqual(v, 0.0028, places=4)
    v = metrics.fisher_exact_test(a=45, b=15, c=75, d=45)
    self.assertAlmostEqual(v, 0.1307, places=4)
    print("pattern.metrics.fisher_test()")

</t>
<t tx="karstenw.20230303140216.43">def test_chi_squared(self):
    # Assert chi-squared test (upper tail).
    o1, e1 = [[44, 56]], [[50, 50]]
    o2, e2 = [[22, 21, 22, 27, 22, 36]], []
    o3, e3 = [[48, 35, 15, 3]], [[58, 34.5, 7, 0.5]]
    o4, e4 = [[36, 14], [30, 25]], []
    o5, e5 = [[46, 71], [37, 83]], [[40.97, 76.02], [42.03, 77.97]]
    v1 = metrics.chi2(o1, e1)
    v2 = metrics.chi2(o2, e2)
    v3 = metrics.chi2(o3, e3)
    v4 = metrics.chi2(o4, e4)
    v5 = metrics.chi2(o5, e5)
    self.assertAlmostEqual(v1[0], 1.4400, places=4)
    self.assertAlmostEqual(v1[1], 0.2301, places=4)
    self.assertAlmostEqual(v2[0], 6.7200, places=4)
    self.assertAlmostEqual(v2[1], 0.2423, places=4)
    self.assertAlmostEqual(v3[0], 23.3742, places=4)
    self.assertAlmostEqual(v4[0], 3.4177, places=4)
    self.assertAlmostEqual(v5[0], 1.8755, places=4)
    print("pattern.metrics.chi2()")

</t>
<t tx="karstenw.20230303140216.44">def test_chi_squared_p(self):
    # Assert chi-squared P-value (upper tail).
    for df, X2 in [
      (1, (3.85, 5.05, 6.65, 7.90)),
      (2, (6.00, 7.40, 9.25, 10.65)),
      (3, (7.85, 9.40, 11.35, 12.85)),
      (4, (9.50, 11.15, 13.30, 14.90)),
      (5, (11.10, 12.85, 15.10, 16.80))]:
        for i, x2 in enumerate(X2):
            v = metrics.chi2p(x2, df, tail=metrics.UPPER)
            self.assertTrue(v &lt; (0.05, 0.025, 0.01, 0.005)[i])
    print("pattern.metrics.chi2p()")

</t>
<t tx="karstenw.20230303140216.45">def test_kolmogorov_smirnov(self):
    v = metrics.ks2([1, 2, 3], [1, 2, 4])
    self.assertAlmostEqual(v[0], 0.3333, places=4)
    self.assertAlmostEqual(v[1], 0.9762, places=4)
    print("pattern.metrics.ks2()")


</t>
<t tx="karstenw.20230303140216.46">class TestSpecialFunctions(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140216.47">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140216.48">def test_gamma(self):
    # Assert complete gamma function.
    v = metrics.gamma(0.5)
    self.assertAlmostEqual(v, math.sqrt(math.pi), places=4)
    print("pattern.metrics.gamma()")

</t>
<t tx="karstenw.20230303140216.49">def test_gammai(self):
    # Assert incomplete gamma function.
    v = metrics.gammai(a=1, x=2)
    self.assertAlmostEqual(v, 0.1353, places=4)
    print("pattern.metrics.gammai()")

</t>
<t tx="karstenw.20230303140216.5">def test_confustion_matrix(self):
    # Assert 2 true positives (TP) and 1 false positive (FP).
    v = metrics.confusion_matrix(lambda document: True, self.documents)
    self.assertEqual(v, (2, 0, 1, 0))
    # Assert 1 true negative (TN) and 2 false negatives (FN).
    v = metrics.confusion_matrix(lambda document: False, self.documents)
    self.assertEqual(v, (0, 1, 0, 2))
    print("pattern.metrics.confusion_matrix()")

</t>
<t tx="karstenw.20230303140216.50">def test_erfc(self):
    # Assert complementary error function.
    for x, y in [
      (-3.00, 2.000),
      (-2.00, 1.995),
      (-1.00, 1.843),
      (-0.50, 1.520),
      (-0.25, 1.276),
      ( 0.00, 1.000),
      ( 0.25, 0.724),
      ( 0.50, 0.480),
      ( 1.00, 0.157),
      ( 2.00, 0.005),
      ( 3.00, 0.000)]:
        self.assertAlmostEqual(metrics.erfc(x), y, places=3)
    print("pattern.metrics.erfc()")

</t>
<t tx="karstenw.20230303140216.51">def test_kolmogorov(self):
    # Assert Kolmogorov limit distribution.
    self.assertAlmostEqual(metrics.kolmogorov(0.0), 1.0000, places=4)
    self.assertAlmostEqual(metrics.kolmogorov(0.5), 0.9639, places=4)
    self.assertAlmostEqual(metrics.kolmogorov(1.0), 0.2700, places=4)
    self.assertAlmostEqual(metrics.kolmogorov(2.0), 0.0007, places=4)
    self.assertAlmostEqual(metrics.kolmogorov(4.0), 0.0000, places=4)
    print("pattern.metrics.kolmogorov()")

</t>
<t tx="karstenw.20230303140216.52">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestProfiling))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestTextMetrics))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestInterpolation))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestStatistics))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestStatisticalTests))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSpecialFunctions))
    return suite

</t>
<t tx="karstenw.20230303140216.6">def test_accuracy(self):
    # Assert 2.0/3.0 (two out of three correct predictions).
    v = metrics.accuracy(lambda document: True, self.documents)
    self.assertEqual(v, 2.0 / 3.0)
    print("pattern.metrics.accuracy()")

</t>
<t tx="karstenw.20230303140216.7">def test_precision(self):
    # Assert 2.0/3.0 (2 TP, 1 FP).
    v = metrics.precision(lambda document: True, self.documents)
    self.assertEqual(v, 2.0 / 3.0)
    # Assert 0.0 (no TP).
    v = metrics.precision(lambda document: False, self.documents)
    self.assertEqual(v, 0.0)
    print("pattern.metrics.precision()")

</t>
<t tx="karstenw.20230303140216.8">def test_recall(self):
    # Assert 1.0 (no FN).
    v = metrics.recall(lambda document: True, self.documents)
    self.assertEqual(v, 1.0)
    # Assert 0.0 (no TP).
    v = metrics.recall(lambda document: False, self.documents)
    self.assertEqual(v, 0.0)
    print("pattern.metrics.recall()")

</t>
<t tx="karstenw.20230303140216.9">def test_F1(self):
    # Assert 0.8 (F1 for precision=2/3 and recall=1).
    v = metrics.F1(lambda document: True, self.documents)
    self.assertEqual(v, 0.8)
    self.assertEqual(v, metrics.F(lambda document: True, self.documents, beta=1))
    print("pattern.metrics.F1()")

</t>
<t tx="karstenw.20230303140218.1">from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest

from pattern import graph
from pattern.graph import commonsense

from builtins import str, bytes, int, dict
from builtins import map, zip, filter
from builtins import object, range

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140218.10">def test_edge(self):
    # Assert node edges.
    n1 = self.g["a"]
    n2 = self.g["b"]
    self.assertTrue(n1.edges[0].node1.id == "a")
    self.assertTrue(n1.edges[0].node2.id == "b")
    self.assertTrue(n1.links[0].id == "b")
    self.assertTrue(n1.links[0] == self.g.edges[0].node2)
    self.assertTrue(n1.links.edge("b") == self.g.edges[0])
    self.assertTrue(n1.links.edge(n2) == self.g.edges[0])
    print("pattern.graph.Node.links")
    print("pattern.graph.Node.edges")

</t>
<t tx="karstenw.20230303140218.11">def test_flatten(self):
    # Assert node spreading activation.
    n = self.g["a"]
    self.assertTrue(set(n.flatten(depth=0)) == set([n]))
    self.assertTrue(set(n.flatten(depth=1)) == set([n, n.links[0]]))
    self.assertTrue(set(n.flatten(depth=2)) == set(self.g.nodes))
    print("pattern.graph.Node.flatten()")

</t>
<t tx="karstenw.20230303140218.12">def test_text(self):
    n = self.g.add_node("d", text=None)
    self.assertTrue(n.text is None)
    print("pattern.graph.Node.text")

</t>
<t tx="karstenw.20230303140218.13">class TestEdge(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140218.14">def setUp(self):
    # Create test graph.
    self.g = graph.Graph()
    self.g.add_node("a")
    self.g.add_node("b")
    self.g.add_edge("a", "b", weight=0.0, length=1.0, type="is-a", stroke=(0, 0, 0, 1), strokewidth=1)

</t>
<t tx="karstenw.20230303140218.15">def test_edge(self):
    # Assert edge properties.
    e = self.g.edges[0]
    self.assertTrue(isinstance(e, graph.Edge))
    self.assertTrue(e.node1 == self.g["a"])
    self.assertTrue(e.node2 == self.g["b"])
    self.assertTrue(e.weight == 0.0)
    self.assertTrue(e.length == 1.0)
    self.assertTrue(e.type == "is-a")
    self.assertTrue(e.stroke == (0, 0, 0, 1))
    self.assertTrue(e.strokewidth == 1)
    print("pattern.graph.Edge")

</t>
<t tx="karstenw.20230303140218.16">class TestGraph(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140218.17">def setUp(self):
    # Create test graph.
    self.g = graph.Graph(layout=graph.SPRING, distance=10.0)
    self.g.add_node("a")
    self.g.add_node("b")
    self.g.add_node("c")
    self.g.add_edge("a", "b")
    self.g.add_edge("b", "c")

</t>
<t tx="karstenw.20230303140218.18">def test_graph(self):
    # Assert graph properties.
    g = self.g.copy()
    self.assertTrue(len(g.nodes) == 3)
    self.assertTrue(len(g.edges) == 2)
    self.assertTrue(g.distance == 10.0)
    self.assertTrue(g.density == 2 / 3.0)
    self.assertTrue(g.is_complete == False)
    self.assertTrue(g.is_sparse == False)
    self.assertTrue(g.is_dense)
    self.assertTrue(g._adjacency is None)
    self.assertTrue(isinstance(g.layout, graph.GraphLayout))
    self.assertTrue(isinstance(g.layout, graph.GraphSpringLayout))
    print("pattern.graph.Graph")

</t>
<t tx="karstenw.20230303140218.19">def test_graph_nodes(self):
    # Assert graph nodes.
    g = self.g.copy()
    g.append(graph.Node, "d")
    g.add_node("e", base=graph.Node, root=True)
    self.assertTrue("d" in g)
    self.assertTrue("e" in g)
    self.assertTrue(g.root == g["e"])
    self.assertTrue(g["e"] == g.node("e") == g.nodes[-1])
    g.remove(g["d"])
    g.remove(g["e"])
    self.assertTrue("d" not in g)
    self.assertTrue("e" not in g)
    print("pattern.graph.Graph.add_node()")

</t>
<t tx="karstenw.20230303140218.2">class TestUtilityFunctions(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140218.20">def test_graph_edges(self):
    # Assert graph edges.
    g = self.g.copy()
    v1 = g.add_edge("d", "e") # Automatically create Node(d) and Node(e).
    v2 = g.add_edge("d", "e") # Yields existing edge.
    v3 = g.add_edge("e", "d") # Opposite direction.
    self.assertEqual(v1, v2)
    self.assertEqual(v2, g.edge("d", "e"))
    self.assertEqual(v3, g.edge("e", "d"))
    self.assertEqual(g["d"].links.edge(g["e"]), v2)
    self.assertEqual(g["e"].links.edge(g["d"]), v3)
    g.remove(g["d"])
    g.remove(g["e"])
    # Edges d-&gt;e and e-&gt;d should now be removed automatically.
    self.assertEqual(len(g.edges), 2)
    print("pattern.graph.Graph.add_edge()")

</t>
<t tx="karstenw.20230303140218.21">def test_cache(self):
    # Assert adjacency cache is flushed when nodes, edges or direction changes.
    g = self.g.copy()
    g.eigenvector_centrality()
    self.assertEqual(g._adjacency[0]["a"], {})
    self.assertEqual(g._adjacency[0]["b"]["a"], 1.0)
    g.add_node("d")
    g.add_node("e")
    self.assertEqual(g._adjacency, None)
    g.betweenness_centrality()
    self.assertEqual(g._adjacency[0]["a"]["b"], 1.0)
    self.assertEqual(g._adjacency[0]["b"]["a"], 1.0)
    g.add_edge("d", "e", weight=0.0)
    g.remove(g.node("d"))
    g.remove(g.node("e"))
    print("pattern.graph.Graph._adjacency")

</t>
<t tx="karstenw.20230303140218.22">def test_paths(self):
    # Assert node paths.
    g = self.g.copy()
    self.assertEqual(g.paths("a", "c"), g.paths(g["a"], g["c"]))
    self.assertEqual(g.paths("a", "c"), [[g["a"], g["b"], g["c"]]])
    self.assertEqual(g.paths("a", "c", length=2), [])
    # Assert node shortest paths.
    g.add_edge("a", "c")
    self.assertEqual(g.paths("a", "c", length=2), [[g["a"], g["c"]]])
    self.assertEqual(g.shortest_path("a", "c"), [g["a"], g["c"]])
    self.assertEqual(g.shortest_path("c", "a"), [g["c"], g["a"]])
    self.assertEqual(g.shortest_path("c", "a", directed=True), None)
    g.remove(g.edge("a", "c"))
    g.add_node("d")
    self.assertEqual(g.shortest_path("a", "d"), None)
    self.assertEqual(g.shortest_paths("a")["b"], [g["a"], g["b"]])
    self.assertEqual(g.shortest_paths("a")["c"], [g["a"], g["b"], g["c"]])
    self.assertEqual(g.shortest_paths("a")["d"], None)
    self.assertEqual(g.shortest_paths("c", directed=True)["a"], None)
    g.remove(g["d"])
    print("pattern.graph.Graph.paths()")
    print("pattern.graph.Graph.shortest_path()")
    print("pattern.graph.Graph.shortest_paths()")

</t>
<t tx="karstenw.20230303140218.23">def test_eigenvector_centrality(self):
    # Assert eigenvector centrality.
    self.assertEqual(self.g["a"]._weight, None)
    v = self.g.eigenvector_centrality()
    self.assertTrue(isinstance(v["a"], float))
    self.assertTrue(v["a"] == v[self.g.node("a")])
    self.assertTrue(v["a"] &lt; v["c"])
    self.assertTrue(v["b"] &lt; v["c"])
    print("pattern.graph.Graph.eigenvector_centrality()")

</t>
<t tx="karstenw.20230303140218.24">def test_betweenness_centrality(self):
    # Assert betweenness centrality.
    self.assertEqual(self.g["a"]._centrality, None)
    v = self.g.betweenness_centrality()
    self.assertTrue(isinstance(v["a"], float))
    self.assertTrue(v["a"] == v[self.g.node("a")])
    self.assertTrue(v["a"] &lt; v["b"])
    self.assertTrue(v["c"] &lt; v["b"])
    print("pattern.graph.Graph.betweenness_centrality()")

</t>
<t tx="karstenw.20230303140218.25">def test_sorted(self):
    # Assert graph node sorting
    o1 = self.g.sorted(order=graph.WEIGHT, threshold=0.0)
    o2 = self.g.sorted(order=graph.CENTRALITY, threshold=0.0)
    self.assertEqual(o1[0], self.g["c"])
    self.assertEqual(o2[0], self.g["b"])
    print("pattern.graph.Graph.sorted()")

</t>
<t tx="karstenw.20230303140218.26">def test_prune(self):
    # Assert leaf pruning.
    g = self.g.copy()
    g.prune(1)
    self.assertEqual(len(g), 1)
    self.assertEqual(g.nodes, [g["b"]])
    print("pattern.graph.Graph.prune()")

</t>
<t tx="karstenw.20230303140218.27">def test_fringe(self):
    # Assert leaf fetching.
    g = self.g.copy()
    self.assertEqual(g.fringe(0), [g["a"], g["c"]])
    self.assertEqual(g.fringe(1), [g["a"], g["b"], g["c"]])
    print("pattern.graph.Graph.fringe()")

</t>
<t tx="karstenw.20230303140218.28">def test_split(self):
    # Asset subgraph splitting.
    self.assertTrue(isinstance(self.g.split(), list))
    self.assertTrue(isinstance(self.g.split()[0], graph.Graph))
    print("pattern.graph.Graph.split()")

</t>
<t tx="karstenw.20230303140218.29">def test_update(self):
    # Assert node position after updating layout algorithm.
    self.g.update()
    for n in self.g.nodes:
        self.assertTrue(n.x != 0)
        self.assertTrue(n.y != 0)
    self.g.layout.reset()
    for n in self.g.nodes:
        self.assertTrue(n.x == 0)
        self.assertTrue(n.y == 0)
    print("pattern.graph.Graph.update()")

</t>
<t tx="karstenw.20230303140218.3">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140218.30">def test_copy(self):
    # Assert deep copy of Graph.
    g1 = self.g
    g2 = self.g.copy()
    self.assertTrue(set(g1) == set(g2))         # Same node id's.
    self.assertTrue(id(g1["a"]) != id(g2["b"])) # Different node objects.
    g3 = self.g.copy(nodes=[self.g["a"], self.g["b"]])
    g3 = self.g.copy(nodes=["a", "b"])
    self.assertTrue(len(g3.nodes), 2)
    self.assertTrue(len(g3.edges), 1)
    # Assert copy with subclasses of Node and Edge.

    class MyNode(graph.Node):
        pass

    class MyEdge(graph.Edge):
        pass
    g4 = graph.Graph()
    g4.append(MyNode, "a")
    g4.append(MyNode, "b")
    g4.append(MyEdge, "a", "b")
    g4 = g4.copy()
    self.assertTrue(isinstance(g4.nodes[0], MyNode))
    self.assertTrue(isinstance(g4.edges[0], MyEdge))
    print("pattern.graph.Graph.copy()")

</t>
<t tx="karstenw.20230303140218.31">class TestGraphLayout(unittest.TestCase):

    @others
</t>
<t tx="karstenw.20230303140218.32">def setUp(self):
    # Create test graph.
    self.g = graph.Graph(layout=graph.SPRING, distance=10.0)
    self.g.add_node("a")
    self.g.add_node("b")
    self.g.add_node("c")
    self.g.add_edge("a", "b")
    self.g.add_edge("b", "c")

</t>
<t tx="karstenw.20230303140218.33">def test_layout(self):
    # Assert GraphLayout properties.
    gl = graph.GraphLayout(graph=self.g)
    self.assertTrue(gl.graph == self.g)
    self.assertTrue(gl.bounds == (0, 0, 0, 0))
    self.assertTrue(gl.iterations == 0)
    gl.update()
    self.assertTrue(gl.iterations == 1)
    print("pattern.graph.GraphLayout")


</t>
<t tx="karstenw.20230303140218.34">class TestGraphSpringLayout(TestGraphLayout):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140218.35">def test_layout(self):
    # Assert GraphSpringLayout properties.
    gl = self.g.layout
    self.assertTrue(gl.graph == self.g)
    self.assertTrue(gl.k == 4.0)
    self.assertTrue(gl.force == 0.01)
    self.assertTrue(gl.repulsion == 50)
    self.assertTrue(gl.bounds == (0, 0, 0, 0))
    self.assertTrue(gl.iterations == 0)
    gl.update()
    self.assertTrue(gl.iterations == 1)
    self.assertTrue(gl.bounds[0] &lt; 0)
    self.assertTrue(gl.bounds[1] &lt; 0)
    self.assertTrue(gl.bounds[2] &gt; 0)
    self.assertTrue(gl.bounds[3] &gt; 0)
    print("pattern.graph.GraphSpringLayout")

</t>
<t tx="karstenw.20230303140218.36">def test_distance(self):
    # Assert 2D distance.
    n1 = graph.Node()
    n2 = graph.Node()
    n1.x = -100
    n2.x = +100
    d = self.g.layout._distance(n1, n2)
    self.assertEqual(d, (200.0, 0.0, 200.0, 40000.0))
    print("pattern.graph.GraphSpringLayout._distance")

</t>
<t tx="karstenw.20230303140218.37">def test_repulsion(self):
    # Assert repulsive node force.
    gl = self.g.layout
    d1 = gl._distance(self.g["a"], self.g["c"])[2]
    gl.update()
    d2 = gl._distance(self.g["a"], self.g["c"])[2]
    self.assertTrue(d2 &gt; d1)
    self.g.layout.reset()
    print("pattern.graph.GraphSpringLayout._repulse()")

</t>
<t tx="karstenw.20230303140218.38">def test_attraction(self):
    # Assert attractive edge force.
    gl = self.g.layout
    self.g["a"].x = -100
    self.g["b"].y = +100
    d1 = gl._distance(self.g["a"], self.g["b"])[2]
    gl.update()
    d2 = gl._distance(self.g["a"], self.g["b"])[2]
    self.assertTrue(d2 &lt; d1)
    print("pattern.graph.GraphSpringLayout._attract()")

</t>
<t tx="karstenw.20230303140218.39">class TestGraphTraversal(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140218.4">def test_deepcopy(self):
    # Object with a copy() method are responsible for deep-copying themselves.
    class MyObject(object):
        def __init__(self, i):
            self.i = i

        def copy(self):
            return MyObject(graph.deepcopy(self.i))
    # Assert deep copy for different types.
    for o1 in (
      None, True, False,
      "a",
      1, 1.0, int(1), complex(1),
      list([1]), tuple([1]), set([1]), frozenset([1]),
      dict(a=1), {frozenset(["a"]): 1}, {MyObject(1): 1},
      MyObject(1)):
        o2 = graph.deepcopy(o1)
        if isinstance(o2, (list, tuple, set, dict, MyObject)):
            self.assertTrue(id(o1) != id(o2))
    print("pattern.graph.deepcopy()")

</t>
<t tx="karstenw.20230303140218.40">def setUp(self):
    # Create test graph.
    self.g = graph.Graph()
    self.g.add_edge("a", "b", weight=0.5)
    self.g.add_edge("a", "c")
    self.g.add_edge("b", "d")
    self.g.add_edge("d", "e")
    self.g.add_node("x")

</t>
<t tx="karstenw.20230303140218.41">def test_search(self):
    # Assert depth-first vs. breadth-first search.
    def visit(node):
        a.append(node)

    def traversable(node, edge):
        if edge.node2.id == "e":
            return False
    g = self.g
    a = []
    graph.depth_first_search(g["a"], visit, traversable)
    self.assertEqual(a, [g["a"], g["b"], g["d"], g["c"]])
    a = []
    graph.breadth_first_search(g["a"], visit, traversable)
    self.assertEqual(a, [g["a"], g["b"], g["c"], g["d"]])
    print("pattern.graph.depth_first_search()")
    print("pattern.graph.breadth_first_search()")

</t>
<t tx="karstenw.20230303140218.42">def test_paths(self):
    # Assert depth-first all paths.
    g = self.g.copy()
    g.add_edge("a", "d")
    for id1, id2, length, path in (
      ("a", "a", 1, [["a"]]),
      ("a", "d", 3, [["a", "d"], ["a", "b", "d"]]),
      ("a", "d", 2, [["a", "d"]]),
      ("a", "d", 1, []),
      ("a", "x", 1, [])):
        p = graph.paths(g, id1, id2, length)
        self.assertEqual(p, path)
    print("pattern.graph.paths()")

</t>
<t tx="karstenw.20230303140218.43">def test_edges(self):
    # Assert path of nodes to edges.
    g = self.g
    p = [g["a"], g["b"], g["d"], g["x"]]
    e = list(graph.edges(p))
    self.assertEqual(e, [g.edge("a", "b"), g.edge("b", "d"), None])
    print("pattern.graph.edges()")

</t>
<t tx="karstenw.20230303140218.44">def test_adjacency(self):
    # Assert adjacency map with different settings.
    a = [
        graph.adjacency(self.g),
        graph.adjacency(self.g, directed=True),
        graph.adjacency(self.g, directed=True, reversed=True),
        graph.adjacency(self.g, stochastic=True),
        graph.adjacency(self.g, heuristic=lambda id1, id2: 0.1),
    ]
    for i in range(len(a)):
        a[i] = sorted((id1, sorted((id2, round(w, 2)) for id2, w in p.items())) for id1, p in a[i].items())
    self.assertEqual(a[0], [
        ("a", [("b", 0.75), ("c", 1.0)]),
        ("b", [("a", 0.75), ("d", 1.0)]),
        ("c", [("a", 1.0)]),
        ("d", [("b", 1.0), ("e", 1.0)]),
        ("e", [("d", 1.0)]),
        ("x", [])])
    self.assertEqual(a[1], [
        ("a", [("b", 0.75), ("c", 1.0)]),
        ("b", [("d", 1.0)]),
        ("c", []),
        ("d", [("e", 1.0)]),
        ("e", []),
        ("x", [])])
    self.assertEqual(a[2], [
        ("a", []),
        ("b", [("a", 0.75)]),
        ("c", [("a", 1.0)]),
        ("d", [("b", 1.0)]),
        ("e", [("d", 1.0)]),
        ("x", [])])
    self.assertEqual(a[3], [
        ("a", [("b", 0.43), ("c", 0.57)]),
        ("b", [("a", 0.43), ("d", 0.57)]),
        ("c", [("a", 1.0)]),
        ("d", [("b", 0.5), ("e", 0.5)]),
        ("e", [("d", 1.0)]),
        ("x", [])])
    self.assertEqual(a[4], [
        ("a", [("b", 0.85), ("c", 1.1)]),
        ("b", [("a", 0.85), ("d", 1.1)]),
        ("c", [("a", 1.1)]),
        ("d", [("b", 1.1), ("e", 1.1)]),
        ("e", [("d", 1.1)]),
        ("x", [])])
    print("pattern.graph.adjacency()")

</t>
<t tx="karstenw.20230303140218.45">def test_dijkstra_shortest_path(self):
    # Assert Dijkstra's algorithm (node1 -&gt; node2).
    g = self.g.copy()
    g.add_edge("d", "a")
    for id1, id2, heuristic, directed, path in (
      ("a", "d", None, False, ["a", "d"]),
      ("a", "d", None, True, ["a", "b", "d"]),
      ("a", "d", lambda id1, id2: id1 == "d" and id2 == "a" and 1 or 0, False, ["a", "b", "d"])):
        p = graph.dijkstra_shortest_path(g, id1, id2, heuristic, directed)
        self.assertEqual(p, path)
    print("pattern.graph.dijkstra_shortest_path()")

</t>
<t tx="karstenw.20230303140218.46">def test_dijkstra_shortest_paths(self):
    # Assert Dijkstra's algorithm (node1 -&gt; all).
    g = self.g.copy()
    g.add_edge("d", "a")
    a = [
        graph.dijkstra_shortest_paths(g, "a"),
        graph.dijkstra_shortest_paths(g, "a", directed=True),
        graph.dijkstra_shortest_paths(g, "a", heuristic=lambda id1, id2: id1 == "d" and id2 == "a" and 1 or 0)
    ]
    for i in range(len(a)):
        a[i] = sorted(a[i].items())
    self.assertEqual(a[0], [
        ("a", ["a"]),
        ("b", ["a", "b"]),
        ("c", ["a", "c"]),
        ("d", ["a", "d"]),
        ("e", ["a", "d", "e"]),
        ("x", None)])
    self.assertEqual(a[1], [
        ("a", ["a"]),
        ("b", ["a", "b"]),
        ("c", ["a", "c"]),
        ("d", ["a", "b", "d"]),
        ("e", ["a", "b", "d", "e"]),
        ("x", None)])
    self.assertEqual(a[2], [
        ("a", ["a"]),
        ("b", ["a", "b"]),
        ("c", ["a", "c"]),
        ("d", ["a", "b", "d"]),
        ("e", ["a", "b", "d", "e"]),
        ("x", None)])
    print("pattern.graph.dijkstra_shortest_paths()")

</t>
<t tx="karstenw.20230303140218.47">def test_floyd_warshall_all_pairs_distance(self):
    # Assert all pairs path distance.
    p1 = graph.floyd_warshall_all_pairs_distance(self.g)
    p2 = sorted((id1, sorted((id2, round(w, 2)) for id2, w in p.items())) for id1, p in p1.items())
    self.assertEqual(p2, [
        ("a", [("a", 0.00), ("b", 0.75), ("c", 1.00), ("d", 1.75), ("e", 2.75)]),
        ("b", [("a", 0.75), ("b", 0.00), ("c", 1.75), ("d", 1.00), ("e", 2.00)]),
        ("c", [("a", 1.00), ("b", 1.75), ("c", 2.00), ("d", 2.75), ("e", 3.75)]),
        ("d", [("a", 1.75), ("b", 1.00), ("c", 2.75), ("d", 0.00), ("e", 1.00)]),
        ("e", [("a", 2.75), ("b", 2.00), ("c", 3.75), ("d", 1.00), ("e", 2.00)]),
        ("x", [])])
    # Assert predecessor tree.
    self.assertEqual(graph.predecessor_path(p1.predecessors, "a", "d"), ["a", "b", "d"])
    print("pattern.graph.floyd_warshall_all_pairs_distance()")

</t>
<t tx="karstenw.20230303140218.48">class TestGraphPartitioning(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140218.49">def setUp(self):
    # Create test graph.
    self.g = graph.Graph()
    self.g.add_edge("a", "b", weight=0.5)
    self.g.add_edge("a", "c")
    self.g.add_edge("b", "d")
    self.g.add_edge("d", "e")
    self.g.add_edge("x", "y")
    self.g.add_node("z")

</t>
<t tx="karstenw.20230303140218.5">def test_unique(self):
    # Assert list copy with unique items.
    v = graph.unique([1, 1, 1])
    self.assertEqual(len(v), 1)
    self.assertEqual(v[0], 1)
    print("pattern.graph.unique()")

</t>
<t tx="karstenw.20230303140218.50">def test_union(self):
    self.assertEqual(graph.union([1, 2], [2, 3]), [1, 2, 3])

</t>
<t tx="karstenw.20230303140218.51">def test_intersection(self):
    self.assertEqual(graph.intersection([1, 2], [2, 3]), [2])

</t>
<t tx="karstenw.20230303140218.52">def test_difference(self):
    self.assertEqual(graph.difference([1, 2], [2, 3]), [1])

</t>
<t tx="karstenw.20230303140218.53">def test_partition(self):
    # Assert unconnected subgraph partitioning.
    g = graph.partition(self.g)
    self.assertTrue(len(g) == 3)
    self.assertTrue(isinstance(g[0], graph.Graph))
    self.assertTrue(sorted(g[0].keys()), ["a", "b", "c", "d", "e"])
    self.assertTrue(sorted(g[1].keys()), ["x", "y"])
    self.assertTrue(sorted(g[2].keys()), ["z"])
    print("pattern.graph.partition()")

</t>
<t tx="karstenw.20230303140218.54">def test_clique(self):
    # Assert node cliques.
    v = graph.clique(self.g, "a")
    self.assertEqual(v, ["a", "b"])
    self.g.add_edge("b", "c")
    v = graph.clique(self.g, "a")
    self.assertEqual(v, ["a", "b", "c"])
    v = graph.cliques(self.g, 2)
    self.assertEqual(v, [["a", "b", "c"], ["b", "d"], ["d", "e"], ["x", "y"]])
    print("pattern.graph.clique()")
    print("pattern.graph.cliques()")

</t>
<t tx="karstenw.20230303140218.55">class TestGraphMaintenance(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140218.56">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140218.57">def test_unlink(self):
    # Assert remove all edges to/from Node(a).
    g = graph.Graph()
    g.add_edge("a", "b")
    g.add_edge("a", "c")
    graph.unlink(g, g["a"])
    self.assertTrue(len(g.edges) == 0)
    # Assert remove edges between Node(a) and Node(b)
    g = graph.Graph()
    g.add_edge("a", "b")
    g.add_edge("a", "c")
    graph.unlink(g, g["a"], "b")
    self.assertTrue(len(g.edges) == 1)
    print("pattern.graph.unlink()")

</t>
<t tx="karstenw.20230303140218.58">def test_redirect(self):
    # Assert transfer connections of Node(a) to Node(d).
    g = graph.Graph()
    g.add_edge("a", "b")
    g.add_edge("c", "a")
    g.add_node("d")
    graph.redirect(g, g["a"], "d")
    self.assertTrue(len(g["a"].edges) == 0)
    self.assertTrue(len(g["d"].edges) == 2)
    self.assertTrue(g.edge("d", "c").node1 == g["c"])
    print("pattern.graph.redirect()")

</t>
<t tx="karstenw.20230303140218.59">def test_cut(self):
    # Assert unlink Node(b) and redirect a-&gt;c and a-&gt;d.
    g = graph.Graph()
    g.add_edge("a", "b")
    g.add_edge("b", "c")
    g.add_edge("b", "d")
    graph.cut(g, g["b"])
    self.assertTrue(len(g["b"].edges) == 0)
    self.assertTrue(g.edge("a", "c") is not None)
    self.assertTrue(g.edge("a", "d") is not None)
    print("pattern.graph.cut()")

</t>
<t tx="karstenw.20230303140218.6">def test_coordinates(self):
    # Assert 2D coordinates.
    x, y = graph.coordinates(10, 10, 100, 30)
    self.assertAlmostEqual(x, 96.60, places=2)
    self.assertAlmostEqual(y, 60.00, places=2)
    print("pattern.graph.coordinates()")

</t>
<t tx="karstenw.20230303140218.60">def test_insert(self):
    g = graph.Graph()
    g.add_edge("a", "b")
    g.add_node("c")
    graph.insert(g, g["c"], g["a"], g["b"])
    self.assertTrue(g.edge("a", "b") is None)
    self.assertTrue(g.edge("a", "c") is not None)
    self.assertTrue(g.edge("c", "b") is not None)
    print("pattern.graph.insert()")

</t>
<t tx="karstenw.20230303140218.61">class TestGraphCommonsense(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140218.62">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140218.63">def test_halo(self):
    # Assert concept halo (e.g., latent related concepts).
    g = commonsense.Commonsense()
    v = [concept.id for concept in g["rose"].halo]
    self.assertTrue("red" in v)
    self.assertTrue("romance" in v)
    # Concept.properties is the list of properties (adjectives) in the halo.
    v = g["rose"].properties
    self.assertTrue("red" in v)
    self.assertTrue("romance" not in v)
    print("pattern.graph.commonsense.Concept.halo")
    print("pattern.graph.commonsense.Concept.properties")

</t>
<t tx="karstenw.20230303140218.64">def test_field(self):
    # Assert semantic field (e.g., concept taxonomy).
    g = commonsense.Commonsense()
    v = [concept.id for concept in g.field("color")]
    self.assertTrue("red" in v)
    self.assertTrue("green" in v)
    self.assertTrue("blue" in v)
    print("pattern.graph.commonsense.Commonsense.field()")

</t>
<t tx="karstenw.20230303140218.65">def test_similarity(self):
    # Assert that tiger is more similar to lion than to spoon
    # (which is common sense).
    g = commonsense.Commonsense()
    w1 = g.similarity("tiger", "lion")
    w2 = g.similarity("tiger", "spoon")
    self.assertTrue(w1 &gt; w2)
    print("pattern.graph.commonsense.Commonsense.similarity()")

</t>
<t tx="karstenw.20230303140218.66">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUtilityFunctions))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestNode))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestEdge))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestGraph))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestGraphLayout))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestGraphSpringLayout))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestGraphTraversal))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestGraphPartitioning))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestGraphMaintenance))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestGraphCommonsense))
    return suite

</t>
<t tx="karstenw.20230303140218.7">class TestNode(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140218.8">def setUp(self):
    # Create test graph.
    self.g = graph.Graph()
    self.g.add_node("a", radius=5, stroke=(0, 0, 0, 1), strokewidth=1, fill=None, text=(0, 0, 0, 1))
    self.g.add_node("b", radius=5)
    self.g.add_node("c", radius=5)
    self.g.add_edge("a", "b")
    self.g.add_edge("b", "c")

</t>
<t tx="karstenw.20230303140218.9">def test_node(self):
    # Assert node properties.
    n = self.g["a"]
    self.assertTrue(isinstance(n, graph.Node))
    self.assertTrue(n               == self.g["a"])
    self.assertTrue(n               != self.g["b"])
    self.assertTrue(n.graph         == self.g)
    self.assertTrue(n._distance     == self.g.distance)
    self.assertTrue(n.id            == "a")
    self.assertTrue(n.x             == 0.0)
    self.assertTrue(n.y             == 0.0)
    self.assertTrue(n.force.x       == graph.Vector(0.0, 0.0).x)
    self.assertTrue(n.force.y       == graph.Vector(0.0, 0.0).y)
    self.assertTrue(n.radius        == 5)
    self.assertTrue(n.fill is None)
    self.assertTrue(n.stroke        == (0, 0, 0, 1))
    self.assertTrue(n.strokewidth   == 1)
    self.assertTrue(n.text.string   == "a")
    self.assertTrue(n.text.width    == 85)
    self.assertTrue(n.text.fill     == (0, 0, 0, 1))
    self.assertTrue(n.text.fontsize == 11)
    self.assertTrue(n.fixed         == False)
    self.assertTrue(n.weight        == 0)
    self.assertTrue(n.centrality    == 0)
    print("pattern.graph.Node")

</t>
<t tx="karstenw.20230303140220.1">from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
import time
import re
import random

from pattern import search
from pattern.en import Sentence, parse

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140220.10">class TestTaxonomy(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140220.11">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140220.12">def test_taxonomy(self):
    # Assert Taxonomy search.
    t = search.Taxonomy()
    t.append("King Arthur", type="knight", value=1)
    t.append("Sir Bedevere", type="knight", value=2)
    t.append("Sir Lancelot", type="knight", value=3)
    t.append("Sir Gallahad", type="knight", value=4)
    t.append("Sir Robin", type="knight", value=5)
    t.append("John Cleese", type="Sir Lancelot")
    t.append("John Cleese", type="Basil Fawlty")
    # Matching is case-insensitive, results are lowercase.
    self.assertTrue("John Cleese" in t)
    self.assertTrue("john cleese" in t)
    self.assertEqual(t.classify("King Arthur"), "knight")
    self.assertEqual(t.value("King Arthur"), 1)
    self.assertEqual(t.parents("John Cleese"), ["basil fawlty", "sir lancelot"])
    self.assertEqual(t.parents("John Cleese", recursive=True), [
        "basil fawlty",
        "sir lancelot",
        "knight"])
    self.assertEqual(t.children("knight"), [
        "sir robin",
        "sir gallahad",
        "sir lancelot",
        "sir bedevere",
        "king arthur"])
    self.assertEqual(t.children("knight", recursive=True), [
        "sir robin",
        "sir gallahad",
        "sir lancelot",
        "sir bedevere",
        "king arthur",
        "john cleese"])
    print("pattern.search.Taxonomy")

</t>
<t tx="karstenw.20230303140220.13">def test_classifier(self):
    # Assert taxonomy classifier + keyword arguments.
    c1 = search.Classifier(parents=lambda word, chunk=None: word.endswith("ness") and ["quality"] or [])
    c2 = search.Classifier(parents=lambda word, chunk=None: chunk == "VP" and ["action"] or [])
    t = search.Taxonomy()
    t.classifiers.append(c1)
    t.classifiers.append(c2)
    self.assertEqual(t.classify("fuzziness"), "quality")
    self.assertEqual(t.classify("run", chunk="VP"), "action")
    print("pattern.search.Classifier")

</t>
<t tx="karstenw.20230303140220.14">def test_wordnet_classifier(self):
    # Assert WordNet classifier parents &amp; children.
    c = search.WordNetClassifier()
    t = search.Taxonomy()
    t.classifiers.append(c)
    self.assertEqual(t.classify("cat"), "feline")
    self.assertEqual(t.classify("dog"), "canine")
    self.assertTrue("domestic_cat" in t.children("cat"))
    self.assertTrue("puppy" in t.children("dog"))
    print("pattern.search.WordNetClassifier")

</t>
<t tx="karstenw.20230303140220.15">class TestConstraint(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140220.16">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140220.17">def _test_constraint(self, constraint, **kwargs):
    # Assert Constraint property values with given optional parameters.
    self.assertEqual(constraint.words,    kwargs.get("words",    []))
    self.assertEqual(constraint.tags,     kwargs.get("tags",     []))
    self.assertEqual(constraint.chunks,   kwargs.get("chunks",   []))
    self.assertEqual(constraint.roles,    kwargs.get("roles",    []))
    self.assertEqual(constraint.taxa,     kwargs.get("taxa",     []))
    self.assertEqual(constraint.optional, kwargs.get("optional", False))
    self.assertEqual(constraint.multiple, kwargs.get("multiple", False))
    self.assertEqual(constraint.first,    kwargs.get("first",    False))
    self.assertEqual(constraint.exclude,  kwargs.get("exclude",  None))
    self.assertEqual(constraint.taxonomy, kwargs.get("taxonomy", search.taxonomy))

</t>
<t tx="karstenw.20230303140220.18">def test_fromstring(self):
    # Assert Constraint string syntax.
    for s, kwargs in (
      (        "cats", dict( words = ["cats"])),
      (        "Cat*", dict( words = ["cat*"])),
      (   "\\[cat\\]", dict( words = ["[cat]"])),
      ("[black cats]", dict( words = ["black cats"])),
      (  "black_cats", dict( words = ["black cats"])),
      ("black\\_cats", dict( words = ["black_cats"])),
      (         "NNS", dict(  tags = ["NNS"])),
      (     "NN*|VB*", dict(  tags = ["NN*", "VB*"])),
      (          "NP", dict(chunks = ["NP"])),
      (         "SBJ", dict( roles = ["SBJ"])),
      (        "CATS", dict(  taxa = ["cats"])),
      (       "cats?", dict( words = ["cats"], optional=True)),
      (      "(cats)", dict( words = ["cats"], optional=True)),
      (  "\\(cats\\)", dict( words = ["(cats)"])),
      (       "cats+", dict( words = ["cats"], multiple=True)),
      (     "cats\\+", dict( words = ["cats+"])),
      (   "cats+dogs", dict( words = ["cats+dogs"])),
      (     "(cats+)", dict( words = ["cats"], optional=True, multiple=True)),
      (     "(cats)+", dict( words = ["cats"], optional=True, multiple=True)),
      (      "cats+?", dict( words = ["cats"], optional=True, multiple=True)),
      (      "cats?+", dict( words = ["cats"], optional=True, multiple=True)),
      ( "^[fat cat]?", dict( words = ["fat cat"], first=True, optional=True)),
      ( "[^fat cat?]", dict( words = ["fat cat"], first=True, optional=True)),
      ( "cats\\|dogs", dict( words = ["cats|dogs"])),
      (   "cats|dogs", dict( words = ["cats", "dogs"])),
      (        "^cat", dict( words = ["cat"], first=True)),
      (      "\\^cat", dict( words = ["^cat"])),
      (     "(cat*)+", dict( words = ["cat*"], optional=True, multiple=True)),
      ( "^black_cat+", dict( words = ["black cat"], multiple=True, first=True)),
      (  "black\[cat", dict( words = ["black[cat"])),
      (  "black\(cat", dict( words = ["black(cat"])),
      (  "black\{cat", dict( words = ["black{cat"])),
      (  "black\|cat", dict( words = ["black|cat"])),
      (  "black\!cat", dict( words = ["black!cat"])),
      (  "black\^cat", dict( words = ["black^cat"])),
      (  "black\+cat", dict( words = ["black+cat"])),
      (  "black\?cat", dict( words = ["black?cat"])),
      (    "cats|NN*", dict( words = ["cats"], tags=["NN*"]))):
        self._test_constraint(search.Constraint.fromstring(s), **kwargs)
    # Assert non-alpha taxonomy items.
    t = search.Taxonomy()
    t.append("0.5", type="0.5")
    t.append("half", type="0.5")
    v = search.Constraint.fromstring("0.5", taxonomy=t)
    # Assert non-alpha words without taxonomy.
    self.assertTrue(v.taxa == ["0.5"])
    v = search.Constraint.fromstring("0.5")
    # Assert exclude Constraint.
    self.assertTrue(v.words == ["0.5"])
    v = search.Constraint.fromstring("\\!cats|!dogs|!fish")
    self.assertTrue(v.words == ["!cats"])
    self.assertTrue(v.exclude.words == ["dogs", "fish"])
    print("pattern.search.Constraint.fromstring")
    print("pattern.search.Constraint.fromstring")

</t>
<t tx="karstenw.20230303140220.19">def test_match(self):
    # Assert Constraint-Word matching.
    R = search.Constraint.fromstring
    S = lambda s: Sentence(parse(s, relations=True, lemmata=True))
    W = lambda s, tag=None, index=0: search.Word(None, s, tag, index)
    for constraint, tests in (
      (R("cat|dog"),  [(W("cat"), 1), (W("dog"), 1), (W("fish"), 0)]),
      (R("cat*"),     [(W("cats"), 1)]),
      (R("*cat"),     [(W("tomcat"), 1)]),
      (R("c*t|d*g"),  [(W("cat"), 1), (W("cut"), 1), (W("dog"), 1), (W("dig"), 1)]),
      (R("cats|NN*"), [(W("cats", "NNS"), 1), (W("cats"), 0)]),
      (R("^cat"),     [(W("cat", "NN", index=0), 1), (W("cat", "NN", index=1), 0)]),
      (R("*|!cat"),   [(W("cat"), 0), (W("dog"), 1), (W("fish"), 1)]),
      (R("my cat"),   [(W("cat"), 0)]),
      (R("my cat"),   [(S("my cat").words[1], 1)]),  # "my cat" is an overspecification of "cat"
      (R("my_cat"),   [(S("my cat").words[1], 1)]),
      (R("cat|NP"),   [(S("my cat").words[1], 1)]),
      (R("dog|VP"),   [(S("my dog").words[1], 0)]),
      (R("cat|SBJ"),  [(S("the cat is sleeping").words[1], 1)]),
      (R("dog"),      [(S("MY DOGS").words[1], 1)]), # lemma matches
      (R("dog"),      [(S("MY DOG").words[1], 1)])): # case-insensitive
        for test, b in tests:
            self.assertEqual(constraint.match(test), bool(b))
    # Assert Constraint-Taxa matching.
    t = search.Taxonomy()
    t.append("Tweety", type="bird")
    t.append("Steven", type="bird")
    v = search.Constraint.fromstring("BIRD", taxonomy=t)
    self.assertTrue(v.match(W("bird")))
    self.assertTrue(v.match(S("tweeties")[0]))
    self.assertTrue(v.match(W("Steven")))
    print("pattern.search.Constraint.match()")

</t>
<t tx="karstenw.20230303140220.2">class TestUtilityFunctions(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140220.20">def test_string(self):
    # Assert Constraint.string.
    v = search.Constraint()
    v.words = ["Steven\\*"]
    v.tags = ["NN*"]
    v.roles = ["SBJ"]
    v.taxa = ["(associate) professor"]
    v.exclude = search.Constraint(["bird"])
    v.multiple = True
    v.first = True
    self.assertEqual(v.string, "^[Steven\\*|NN*|SBJ|\(ASSOCIATE\)_PROFESSOR|!bird]+")
    print("pattern.search.Constraint.string")

</t>
<t tx="karstenw.20230303140220.21">class TestPattern(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140220.22">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140220.23">def test_pattern(self):
    # Assert Pattern properties.
    v = search.Pattern([
        search.Constraint("a|an|the"),
        search.Constraint("JJ*"),
        search.Constraint("cat")], search.STRICT)
    self.assertEqual(len(v), 3)
    self.assertEqual(v.strict, True)
    print("pattern.search.Pattern")

</t>
<t tx="karstenw.20230303140220.24">def test_fromstring(self):
    # Assert Pattern string syntax.
    v = search.Pattern.fromstring("a|an|the JJ*? cat*")
    self.assertEqual(v[0].words,    ["a", "an", "the"])
    self.assertEqual(v[1].tags,     ["JJ*"])
    self.assertEqual(v[1].optional, True)
    self.assertEqual(v[2].words,    ["cat*"])
    # Assert escaped control characters.
    v = search.Pattern.fromstring("[\\[Figure 1\\]] VP")
    self.assertEqual(v[0].words,    ["[figure 1]"])
    self.assertEqual(v[1].chunks,   ["VP"])
    # Assert messy syntax (fix brackets and whitespace, don't fix empty options).
    v = search.Pattern.fromstring("[avoid][|!|messy  |syntax |]")
    self.assertEqual(v[0].words,    ["avoid"])
    self.assertEqual(v[1].words,    ["", "messy", "syntax", ""])
    self.assertEqual(v[1].exclude.words, [""]) # "!" = exclude everything
    print("pattern.search.Pattern.fromstring()")

</t>
<t tx="karstenw.20230303140220.25">def test_match(self):
    # Assert Pattern.match()
    P = search.Pattern.fromstring
    X = search.STRICT
    S = lambda s: Sentence(parse(s, relations=True, lemmata=True))
    for i, (pattern, test, match) in enumerate((
      (P("^rabbit"),                  "white rabbit",     None),                  #  0
      (P("^rabbit"),                        "rabbit",     "rabbit"),              #  1
      (P("rabbit"),               "big white rabbit",     "rabbit"),              #  2
      (P("rabbit*"),              "big white rabbits",    "rabbits"),             #  3
      (P("JJ|NN"),              S("big white rabbits"),   "big"),                 #  4
      (P("JJ+"),                S("big white rabbits"),   "big white"),           #  5
      (P("JJ+ NN*"),            S("big white rabbits"),   "big white rabbits"),   #  6
      (P("JJ black|white NN*"), S("big white rabbits"),   "big white rabbits"),   #  7
      (P("NP"),                 S("big white rabbit"),    "big white rabbit"),    #  8
      (P("big? rabbit", X),     S("big white rabbit"),    "rabbit"),              #  9 strict
      (P("big? rabbit|NN"),     S("big white rabbit"),    "rabbit"),              # 10 explicit
      (P("big? rabbit"),        S("big white rabbit"),    "big white rabbit"),    # 11 greedy
      (P("rabbit VP JJ"),       S("the rabbit was huge"), "the rabbit was huge"), # 12
      (P("rabbit be JJ"),       S("the rabbit was huge"), "the rabbit was huge"), # 13 lemma
      (P("rabbit be JJ", X),    S("the rabbit was huge"), "rabbit was huge"),     # 14
      (P("rabbit is JJ"),       S("the rabbit was huge"), None),                  # 15
      (P("the NP"),             S("the rabid rodents"),   "the rabid rodents"),   # 16 overlap
      (P("t*|r*+"),             S("the rabid rodents"),   "the rabid rodents"),   # 17
      (P("(DT) JJ? NN*"),       S("the rabid rodents"),   "the rabid rodents"),   # 18
      (P("(DT) JJ? NN*"),       S("the rabbit"),          "the rabbit"),          # 19
      (P("rabbit"),             S("the big rabbit"),      "the big rabbit"),      # 20 greedy
      (P("eat carrot"),         S("is eating a carrot"),  "is eating a carrot"),  # 21
      (P("eat carrot|NP"),      S("is eating a carrot"),  "is eating a carrot"),  # 22
      (P("eat NP"),             S("is eating a carrot"),  "is eating a carrot"),  # 23
      (P("eat a"),              S("is eating a carrot"),  "is eating a"),         # 24
      (P("!NP carrot"),         S("is eating a carrot"),  "is eating a carrot"),  # 25
      (P("eat !pizza"),         S("is eating a carrot"),  "is eating a carrot"),  # 26
      (P("eating a"),           S("is eating a carrot"),  "is eating a"),         # 27
      (P("eating !carrot", X),  S("is eating a carrot"),  "eating a"),            # 28
      (P("eat !carrot"),        S("is eating a carrot"),  None),                  # 28 NP chunk is a carrot
      (P("eat !DT"),            S("is eating a carrot"),  None),                  # 30 eat followed by DT
      (P("eat !NN"),            S("is eating a carrot"),  "is eating a"),         # 31 a/DT is not NN
      (P("!be carrot"),         S("is eating a carrot"),  "is eating a carrot"),  # 32 is eating == eat != is
      (P("!eat|VP carrot"),     S("is eating a carrot"),  None),                  # 33 VP chunk == eat
      (P("white_rabbit"),       S("big white rabbit"),    None),                  # 34
      (P("[white rabbit]"),     S("big white rabbit"),    None),                  # 35
      (P("[* white rabbit]"),   S("big white rabbit"),    "big white rabbit"),    # 36
      (P("[big * rabbit]"),     S("big white rabbit"),    "big white rabbit"),    # 37
      (P("big [big * rabbit]"), S("big white rabbit"),    "big white rabbit"),    # 38
      (P("[*+ rabbit]"),        S("big white rabbit"),    None),                  # 39 bad pattern: "+" is literal
    )):
        m = pattern.match(test)
        self.assertTrue(getattr(m, "string", None) == match)
    # Assert chunk with head at the front.
    s = S("Felix the cat")
    self.assertEqual(P("felix").match(s).string, "Felix the cat")
    # Assert negation + custom greedy() function.
    s = S("the big white rabbit")
    g = lambda chunk, constraint: len([w for w in chunk if not constraint.match(w)]) == 0
    self.assertEqual(P("!white").match(s).string, "the big white rabbit") # a rabbit != white
    self.assertEqual(P("!white", greedy=g).match(s), None)                # a white rabbit == white
    # Assert taxonomy items with spaces.
    s = S("Bugs Bunny is a giant talking rabbit.")
    t = search.Taxonomy()
    t.append("rabbit", type="rodent")
    t.append("Bugs Bunny", type="rabbit")
    self.assertEqual(P("RABBIT", taxonomy=t).match(s).string, "Bugs Bunny")
    # Assert None, the syntax cannot handle taxonomy items that span multiple chunks.
    s = S("Elmer Fudd fires a cannon")
    t = search.Taxonomy()
    t.append("fire cannon", type="violence")
    self.assertEqual(P("VIOLENCE").match(s), None)
    # Assert regular expressions.
    s = S("a sack with 3.5 rabbits")
    p = search.Pattern.fromstring("[] NNS")
    p[0].words.append(re.compile(r"[0-9|\.]+"))
    self.assertEqual(p.match(s).string, "3.5 rabbits")
    print("pattern.search.Pattern.match()")

</t>
<t tx="karstenw.20230303140220.26">def test_search(self):
    # Assert one match containing all words.
    v = search.Pattern.fromstring("*+")
    v = v.search("one two three")
    self.assertEqual(v[0].string, "one two three")
    # Assert one match for each word.
    v = search.Pattern.fromstring("*")
    v = v.search("one two three")
    self.assertEqual(v[0].string, "one")
    self.assertEqual(v[1].string, "two")
    self.assertEqual(v[2].string, "three")
    # Assert all variations are matched (sentence starts with a NN* which must be caught).
    v = search.Pattern.fromstring("(DT) JJ?+ NN*")
    v = v.search(Sentence(parse("dogs, black cats and a big white rabbit")))
    self.assertEqual(v[0].string, "dogs")
    self.assertEqual(v[1].string, "black cats")
    self.assertEqual(v[2].string, "a big white rabbit")
    v = search.Pattern.fromstring("NN*")
    print("pattern.search.Pattern.search()")

</t>
<t tx="karstenw.20230303140220.27">def test_convergence(self):
    # Test with random sentences and random patterns to see if it crashes.
    w = ("big", "white", "rabbit", "black", "cats", "is", "was", "going", "to", "sleep", "sleepy", "very", "or")
    x = ("DT?", "JJ?+", "NN*", "VP?", "cat", "[*]")
    for i in range(100):
        s = " ".join(random.choice(w) for i in range(20))
        s = Sentence(parse(s, lemmata=True))
        p = " ".join(random.choice(x) for i in range(5))
        p = search.Pattern.fromstring(p)
        p.search(s)

</t>
<t tx="karstenw.20230303140220.28">def test_compile_function(self):
    # Assert creating and caching Pattern with compile().
    t = search.Taxonomy()
    p = search.compile("JJ?+ NN*", search.STRICT, taxonomy=t)
    self.assertEqual(p.strict, True)
    self.assertEqual(p[0].optional, True)
    self.assertEqual(p[0].tags, ["JJ"])
    self.assertEqual(p[1].tags, ["NN*"])
    self.assertEqual(p[1].taxonomy, t)
    # Assert regular expression input.
    p = search.compile(re.compile(r"[0-9|\.]+"))
    self.assertTrue(isinstance(p[0].words[0], search.regexp))
    # Assert TypeError for other input.
    self.assertRaises(TypeError, search.compile, 1)
    print("pattern.search.compile()")

</t>
<t tx="karstenw.20230303140220.29">def test_match_function(self):
    # Assert match() function.
    s = Sentence(parse("Go on Bors, chop his head off!"))
    m1 = search.match("chop NP off", s, strict=False)
    m2 = search.match("chop NP+ off", s, strict=True)
    self.assertEqual(m1.constituents()[1].string, "his head")
    self.assertEqual(m2.constituents()[1].string, "his head")
    print("pattern.search.match()")

</t>
<t tx="karstenw.20230303140220.3">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140220.30">def test_search_function(self):
    # Assert search() function.
    s = Sentence(parse("Go on Bors, chop his head off!"))
    m = search.search("PRP*? NN*", s)
    self.assertEqual(m[0].string, "Bors")
    self.assertEqual(m[1].string, "his head")
    print("pattern.search.search()")

</t>
<t tx="karstenw.20230303140220.31">def test_escape(self):
    # Assert escape() function.
    self.assertEqual(search.escape("{}[]()_|!*+^."), "\\{\\}\\[\\]\\(\\)\\_\\|\\!\\*\\+\\^.")
    print("pattern.search.escape()")

</t>
<t tx="karstenw.20230303140220.32">class TestMatch(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140220.33">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140220.34">def test_match(self):
    # Assert Match properties.
    s = Sentence(parse("Death awaits you all with nasty, big, pointy teeth."))
    p = search.Pattern(sequence=[
        search.Constraint(tags=["JJ"], optional=True),
        search.Constraint(tags=["NN*"])])
    m = p.search(s)
    self.assertTrue(isinstance(m, list))
    self.assertEqual(m[0].pattern, p)
    self.assertEqual(m[1].pattern, p)
    self.assertEqual(m[0].words, [s.words[0]])
    self.assertEqual(m[1].words, [s.words[-3], s.words[-2]])
    # Assert contraint "NN*" links to "Death" and "teeth", and "JJ" to "pointy".
    self.assertEqual(m[0].constraint(s.words[0]), p[1])
    self.assertEqual(m[1].constraint(s.words[-3]), p[0])
    self.assertEqual(m[1].constraint(s.words[-2]), p[1])
    # Assert constraints "JJ NN*" links to chunk "pointy teeth".
    self.assertEqual(m[1].constraints(s.chunks[-1]), [p[0], p[1]])
    # Assert Match.constituents() by constraint, constraint index and list of indices.
    self.assertEqual(m[1].constituents(), [s.words[-3], s.words[-2]])
    self.assertEqual(m[1].constituents(constraint=p[0]), [s.words[-3]])
    self.assertEqual(m[1].constituents(constraint=1), [s.words[-2]])
    self.assertEqual(m[1].constituents(constraint=(0, 1)), [s.words[-3], s.words[-2]])
    # Assert Match.string.
    self.assertEqual(m[1].string, "pointy teeth")
    print("pattern.search.Match")

</t>
<t tx="karstenw.20230303140220.35">def test_group(self):
    # Assert Match groups.
    s = Sentence(parse("the big black cat eats a tasty fish"))
    m = search.search("DT {JJ+} NN", s)
    self.assertEqual(m[0].group(1).string, "big black")
    self.assertEqual(m[1].group(1).string, "tasty")
    # Assert nested groups (and syntax with additional spaces).
    m = search.search("DT { JJ { JJ { NN }}}", s)
    self.assertEqual(m[0].group(1).string, "big black cat")
    self.assertEqual(m[0].group(2).string, "black cat")
    self.assertEqual(m[0].group(3).string, "cat")
    # Assert chunked groups.
    m = search.search("NP {VP NP}", s)
    v = m[0].group(1, chunked=True)
    self.assertEqual(v[0].string, "eats")
    self.assertEqual(v[1].string, "a tasty fish")
    print("pattern.search.Match.group()")

</t>
<t tx="karstenw.20230303140220.36">def test_group_ordering(self):
    # Assert group parser ordering (opened-first).
    c1 = search.Constraint("1")
    c2 = search.Constraint("2")
    c3 = search.Constraint("3")
    c4 = search.Constraint("4")
    p = search.Pattern([c1, [c2, [[c3], c4]]])
    self.assertEqual(p.groups[0][0].words[0], "2")
    self.assertEqual(p.groups[0][1].words[0], "3")
    self.assertEqual(p.groups[0][2].words[0], "4")
    self.assertEqual(p.groups[1][0].words[0], "3")
    self.assertEqual(p.groups[1][1].words[0], "4")
    self.assertEqual(p.groups[2][0].words[0], "3")
    p = search.Pattern.fromstring("1 {2 {{3} 4}}")
    self.assertEqual(p.groups[0][0].words[0], "2")
    self.assertEqual(p.groups[0][1].words[0], "3")
    self.assertEqual(p.groups[0][2].words[0], "4")
    self.assertEqual(p.groups[1][0].words[0], "3")
    self.assertEqual(p.groups[1][1].words[0], "4")
    self.assertEqual(p.groups[2][0].words[0], "3")
    p = search.Pattern.fromstring("1 {2} {3} 4")
    self.assertEqual(p.groups[0][0].words[0], "2")
    self.assertEqual(p.groups[1][0].words[0], "3")

</t>
<t tx="karstenw.20230303140220.37">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUtilityFunctions))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestTaxonomy))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestConstraint))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestPattern))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestMatch))
    return suite

</t>
<t tx="karstenw.20230303140220.4">def test_match(self):
    # Assert search._match() wildcard matching.
    for s, p, b in (
      ("rabbit" , "rabbit", True),
      ("rabbits", "rabbit*", True),
      ("rabbits", "*abbits", True),
      ("rabbits", "*abbit*", True),
      ("rabbits", "rab*its", True),
      ("rabbits", re.compile(r"ra.*?"), True)):
        self.assertEqual(search._match(s, p), b)
    print("pattern.search._match()")

</t>
<t tx="karstenw.20230303140220.5">def test_unique(self):
    self.assertEqual(search.unique([1, 1, 2, 2]), [1, 2])

</t>
<t tx="karstenw.20230303140220.6">def test_find(self):
    self.assertEqual(search.find(lambda v: v &gt; 2, [1, 2, 3, 4, 5]), 3)

</t>
<t tx="karstenw.20230303140220.7">def test_product(self):
    # Assert combinations of list items.
    self.assertEqual(list(search.product([], repeat=2)), [])   # No possibilities.
    self.assertEqual(list(search.product([1], repeat=0)), [()]) # One possibility: the empty set.
    self.assertEqual(list(search.product([1, 2, 3], repeat=2)),
        [(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3)])
    for n, m in ((1, 9), (2, 81), (3, 729), (4, 6561)):
        v = search.product([1, 2, 3, 4, 5, 6, 7, 8, 9], repeat=n)
        self.assertEqual(len(list(v)), m)
    print("pattern.search.product()")

</t>
<t tx="karstenw.20230303140220.8">def test_variations(self):
    # Assert variations include the original input (the empty list has one variation = itself).
    v = search.variations([])
    self.assertEqual(v, [()])
    # Assert variations = (1,) and ().
    v = search.variations([1], optional=lambda item: item == 1)
    self.assertEqual(v, [(1,), ()])
    # Assert variations = the original input, (2,), (1,) and ().
    v = search.variations([1, 2], optional=lambda item: item in (1, 2))
    self.assertEqual(v, [(1, 2), (2,), (1,), ()])
    # Assert variations are sorted longest-first.
    v = search.variations([1, 2, 3, 4], optional=lambda item: item in (1, 2))
    self.assertEqual(v, [(1, 2, 3, 4), (2, 3, 4), (1, 3, 4), (3, 4)])
    self.assertTrue(len(v[0]) &gt;= len(v[1]) &gt;= len(v[2]), len(v[3]))
    print("pattern.search.variations()")

</t>
<t tx="karstenw.20230303140220.9">def test_odict(self):
    # Assert odict.append() which must be order-preserving.
    v = search.odict()
    v.push(("a", 1))
    v.push(("b", 2))
    v.push(("c", 3))
    v.push(("a", 0))
    v = v.copy()
    self.assertTrue(isinstance(v, dict))
    self.assertEqual(v.keys(), ["a", "c", "b"])
    print("pattern.search.odict()")

</t>
<t tx="karstenw.20230303140222.1">from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
try:
    # Python 2
    from StringIO import StringIO
except ImportError:
    # Python 3
    from io import StringIO

from pattern import text

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140222.10">class TestModel(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140222.11">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140222.12">def test_model(self):
    # Assert SLP language model.
    v = text.Model()
    for i in range(2):
        v.train("black", "JJ", previous=("the", "DT"), next=("cat", "NN"))
        v.train("on", "IN", previous=("sat", "VBD"), next=("the", "DT"))
    self.assertEqual("JJ", v.classify("slack"))
    self.assertEqual("JJ", v.classify("white", previous=("a", "DT"), next=("cat", "NN")))
    self.assertEqual("IN", v.classify("on", previous=("sat", "VBD")))
    self.assertEqual("IN", v.classify("on", next=("the", "")))
    self.assertEqual(["white", "JJ"], v.apply(("white", ""), next=("cat", "")))
    print("pattern.text.Model")

</t>
<t tx="karstenw.20230303140222.13">class TestMorphology(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140222.14">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140222.15">def test_morphology(self):
    # Assert morphological tagging rules.
    f = StringIO("NN s fhassuf 1 NNS x")
    v = text.Morphology(f)
    self.assertEqual(v.apply(
        ["cats", "NN"]),
        ["cats", "NNS"])
    print("pattern.text.Morphology")

</t>
<t tx="karstenw.20230303140222.16">class TestContext(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140222.17">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140222.18">def test_context(self):
    # Assert contextual tagging rules.
    f = StringIO("VBD VB PREVTAG TO")
    v = text.Context(path=f)
    self.assertEqual(v.apply(
        [["to", "TO"], ["be", "VBD"]]),
        [["to", "TO"], ["be", "VB"]])
    print("pattern.text.Context")

</t>
<t tx="karstenw.20230303140222.19">class TestEntities(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140222.2">class TestLexicon(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140222.20">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140222.21">def test_entities(self):
    # Assert named entity recognizer.
    f = StringIO("Schrödinger's cat PERS")
    v = text.Entities(path=f)
    self.assertEqual(v.apply(
        [["Schrödinger's", "NNP"], ["cat", "NN"]]),
        [["Schrödinger's", "NNP-PERS"], ["cat", "NNP-PERS"]])
    print("pattern.text.Entities")

</t>
<t tx="karstenw.20230303140222.22">class TestParser(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140222.23">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140222.24">def test_stringio(self):
    # Assert loading data from file-like strings.
    p = text.Parser(
           lexicon = {"to": "TO", "saw": "VBD"},
        morphology = StringIO("NN s fhassuf 1 NNS x"),
           context = StringIO("VBD VB PREVTAG TO"))
    self.assertEqual(p.parse("cats"), "cats/NNS/B-NP/O")
    self.assertEqual(p.parse("to saw"), "to/TO/B-VP/O saw/VB/I-VP/O")

</t>
<t tx="karstenw.20230303140222.25">def test_find_keywords(self):
    # Assert the intrinsic keyword extraction algorithm.
    p = text.Parser()
    p.lexicon["the"] = "DT"
    p.lexicon["cat"] = "NN"
    p.lexicon["dog"] = "NN"
    v1 = p.find_keywords("the cat")
    v2 = p.find_keywords("cat. cat. dog.")
    v3 = p.find_keywords("cat. dog. dog.")
    v4 = p.find_keywords("the. cat. dog.", frequency={"cat": 1.0, "dog": 0.0})
    self.assertEqual(v1, ["cat"])
    self.assertEqual(v2, ["cat", "dog"])
    self.assertEqual(v3, ["cat", "dog"])
    self.assertEqual(v4, ["dog", "cat"])
    print("pattern.text.Parser.find_keywords()")

</t>
<t tx="karstenw.20230303140222.26">def test_find_tokens(self):
    # Assert the default tokenizer and its optional parameters.
    p = text.Parser()
    v1 = p.find_tokens("Schrödinger's cat is alive!", punctuation="", replace={})
    v2 = p.find_tokens("Schrödinger's cat is dead!", punctuation="!", replace={"'s": " 's"})
    v3 = p.find_tokens("etc.", abbreviations=set())
    v4 = p.find_tokens("etc.", abbreviations=set(("etc.",)))
    self.assertEqual(v1[0], "Schrödinger's cat is alive!")
    self.assertEqual(v2[0], "Schrödinger 's cat is dead !")
    self.assertEqual(v3[0], "etc .")
    self.assertEqual(v4[0], "etc.")
    print("pattern.text.Parser.find_tokens()")

</t>
<t tx="karstenw.20230303140222.27">def test_find_tags(self):
    # Assert the default part-of-speech tagger and its optional parameters.
    p = text.Parser()
    v1 = p.find_tags(["Schrödinger", "cat", "1.0"], lexicon={}, default=("NN?", "NNP?", "CD?"))
    v2 = p.find_tags(["Schrödinger", "cat", "1.0"], lexicon={"1.0": "CD?"})
    v3 = p.find_tags(["Schrödinger", "cat", "1.0"], map=lambda token, tag: (token, tag + "!"))
    v4 = p.find_tags(["observer", "observable"], language="fr")
    v5 = p.find_tags(["observer", "observable"], language="en")
    self.assertEqual(v1, [["Schr\xf6dinger", "NNP?"], ["cat", "NN?"], ["1.0", "CD?"]])
    self.assertEqual(v2, [["Schr\xf6dinger", "NNP"], ["cat", "NN"], ["1.0", "CD?"]])
    self.assertEqual(v3, [["Schr\xf6dinger", "NNP!"], ["cat", "NN!"], ["1.0", "CD!"]])
    self.assertEqual(v4, [["observer", "NN"], ["observable", "NN"]])
    self.assertEqual(v5, [["observer", "NN"], ["observable", "JJ"]])
    print("pattern.text.Parser.find_tags()")

</t>
<t tx="karstenw.20230303140222.28">def test_find_chunks(self):
    # Assert the default phrase chunker and its optional parameters.
    p = text.Parser()
    v1 = p.find_chunks([["", "DT"], ["", "JJ"], ["", "NN"]], language="en")
    v2 = p.find_chunks([["", "DT"], ["", "JJ"], ["", "NN"]], language="es")
    v3 = p.find_chunks([["", "DT"], ["", "NN"], ["", "JJ"]], language="en")
    v4 = p.find_chunks([["", "DT"], ["", "NN"], ["", "JJ"]], language="es")
    self.assertEqual(v1, [["", "DT", "B-NP", "O"], ["", "JJ", "I-NP", "O"], ["", "NN", "I-NP", "O"]])
    self.assertEqual(v2, [["", "DT", "B-NP", "O"], ["", "JJ", "I-NP", "O"], ["", "NN", "I-NP", "O"]])
    self.assertEqual(v3, [["", "DT", "B-NP", "O"], ["", "NN", "I-NP", "O"], ["", "JJ", "B-ADJP", "O"]])
    self.assertEqual(v4, [["", "DT", "B-NP", "O"], ["", "NN", "I-NP", "O"], ["", "JJ", "I-NP", "O"]])
    print("pattern.text.Parser.find_chunks()")

</t>
<t tx="karstenw.20230303140222.29">class TestSentiment(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140222.3">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140222.30">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140222.31">def test_dict(self):
    # Assert weighted average polarity and subjectivity for dictionary.
    s = text.Sentiment()
    v = {":-(": 4, ":-)": 1}
    self.assertEqual(s(v)[0], -0.5)
    self.assertEqual(s(v)[1], +1.0)
    self.assertEqual(s(v).assessments[0], ([":-("], -0.75, 1.0, "mood"))
    self.assertEqual(s(v).assessments[1], ([":-)"], +0.50, 1.0, "mood"))
    print("pattern.text.Sentiment.assessments")

</t>
<t tx="karstenw.20230303140222.32">def test_bag_of_words(self):
    # Assert weighted average polarity and subjectivity for bag-of-words with weighted features.
    from pattern.vector import BagOfWords # Alias for pattern.vector.Document.
    s = text.Sentiment()
    v = BagOfWords({":-(": 4, ":-)": 1})
    self.assertEqual(s(v)[0], -0.5)
    self.assertEqual(s(v)[1], +1.0)
    self.assertEqual(s(v).assessments[0], ([":-("], -0.75, 1.0, "mood"))
    self.assertEqual(s(v).assessments[1], ([":-)"], +0.50, 1.0, "mood"))

</t>
<t tx="karstenw.20230303140222.33">def test_annotate(self):
    # Assert custom annotations.
    s = text.Sentiment()
    s.annotate("inconceivable", polarity=0.9, subjectivity=0.9)
    v = "inconceivable"
    self.assertEqual(s(v)[0], +0.9)
    self.assertEqual(s(v)[1], +0.9)

</t>
<t tx="karstenw.20230303140222.34">class TestMultilingual(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140222.35">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140222.36">def test_language(self):
    # Assert language recognition.
    self.assertEqual(text.language("the cat sat on the mat")[0], "en")
    self.assertEqual(text.language("de kat zat op de mat")[0], "nl")
    self.assertEqual(text.language("le chat s'était assis sur le tapis")[0], "fr")
    print("pattern.text.language()")

</t>
<t tx="karstenw.20230303140222.37">def test_deflood(self):
    # Assert flooding removal.
    self.assertEqual(text.deflood("NIIICE!!!", n=1), "NICE!")
    self.assertEqual(text.deflood("NIIICE!!!", n=2), "NIICE!!")
    print("pattern.text.deflood()")

</t>
<t tx="karstenw.20230303140222.38">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestLexicon))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestFrequency))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestModel))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestMorphology))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestContext))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestEntities))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestParser))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSentiment))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestMultilingual))
    return suite

</t>
<t tx="karstenw.20230303140222.4">def test_lazydict(self):
    # Assert lazy dictionary only has data after one of its methods is called.
    class V(text.lazydict):
        def load(self):
            dict.__setitem__(self, "a", 1)
    v = V()
    self.assertTrue(dict.__len__(v) == 0)
    self.assertTrue(dict.__contains__(v, "a") is False)
    self.assertTrue(len(v), 1)
    self.assertTrue(v["a"] == 1)
    print("pattern.text.lazydict")

</t>
<t tx="karstenw.20230303140222.5">def test_lazylist(self):
    # Assert lazy list only has data after one of its methods is called.
    class V(text.lazylist):
        def load(self):
            list.append(self, "a")
    v = V()
    self.assertTrue(list.__len__(v) == 0)
    self.assertTrue(list.__contains__(v, "a") is False)
    self.assertTrue(len(v), 1)
    self.assertTrue(v[0] == "a")
    print("pattern.text.lazylist")

</t>
<t tx="karstenw.20230303140222.6">def test_lexicon(self):
    # Assert lexicon from file (or file-like string).
    f1 = ";;; Comments. \n schrödinger NNP \n cat NN"
    f2 = StringIO(";;; Comments. \n schrödinger NNP \n cat NN")
    v1 = text.Lexicon(path=f1)
    v2 = text.Lexicon(path=f2)
    self.assertEqual(v1["schrödinger"], "NNP")
    self.assertEqual(v2["schrödinger"], "NNP")
    print("pattern.text.Lexicon")

</t>
<t tx="karstenw.20230303140222.7">class TestFrequency(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140222.8">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140222.9">def test_frequency(self):
    # Assert word frequency from file (or file-like string).
    f1 = ";;; Comments. \n the 1.0000 \n of 0.5040"
    f2 = StringIO(";;; Comments. \n the 1.0000 \n of 0.5040")
    v1 = text.Frequency(path=f1)
    v2 = text.Frequency(path=f2)
    self.assertEqual(v1["of"], 0.504)
    self.assertEqual(v2["of"], 0.504)
    print("pattern.text.Frequency")

</t>
<t tx="karstenw.20230303140224.1">from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

from io import open

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import time
import random
import unittest

from random import seed
seed(0)

from pattern import vector

from pattern.en import Text, Sentence, Word, parse
from pattern.db import Datasheet

try:
    PATH = os.path.dirname(os.path.realpath(__file__))
except:
    PATH = ""


</t>
<t tx="karstenw.20230303140224.10">def test_shuffled(self):
    # Assert shuffled() &lt;=&gt; sorted().
    v1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    v2 = vector.shuffled(v1)
    self.assertTrue(v1 != v2 and v1 == sorted(v2))
    print("pattern.vector.shuffled()")

</t>
<t tx="karstenw.20230303140224.11">def test_chunk(self):
    # Assert list chunk (near-)equal size.
    for a, n, b in (
      ([1, 2, 3, 4, 5], 0, []),
      ([1, 2, 3, 4, 5], 1, [[1, 2, 3, 4, 5]]),
      ([1, 2, 3, 4, 5], 2, [[1, 2, 3], [4, 5]]),
      ([1, 2, 3, 4, 5], 3, [[1, 2], [3, 4], [5]]),
      ([1, 2, 3, 4, 5], 4, [[1, 2], [3], [4], [5]]),
      ([1, 2, 3, 4, 5], 5, [[1], [2], [3], [4], [5]]),
      ([1, 2, 3, 4, 5], 6, [[1], [2], [3], [4], [5], []])):
        self.assertEqual(list(vector.chunk(a, n)), b)
    print("pattern.vector.chunk()")

</t>
<t tx="karstenw.20230303140224.12">def test_readonlydict(self):
    # Assert read-only dict.
    v = vector.readonlydict({"a": 1})
    self.assertTrue(isinstance(v, dict))
    self.assertRaises(vector.ReadOnlyError, v.__setitem__, "a", 2)
    self.assertRaises(vector.ReadOnlyError, v.__delitem__, "a")
    self.assertRaises(vector.ReadOnlyError, v.pop, "a")
    self.assertRaises(vector.ReadOnlyError, v.popitem, ("a", 2))
    self.assertRaises(vector.ReadOnlyError, v.clear)
    self.assertRaises(vector.ReadOnlyError, v.update, {"b": 2})
    self.assertRaises(vector.ReadOnlyError, v.setdefault, "b", 2)
    print("pattern.vector.readonlydict")

</t>
<t tx="karstenw.20230303140224.13">def test_readonlylist(self):
    # Assert read-only list.
    v = vector.readonlylist([1, 2])
    self.assertTrue(isinstance(v, list))
    self.assertRaises(vector.ReadOnlyError, v.__setitem__, 0, 0)
    self.assertRaises(vector.ReadOnlyError, v.__delitem__, 0)
    self.assertRaises(vector.ReadOnlyError, v.append, 3)
    self.assertRaises(vector.ReadOnlyError, v.insert, 2, 3)
    self.assertRaises(vector.ReadOnlyError, v.extend, [3, 4])
    self.assertRaises(vector.ReadOnlyError, v.remove, 1)
    self.assertRaises(vector.ReadOnlyError, v.pop, 0)
    print("pattern.vector.readonlylist")

</t>
<t tx="karstenw.20230303140224.14">class TestStemmer(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140224.15">def setUp(self):
    # Test data from http://snowball.tartarus.org/algorithms/english/stemmer.html
    self.input = [
        'consign', 'consigned', 'consigning', 'consignment', 'consist', 'consisted', 'consistency',
        'consistent', 'consistently', 'consisting', 'consists', 'consolation', 'consolations',
        'consolatory', 'console', 'consoled', 'consoles', 'consolidate', 'consolidated', 'consolidating',
        'consoling', 'consolingly', 'consols', 'consonant', 'consort', 'consorted', 'consorting',
        'conspicuous', 'conspicuously', 'conspiracy', 'conspirator', 'conspirators', 'conspire',
        'conspired', 'conspiring', 'constable', 'constables', 'constance', 'constancy', 'constant',
        'generate', 'generates', 'generated', 'generating', 'general', 'generally', 'generic',
        'generically', 'generous', 'generously', 'knack', 'knackeries', 'knacks', 'knag', 'knave',
        'knaves', 'knavish', 'kneaded', 'kneading', 'knee', 'kneel', 'kneeled', 'kneeling', 'kneels',
        'knees', 'knell', 'knelt', 'knew', 'knick', 'knif', 'knife', 'knight', 'knightly', 'knights',
        'knit', 'knits', 'knitted', 'knitting', 'knives', 'knob', 'knobs', 'knock', 'knocked', 'knocker',
        'knockers', 'knocking', 'knocks', 'knopp', 'knot', 'knots', 'skies', 'spy'
    ]
    self.output = [
        'consign', 'consign', 'consign', 'consign', 'consist', 'consist', 'consist', 'consist', 'consist',
        'consist', 'consist', 'consol', 'consol', 'consolatori', 'consol', 'consol', 'consol', 'consolid',
        'consolid', 'consolid', 'consol', 'consol', 'consol', 'conson', 'consort', 'consort', 'consort',
        'conspicu', 'conspicu', 'conspiraci', 'conspir', 'conspir', 'conspir', 'conspir', 'conspir',
        'constabl', 'constabl', 'constanc', 'constanc', 'constant', 'generat', 'generat', 'generat',
        'generat', 'general', 'general', 'generic', 'generic', 'generous', 'generous', 'knack', 'knackeri',
        'knack', 'knag', 'knave', 'knave', 'knavish', 'knead', 'knead', 'knee', 'kneel', 'kneel', 'kneel',
        'kneel', 'knee', 'knell', 'knelt', 'knew', 'knick', 'knif', 'knife', 'knight', 'knight', 'knight',
        'knit', 'knit', 'knit', 'knit', 'knive', 'knob', 'knob', 'knock', 'knock', 'knocker', 'knocker',
        'knock', 'knock', 'knopp', 'knot', 'knot', 'sky', 'spi'
    ]

</t>
<t tx="karstenw.20230303140224.16">def test_stem(self):
    # Assert the accuracy of the stemmer.
    i = 0
    n = len(self.input)
    for a, b in zip(self.input, self.output):
        if vector.stemmer.stem(a, cached=True) == b:
            i += 1
    self.assertEqual(float(i) / n, 1.0)
    print("pattern.vector.stemmer.stem()")

</t>
<t tx="karstenw.20230303140224.17">def test_stem_case_sensitive(self):
    # Assert stemmer case-sensitivity.
    for a, b in (
      ("Ponies", "Poni"),
      ("pONIES", "pONI"),
      ("SKiES", "SKy"),
      ("cosmos", "cosmos")):
        self.assertEqual(vector.stemmer.stem(a), b)
    print("pattern.vector.stemmer.case_sensitive()")

</t>
<t tx="karstenw.20230303140224.18">class TestDocument(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140224.19">def setUp(self):
    # Test file for loading and saving documents.
    self.path = "test_document2.txt"

</t>
<t tx="karstenw.20230303140224.2">def model(top=None):
    """ Returns a Model of e-mail messages.
        Document type=True =&gt; HAM, False =&gt; SPAM.
        Documents are mostly of a technical nature (developer forum posts).
    """
    documents = []
    for score, message in Datasheet.load(os.path.join(PATH, "corpora", "spam-apache.csv")):
        document = vector.Document(message, stemmer="porter", top=top, type=int(score) &gt; 0)
        documents.append(document)
    return vector.Model(documents)

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140224.20">def tearDown(self):
    if os.path.exists(self.path):
        os.remove(self.path)

</t>
<t tx="karstenw.20230303140224.21">def test_stopwords(self):
    # Assert common stop words.
    for w in ("a", "am", "an", "and", "i", "the", "therefore", "they", "what", "while"):
        self.assertTrue(w in vector.stopwords["en"])
    print("pattern.vector.stopwords")

</t>
<t tx="karstenw.20230303140224.22">def test_words(self):
    # Assert word split algorithm (default treats lines as spaces and ignores numbers).
    s = "The cat sat on the\nmat. 1 11."
    v = vector.words(s, filter=lambda w: w.isalpha())
    self.assertEqual(v, ["The", "cat", "sat", "on", "the", "mat"])
    # Assert custom word filter.
    v = vector.words(s, filter=lambda w: True)
    self.assertEqual(v, ["The", "cat", "sat", "on", "the", "mat", "1", "11"])
    print("pattern.vector.words()")

</t>
<t tx="karstenw.20230303140224.23">def test_stem(self):
    # Assert stem with PORTER, LEMMA and pattern.en.Word.
    s = "WOLVES"
    v1 = vector.stem(s, stemmer=None)
    v2 = vector.stem(s, stemmer=vector.PORTER)
    v3 = vector.stem(s, stemmer=vector.LEMMA)
    v4 = vector.stem(s, stemmer=lambda w: "wolf*")
    v5 = vector.stem(Word(None, s, lemma="wolf*"), stemmer=vector.LEMMA)
    v6 = vector.stem(Word(None, s, type="NNS"), stemmer=vector.LEMMA)
    self.assertEqual(v1, "wolves")
    self.assertEqual(v2, "wolv")
    self.assertEqual(v3, "wolf")
    self.assertEqual(v4, "wolf*")
    self.assertEqual(v5, "wolf*")
    self.assertEqual(v6, "wolf")
    # Assert unicode output.
    self.assertTrue(isinstance(v1, str))
    self.assertTrue(isinstance(v2, str))
    self.assertTrue(isinstance(v3, str))
    self.assertTrue(isinstance(v4, str))
    self.assertTrue(isinstance(v5, str))
    self.assertTrue(isinstance(v6, str))
    print("pattern.vector.stem()")

</t>
<t tx="karstenw.20230303140224.24">def test_count(self):
    # Assert wordcount with stemming, stopwords and pruning.
    w = ["The", "cats", "sat", "on", "the", "mat", "."]
    v1 = vector.count(w)
    v2 = vector.count(w, stemmer=vector.LEMMA)
    v3 = vector.count(w, exclude=["."])
    v4 = vector.count(w, stopwords=True)
    v5 = vector.count(w, stopwords=True, top=3)
    v6 = vector.count(w, stopwords=True, top=3, threshold=1)
    v7 = vector.count(w, dict=vector.readonlydict, cached=False)
    self.assertEqual(v1, {"cats": 1, "sat": 1, "mat": 1, ".": 1})
    self.assertEqual(v2, {"cat": 1, "sat": 1, "mat": 1, ".": 1})
    self.assertEqual(v3, {"cats": 1, "sat": 1, "mat": 1})
    self.assertEqual(v4, {"the": 2, "cats": 1, "sat": 1, "on": 1, "mat": 1, ".": 1})
    self.assertEqual(v5, {"the": 2, "cats": 1, ".": 1})
    self.assertEqual(v6, {"the": 2})
    # Assert custom dict class.
    self.assertTrue(isinstance(v7, vector.readonlydict))
    print("pattern.vector.count()")

</t>
<t tx="karstenw.20230303140224.25">def test_document(self):
    # Assert Document properties.
    # Test with different input types.
    for constructor, w in (
      (vector.Document, "The cats sit on the mat."),
      (vector.Document, ["The", "cats", "sit", "on", "the", "mat"]),
      (vector.Document, {"cat": 1, "mat": 1, "sit": 1}),
      (vector.Document, Text(parse("The cats sat on the mat."))),
      (vector.Document, Sentence(parse("The cats sat on the mat.")))):
        # Test copy.
        v = constructor(w, stemmer=vector.LEMMA, stopwords=False, name="Cat", type="CAT")
        v = v.copy()
        # Test properties.
        self.assertEqual(v.name, "Cat")
        self.assertEqual(v.type, "CAT")
        self.assertEqual(v.count, 3)
        self.assertEqual(v.terms, {"cat": 1, "mat": 1, "sit": 1})
        # Test iterator decoration.
        self.assertEqual(sorted(v.features), ["cat", "mat", "sit"])
        self.assertEqual(sorted(v), ["cat", "mat", "sit"])
        self.assertEqual(len(v), 3)
        self.assertEqual(v["cat"], 1)
        self.assertEqual("cat" in v, True)
    print("pattern.vector.Document")

</t>
<t tx="karstenw.20230303140224.26">def test_document_load(self):
    # Assert save + load document integrity.
    v1 = "The cats are purring on the mat."
    v1 = vector.Document(v1, stemmer=vector.PORTER, stopwords=True, name="Cat", type="CAT")
    v1.save(self.path)
    v2 = vector.Document.load(self.path)
    self.assertEqual(v1.name, v2.name)
    self.assertEqual(v1.type, v2.type)
    self.assertEqual(v1.vector, v2.vector)
    print("pattern.vector.Document.save()")
    print("pattern.vector.Document.load()")

</t>
<t tx="karstenw.20230303140224.27">def test_document_vector(self):
    # Assert Vector properties.
    # Test copy.
    v = vector.Document("the cat sat on the mat").vector
    v = v.copy()
    # Test properties.
    self.assertTrue(isinstance(v, dict))
    self.assertTrue(isinstance(v, vector.Vector))
    self.assertTrue(isinstance(v.id, int))
    self.assertEqual(sorted(v.features), ["cat", "mat", "sat"])
    self.assertEqual(v.weight, vector.TF)
    self.assertAlmostEqual(v.norm, 0.58, places=2)
    self.assertAlmostEqual(v["cat"], 0.33, places=2)
    self.assertAlmostEqual(v["sat"], 0.33, places=2)
    self.assertAlmostEqual(v["mat"], 0.33, places=2)
    # Test copy + update.
    v = v({"cat": 1, "sat": 1, "mat": 1})
    self.assertEqual(sorted(v.features), ["cat", "mat", "sat"])
    self.assertAlmostEqual(v["cat"], 1.00, places=2)
    self.assertAlmostEqual(v["sat"], 1.00, places=2)
    self.assertAlmostEqual(v["mat"], 1.00, places=2)
    print("pattern.vector.Document.vector")

</t>
<t tx="karstenw.20230303140224.28">def test_document_keywords(self):
    # Assert Document.keywords() based on term frequency.
    v = vector.Document(["cat", "cat", "cat", "sat", "sat", "mat"]).keywords(top=2)
    self.assertEqual(len(v), 2)
    self.assertEqual(v[0][1], "cat")
    self.assertEqual(v[1][1], "sat")
    self.assertAlmostEqual(v[0][0], 0.50, places=2)
    self.assertAlmostEqual(v[1][0], 0.33, places=2)
    print("pattern.vector.Document.keywords()")

</t>
<t tx="karstenw.20230303140224.29">def test_tf(self):
    # Assert Document.term_frequency() (= weights used in Vector for orphaned documents).
    v = vector.Document("the cat sat on the mat")
    for feature, weight in v.vector.items():
        self.assertEqual(v.term_frequency(feature), weight)
        self.assertAlmostEqual(v.term_frequency(feature), 0.33, places=2)
    print("pattern.vector.Document.tf()")

</t>
<t tx="karstenw.20230303140224.3">class TestUnicode(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140224.30">def test_tfidf(self):
    # Assert tf-idf for documents not in a model.
    v = [[0.0, 0.4, 0.6], [0.6, 0.4, 0.0]]
    v = [dict(enumerate(v)) for v in v]
    m = vector.Model([vector.Document(x) for x in v], weight=vector.TFIDF)
    v = [vector.sparse(v) for v in vector.tf_idf(v)]
    self.assertEqual(sorted(m[0].vector.items()), sorted(v[0].items()))
    self.assertAlmostEqual(v[0][2], 0.42, places=2)
    self.assertAlmostEqual(v[1][0], 0.42, places=2)
    print("pattern.vector.tf_idf()")

</t>
<t tx="karstenw.20230303140224.31">def test_cosine_similarity(self):
    # Test cosine similarity for documents not in a model.
    v1 = vector.Document("the cat sat on the mat")
    v2 = vector.Document("a cat with a hat")
    self.assertAlmostEqual(v1.cosine_similarity(v2), 0.41, places=2)
    print("pattern.vector.Document.similarity()")
    print("pattern.vector.cosine_similarity()")
    print("pattern.vector.l2_norm()")

</t>
<t tx="karstenw.20230303140224.32">class TestModel(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140224.33">def setUp(self):
    # Test model.
    self.model = vector.Model(documents=(
        vector.Document("cats purr", name="cat1", type="cåt"),
        vector.Document("cats meow", name="cat2", type="cåt"),
        vector.Document("dogs howl", name="dog1", type="døg"),
        vector.Document("dogs bark", name="dog2", type="døg")
    ))

</t>
<t tx="karstenw.20230303140224.34">def test_model(self):
    # Assert Model properties.
    v = self.model
    self.assertEqual(list(v), v.documents)
    self.assertEqual(len(v), 4)
    self.assertEqual(sorted(v.terms), ["bark", "cats", "dogs", "howl", "meow", "purr"])
    self.assertEqual(sorted(v.terms), sorted(v.vector.keys()))
    self.assertEqual(v.weight, vector.TFIDF)
    self.assertEqual(v.lsa, None)
    self.assertEqual(v.vectors, [d.vector for d in v.documents])
    self.assertAlmostEqual(v.density, 0.22, places=2)
    print("pattern.vector.Model")

</t>
<t tx="karstenw.20230303140224.35">def test_model_append(self):
    # Assert Model.append().
    self.assertRaises(vector.ReadOnlyError, self.model.documents.append, None)
    self.model.append(vector.Document("birds chirp", name="bird"))
    self.assertEqual(self.model[0]._vector, None)
    self.assertEqual(len(self.model), 5)
    self.model.remove(self.model.document("bird"))
    print("pattern.vector.Model.append()")

</t>
<t tx="karstenw.20230303140224.36">def test_model_save(self):
    # Assert Model save &amp; load.
    self.model.save("test_model.pickle", update=True)
    self.model._update()
    model = vector.Model.load("test_model.pickle")
    # Assert that the precious cache is saved and reloaded.
    self.assertTrue(len(model._df) &gt; 0)
    self.assertTrue(len(model._cos) &gt; 0)
    self.assertTrue(len(model.vectors) &gt; 0)
    os.remove("test_model.pickle")
    print("pattern.vector.Model.save()")
    print("pattern.vector.Model.load()")

</t>
<t tx="karstenw.20230303140224.37">def test_model_export(self):
    # Assert Orange and Weka ARFF export formats.
    for format, src in (
        (vector.ORANGE,
            "bark\tcats\tdogs\thowl\tmeow\tpurr\tm#name\tc#type\n"
            "0\t0.3466\t0\t0\t0\t0.6931\tcat1\tcåt\n"
            "0\t0.3466\t0\t0\t0.6931\t0\tcat2\tcåt\n"
            "0\t0\t0.3466\t0.6931\t0\t0\tdog1\tdøg\n"
            "0.6931\t0\t0.3466\t0\t0\t0\tdog2\tdøg"),
        (vector.WEKA,
            "@RELATION 5885744\n"
            "@ATTRIBUTE bark NUMERIC\n"
            "@ATTRIBUTE cats NUMERIC\n"
            "@ATTRIBUTE dogs NUMERIC\n"
            "@ATTRIBUTE howl NUMERIC\n"
            "@ATTRIBUTE meow NUMERIC\n"
            "@ATTRIBUTE purr NUMERIC\n"
            "@ATTRIBUTE class {døg,cåt}\n"
            "@DATA\n0,0.3466,0,0,0,0.6931,cåt\n"
            "0,0.3466,0,0,0.6931,0,cåt\n"
            "0,0,0.3466,0.6931,0,0,døg\n"
            "0.6931,0,0.3466,0,0,0,døg")):
        self.model.export("test_%s.txt" % format, format=format)
        v = open("test_%s.txt" % format, encoding="utf-8").read()
        v = v.replace("\r\n", "\n")
        for line in src.split("\n"):
            self.assertTrue(line in src)
        os.remove("test_%s.txt" % format)
    print("pattern.vector.Model.export()")

</t>
<t tx="karstenw.20230303140224.38">def test_df(self):
    # Assert document frequency: "cats" appears in 1/2 documents,"purr" in 1/4.
    self.assertEqual(self.model.df("cats"), 0.50)
    self.assertEqual(self.model.df("purr"), 0.25)
    self.assertEqual(self.model.df("????"), 0.00)
    print("pattern.vector.Model.df()")

</t>
<t tx="karstenw.20230303140224.39">def test_idf(self):
    # Assert inverse document frequency: log(1/df).
    self.assertAlmostEqual(self.model.idf("cats"), 0.69, places=2)
    self.assertAlmostEqual(self.model.idf("purr"), 1.39, places=2)
    self.assertEqual(self.model.idf("????"), None)
    print("pattern.vector.Model.idf()")

</t>
<t tx="karstenw.20230303140224.4">def setUp(self):
    # Test data with different (or wrong) encodings.
    self.strings = (
        "ünîcøde",
        "ünîcøde".encode("utf-16"),
        "ünîcøde".encode("latin-1"),
        "ünîcøde".encode("windows-1252"),
        "ünîcøde",
        "אוניקאָד"
    )

</t>
<t tx="karstenw.20230303140224.40">def test_tfidf(self):
    # Assert term frequency - inverse document frequency: tf * idf.
    self.assertAlmostEqual(self.model[0].tfidf("cats"), 0.35, places=2) # 0.50 * 0.69
    self.assertAlmostEqual(self.model[0].tfidf("purr"), 0.69, places=2) # 0.50 * 1.39
    self.assertAlmostEqual(self.model[0].tfidf("????"), 0.00, places=2)
    print("pattern.vector.Document.tfidf()")

</t>
<t tx="karstenw.20230303140224.41">def test_frequent_concept_sets(self):
    # Assert Apriori algorithm.
    v = self.model.frequent(threshold=0.5)
    if sys.version &gt; "3":
        self.assertCountEqual(sorted(list(v.keys())), [frozenset(["dogs"]), frozenset(["cats"])])
    else:
        self.assertItemsEqual(sorted(list(v.keys())), [frozenset(["dogs"]), frozenset(["cats"])])
    print("pattern.vector.Model.frequent()")

</t>
<t tx="karstenw.20230303140224.42">def test_cosine_similarity(self):
    # Assert document cosine similarity.
    v1 = self.model.similarity(self.model[0], self.model[1])
    v2 = self.model.similarity(self.model[0], self.model[2])
    v3 = self.model.similarity(self.model[0], vector.Document("cats cats"))
    self.assertAlmostEqual(v1, 0.20, places=2)
    self.assertAlmostEqual(v2, 0.00, places=2)
    self.assertAlmostEqual(v3, 0.45, places=2)
    # Assert that Model.similarity() is aware of LSA reduction.
    self.model.reduce(2)
    v1 = self.model.similarity(self.model[0], self.model[1])
    v2 = self.model.similarity(self.model[0], self.model[2])
    self.assertAlmostEqual(v1, 1.00, places=2)
    self.assertAlmostEqual(v2, 0.00, places=2)
    self.model.lsa = None
    print("pattern.vector.Model.similarity()")

</t>
<t tx="karstenw.20230303140224.43">def test_nearest_neighbors(self):
    # Assert document nearest-neighbor search.
    v1 = self.model.neighbors(self.model[0])
    v2 = self.model.neighbors(vector.Document("cats meow"))
    v3 = self.model.neighbors(vector.Document("????"))
    self.assertEqual(v1[0][1], self.model[1])
    self.assertEqual(v2[0][1], self.model[1])
    self.assertEqual(v2[1][1], self.model[0])
    self.assertAlmostEqual(v1[0][0], 0.20, places=2)
    self.assertAlmostEqual(v2[0][0], 0.95, places=2)
    self.assertAlmostEqual(v2[1][0], 0.32, places=2)
    self.assertTrue(len(v3) == 0)
    print("pattern.vector.Model.neighbors()")

</t>
<t tx="karstenw.20230303140224.44">def test_search(self):
    # Assert document vector space search.
    v1 = self.model.search(self.model[0])
    v2 = self.model.search(vector.Document("cats meow"))
    v3 = self.model.search(vector.Document("????"))
    v4 = self.model.search("meow")
    v5 = self.model.search(["cats", "meow"])
    self.assertEqual(v1, self.model.neighbors(self.model[0]))
    self.assertEqual(v2[0][1], self.model[1])
    self.assertEqual(v3, [])
    self.assertEqual(v4[0][1], self.model[1])
    self.assertEqual(v5[0][1], self.model[1])
    self.assertAlmostEqual(v4[0][0], 0.89, places=2)
    self.assertAlmostEqual(v5[0][0], 1.00, places=2)
    print("pattern.vector.Model.search()")

</t>
<t tx="karstenw.20230303140224.45">def test_distance(self):
    # Assert Model document distance.
    v1 = self.model.distance(self.model[0], self.model[1], method=vector.COSINE)
    v2 = self.model.distance(self.model[0], self.model[2], method=vector.COSINE)
    v3 = self.model.distance(self.model[0], self.model[2], method=vector.EUCLIDEAN)
    self.assertAlmostEqual(v1, 0.8, places=1)
    self.assertAlmostEqual(v2, 1.0, places=1)
    self.assertAlmostEqual(v3, 1.2, places=1)
    print("pattern.vector.Model.distance()")

</t>
<t tx="karstenw.20230303140224.46">def test_cluster(self):
    # Assert Model document clustering.
    v1 = self.model.cluster(method=vector.KMEANS, k=10)
    v2 = self.model.cluster(method=vector.HIERARCHICAL, k=1)
    self.assertTrue(isinstance(v1, list) and len(v1) == 10)
    self.assertTrue(isinstance(v2, vector.Cluster))

    def _test_clustered_documents(cluster):
        if self.model[0] in cluster:
            self.assertTrue(self.model[1] in cluster \
                    and not self.model[2] in cluster)
        if self.model[2] in cluster:
            self.assertTrue(self.model[3] in cluster \
                    and not self.model[1] in cluster)
    v2.traverse(_test_clustered_documents)
    print("pattern.vector.Model.cluster()")

</t>
<t tx="karstenw.20230303140224.47">def test_centroid(self):
    # Assert centroid of recursive Cluster.
    v = vector.Cluster(({"a": 1}, vector.Cluster(({"a": 2}, {"a": 4}))))
    self.assertAlmostEqual(vector.centroid(v)["a"], 2.33, places=2)
    print("pattern.vector.centroid()")

</t>
<t tx="karstenw.20230303140224.48">def test_lsa(self):
    # Assert Model.reduce() LSA reduction.
    self.model.reduce(2)
    self.assertTrue(isinstance(self.model.lsa, vector.LSA))
    self.model.lsa = None
    print("pattern.vector.Model.reduce()")

</t>
<t tx="karstenw.20230303140224.49">def test_feature_selection(self):
    # Assert information gain feature selection.
    m = vector.Model((
        vector.Document("the cat sat on the mat", type="cat", stopwords=True),
        vector.Document("the dog howled at the moon", type="dog", stopwords=True)
    ))
    v = m.feature_selection(top=3, method=vector.IG, threshold=0.0)
    self.assertEqual(v, ["at", "cat", "dog"])
    # Assert Model.filter().
    v = m.filter(v)
    self.assertTrue("at" in v.terms)
    self.assertTrue("cat" in v.terms)
    self.assertTrue("dog" in v.terms)
    self.assertTrue("the" not in v.terms)
    self.assertTrue("mat" not in v.terms)
    print("pattern.vector.Model.feature_selection()")
    print("pattern.vector.Model.filter()")

</t>
<t tx="karstenw.20230303140224.5">def test_decode_utf8(self):
    # Assert unicode.
    for s in self.strings:
        self.assertTrue(isinstance(vector.decode_utf8(s), str))
    print("pattern.vector.decode_utf8()")

</t>
<t tx="karstenw.20230303140224.50">def test_information_gain(self):
    # Assert information gain weights.
    # Example from http://www.comp.lancs.ac.uk/~kc/Lecturing/csc355/DecisionTrees_given.pdf
    m = vector.Model([
        vector.Document({"wind": 1}, type=False),
        vector.Document({"wind": 0}, type=True),
        vector.Document({"wind": 0}, type=True),
        vector.Document({"wind": 0}, type=True),
        vector.Document({"wind": 1}, type=True),
        vector.Document({"wind": 1}, type=False),
        vector.Document({"wind": 1}, type=False)], weight=None
    )
    self.assertAlmostEqual(m.information_gain("wind"), 0.52, places=2)
    # Example from http://rutcor.rutgers.edu/~amai/aimath02/PAPERS/14.pdf
    m = vector.Model([
        vector.Document({"3": 1}, type=True),
        vector.Document({"3": 5}, type=True),
        vector.Document({"3": 1}, type=False),
        vector.Document({"3": 7}, type=True),
        vector.Document({"3": 2}, type=False),
        vector.Document({"3": 2}, type=True),
        vector.Document({"3": 6}, type=False),
        vector.Document({"3": 4}, type=True),
        vector.Document({"3": 0}, type=False),
        vector.Document({"3": 9}, type=True)], weight=None
    )
    self.assertAlmostEqual(m.ig("3"), 0.571, places=3)
    self.assertAlmostEqual(m.gr("3"), 0.195, places=3)
    print("patten.vector.Model.information_gain()")
    print("patten.vector.Model.gain_ratio()")

</t>
<t tx="karstenw.20230303140224.51">def test_entropy(self):
    # Assert Shannon entropy calculcation.
    self.assertAlmostEqual(vector.entropy([1, 1]), 1.00, places=2)
    self.assertAlmostEqual(vector.entropy([2, 1]), 0.92, places=2)
    self.assertAlmostEqual(vector.entropy([0.5, 0.5]), 1.00, places=2)
    self.assertAlmostEqual(vector.entropy([0.6]), 0.44, places=2)
    print("pattern.vector.entropy()")

</t>
<t tx="karstenw.20230303140224.52">def test_condensed_nearest_neighbor(self):
    # Assert CNN for data reduction.
    v = vector.Model((
        vector.Document("woof", type="dog"),
        vector.Document("meow", type="cat"),  # redundant
        vector.Document("meow meow", type="cat")
    ))
    self.assertTrue(len(v.cnn()) &lt; len(v))
    print("pattern.vector.Model.condensed_nearest_neighbor()")

</t>
<t tx="karstenw.20230303140224.53">def test_classifier(self):
    # Assert that the model classifier is correctly saved and loaded.
    p = "test.model.tmp"
    v = vector.Model([vector.Document("chirp", type="bird")])
    v.train(vector.SVM)
    v.save(p)
    v = vector.Model.load(p)
    self.assertTrue(isinstance(v.classifier, vector.SVM))
    os.unlink(p)
    print("pattern.vector.Model.classifier")
    print("pattern.vector.Model.train()")

</t>
<t tx="karstenw.20230303140224.54">class TestApriori(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140224.55">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140224.56">def test_apriori(self):
    # Assert frequent sets frequency.
    v = vector.apriori((
        [1, 2, 4],
        [1, 2, 5],
        [1, 3, 6],
        [1, 3, 7]
    ), support=0.5)
    self.assertTrue(len(v), 3)
    self.assertEqual(v[frozenset((1, ))], 1.0)
    self.assertEqual(v[frozenset((1, 2))], 0.5)
    self.assertEqual(v[frozenset((2, ))], 0.5)
    self.assertEqual(v[frozenset((3, ))], 0.5)

</t>
<t tx="karstenw.20230303140224.57">class TestLSA(unittest.TestCase):

    model = None

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140224.58">def setUp(self):
    # Test spam model for reduction.
    if self.__class__.model is None:
        self.__class__.model = model(top=250)
    self.model = self.__class__.model
    random.seed(0)

</t>
<t tx="karstenw.20230303140224.59">def tearDown(self):
    random.seed()

</t>
<t tx="karstenw.20230303140224.6">def test_encode_utf8(self):
    # Assert Python bytestring.
    for s in self.strings:
        self.assertTrue(isinstance(vector.encode_utf8(s), bytes))
    print("pattern.vector.encode_utf8()")

</t>
<t tx="karstenw.20230303140224.60">def test_lsa(self):
    # Assert LSA properties.
    k = 100
    lsa = vector.LSA(self.model, k)
    self.assertEqual(lsa.model, self.model)
    self.assertEqual(lsa.vectors, lsa.u)
    self.assertEqual(set(lsa.terms), set(self.model.vector.keys()))
    self.assertTrue(isinstance(lsa.u, dict))
    self.assertTrue(isinstance(lsa.sigma, list))
    self.assertTrue(isinstance(lsa.vt, list))
    self.assertTrue(len(lsa.u), len(self.model))
    self.assertTrue(len(lsa.sigma), len(self.model) - k)
    self.assertTrue(len(lsa.vt), len(self.model) - k)
    for document in self.model:
        v = lsa.vectors[document.id]
        self.assertTrue(isinstance(v, vector.Vector))
        self.assertTrue(len(v) &lt;= k)
    print("pattern.vector.LSA")

</t>
<t tx="karstenw.20230303140224.61">def test_lsa_concepts(self):
    # Assert LSA concept space.
    model = vector.Model((
        vector.Document("cats purr"),
        vector.Document("cats meow"),
        vector.Document("dogs howl"),
        vector.Document("dogs bark")
    ))
    model.reduce(2)
    # Intuitively, we'd expect two concepts:
    # 1) with cats + purr + meow grouped together,
    # 2) with dogs + howl + bark grouped together.
    i1, i2 = 0, 0
    for i, concept in enumerate(model.lsa.concepts):
        self.assertTrue(isinstance(concept, dict))
        if concept["cats"] &gt; 0.5:
            self.assertTrue(concept["purr"] &gt; 0.5)
            self.assertTrue(concept["meow"] &gt; 0.5)
            self.assertTrue(concept["howl"] == 0.0)
            self.assertTrue(concept["bark"] == 0.0)
            i1 = i
        if concept["dogs"] &gt; 0.5:
            self.assertTrue(concept["howl"] &gt; 0.5)
            self.assertTrue(concept["bark"] &gt; 0.5)
            self.assertTrue(concept["purr"] == 0.0)
            self.assertTrue(concept["meow"] == 0.0)
            i2 = i
    # We'd expect the "cat" documents to score high on the "cat" concept vector.
    # We'd expect the "dog" documents to score high on the "dog" concept vector.
    v1 = model.lsa[model.documents[0].id]
    v2 = model.lsa[model.documents[2].id]
    self.assertTrue(v1.get(i1, 0) &gt; 0.7)
    self.assertTrue(v1.get(i2, 0) == 0.0)
    self.assertTrue(v2.get(i1, 0) == 0.0)
    self.assertTrue(v2.get(i2, 0) &gt; 0.7)
    # Assert LSA.transform() for unknown documents.
    v = model.lsa.transform(vector.Document("cats dogs"))
    self.assertAlmostEqual(v[0], 0.34, places=2)
    self.assertAlmostEqual(v[1], 0.34, places=2)
    print("pattern.vector.LSA.concepts")
    print("pattern.vector.LSA.transform()")

</t>
<t tx="karstenw.20230303140224.62">def test_model_reduce(self):
    # Test time and accuracy of model with sparse vectors of maximum 250 features.
    t1 = time.time()
    A1, P1, R1, F1, stdev = vector.KNN.test(self.model, folds=10)
    t1 = time.time() - t1
    # Test time and accuracy of model with reduced vectors of 20 features.
    self.model.reduce(dimensions=20)
    t2 = time.time()
    A2, P2, R2, F2, stdev = vector.KNN.test(self.model, folds=10)
    t2 = time.time() - t2
    self.assertTrue(len(self.model.lsa[self.model.documents[0].id]) == 20)
    self.assertTrue(t2 * 2 &lt; t1)       # KNN over 2x faster.
    self.assertTrue(abs(F1 - F2) &lt; 0.06) # Difference in F-score = 1-6%.
    self.model.lsa = None
    print("pattern.vector.Model.reduce()")

</t>
<t tx="karstenw.20230303140224.63">class TestClustering(unittest.TestCase):

    model = None

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140224.64">def setUp(self):
    # Test spam model for clustering.
    if self.__class__.model is None:
        self.__class__.model = model(top=10)
    self.model = self.__class__.model
    random.seed(0)

</t>
<t tx="karstenw.20230303140224.65">def tearDown(self):
    random.seed()

</t>
<t tx="karstenw.20230303140224.66">def test_features(self):
    # Assert unique list of vector keys.
    v = vector.features(vectors=[{"cat": 1}, {"dog": 1}])
    self.assertEqual(sorted(v), ["cat", "dog"])
    print("pattern.vector.features()")

</t>
<t tx="karstenw.20230303140224.67">def test_mean(self):
    # Assert iterator mean.
    self.assertEqual(vector.mean([], 0), 0)
    self.assertEqual(vector.mean([1, 1.5, 2], 3), 1.5)
    self.assertEqual(vector.mean(range(4), 4), 1.5)
    print("pattern.vector.mean()")

</t>
<t tx="karstenw.20230303140224.68">def test_centroid(self):
    # Assert center of list of vectors.
    v = vector.centroid([{"cat": 1}, {"cat": 0.5, "dog": 1}], features=["cat", "dog"])
    self.assertEqual(v, {"cat": 0.75, "dog": 0.5})
    print("pattern.vector.centroid()")

</t>
<t tx="karstenw.20230303140224.69">def test_distance(self):
    # Assert distance metrics.
    v1 = vector.Vector({"cat": 1})
    v2 = vector.Vector({"cat": 0.5, "dog": 1})
    for d, method in (
      (0.55, vector.COSINE),    # 1 - ((1*0.5 + 0*1) / (sqrt(1**2 + 0**2) * sqrt(0.5**2 + 1**2)))
      (1.25, vector.EUCLIDEAN), # (1-0.5)**2 + (0-1)**2
      (1.50, vector.MANHATTAN), # abs(1-0.5) + abs(0-1)
      (1.00, vector.HAMMING),   # (True + True) / 2
      (1.11, lambda v1, v2: 1.11)):
        self.assertAlmostEqual(vector.distance(v1, v2, method), d, places=2)
    print("pattern.vector.distance()")

</t>
<t tx="karstenw.20230303140224.7">class TestUtilityFunctions(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140224.70">def test_distancemap(self):
    # Assert distance caching mechanism.
    v1 = vector.Vector({"cat": 1})
    v2 = vector.Vector({"cat": 0.5, "dog": 1})
    m = vector.DistanceMap(method=vector.COSINE)
    for i in range(100):
        self.assertAlmostEqual(m.distance(v1, v2), 0.55, places=2)
        self.assertAlmostEqual(m._cache[(v1.id, v2.id)], 0.55, places=2)
    print("pattern.vector.DistanceMap")

</t>
<t tx="karstenw.20230303140224.71">def _test_k_means(self, seed):
    # Assert k-means clustering accuracy.
    A = []
    n = 100
    m = dict((d.vector.id, d.type) for d in self.model[:n])
    for i in range(30):
        # Create two clusters of vectors.
        k = vector.kmeans([d.vector for d in self.model[:n]], k=2, seed=seed)
        # Measure the number of spam in each clusters.
        # Ideally, we have a cluster without spam and one with only spam.
        i = len([1 for v in k[0] if m[v.id] == False])
        j = len([1 for v in k[1] if m[v.id] == False])
        A.append(max(i, j) * 2.0 / n)
    # Return average accuracy after 10 tests.
    return sum(A) / 30.0

</t>
<t tx="karstenw.20230303140224.72">def test_k_means_random(self):
    # Assert k-means with random initialization.
    v = self._test_k_means(seed=vector.RANDOM)
    self.assertTrue(v &gt;= 0.6)
    print("pattern.vector.kmeans(seed=RANDOM)")

</t>
<t tx="karstenw.20230303140224.73">def test_k_means_kmpp(self):
    # Assert k-means with k-means++ initialization.
    # Note: vectors contain the top 10 features - see setUp().
    # If you include more features (more noise?) accuracy and performance will drop.
    v = self._test_k_means(seed=vector.KMPP)
    self.assertTrue(v &gt;= 0.8)
    print("pattern.vector.kmeans(seed=KMPP)")

</t>
<t tx="karstenw.20230303140224.74">def test_hierarchical(self):
    # Assert cluster contains nested clusters and/or vectors.
    def _test_cluster(cluster):
        for nested in cluster:
            if isinstance(nested, vector.Cluster):
                v1 = set((v.id for v in nested.flatten()))
                v2 = set((v.id for v in cluster.flatten()))
                self.assertTrue(nested.depth &lt; cluster.depth)
                self.assertTrue(v1.issubset(v2))
            else:
                self.assertTrue(isinstance(nested, vector.Vector))
        self.assertTrue(isinstance(cluster, list))
        self.assertTrue(isinstance(cluster.depth, int))
        self.assertTrue(isinstance(cluster.flatten(), list))
    n = 50
    m = dict((d.vector.id, d.type) for d in self.model[:n])
    h = vector.hierarchical([d.vector for d in self.model[:n]], k=2)
    h.traverse(_test_cluster)
    # Assert the accuracy of hierarchical clustering (shallow test).
    # Assert that cats are separated from dogs.
    v = (
        vector.Vector({"feline": 1, " lion": 1, "mane": 1}),
        vector.Vector({"feline": 1, "tiger": 1, "stripe": 1}),
        vector.Vector({"canine": 1, "wolf": 1, "howl": 1}),
        vector.Vector({"canine": 1, "dog": 1, "bark": 1})
    )
    h = vector.hierarchical(v)
    self.assertTrue(len(h[0][0]) == 2)
    self.assertTrue(len(h[0][1]) == 2)
    self.assertTrue(v[0] in h[0][0] and v[1] in h[0][0] or v[0] in h[0][1] and v[1] in h[0][1])
    self.assertTrue(v[2] in h[0][0] and v[3] in h[0][0] or v[2] in h[0][1] and v[3] in h[0][1])
    print("pattern.vector.Cluster()")
    print("pattern.vector.hierarchical()")

</t>
<t tx="karstenw.20230303140224.75">class TestClassifier(unittest.TestCase):

    model = None

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140224.76">def setUp(self):
    # Test model for training classifiers.
    if self.__class__.model is None:
        self.__class__.model = model()
    self.model = self.__class__.model

</t>
<t tx="karstenw.20230303140224.77">def _test_classifier(self, Classifier, **kwargs):
    # Assert classifier training + prediction for trivial cases.
    v = Classifier(**kwargs)
    test_doc1 = None
    test_doc2 = None

    for document in self.model:
        if isinstance(v, vector.IGTree):
            if test_doc1 is None and document.type is True:
                test_doc1 = document
            if test_doc2 is None and document.type is False:
                test_doc2 = document
        v.train(document)

    for type, message in (
      (False, "win money"),
      (True, "fix bug")):
        if not isinstance(v, vector.IGTree):
            self.assertEqual(v.classify(message), type)

    if isinstance(v, vector.IGTree):
        self.assertEqual(v.classify(test_doc1), True)
        self.assertEqual(v.classify(test_doc2), False)

    # Assert classifier properties.
    self.assertEqual(v.binary, True)
    self.assertEqual(sorted(v.classes), [False, True])
    self.assertTrue(isinstance(v.features, list))
    self.assertTrue("ftp" in v.features)
    # Assert saving + loading.
    v.save(Classifier.__name__)
    v = Classifier.load(Classifier.__name__)
    if not isinstance(v, vector.IGTree):
        self.assertEqual(v.classify("win money"), False)
        self.assertEqual(v.classify("fix bug"), True)
    os.remove(Classifier.__name__)
    # Assert untrained classifier returns None.
    v = Classifier(**kwargs)
    self.assertEqual(v.classify("herring"), None)
    print("pattern.vector.%s.train()" % Classifier.__name__)
    print("pattern.vector.%s.classify()" % Classifier.__name__)
    print("pattern.vector.%s.save()" % Classifier.__name__)

</t>
<t tx="karstenw.20230303140224.78">def test_classifier_vector(self):
    # Assert Classifier._vector() (translates input from train() and classify() to a Vector).
    v = vector.Classifier()._vector
    self.assertEqual(("cat", {"cat": 0.5, "purs": 0.5}), v(vector.Document("the cat purs", type="cat")))
    self.assertEqual(("cat", {"cat": 0.5, "purs": 0.5}), v({"cat": 0.5, "purs": 0.5}, type="cat"))
    self.assertEqual(("cat", {"cat": 0.5, "purs": 0.5}), v(["cat", "purs"], type="cat"))
    self.assertEqual(("cat", {"cat": 0.5, "purs": 0.5}), v("cat purs", type="cat"))
    print("pattern.vector.Classifier._vector()")

</t>
<t tx="karstenw.20230303140224.79">def test_nb(self):
    # Assert Bayesian probability classification.
    self._test_classifier(vector.NB)
    # Assert the accuracy of the classifier.
    A, P, R, F, o = vector.NB.test(self.model, folds=10, method=vector.BERNOUILLI)
    #print(A, P, R, F, o)
    self.assertTrue(P &gt;= 0.88)
    self.assertTrue(R &gt;= 0.89)
    self.assertTrue(F &gt;= 0.88)

</t>
<t tx="karstenw.20230303140224.8">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140224.80">def test_igtree(self):
    # Assert information gain tree classification.
    self._test_classifier(vector.IGTree, method=vector.GAINRATIO)
    # Assert the accuracy of the classifier.
    A, P, R, F, o = vector.IGTREE.test(self.model, folds=10, method=vector.GAINRATIO)
    #print(A, P, R, F, o)
    self.assertTrue(P &gt;= 0.87)
    self.assertTrue(R &gt;= 0.88)
    self.assertTrue(F &gt;= 0.89)

</t>
<t tx="karstenw.20230303140224.81">def test_knn(self):
    # Assert nearest-neighbor classification.
    self._test_classifier(vector.KNN, k=10, distance=vector.COSINE)
    # Assert the accuracy of the classifier.
    A, P, R, F, o = vector.KNN.test(self.model, folds=10, k=2, distance=vector.COSINE)
    #print(A, P, R, F, o)
    self.assertTrue(P &gt;= 0.91)
    self.assertTrue(R &gt;= 0.92)
    self.assertTrue(F &gt;= 0.92)

</t>
<t tx="karstenw.20230303140224.82">def test_slp(self):
    random.seed(1)
    # Assert single-layer averaged perceptron classification.
    self._test_classifier(vector.SLP)
    # Assert the accuracy of the classifier.
    A, P, R, F, o = vector.SLP.test(self.model, folds=10, iterations=3)
    #print(A, P, R, F, o)
    self.assertTrue(P &gt;= 0.90)
    self.assertTrue(R &gt;= 0.91)
    self.assertTrue(F &gt;= 0.91)

</t>
<t tx="karstenw.20230303140224.83">def test_svm(self):
    try:
        from pattern.vector import svm
    except ImportError as e:
        print(e)
        return
    # Assert support vector classification.
    self._test_classifier(vector.SVM, type=vector.SVC, kernel=vector.LINEAR)
    # Assert the accuracy of the classifier.
    A, P, R, F, o = vector.SVM.test(self.model, folds=10, type=vector.SVC, kernel=vector.LINEAR)
    #print(A, P, R, F, o)
    self.assertTrue(P &gt;= 0.93)
    self.assertTrue(R &gt;= 0.93)
    self.assertTrue(F &gt;= 0.93)

</t>
<t tx="karstenw.20230303140224.84">def test_liblinear(self):
    # If LIBLINEAR can be loaded,
    # assert that it is used for linear SVC (= 10x faster).
    try:
        from pattern.vector import svm
    except ImportError as e:
        print(e)
        return
    if svm.LIBLINEAR:
        classifier1 = vector.SVM(
                  type =  vector.CLASSIFICATION,
                kernel =  vector.LINEAR,
            extensions = (vector.LIBSVM, vector.LIBLINEAR))
        classifier2 = vector.SVM(
                  type =  vector.CLASSIFICATION,
                kernel =  vector.RBF,
            extensions = (vector.LIBSVM, vector.LIBLINEAR))
        classifier3 = vector.SVM(
                  type =  vector.CLASSIFICATION,
                kernel =  vector.LINEAR,
            extensions = (vector.LIBSVM,))
        self.assertEqual(classifier1.extension, vector.LIBLINEAR)
        self.assertEqual(classifier2.extension, vector.LIBSVM)
        self.assertEqual(classifier3.extension, vector.LIBSVM)
    print("pattern.vector.svm.LIBSVM")
    print("pattern.vector.svm.LIBLINEAR")

</t>
<t tx="karstenw.20230303140224.85">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUnicode))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUtilityFunctions))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestStemmer))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestDocument))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestModel))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestApriori))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestLSA))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestClustering))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestClassifier))
    return suite

</t>
<t tx="karstenw.20230303140224.9">def test_shi(self):
    # Assert integer hashing algorithm.
    for a, b in (
      (   100, "1c"),
      (  1000, "G8"),
      ( 10000, "2bI"),
      (100000, "Q0u")):
        self.assertEqual(vector.shi(a), b)
    print("pattern.vector.shi()")

</t>
<t tx="karstenw.20230303140227.1">from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range, next

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
import unittest
import time
import warnings

from pattern import web

try:
    PATH = os.path.dirname(os.path.realpath(__file__))
except:
    PATH = ""

#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140227.10">class TestURL(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140227.100">def test_languages(self):
    # Assert "BE" =&gt; "fr" + "nl".
    self.assertEqual(web.locale.languages("be"), ["fr", "nl"])
    print("pattern.web.locale.languages()")

</t>
<t tx="karstenw.20230303140227.101">def test_regions(self):
    # Assert "nl" =&gt; "NL" + "BE".
    self.assertEqual(web.locale.regions("nl"), ["NL", "BE"])
    print("pattern.web.locale.regions()")

</t>
<t tx="karstenw.20230303140227.102">def test_regionalize(self):
    # Assert "nl" =&gt; "nl-NL" + "nl-BE".
    self.assertEqual(web.locale.regionalize("nl"), ["nl-NL", "nl-BE"])
    print("pattern.web.locale.regionalize()")

</t>
<t tx="karstenw.20230303140227.103">def test_geocode(self):
    # Assert region geocode.
    v = web.locale.geocode("brussels")
    self.assertAlmostEqual(v[0], 50.83, places=2)
    self.assertAlmostEqual(v[1], 4.33, places=2)
    self.assertEqual(v[2], "nl")
    self.assertEqual(v[3], "Belgium")
    print("pattern.web.locale.geocode()")

</t>
<t tx="karstenw.20230303140227.104">def test_correlation(self):
    # Test the correlation between locale.LANGUAGE_REGION and locale.GEOCODE.
    # It should increase as new languages and locations are added.
    i = 0
    n = len(web.locale.GEOCODE)
    for city, (latitude, longitude, language, region) in web.locale.GEOCODE.items():
        if web.locale.encode_region(region) is not None:
            i += 1
    self.assertTrue(float(i) / n &gt; 0.60)

</t>
<t tx="karstenw.20230303140227.105">class TestMail(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140227.106">def setUp(self):
    self.username = ""
    self.password = ""
    self.service = web.GMAIL
    self.port = 993
    self.SSL = True
    self.query1 = "google" # FROM-field query in Inbox.
    self.query2 = "viagra" # SUBJECT-field query in Spam.

</t>
<t tx="karstenw.20230303140227.107">def test_mail(self):
    if not self.username or not self.password:
        return
    # Assert web.imap.Mail.
    m = web.Mail(self.username, self.password, service=self.service, port=self.port, secure=self.SSL)
    # Assert web.imap.MailFolder (assuming GMail folders).
    print(m.folders)
    self.assertTrue(len(m.folders) &gt; 0)
    self.assertTrue(len(m.inbox) &gt; 0)
    print("pattern.web.Mail")

</t>
<t tx="karstenw.20230303140227.108">def test_mail_message1(self):
    if not self.username or not self.password or not self.query1:
        return
    # Assert web.imap.Mailfolder.search().
    m = web.Mail(self.username, self.password, service=self.service, port=self.port, secure=self.SSL)
    a = m.inbox.search(self.query1, field=web.FROM)
    self.assertTrue(isinstance(a[0], int))
    # Assert web.imap.Mailfolder.read().
    e = m.inbox.read(a[0], attachments=False, cached=False)
    # Assert web.imap.Message.
    self.assertTrue(isinstance(e, web.imap.Message))
    self.assertTrue(isinstance(e.author, str))
    self.assertTrue(isinstance(e.email_address, str))
    self.assertTrue(isinstance(e.date, str))
    self.assertTrue(isinstance(e.subject, str))
    self.assertTrue(isinstance(e.body, str))
    self.assertTrue(self.query1 in e.author.lower())
    self.assertTrue("@" in e.email_address)
    print("pattern.web.Mail.search(field=FROM)")
    print("pattern.web.Mail.read()")

</t>
<t tx="karstenw.20230303140227.109">def test_mail_message2(self):
    if not self.username or not self.password or not self.query2:
        return
    # Test if we can download some mail attachments.
    # Set query2 to a mail subject of a spam e-mail you know contains an attachment.
    m = web.Mail(self.username, self.password, service=self.service, port=self.port, secure=self.SSL)
    if "spam" in m.folders:
        for id in m.spam.search(self.query2, field=web.SUBJECT):
            e = m.spam.read(id, attachments=True, cached=False)
            if len(e.attachments) &gt; 0:
                self.assertTrue(isinstance(e.attachments[0][1], bytes))
                self.assertTrue(len(e.attachments[0][1]) &gt; 0)
                print("pattern.web.Message.attachments (MIME-type: %s)" % e.attachments[0][0])
    print("pattern.web.Mail.search(field=SUBJECT)")
    print("pattern.web.Mail.read()")

</t>
<t tx="karstenw.20230303140227.11">def setUp(self):
    # Test a live URL that has fast response time
    self.live = "http://www.google.com/"
    # Test a fake URL with the URL parser.
    self.url = "https://username:password@www.domain.com:8080/path/path/page.html?q=1#anchor"
    self.parts = {
        "protocol": "https",
        "username": "username",
        "password": "password",
          "domain": "www.domain.com",
            "port": 8080,
            "path": ["path", "path"],
            "page": "page.html",
           "query": {"q": 1},
          "anchor": "anchor"
    }

</t>
<t tx="karstenw.20230303140227.110">class TestCrawler(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140227.111">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140227.112">def test_link(self):
    # Assert web.Link parser and properties.
    v = web.HTMLLinkParser().parse("""
        &lt;html&gt;
        &lt;head&gt;
            &lt;title&gt;title&lt;/title&gt;
        &lt;/head&gt;
        &lt;body&gt;
            &lt;div id="navigation"&gt;
                &lt;a href="http://www.domain1.com/?p=1" title="1" rel="a"&gt;nav1&lt;/a&gt;
                &lt;a href="http://www.domain2.com/?p=2" title="2" rel="b"&gt;nav1&lt;/a&gt;
            &lt;/div&gt;
        &lt;/body&gt;
        &lt;/html&gt;
    """, "http://www.domain.com/")
    self.assertTrue(v[0].url, "http://www.domain1.com/?p=1")
    self.assertTrue(v[1].url, "http://www.domain1.com/?p=2")
    self.assertTrue(v[0].description, "1")
    self.assertTrue(v[1].description, "2")
    self.assertTrue(v[0].relation, "a")
    self.assertTrue(v[1].relation, "b")
    self.assertTrue(v[0].referrer, "http://www.domain.com/")
    self.assertTrue(v[1].referrer, "http://www.domain.com/")
    self.assertTrue(v[0] &lt; v[1])
    print("pattern.web.HTMLLinkParser")

</t>
<t tx="karstenw.20230303140227.113">def test_crawler_crawl(self):
    # Assert domain filter.
    v = web.Crawler(links=["http://nodebox.net/"], domains=["nodebox.net"], delay=0.5)
    while len(v.visited) &lt; 4:
        v.crawl(throttle=0.1, cached=False)
    for url in v.visited:
        self.assertTrue("nodebox.net" in url)
    self.assertTrue(len(v.history) == 2)
    print("pattern.web.Crawler.crawl()")

</t>
<t tx="karstenw.20230303140227.114">def test_crawler_delay(self):
    # Assert delay for several crawls to a single domain.
    v = web.Crawler(links=["http://nodebox.net/"], domains=["nodebox.net"], delay=1.2)
    v.crawl()
    t = time.time()
    while not v.crawl(throttle=0.1, cached=False):
        pass
    t = time.time() - t
    self.assertTrue(t &gt; 1.0)
    print("pattern.web.Crawler.delay")

</t>
<t tx="karstenw.20230303140227.115">def test_crawler_breadth(self):
    # Assert BREADTH cross-domain preference.
    v = web.Crawler(links=["http://nodebox.net/"], delay=10)
    while len(v.visited) &lt; 4:
        v.crawl(throttle=0.1, cached=False, method=web.BREADTH)
    self.assertTrue(list(v.history.keys())[0] != list(v.history.keys())[1])
    self.assertTrue(list(v.history.keys())[0] != list(v.history.keys())[2])
    self.assertTrue(list(v.history.keys())[1] != list(v.history.keys())[2])
    print("pattern.web.Crawler.crawl(method=BREADTH)")

</t>
<t tx="karstenw.20230303140227.116">def suite():
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestCache))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestUnicode))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestURL))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestPlaintext))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestSearchEngine))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestDOM))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestDocumentParser))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestLocale))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestMail))
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestCrawler))
    return suite

</t>
<t tx="karstenw.20230303140227.12">def test_asynchrous(self):
    # Assert asynchronous function call (returns 1).
    v = web.asynchronous(lambda t: time.sleep(t) or 1, 0.2)
    while not v.done:
        time.sleep(0.1)
    self.assertEqual(v.value, 1)
    print("pattern.web.asynchronous()")

</t>
<t tx="karstenw.20230303140227.13">def test_extension(self):
    # Assert filename extension.
    v = web.extension(os.path.join("pattern", "test", "test-web.py.zip"))
    self.assertEqual(v, ".zip")
    print("pattern.web.extension()")

</t>
<t tx="karstenw.20230303140227.14">def test_urldecode(self):
    # Assert URL decode (inverse of urllib.urlencode).
    v = web.urldecode("?user=me&amp;page=1&amp;q=&amp;")
    self.assertEqual(v, {"user": "me", "page": 1, "q": None})
    print("pattern.web.urldecode()")

</t>
<t tx="karstenw.20230303140227.15">def test_proxy(self):
    # Assert URL proxy.
    v = web.proxy("www.proxy.com", "https")
    self.assertEqual(v, ("www.proxy.com", "https"))
    print("pattern.web.proxy()")

</t>
<t tx="karstenw.20230303140227.16">def test_url_parts(self):
    # Assert URL._parse and URL.parts{}.
    v = web.URL(self.url)
    for a, b in (
      (web.PROTOCOL, self.parts["protocol"]),
      (web.USERNAME, self.parts["username"]),
      (web.PASSWORD, self.parts["password"]),
      (web.DOMAIN,   self.parts["domain"]),
      (web.PORT,     self.parts["port"]),
      (web.PATH,     self.parts["path"]),
      (web.PAGE,     self.parts["page"]),
      (web.QUERY,    self.parts["query"]),
      (web.ANCHOR,   self.parts["anchor"])):
        self.assertEqual(v.parts[a], b)
    print("pattern.web.URL.parts")

</t>
<t tx="karstenw.20230303140227.17">def test_url_query(self):
    # Assert URL.query and URL.querystring.
    v = web.URL(self.url)
    v.query["page"] = 10
    v.query["user"] = None
    self.assertEqual(v.query, {"q": 1, "page": 10, "user": None})
    self.assertEqual(v.querystring, "q=1&amp;page=10&amp;user=")
    # Assert URL.querystring encodes unicode arguments.
    q = ({"ünîcødé": 1.5}, "%C3%BCn%C3%AEc%C3%B8d%C3%A9=1.5")
    v.query = q[0]
    self.assertEqual(v.querystring, q[1])
    # Assert URL.query decodes unicode arguments.
    v = web.URL("http://domain.com?" + q[1])
    self.assertEqual(v.query, q[0])
    print("pattern.web.URL.query")
    print("pattern.web.URL.querystring")

</t>
<t tx="karstenw.20230303140227.18">def test_url_string(self):
    # Assert URL._set_string().
    v = web.URL("")
    v.string = "https://domain.com"
    self.assertEqual(v.parts[web.PROTOCOL], "https")
    self.assertEqual(v.parts[web.DOMAIN], "domain.com")
    self.assertEqual(v.parts[web.PATH], [])
    print("pattern.web.URL.string")

</t>
<t tx="karstenw.20230303140227.19">def test_url(self):
    # Assert URL.copy().
    v = web.URL(self.url)
    v = v.copy()
    # Assert URL.__setattr__().
    v.username = "new-username"
    v.password = "new-password"
    # Assert URL.__getattr__().
    self.assertEqual(v.method, web.GET)
    self.assertEqual(v.protocol, self.parts["protocol"])
    self.assertEqual(v.username, "new-username")
    self.assertEqual(v.password, "new-password")
    self.assertEqual(v.domain, self.parts["domain"])
    self.assertEqual(v.port, self.parts["port"])
    self.assertEqual(v.path, self.parts["path"])
    self.assertEqual(v.page, self.parts["page"])
    self.assertEqual(v.query, self.parts["query"])
    self.assertEqual(v.anchor, self.parts["anchor"])
    print("pattern.web.URL")

</t>
<t tx="karstenw.20230303140227.2">class TestCache(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140227.20">def test_url_open(self):
    # Assert URLError.
    v = web.URL(self.live.replace("http://", "htp://"))
    self.assertRaises(web.URLError, v.open)
    self.assertEqual(v.exists, False)
    # Assert HTTPError.
    v = web.URL(self.live + "iphone/android.html")
    self.assertRaises(web.HTTPError, v.open)
    self.assertRaises(web.HTTP404NotFound, v.open)
    self.assertEqual(v.exists, False)
    # Assert socket connection.
    v = web.URL(self.live)
    self.assertTrue(v.open() is not None)
    self.assertEqual(v.exists, True)
    # Assert user-agent and referer.
    self.assertTrue(v.open(user_agent=web.MOZILLA, referrer=web.REFERRER) is not None)
    print("pattern.web.URL.exists")
    print("pattern.web.URL.open()")

</t>
<t tx="karstenw.20230303140227.21">def test_url_download(self):
    t = time.time()
    v = web.URL(self.live).download(cached=False, throttle=0.25, unicode=True)
    t = time.time() - t
    # Assert unicode content.
    self.assertTrue(isinstance(v, str))
    # Assert download rate limiting.
    self.assertTrue(t &gt;= 0.25)
    print("pattern.web.URL.download()")

</t>
<t tx="karstenw.20230303140227.22">def test_url_mimetype(self):
    # Assert URL MIME-type.
    v = web.URL(self.live).mimetype
    self.assertTrue(v in web.MIMETYPE_WEBPAGE)
    print("pattern.web.URL.mimetype")

</t>
<t tx="karstenw.20230303140227.23">def test_url_headers(self):
    # Assert URL headers.
    v = web.URL(self.live).headers["content-type"].split(";")[0]
    self.assertEqual(v, "text/html")
    print("pattern.web.URL.headers")

</t>
<t tx="karstenw.20230303140227.24">def test_url_redirect(self):
    # Assert URL redirected URL (this depends on where you are).
    # In Belgium, it yields "http://www.google.be/".
    v = web.URL(self.live).redirect
    print("pattern.web.URL.redirect: " + self.live + " =&gt; " + str(v))

</t>
<t tx="karstenw.20230303140227.25">def test_abs(self):
    # Assert absolute URL (special attention for anchors).
    for a, b in (
      ("../page.html", "http://domain.com/path/"),
      (   "page.html", "http://domain.com/home.html")):
        v = web.abs(a, base=b)
        self.assertEqual(v, "http://domain.com/page.html")
    for a, b, c in (
      (     "#anchor", "http://domain.com", "/"),
      (     "#anchor", "http://domain.com/", ""),
      (     "#anchor", "http://domain.com/page", "")):
        v = web.abs(a, base=b)
        self.assertEqual(v, b + c + a) # http://domain.com/#anchor
    print("pattern.web.abs()")

</t>
<t tx="karstenw.20230303140227.26">def test_base(self):
    # Assert base URL domain name.
    self.assertEqual(web.base("http://domain.com/home.html"), "domain.com")
    print("pattern.web.base()")

</t>
<t tx="karstenw.20230303140227.27">def test_oauth(self):
    # Assert OAuth algorithm.
    data = {
        "q": '"cåts, døgs &amp; chîckéns = fün+"',
        "oauth_version": "1.0",
        "oauth_nonce": "0",
        "oauth_timestamp": 0,
        "oauth_consumer_key": "key",
        "oauth_signature_method": "HMAC-SHA1"
    }
    v = web.oauth.sign("http://yboss.yahooapis.com/ysearch/web", data, secret="secret")
    self.assertEqual(v, "RtTu8dxSp3uBzSbsuLAXIWOKfyI=")
    print("pattern.web.oauth.sign()")

</t>
<t tx="karstenw.20230303140227.28">class TestPlaintext(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140227.29">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140227.3">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140227.30">def test_find_urls(self):
    # Assert URL finder with common URL notations.
    for url in (
      "http://domain.co.uk",
      "https://domain.co.uk",
      "www.domain.cu.uk",
      "domain.com",
      "domain.org",
      "domain.net"):
        self.assertEqual(web.find_urls("(" + url + ".")[0], url)
    # Assert case-insensitive, punctuation and &lt;a href=""&gt;.
    # Assert several matches in string.
    self.assertEqual(web.find_urls("HTTP://domain.net")[0], "HTTP://domain.net")
    self.assertEqual(web.find_urls("http://domain.net),};")[0], "http://domain.net")
    self.assertEqual(web.find_urls("http://domain.net\"&gt;domain")[0], "http://domain.net")
    self.assertEqual(web.find_urls("domain.com, domain.net"), ["domain.com", "domain.net"])
    print("pattern.web.find_urls()")

</t>
<t tx="karstenw.20230303140227.31">def test_find_email(self):
    # Assert e-mail finder with common e-mail notations.
    s = "firstname.last+name@domain.ac.co.uk"
    v = web.find_email("(" + s + ".")
    self.assertEqual(v[0], s)
    # Assert several matches in string.
    s = ["me@site1.com", "me@site2.com"]
    v = web.find_email("(" + ",".join(s) + ")")
    self.assertEqual(v, s)
    print("pattern.web.find_email()")

</t>
<t tx="karstenw.20230303140227.32">def test_find_between(self):
    # Assert search between open tag and close tag.
    s = "&lt;script type='text/javascript'&gt;alert(0);&lt;/script&gt;"
    v = web.find_between("&lt;script", "&lt;/script&gt;", s)
    self.assertEqual(v[0], " type='text/javascript'&gt;alert(0);")
    # Assert several matches in string.
    s = "a0ba1b"
    v = web.find_between("a", "b", s)
    self.assertEqual(v, ["0", "1"])
    print("pattern.web.find_between()")

</t>
<t tx="karstenw.20230303140227.33">def test_strip_tags(self):
    # Assert HTML parser and tag stripper.
    for html, plain in (
      ("&lt;b&gt;ünîcøde&lt;/b&gt;", "ünîcøde"),
      ("&lt;img src=""/&gt;", ""),
      ("&lt;p&gt;text&lt;/p&gt;", "text\n\n"),
      ("&lt;li&gt;text&lt;/li&gt;", "* text\n"),
      ("&lt;td&gt;text&lt;/td&gt;", "text\t"),
      ("&lt;br&gt;", "\n"),
      ("&lt;br/&gt;", "\n\n"),
      ("&lt;br /&gt;&lt;br/&gt;&lt;br&gt;", "\n\n\n\n\n")):
        self.assertEqual(web.strip_tags(html), plain)
    # Assert exclude tags and attributes
    v = web.strip_tags("&lt;a href=\"\" onclick=\"\"&gt;text&lt;/a&gt;", exclude={"a": ["href"]})
    self.assertEqual(v, "&lt;a href=\"\"&gt;text&lt;/a&gt;")
    print("pattern.web.strip_tags()")

</t>
<t tx="karstenw.20230303140227.34">def test_strip_element(self):
    # Assert strip &lt;p&gt; elements.
    v = web.strip_element(" &lt;p&gt;&lt;p&gt;&lt;/p&gt;text&lt;/p&gt; &lt;b&gt;&lt;P&gt;&lt;/P&gt;&lt;/b&gt;", "p")
    self.assertEqual(v, "  &lt;b&gt;&lt;/b&gt;")
    print("pattern.web.strip_element()")

</t>
<t tx="karstenw.20230303140227.35">def test_strip_between(self):
    # Assert strip &lt;p&gt; elements.
    v = web.strip_between("&lt;p", "&lt;/p&gt;", " &lt;p&gt;&lt;p&gt;&lt;/p&gt;text&lt;/p&gt; &lt;b&gt;&lt;P&gt;&lt;/P&gt;&lt;/b&gt;")
    self.assertEqual(v, " text&lt;/p&gt; &lt;b&gt;&lt;/b&gt;")
    print("pattern.web.strip_between()")

</t>
<t tx="karstenw.20230303140227.36">def test_strip_javascript(self):
    # Assert strip &lt;script&gt; elements.
    v = web.strip_javascript(" &lt;script type=\"text/javascript\"&gt;text&lt;/script&gt; ")
    self.assertEqual(v, "  ")
    print("pattern.web.strip_javascript()")

</t>
<t tx="karstenw.20230303140227.37">def test_strip_inline_css(self):
    # Assert strip &lt;style&gt; elements.
    v = web.strip_inline_css(" &lt;style type=\"text/css\"&gt;text&lt;/style&gt; ")
    self.assertEqual(v, "  ")
    print("pattern.web.strip_inline_css()")

</t>
<t tx="karstenw.20230303140227.38">def test_strip_comments(self):
    # Assert strip &lt;!-- --&gt; elements.
    v = web.strip_comments(" &lt;!-- text --&gt; ")
    self.assertEqual(v, "  ")
    print("pattern.web.strip_comments()")

</t>
<t tx="karstenw.20230303140227.39">def test_strip_forms(self):
    # Assert strip &lt;form&gt; elements.
    v = web.strip_forms(" &lt;form method=\"get\"&gt;text&lt;/form&gt; ")
    self.assertEqual(v, "  ")
    print("pattern.web.strip_forms()")

</t>
<t tx="karstenw.20230303140227.4">def test_cache(self):
    # Assert cache unicode.
    k, v = "test", "ünîcødé"
    web.cache[k] = v
    self.assertTrue(isinstance(web.cache[k], str))
    self.assertEqual(web.cache[k], v)
    self.assertEqual(web.cache.age(k), 0)
    del web.cache[k]
    print("pattern.web.Cache")

</t>
<t tx="karstenw.20230303140227.40">def test_encode_entities(self):
    # Assert HTML entity encoder (e.g., "&amp;" =&gt; "&amp;&amp;amp;")
    for a, b in (
      ("&amp;#201;", "&amp;#201;"),
      ("&amp;", "&amp;amp;"),
      ("&lt;", "&amp;lt;"),
      ("&gt;", "&amp;gt;"),
      ('"', "&amp;quot;"),
      ("'", "&amp;#39;")):
        self.assertEqual(web.encode_entities(a), b)
    print("pattern.web.encode_entities()")

</t>
<t tx="karstenw.20230303140227.41">def test_decode_entities(self):
    # Assert HMTL entity decoder (e.g., "&amp;amp;" =&gt; "&amp;")
    for a, b in (
      ("&amp;#38;", "&amp;"),
      ("&amp;amp;", "&amp;"),
      ("&amp;#x0026;", "&amp;"),
      ("&amp;#160;", "\xa0"),
      ("&amp;foo;", "&amp;foo;")):
        self.assertEqual(web.decode_entities(a), b)
    print("pattern.web.decode_entities()")

</t>
<t tx="karstenw.20230303140227.42">def test_collapse_spaces(self):
    # Assert collapse multiple spaces.
    for a, b in (
      ("    ", ""),
      (" .. ", ".."),
      (".  .", ". ."),
      (". \n", "."),
      ("\xa0", "")):
        self.assertEqual(web.collapse_spaces(a), b)
    # Assert preserve indendation.
    self.assertEqual(web.collapse_spaces("  . \n", indentation=True), "  .")
    print("pattern.web.collapse_spaces()")

</t>
<t tx="karstenw.20230303140227.43">def test_collapse_tabs(self):
    # Assert collapse multiple tabs to 1 space.
    for a, b in (
      ("\t\t\t", ""),
      ("\t..\t", ".."),
      (".\t\t.", ". ."),
      (".\t\n", ".")):
        self.assertEqual(web.collapse_tabs(a), b)
    # Assert preserve indendation.
    self.assertEqual(web.collapse_tabs("\t\t .\t\n", indentation=True), "\t\t .")
    print("pattern.web.collapse_tabs()")

</t>
<t tx="karstenw.20230303140227.44">def test_collapse_linebreaks(self):
    # Assert collapse multiple linebreaks.
    for a, b in (
      ("\n\n\n", "\n"),
      (".\n\n.", ".\n."),
      (".\r\n.", ".\n."),
      (".\n  .", ".\n  ."),
      (" \n  .", "\n  .")):
        self.assertEqual(web.collapse_linebreaks(a), b)
    print("pattern.web.collapse_linebreaks()")

</t>
<t tx="karstenw.20230303140227.45">def test_plaintext(self):
    # Assert plaintext:
    # - strip &lt;script&gt;, &lt;style&gt;, &lt;form&gt;, &lt;!-- --&gt; elements,
    # - strip tags,
    # - decode entities,
    # - collapse whitespace,
    html = """
        &lt;html&gt;
        &lt;head&gt;
            &lt;title&gt;tags &amp;amp; things&lt;/title&gt;
        &lt;/head&gt;
        &lt;body&gt;
            &lt;div id="content"&gt;       \n\n\n\
                &lt;!-- main content --&gt;
                &lt;script type="text/javascript&gt;"alert(0);&lt;/script&gt;
                &lt;h1&gt;title1&lt;/h1&gt;
                &lt;h2&gt;title2&lt;/h2&gt;
                &lt;p&gt;paragraph1&lt;/p&gt;
                &lt;p&gt;paragraph2 &lt;a href="http://www.domain.com" onclick="alert(0);"&gt;link&lt;/a&gt;&lt;/p&gt;
                &lt;ul&gt;
                    &lt;li&gt;item1&amp;nbsp;&amp;nbsp;&amp;nbsp;xxx&lt;/li&gt;
                    &lt;li&gt;item2&lt;/li&gt;
                &lt;ul&gt;
            &lt;/div&gt;
            &lt;br /&gt;
            &lt;br /&gt;
        &lt;/body&gt;
        &lt;/html&gt;
    """
    self.assertEqual(web.plaintext(html, keep={"a": "href"}),
        "tags &amp; things\n\ntitle1\n\ntitle2\n\nparagraph1\n\nparagraph2 " + \
        "&lt;a href=\"http://www.domain.com\"&gt;link&lt;/a&gt;\n\n* item1 xxx\n* item2")
    print("pattern.web.plaintext()")

</t>
<t tx="karstenw.20230303140227.46">class TestSearchEngine(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140227.47">def setUp(self):
    # Test data for all search engines:
    # {api: (source, license, Engine)}.
    self.api = {
        "Google": (web.GOOGLE,      web.GOOGLE_LICENSE,      web.Google),
         "Yahoo": (web.YAHOO,       web.YAHOO_LICENSE,       web.Yahoo),
          "Bing": (web.BING,        web.BING_LICENSE,        web.Bing),
       "Twitter": (web.TWITTER,     web.TWITTER_LICENSE,     web.Twitter),
     "Wikipedia": (web.MEDIAWIKI,   web.WIKIPEDIA_LICENSE,   web.Wikipedia),
         "Wikia": (web.MEDIAWIKI,   web.MEDIAWIKI_LICENSE,   web.Wikia),
        "Flickr": (web.FLICKR,      web.FLICKR_LICENSE,      web.Flickr),
      "Facebook": (web.FACEBOOK,    web.FACEBOOK_LICENSE,    web.Facebook),
   "ProductWiki": (web.PRODUCTWIKI, web.PRODUCTWIKI_LICENSE, web.ProductWiki)
    }

</t>
<t tx="karstenw.20230303140227.48">def _test_search_engine(self, api, source, license, Engine, query="today", type=web.SEARCH):
    # Assert SearchEngine standard interface for any api:
    # Google, Yahoo, Bing, Twitter, Wikipedia, Flickr, Facebook, ProductWiki, Newsfeed.
    # SearchEngine.search() returns a list of Result objects with unicode fields,
    # except Wikipedia which returns a WikipediaArticle (MediaWikiArticle subclass).
    if api == "Yahoo" and license == ("", ""):
        return
    t = time.time()
    e = Engine(license=license, throttle=0.25, language="en")
    v = e.search(query, type, start=1, count=1, cached=False)
    t = time.time() - t
    self.assertTrue(t &gt;= 0.25)
    self.assertEqual(e.license, license)
    self.assertEqual(e.throttle, 0.25)
    self.assertEqual(e.language, "en")
    self.assertEqual(v.query, query)
    if source != web.MEDIAWIKI:
        self.assertEqual(v.source, source)
        self.assertEqual(v.type, type)
        self.assertEqual(len(v), 1)
        self.assertTrue(isinstance(v[0], web.Result))
        self.assertTrue(isinstance(v[0].url, str))
        self.assertTrue(isinstance(v[0].title, str))
        self.assertTrue(isinstance(v[0].description, str))
        self.assertTrue(isinstance(v[0].language, str))
        self.assertTrue(isinstance(v[0].author, (str, tuple)))
        self.assertTrue(isinstance(v[0].date, str))
    else:
        self.assertTrue(isinstance(v, web.MediaWikiArticle))
    # Assert zero results for start &lt; 1 and count &lt; 1.
    v1 = e.search(query, start=0)
    v2 = e.search(query, count=0)
    if source != web.MEDIAWIKI:
        self.assertEqual(len(v1), 0)
        self.assertEqual(len(v2), 0)
    else:
        self.assertTrue(isinstance(v1, web.MediaWikiArticle))
        self.assertEqual(v2, None)
    # Assert SearchEngineTypeError for unknown type.
    self.assertRaises(web.SearchEngineTypeError, e.search, query, type="crystall-ball")
    print("pattern.web.%s.search()" % api)

</t>
<t tx="karstenw.20230303140227.49">def test_search_google(self):
    self._test_search_engine("Google", *self.api["Google"])

</t>
<t tx="karstenw.20230303140227.5">class TestUnicode(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140227.50">def test_search_yahoo(self):
    self._test_search_engine("Yahoo", *self.api["Yahoo"])

</t>
<t tx="karstenw.20230303140227.51">@unittest.skip('Bing Search API has no free quota')
def test_search_bing(self):
    self._test_search_engine("Bing", *self.api["Bing"])

</t>
<t tx="karstenw.20230303140227.52">def test_search_twitter(self):
    self._test_search_engine("Twitter", *self.api["Twitter"])

</t>
<t tx="karstenw.20230303140227.53">@unittest.skip('Mediawiki/Wikipedia API or appearance changed')
def test_search_wikipedia(self):
    self._test_search_engine("Wikipedia", *self.api["Wikipedia"])

</t>
<t tx="karstenw.20230303140227.54">@unittest.skip('Mediawiki API or appearance changed')
def test_search_wikia(self):
    self._test_search_engine("Wikia", *self.api["Wikia"], **{"query": "games"})

</t>
<t tx="karstenw.20230303140227.55">def test_search_flickr(self):
    self._test_search_engine("Flickr", *self.api["Flickr"], **{"type": web.IMAGE})

</t>
<t tx="karstenw.20230303140227.56">@unittest.skip('Facebook API changed')
def test_search_facebook(self):
    self._test_search_engine("Facebook", *self.api["Facebook"])

</t>
<t tx="karstenw.20230303140227.57">@unittest.skip('ProductWiki is deprecated')
def test_search_productwiki(self):
    self._test_search_engine("ProductWiki", *self.api["ProductWiki"], **{"query": "computer"})

</t>
<t tx="karstenw.20230303140227.58">def test_search_newsfeed(self):
    for feed, url in web.feeds.items():
        self._test_search_engine("Newsfeed", url, None, web.Newsfeed, query=url, type=web.NEWS)

</t>
<t tx="karstenw.20230303140227.59">def _test_results(self, api, source, license, Engine, type=web.SEARCH, query="today", baseline=[6, 6, 6, 0]):
    # Assert SearchEngine result content.
    # We expect to find http:// URL's and descriptions containing the search query.
    if api == "Yahoo" and license == ("", ""):
        return
    i1 = 0
    i2 = 0
    i3 = 0
    i4 = 0
    e = Engine(license=license, language="en", throttle=0.25)
    for result in e.search(query, type, count=10, cached=False):
        i1 += int(result.url.startswith("http"))
        i2 += int(query in result.url.lower())
        i2 += int(query in result.title.lower())
        i2 += int(query in result.description.lower())
        i3 += int(result.language == "en")
        i4 += int(result.url.endswith(("jpg", "png", "gif")))
        #print(result.url)
        #print(result.title)
        #print(result.description)
    #print(i1, i2, i3, i4)
    self.assertTrue(i1 &gt;= baseline[0]) # url's starting with "http"
    self.assertTrue(i2 &gt;= baseline[1]) # query in url + title + description
    self.assertTrue(i3 &gt;= baseline[2]) # language "en"
    self.assertTrue(i4 &gt;= baseline[3]) # url's ending with "jpg", "png" or "gif"
    print("pattern.web.%s.Result(type=%s)" % (api, type.upper()))

</t>
<t tx="karstenw.20230303140227.6">def setUp(self):
    # Test data with different (or wrong) encodings.
    self.strings = (
        "ünîcøde",
        "ünîcøde".encode("utf-16"),
        "ünîcøde".encode("latin-1"),
        "ünîcøde".encode("windows-1252"),
         "ünîcøde",
        "אוניקאָד"
    )

</t>
<t tx="karstenw.20230303140227.60">def test_results_google(self):
    self._test_results("Google", *self.api["Google"])

</t>
<t tx="karstenw.20230303140227.61">def test_results_yahoo(self):
    self._test_results("Yahoo", *self.api["Yahoo"])

</t>
<t tx="karstenw.20230303140227.62">def test_results_yahoo_images(self):
    self._test_results("Yahoo", *self.api["Yahoo"], **{"type": web.IMAGE, "baseline": [6, 6, 0, 6]})

</t>
<t tx="karstenw.20230303140227.63">def test_results_yahoo_news(self):
    self._test_results("Yahoo", *self.api["Yahoo"], **{"type": web.NEWS})

</t>
<t tx="karstenw.20230303140227.64">@unittest.skip('Bing API changed')
def test_results_bing(self):
    self._test_results("Bing", *self.api["Bing"])

</t>
<t tx="karstenw.20230303140227.65">@unittest.skip('Bing API changed')
def test_results_bing_images(self):
    self._test_results("Bing", *self.api["Bing"], **{"type": web.IMAGE, "baseline": [6, 6, 0, 6]})

</t>
<t tx="karstenw.20230303140227.66">@unittest.skip('Bing API changed')
def test_results_bing_news(self):
    self._test_results("Bing", *self.api["Bing"], **{"type": web.NEWS})

</t>
<t tx="karstenw.20230303140227.67">def test_results_twitter(self):
    self._test_results("Twitter", *self.api["Twitter"])

</t>
<t tx="karstenw.20230303140227.68">def test_results_flickr(self):
    self._test_results("Flickr", *self.api["Flickr"], **{"baseline": [6, 6, 0, 6]})

</t>
<t tx="karstenw.20230303140227.69">@unittest.skip('Facebook API changed')
def test_results_facebook(self):
    self._test_results("Facebook", *self.api["Facebook"], **{"baseline": [0, 1, 0, 0]})

</t>
<t tx="karstenw.20230303140227.7">def test_decode_utf8(self):
    # Assert unicode.
    for s in self.strings:
        self.assertTrue(isinstance(web.decode_utf8(s), str))
    print("pattern.web.decode_utf8()")

</t>
<t tx="karstenw.20230303140227.70">def test_google_translate(self):
    try:
        # Assert Google Translate API.
        # Requires license with billing enabled.
        source, license, Engine = self.api["Google"]
        v = Engine(license, throttle=0.25).translate("thé", input="fr", output="en", cached=False)
        self.assertEqual(v, "tea")
        print("pattern.web.Google.translate()")
    except web.HTTP401Authentication:
        pass

</t>
<t tx="karstenw.20230303140227.71">def test_google_identify(self):
    try:
        # Assert Google Translate API (language detection).
        # Requires license with billing enabled.
        source, license, Engine = self.api["Google"]
        v = Engine(license, throttle=0.25).identify("L'essence des mathématiques, c'est la liberté!", cached=False)
        self.assertEqual(v[0], "fr")
        print("pattern.web.Google.identify()")
    except web.HTTP401Authentication:
        pass

</t>
<t tx="karstenw.20230303140227.72">def test_twitter_author(self):
    self.assertEqual(web.author("me"), "from:me")
    print("pattern.web.author()")

</t>
<t tx="karstenw.20230303140227.73">def test_twitter_hashtags(self):
    self.assertEqual(web.hashtags("#cat #dog"), ["#cat", "#dog"])
    print("pattern.web.hashtags()")

</t>
<t tx="karstenw.20230303140227.74">def test_twitter_retweets(self):
    self.assertEqual(web.retweets("RT @me: blah"), ["@me"])
    print("pattern.web.retweets()")

</t>
<t tx="karstenw.20230303140227.75">def _test_search_image_size(self, api, source, license, Engine):
    # Assert image URL's for different sizes actually exist.
    if api == "Yahoo" and license == ("", ""):
        return
    e = Engine(license, throttle=0.25)
    for size in (web.TINY, web.SMALL, web.MEDIUM, web.LARGE):
        v = e.search("cats", type=web.IMAGE, count=1, size=size, cached=False)
        self.assertEqual(web.URL(v[0].url).exists, True)
        print("pattern.web.%s.search(type=IMAGE, size=%s)" % (api, size.upper()))

</t>
<t tx="karstenw.20230303140227.76">def test_yahoo_image_size(self):
    self._test_search_image_size("Yahoo", *self.api["Yahoo"])

</t>
<t tx="karstenw.20230303140227.77">@unittest.skip('Bing Search API has no free quota')
def test_bing_image_size(self):
    self._test_search_image_size("Bing", *self.api["Bing"])

</t>
<t tx="karstenw.20230303140227.78">def test_flickr_image_size(self):
    self._test_search_image_size("Flickr", *self.api["Flickr"])

</t>
<t tx="karstenw.20230303140227.79">@unittest.skip('Mediawiki/Wikipedia API or appearance changed')
def test_wikipedia_list(self):
    # Assert WikipediaArticle.list(), an iterator over all article titles.
    source, license, Engine = self.api["Wikipedia"]
    v = Engine(license).list(start="a", count=1)
    v = [next(v) for i in range(2)]
    self.assertTrue(len(v) == 2)
    self.assertTrue(v[0].lower().startswith("a"))
    self.assertTrue(v[1].lower().startswith("a"))
    print("pattern.web.Wikipedia.list()")

</t>
<t tx="karstenw.20230303140227.8">def test_encode_utf8(self):
    # Assert Python bytestring.
    for s in self.strings:
        self.assertTrue(isinstance(web.encode_utf8(s), bytes))
    print("pattern.web.encode_utf8()")

</t>
<t tx="karstenw.20230303140227.80">def test_wikipedia_all(self):
    # Assert WikipediaArticle.all(), an iterator over WikipediaArticle objects.
    source, license, Engine = self.api["Wikipedia"]
    v = Engine(license).all(start="a", count=1)
    v = [next(v) for i in range(1)]
    self.assertTrue(len(v) == 1)
    self.assertTrue(isinstance(v[0], web.WikipediaArticle))
    self.assertTrue(v[0].title.lower().startswith("a"))
    print("pattern.web.Wikipedia.all()")

</t>
<t tx="karstenw.20230303140227.81">@unittest.skip('Mediawiki/Wikipedia API or appearance changed')
def test_wikipedia_article(self):
    source, license, Engine = self.api["Wikipedia"]
    v = Engine(license).search("cat", cached=False)
    # Assert WikipediaArticle properties.
    self.assertTrue(isinstance(v.title, str))
    self.assertTrue(isinstance(v.string, str))
    self.assertTrue(isinstance(v.links, list))
    self.assertTrue(isinstance(v.categories, list))
    self.assertTrue(isinstance(v.external, list))
    self.assertTrue(isinstance(v.media, list))
    self.assertTrue(isinstance(v.languages, dict))
    # Assert WikipediaArticle properties content.
    self.assertTrue(v.string == v.plaintext())
    self.assertTrue(v.html == v.source)
    self.assertTrue("&lt;/div&gt;"  in v.source)
    self.assertTrue("cat"     in v.title.lower())
    self.assertTrue("Felis"   in v.links)
    self.assertTrue("Felines" in v.categories)
    self.assertTrue("en" == v.language)
    self.assertTrue("fr"      in v.languages)
    self.assertTrue("chat"    in v.languages["fr"].lower())
    self.assertTrue(v.external[0].startswith("http"))
    self.assertTrue(v.media[0].endswith(("jpg", "png", "gif", "svg")))
    print("pattern.web.WikipediaArticle")

</t>
<t tx="karstenw.20230303140227.82">@unittest.skip('Mediawiki/Wikipedia API or appearance changed')
def test_wikipedia_article_sections(self):
    # Assert WikipediaArticle.sections structure.
    # The test may need to be modified if the Wikipedia "Cat" article changes.
    source, license, Engine = self.api["Wikipedia"]
    v = Engine(license).search("cat", cached=False)
    s1 = s2 = s3 = None
    for section in v.sections:
        if section.title == "Behavior":
            s1 = section
        if section.title == "Grooming":
            s2 = section
        if section.title == "Play":
            s3 = section
        self.assertTrue(section.article == v)
        self.assertTrue(section.level == 0 or section.string.startswith(section.title))
    # Test section depth.
    self.assertTrue(s1.level == 1)
    self.assertTrue(s2.level == 2)
    self.assertTrue(s2.level == 2)
    # Test section parent-child structure.
    self.assertTrue(s2 in s1.children) # Behavior =&gt; Grooming
    self.assertTrue(s3 in s1.children) # Behavior =&gt; Play
    self.assertTrue(s2.parent == s1)
    self.assertTrue(s3.parent == s1)
    # Test section content.
    self.assertTrue("hairballs" in s2.content)
    self.assertTrue("laser pointer" in s3.content)
    # Test section tables.
    # XXX should test &lt;td colspan="x"&gt; more thoroughly.
    self.assertTrue(len(v.sections[1].tables) &gt; 0)
    print("pattern.web.WikipediaSection")

</t>
<t tx="karstenw.20230303140227.83">@unittest.skip('ProductWiki is deprecated')
def test_productwiki(self):
    # Assert product reviews and score.
    source, license, Engine = self.api["ProductWiki"]
    v = Engine(license).search("computer", cached=False)
    self.assertTrue(isinstance(v[0].reviews, list))
    self.assertTrue(isinstance(v[0].score, int))
    print("pattern.web.ProductWiki.Result.reviews")
    print("pattern.web.ProductWiki.Result.score")

</t>
<t tx="karstenw.20230303140227.84">class TestDOM(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140227.85">def setUp(self):
    # Test HTML document.
    self.html = """
        &lt;!doctype html&gt;
        &lt;html lang="en"&gt;
        &lt;head&gt;
            &lt;title&gt;title&lt;/title&gt;
            &lt;meta charset="utf-8" /&gt;
        &lt;/head&gt;
        &lt;body id="front" class="comments"&gt;
            &lt;script type="text/javascript"&gt;alert(0);&lt;/script&gt;
            &lt;div id="navigation"&gt;
                &lt;a href="nav1.html"&gt;nav1&lt;/a&gt; | 
                &lt;a href="nav2.html"&gt;nav2&lt;/a&gt; | 
                &lt;a href="nav3.html"&gt;nav3&lt;/a&gt;
            &lt;/div&gt;
            &lt;div id="content"&gt;
                &lt;P class="comment"&gt;
                    &lt;span class="date"&gt;today&lt;/span&gt;
                    &lt;span class="author"&gt;me&lt;/span&gt;
                    Blah blah
                &lt;/P&gt;
                &lt;P class="class1 class2"&gt;
                    Blah blah
                &lt;/P&gt;
                &lt;p&gt;Read more&lt;/p&gt;
            &lt;/div&gt;
        &lt;/body&gt;
        &lt;/html&gt;
    """

</t>
<t tx="karstenw.20230303140227.86">def test_node_document(self):
    # Assert Node properties.
    v1 = web.Document(self.html)
    self.assertEqual(v1.type, web.DOCUMENT)
    self.assertEqual(v1.source[:10], "&lt;!DOCTYPE ") # Note: BeautifulSoup strips whitespace.
    self.assertEqual(v1.parent, None)
    # Assert Node traversal.
    v2 = v1.children[0].next
    self.assertEqual(v2.type, web.ELEMENT)
    self.assertEqual(v2.previous, v1.children[0])
    # Assert Document properties.
    v3 = v1.declaration
    self.assertEqual(v3, v1.children[0])
    self.assertEqual(v3.parent, v1)
    self.assertEqual(v3.source, "html")
    self.assertEqual(v1.head.type, web.ELEMENT)
    self.assertEqual(v1.body.type, web.ELEMENT)
    self.assertTrue(v1.head.source.startswith("&lt;head"))
    self.assertTrue(v1.body.source.startswith("&lt;body"))
    print("pattern.web.Node")
    print("pattern.web.DOM")

</t>
<t tx="karstenw.20230303140227.87">def test_node_traverse(self):
    # Assert Node.traverse() (must visit all child nodes recursively).
    self.b = False

    def visit(node):
        if node.type == web.ELEMENT and node.tag == "span":
            self.b = True
    v = web.DOM(self.html)
    v.traverse(visit)
    self.assertEqual(self.b, True)
    print("pattern.web.Node.traverse()")

</t>
<t tx="karstenw.20230303140227.88">def test_element(self):
    # Assert Element properties (test &lt;body&gt;).
    v = web.DOM(self.html).body
    self.assertEqual(v.tag, "body")
    self.assertEqual(v.attributes["id"], "front")
    self.assertEqual(v.attributes["class"], ["comments"])
    self.assertTrue(v.content.startswith("\n&lt;script"))
    # Assert Element.getElementsByTagname() (test navigation links).
    a = v.by_tag("a")
    self.assertEqual(len(a), 3)
    self.assertEqual(a[0].content, "nav1")
    self.assertEqual(a[1].content, "nav2")
    self.assertEqual(a[2].content, "nav3")
    # Assert Element.getElementsByClassname() (test &lt;p class="comment"&gt;).
    a = v.by_class("comment")
    self.assertEqual(a[0].tag, "p")
    self.assertEqual(a[0].by_tag("span")[0].attributes["class"], ["date"])
    self.assertEqual(a[0].by_tag("span")[1].attributes["class"], ["author"])
    for selector in (".comment", "p.comment", "*.comment"):
        self.assertEqual(v.by_tag(selector)[0], a[0])
    # Assert Element.getElementById() (test &lt;div id="content"&gt;).
    e = v.by_id("content")
    self.assertEqual(e.tag, "div")
    self.assertEqual(e, a[0].parent)
    for selector in ("#content", "div#content", "*#content"):
        self.assertEqual(v.by_tag(selector)[0], e)
    # Assert Element.getElementByAttribute() (test on &lt;a href=""&gt;).
    a = v.by_attribute(href="nav1.html")
    self.assertEqual(a[0].content, "nav1")
    print("pattern.web.Element")
    print("pattern.web.Element.by_tag()")
    print("pattern.web.Element.by_class()")
    print("pattern.web.Element.by_id()")
    print("pattern.web.Element.by_attribute()")

</t>
<t tx="karstenw.20230303140227.89">def test_selector(self):
    # Assert DOM CSS selectors with multiple classes.
    v = web.DOM(self.html).body
    p = v("p.class1")
    self.assertEqual(len(p), 1)
    self.assertTrue("class1" in p[0].attributes["class"])
    p = v("p.class2")
    self.assertEqual(len(p), 1)
    self.assertTrue("class2" in p[0].attributes["class"])
    p = v("p.class1.class2")
    self.assertEqual(len(p), 1)
    self.assertTrue("class1" in p[0].attributes["class"])
    self.assertTrue("class2" in p[0].attributes["class"])
    e = p[0]
    self.assertEqual(e, v("p[class='class1 class2']")[0])
    self.assertEqual(e, v("p[class^='class1']")[0])
    self.assertEqual(e, v("p[class$='class2']")[0])
    self.assertEqual(e, v("p[class*='class']")[0])
    self.assertEqual(e, v("p:contains('blah')")[1])
    self.assertTrue(web.Selector("p[class='class1 class2']").match(e))
    print("pattern.web.Selector()")

</t>
<t tx="karstenw.20230303140227.9">def test_fix(self):
    # Assert fix for common Unicode mistakes.
    self.assertEqual(web.fix("clichÃ©"), "cliché")
    self.assertEqual(web.fix("clichÃ©"), "cliché")
    self.assertEqual(web.fix("cliché"), "cliché")
    self.assertEqual(web.fix("â€“"), "–")

</t>
<t tx="karstenw.20230303140227.90">class TestDocumentParser(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140227.91">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140227.92">def test_pdf(self):
    # Assert PDF to string parser.
    s = web.parsedoc(os.path.join(PATH, "corpora", "carroll-wonderland.pdf"))
    self.assertTrue("Curiouser and curiouser!" in s)
    self.assertTrue(isinstance(s, str))
    print("pattern.web.parsepdf()")

</t>
<t tx="karstenw.20230303140227.93">def test_docx(self):
    # Assert PDF to string parser.
    s = web.parsedoc(os.path.join(PATH, "corpora", "carroll-lookingglass.docx"))
    self.assertTrue("'Twas brillig, and the slithy toves" in s)
    self.assertTrue(isinstance(s, str))
    print("pattern.web.parsedocx()")

</t>
<t tx="karstenw.20230303140227.94">class TestLocale(unittest.TestCase):

    @others
#---------------------------------------------------------------------------------------------------
# You need to define a username, password and mailbox to test on.


</t>
<t tx="karstenw.20230303140227.95">def setUp(self):
    pass

</t>
<t tx="karstenw.20230303140227.96">def test_encode_language(self):
    # Assert "Dutch" =&gt; "nl".
    self.assertEqual(web.locale.encode_language("dutch"), "nl")
    self.assertEqual(web.locale.encode_language("?????"), None)
    print("pattern.web.locale.encode_language()")

</t>
<t tx="karstenw.20230303140227.97">def test_decode_language(self):
    # Assert "nl" =&gt; "Dutch".
    self.assertEqual(web.locale.decode_language("nl"), "Dutch")
    self.assertEqual(web.locale.decode_language("NL"), "Dutch")
    self.assertEqual(web.locale.decode_language("??"), None)
    print("pattern.web.locale.decode_language()")

</t>
<t tx="karstenw.20230303140227.98">def test_encode_region(self):
    # Assert "Belgium" =&gt; "BE".
    self.assertEqual(web.locale.encode_region("belgium"), "BE")
    self.assertEqual(web.locale.encode_region("???????"), None)
    print("pattern.web.locale.encode_region()")

</t>
<t tx="karstenw.20230303140227.99">def test_decode_region(self):
    # Assert "BE" =&gt; "Belgium".
    self.assertEqual(web.locale.decode_region("be"), "Belgium")
    self.assertEqual(web.locale.decode_region("BE"), "Belgium")
    self.assertEqual(web.locale.decode_region("??"), None)
    print("pattern.web.locale.decode_region()")

</t>
<t tx="karstenw.20230303140243.1"></t>
<t tx="karstenw.20230303140246.1"></t>
<t tx="karstenw.20230303140259.1">#### PATTERN | VECTOR ##############################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################
# Vector space model, based on cosine similarity using tf-idf.
# Documents (e.g., a sentence or a text) are represented as bag-of-words:
# the unordered words in the document and their (relative frequency).
# The dictionary of word =&gt; frequency items is called the document vector.
# The frequency weight is either TF or TF-IDF (term frequency-inverse document frequency, i.e.,
# the relevance of a word in a document offset by the frequency of the word in all documents).
# Documents can be grouped in a Model to calculate TF-IDF and cosine similarity,
# which measures similarity (0.0-1.0) between documents based on the cosine distance metric.
# A document cay have a type (or label). A model of labeled documents can be used to train
# a classifier. A classifier can be used to predict the label of unlabeled documents.
# This is called supervised machine learning (since we provide labeled training examples).
# Unsupervised machine learning or clustering can be used to group unlabeled documents
# into subsets based on their similarity.

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140319.1">from __future__ import print_function
from __future__ import unicode_literals
from __future__ import absolute_import
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range, next

from collections import OrderedDict

from . import stemmer
_stemmer = stemmer

import sys
import os
import re
import glob
import heapq
import codecs
import tempfile

try:
    # Python 2
    import cPickle as pickle
except ImportError:
    # Python 3
    import pickle

import gzip
import types

from math import log, exp, sqrt, tanh
from time import time
from random import random, randint, uniform, choice, sample, seed
from itertools import chain
from bisect import insort
from operator import itemgetter
from collections import defaultdict

from io import open

import numpy as np
# import scipy
import pdb

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

try:
    from pattern.text import singularize, predicative, conjugate, tokenize
except:
    try:
        import sys
        sys.path.insert(0, os.path.join(MODULE, ".."))
        from text import singularize, predicative, conjugate, tokenize
    except:
        singularize = lambda w, **k: w
        predicative = lambda w, **k: w
        conjugate   = lambda w, t, **k: w
        tokenize    = lambda s: list(filter(len,
                                    re.split(r"(.*?[\.|\?|\!])",
                                        re.sub(r"(\.|\?|\!|,|;|:)", " \\1", s))))

from pattern.helpers import encode_string, decode_string

decode_utf8 = decode_string
encode_utf8 = encode_string


</t>
<t tx="karstenw.20230303140319.10">def __init__(self, *args, **kwargs):
    self._f = False
    super(readonlyodict, self).__init__(*args, **kwargs)
    self._f = True

</t>
<t tx="karstenw.20230303140319.100">def _get_weight(self):
    return self._weight

</t>
<t tx="karstenw.20230303140319.101">def _set_weight(self, w):
    self._update() # Clear the cache.
    self._weight = w

weight = property(_get_weight, _set_weight)

</t>
<t tx="karstenw.20230303140319.102">@classmethod
def load(cls, path):
    """ Loads the model from a gzipped pickle file created with Model.save().
    """
    model = pickle.loads(gzip.GzipFile(path, "rb").read())
    # Deserialize Model.classifier.
    if model.classifier:
        p = path + ".tmp"
        f = open(p, "wb")
        f.write(model.classifier)
        f.close()
        model._classifier = Classifier.load(p)
        os.remove(p)
    return model

</t>
<t tx="karstenw.20230303140319.103">def save(self, path, update=False, final=False):
    """ Saves the model as a gzipped pickle file at the given path.
        The advantage is that cached vectors and cosine similarity are stored.
    """
    # Update the cache before saving.
    if update:
        classes = self.classes
        self.document_frequency("")        # set self._df
        self.inverted_index                # set self._inverted
        self.vector                        # set self._vector
        self.posterior_probability("", "") # set self._pp
        self.chi_squared("")               # set self._x2
        self.information_gain("")          # set self._ig + self._gr
        for d1 in self.documents:          # set self._cos
            for d2 in self.documents:
                self.cosine_similarity(d1, d2)
    # Serialize Model.classifier.
    if self._classifier:
        p = path + ".tmp"
        self._classifier.save(p, final)
        self._classifier = open(p, "rb").read()
        os.remove(p)
    f = gzip.GzipFile(path, "wb")
    f.write(pickle.dumps(self, 1))  # 1 = binary
    f.close()

</t>
<t tx="karstenw.20230303140319.104">def export(self, path, format=ORANGE, **kwargs):
    """ Exports the model as a file for other machine learning applications,
        e.g., Orange or Weka.
    """
    # The Document.vector space is exported without cache or LSA concept space.
    keys = sorted(self.vector.keys())
    s = []
    # Orange tab format:
    if format.lower() == ORANGE:
        s.append("\t".join(keys + ["m#name", "c#type"]))
        for document in self.documents:
            v = document.vector
            v = [v.get(k, 0) for k in keys]
            v = "\t".join(x == 0 and "0" or "%.4f" % x for x in v)
            v = "%s\t%s\t%s" % (v, document.name or "", document.type or "")
            s.append(v)
    # Weka ARFF format:
    if format.lower() == WEKA:
        s.append("@RELATION %s" % kwargs.get("name", hash(self)))
        s.append("\n".join("@ATTRIBUTE %s NUMERIC" % k for k in keys))
        s.append("@ATTRIBUTE class {%s}" % ",".join(set(d.type or "" for d in self.documents)))
        s.append("@DATA")
        for document in self.documents:
            v = document.vector
            v = [v.get(k, 0) for k in keys]
            v = ",".join(x == 0 and "0" or "%.4f" % x for x in v)
            v = "%s,%s" % (v, document.type or "")
            s.append(v)
    s = "\n".join(s)
    f = open(path, "w", encoding="utf-8")
    f.write(decode_utf8(s))
    f.close()

</t>
<t tx="karstenw.20230303140319.105">def _update(self):
    # Ensures that all document vectors are recalculated
    # when a document is added or deleted (= new features).
    self._df = {}
    self._cos = {}
    self._pp = {}
    self._x2 = {}
    self._ig = {}
    self._gr = {}
    self._inverted = {}
    self._vector = None
    self._classifier = None
    self._lsa = None
    for document in self.documents:
        document._vector = None

</t>
<t tx="karstenw.20230303140319.106">def __len__(self):
    return len(self.documents)

</t>
<t tx="karstenw.20230303140319.107">def __iter__(self):
    return iter(self.documents)

</t>
<t tx="karstenw.20230303140319.108">def __getitem__(self, i):
    return self.documents.__getitem__(i)

</t>
<t tx="karstenw.20230303140319.109">def __delitem__(self, i):
    d = list.pop(self.documents, i)
    d._model = None
    self._index.pop(d.name, None)
    self._update()

</t>
<t tx="karstenw.20230303140319.11">@classmethod
def fromkeys(cls, k, default=None):
    return readonlyodict((k, default) for k in k)

</t>
<t tx="karstenw.20230303140319.110">def clear(self):
    self._documents = readonlylist()
    self._update()

</t>
<t tx="karstenw.20230303140319.111">def append(self, document):
    """ Appends the given Document to the model.
        If Model.weight != TF, the cache of vectors and cosine similarity is cleared
        (feature weights will be different now that there is a new document).
    """
    if not isinstance(document, Document):
        document = Document(document)
    if document.name is not None:
        self._index[document.name] = document
    document._model = self
    list.append(self.documents, document)
    if self._weight not in (TF, BINARY, None):
        self._update()

</t>
<t tx="karstenw.20230303140319.112">def extend(self, documents):
    """ Extends the model with the given list of documents.
    """
    documents = list(documents)
    for i, document in enumerate(documents):
        if not isinstance(document, Document):
            documents[i] = Document(document)
        if document.name is not None:
            self._index[document.name] = document
        document._model = self
    list.extend(self.documents, documents)
    if self._weight not in (TF, BINARY, None):
        self._update()

</t>
<t tx="karstenw.20230303140319.113">def remove(self, document):
    """ Removes the given Document from the model, and sets Document.model=None.
    """
    self.__delitem__(self.documents.index(document))

</t>
<t tx="karstenw.20230303140319.114">def document(self, name):
    """ Returns the Document with the given name (assuming document names are unique).
    """
    if name in self._index:
        return self._index[name]

doc = document

</t>
<t tx="karstenw.20230303140319.115">def keywords(self, top=10, normalized=True):
    """ Returns a sorted list of (relevance, word)-tuples that are top keywords in the model.
        With normalized=True, weights are normalized between 0.0 and 1.0 (their sum will be 1.0).
    """
    self.df(None) # Populate document frequency cache.
    n = normalized and sum(self._df.values()) or 1.0
    v = ((f / n, w) for w, f in self._df.items())
    v = heapq.nsmallest(top, v, key=lambda v: (-v[0], v[1]))
    return v

</t>
<t tx="karstenw.20230303140319.116">def document_frequency(self, word):
    """ Returns the document frequency for the given word or feature.
        Returns 0 if there are no documents in the model (e.g. no word frequency).
        df = number of documents containing the word / number of documents.
        The more occurences of the word across the model, the higher its df weight.
    """
    if len(self.documents) == 0:
        return 0.0
    if len(self._df) == 0:
        # Caching document frequency for each word gives a 300x performance boost
        # (i.e., calculated all at once). Drawback is if you need it for just one word.
        df = self._df
        for d in self.documents:
            for w, f in d.terms.items():
                if f != 0:
                    df[w] = (w in df) and df[w] + 1 or 1.0
        for w in df:
            df[w] /= float(len(self.documents))
    return self._df.get(word, 0.0)

df = document_frequency

</t>
<t tx="karstenw.20230303140319.117">def inverse_document_frequency(self, word, base=2.71828):
    """ Returns the inverse document frequency for the given word or feature.
        Returns None if the word is not in the model, or if there are no documents in the model.
        Using the natural logarithm:
        idf = log(1/df)
        The more occurences of the word, the lower its idf weight (log() makes it grow slowly).
    """
    df = self.df(word)
    if df == 0.0:
        return None
    if df == 1.0:
        return 0.0
    return log(1.0 / df, base)

idf = inverse_document_frequency

</t>
<t tx="karstenw.20230303140319.118">@property
def inverted_index(self):
    """ Yields a dictionary of (word, set([document1, document2, ...]))-items. 
    """
    if not self._inverted:
        m = {}
        for d in self.documents:
            for w in d.terms:
                if w not in m:
                    m[w] = set()
                m[w].add(d)
        self._inverted = m
    return self._inverted

inverted = inverted_index

</t>
<t tx="karstenw.20230303140319.119">@property
def vector(self):
    """ Returns a Vector dict of (word, 0.0)-items from the vector space model.
        It includes all words from all documents (i.e. it is the dimension of the vector space).
        Model.vector(document) yields a vector with the feature weights of the given document.
    """
    # Notes:
    # 1) Model.vector is the dictionary of all (word, 0.0)-items.
    # 2) Model.vector(document) returns a copy with the document's word frequencies.
    #    This is the full vector, as opposed to the sparse Document.vector.
    #    Words in a document that are not in the model are ignored,
    #    i.e., the document was not in the model, this can be the case in Model.search().
    # See: Vector.__call__().
    if not self._vector:
        self._vector = Vector(((w, 0.0) for w in chain(*(d.terms for d in self.documents))), sparse=False)
    return self._vector

</t>
<t tx="karstenw.20230303140319.12">def __setitem__(self, *args, **kwargs):
    if self._f:
        raise ReadOnlyError
    return super(readonlyodict, self).__setitem__(*args, **kwargs)

</t>
<t tx="karstenw.20230303140319.120">@property
def vectors(self):
    """ Yields a list of all document vectors.
    """
    return [d.vector for d in self.documents]

</t>
<t tx="karstenw.20230303140319.121">@property
def density(self):
    """ Yields the overall word coverage as a number between 0.0-1.0.
    """
    return float(sum(len(d.vector) for d in self.documents)) / len(self.vector) ** 2

# Following methods rely on Document.vector:
# frequent sets, cosine similarity, nearest neighbors, search, clustering,
# information gain, latent semantic analysis.

</t>
<t tx="karstenw.20230303140319.122">def frequent_concept_sets(self, threshold=0.5):
    """ Returns a dictionary of (set(feature), frequency) 
        of feature combinations with a frequency above the given threshold.
    """
    return apriori([d.terms for d in self.documents], support=threshold)

sets = frequent = frequent_concept_sets

</t>
<t tx="karstenw.20230303140319.123">def cosine_similarity(self, document1, document2):
    """ Returns the similarity between two documents in the model as a number between 0.0-1.0,
        based on the document feature weight (e.g., tf * idf of words in the text).
        cos = dot(v1, v2) / (norm(v1) * norm(v2))
    """
    # If we already calculated similarity between two given documents,
    # it is available in cache for reuse.
    id1 = document1.id
    id2 = document2.id
    if (id1, id2) in self._cos:
        return self._cos[(id1, id2)]
    if (id2, id1) in self._cos:
        return self._cos[(id2, id1)]
    # Calculate the matrix multiplication of the document vectors.
    if not getattr(self, "lsa", None):
        v1 = document1.vector
        v2 = document2.vector
        s = cosine_similarity(v1, v2)
    else:
        # Using LSA concept space:
        v1 = id1 in self.lsa and self.lsa[id1] or self._lsa.transform(document1)
        v2 = id2 in self.lsa and self.lsa[id2] or self._lsa.transform(document2)
        s = cosine_similarity(v1, v2)
    # Cache the similarity weight for reuse.
    if document1.model == self and \
       document2.model == self:
        self._cos[(id1, id2)] = s
    return s

similarity = cos = cosine_similarity

</t>
<t tx="karstenw.20230303140319.124">def nearest_neighbors(self, document, top=10):
    """ Returns a list of (similarity, document)-tuples in the model, 
        sorted by cosine similarity to the given document.
    """
    v = ((self.cosine_similarity(document, d), d) for d in self.documents)
    # Filter the input document from the matches.
    # Filter documents that score zero, and return the top.
    v = [(w, d) for w, d in v if w &gt; 0 and d.id != document.id]
    v = heapq.nsmallest(top, v, key=lambda v: (-v[0], v[1]))
    return v

similar = related = neighbors = nn = nearest_neighbors

</t>
<t tx="karstenw.20230303140319.125">def vector_space_search(self, words=[], **kwargs):
    """ Returns related documents from the model as a list of (similarity, document)-tuples.
        The given words can be a string (one word), a list or tuple of words, or a Document.
    """
    top = kwargs.pop("top", 10)
    if not isinstance(words, Document):
        kwargs.setdefault("filter", lambda w: w) # pass-through.
        kwargs.setdefault("stopwords", True)
        words = Document(words)
    if len([w for w in words if w in self.vector]) == 0:
        return []
    m, words._model = words._model, self # So we can calculate tf-idf.
    n, words._model = self.nearest_neighbors(words, top), m
    words._model = m
    return n

search = vector_space_search

</t>
<t tx="karstenw.20230303140319.126">def distance(self, document1, document2, *args, **kwargs):
    """ Returns the distance (COSINE, EUCLIDEAN, ...) between two document vectors (0.0-1.0).
    """
    return distance(document1.vector, document2.vector, *args, **kwargs)

</t>
<t tx="karstenw.20230303140319.127">class Apriori(object):

    @others
apriori = Apriori()

#### LATENT SEMANTIC ANALYSIS ######################################################################
# Based on:
# http://en.wikipedia.org/wiki/Latent_semantic_analysis
# http://blog.josephwilk.net/projects/latent-semantic-analysis-in-python.html


</t>
<t tx="karstenw.20230303140319.128">def __init__(self):
    self._candidates = []
    self._support = {}

</t>
<t tx="karstenw.20230303140319.129">def C1(self, sets):
    """ Returns the unique features from all sets as a list of (hashable) frozensets.
    """
    return [frozenset([v]) for v in set(chain(*sets))]

</t>
<t tx="karstenw.20230303140319.13">def __delitem__(self, *args, **kwargs):
    if self._f:
        raise ReadOnlyError
    return super(readonlyodict, self).__delitem__(*args, **kwargs)

</t>
<t tx="karstenw.20230303140319.130">def Ck(self, sets):
    """ For the given sets of length k, returns combined candidate sets of length k+1.
    """
    Ck = []
    for i, s1 in enumerate(sets):
        for j, s2 in enumerate(sets[i + 1:]):
            if set(list(s1)[:-1]) == set(list(s2)[:-1]):
                Ck.append(s1 | s2)
    return Ck

</t>
<t tx="karstenw.20230303140319.131">def Lk(self, sets, candidates, support=0.0):
    """ Prunes candidate sets whose frequency &lt; support threshold.
        Returns a dictionary of (candidate set, frequency)-items.
    """
    Lk, x = {}, 1.0 / (len(sets) or 1) # relative count
    for s1 in candidates:
        for s2 in sets:
            if s1.issubset(s2):
                Lk[s1] = s1 in Lk and Lk[s1] + x or x
    return dict((s, f) for s, f in Lk.items() if f &gt;= support)

</t>
<t tx="karstenw.20230303140319.132">def __call__(self, sets=[], support=0.5):
    """ Returns a dictionary of (set(features), frequency)-items.
        The given support (0.0-1.0) is the relative amount of documents
        in which a combination of features must appear.
    """
    sets = [set(iterable) for iterable in sets]
    C1 = self.C1(sets)
    L1 = self.Lk(sets, C1, support)
    self._candidates = [list(L1.keys())]
    self._support = L1
    while True:
        # Terminate when no further extensions are found.
        if len(self._candidates[-1]) == 0:
            break
        # Extend frequent subsets one item at a time.
        Ck = self.Ck(self._candidates[-1])
        Lk = self.Lk(sets, Ck, support)
        self._candidates.append(list(Lk.keys()))
        self._support.update(Lk)
    return self._support

</t>
<t tx="karstenw.20230303140319.133">class LSA(object):

    @others
# LSA cache for Model.vector_space_search() shouldn't be stored with Model.save()
# (so it is a global instead of a property of the LSA class).
_lsa_transform_cache = {}

#def iter2array(iterator, typecode):
#    a = np.array([next(iterator)], typecode)
#    shape0 = a.shape[1:]
#    for (i, item) in enumerate(iterator):
#        a.resize((i+2,) + shape0)
#        a[i+1] = item
#    return a

#def filter(matrix, min=0):
#    columns = np.max(matrix, axis=0)
#    columns = [i for i, v in enumerate(columns) if v &lt;= min] # Indices of removed columns.
#    matrix = np.delete(matrix, columns, axis=1)
#    return matrix, columns

#### CLUSTERING ####################################################################################
# Clustering can be used to categorize a set of unlabeled documents.
# Clustering is an unsupervised machine learning method that partitions a set of vectors into
# subsets, using a distance metric to determine how similar two vectors are.
# For example, for (x, y)-points in 2D space we can use Euclidean distance ("as the crow flies").
# The k_means() and hierarchical() functions work with Vector objects or dictionaries.


</t>
<t tx="karstenw.20230303140319.134">def __init__(self, model, k=NORM):
    """ Latent Semantic Analysis is a statistical machine learning method based on 
        singular value decomposition (SVD), and related to principal component analysis (PCA).
        Closely related features (words) in the model are combined into "concepts".
        Documents then get a concept vector that is an approximation of the original vector,
        but with reduced dimensionality so that cosine similarity and clustering run faster.
    """
    # Calling Model.vector() in a loop is quite slow, we should refactor this:
    matrix = [list(model.vector(d).values()) for d in model.documents]
    matrix = np.array(matrix)
    # Singular value decomposition, where u * sigma * vt = svd(matrix).
    # Sigma is the diagonal matrix of singular values,
    # u has document rows and concept columns, vt has concept rows and term columns.
    u, sigma, vt = np.linalg.svd(matrix, full_matrices=False)

    # Note: now np.dot(np.dot(u, np.diag(sigma)), vt) !≈ matrix
    # assert np.allclose(np.dot(np.dot(u, np.diag(sigma)), vt), matrix)

    # Delete the smallest coefficients in the diagonal matrix (i.e., at the end of the list).
    # The difficulty and weakness of LSA is knowing how many dimensions to reduce
    # (generally L2-norm is used).
    if k == L1:
        k = int(round(np.linalg.norm(sigma, 1)))
    if k == L2 or k == NORM:
        k = int(round(np.linalg.norm(sigma, 2)))
    if k == TOP300:
        k = max(0, len(sigma) - 300)
    if isinstance(k, int):
        k = max(0, len(sigma) - k)
    if type(k).__name__ == "function":
        k = max(0, int(k(sigma)))
    #print(np.dot(u, np.dot(np.diag(sigma), vt)))
    # Apply dimension reduction.
    # The maximum length of a concept vector = the number of documents.
    assert k &lt; len(model.documents), \
        "can't create more dimensions than there are documents"
    tail = lambda x, i: list(range(len(x) - i, len(x)))
    u, sigma, vt = (
        np.delete(u, tail(u[0], k), axis=1),
        np.delete(sigma, tail(sigma, k), axis=0),
        np.delete(vt, tail(vt, k), axis=0)
    )

    # In some numpy versions, np.linalg.svd seems to yield negative components. SVD decomposition is not unique.
    u, sigma, vt = list(map(np.abs, (u, sigma, vt)))

    # Store as Python dict and lists so we can pickle it.
    self.model = model
    self._terms = dict(enumerate(model.vector().keys())) # Vt-index =&gt; word.
    self.u, self.sigma, self.vt = (
        dict((d.id, Vector((i, float(x)) for i, x in enumerate(v))) for d, v in zip(model, u)),
        list(sigma),
        [[float(x) for x in v] for v in vt]
    )

</t>
<t tx="karstenw.20230303140319.135">@property
def terms(self):
    """ Yields a list of all terms, identical to LSA.model.vector.keys().
    """
    return list(self._terms.values())

features = words = terms

</t>
<t tx="karstenw.20230303140319.136">@property
def concepts(self):
    """ Yields a list of all concepts, each a dictionary of (word, weight)-items.
    """
    # Round the weight so 9.0649330400000009e-17 becomes a more meaningful 0.0.
    return [dict((self._terms[i], round(w, 15)) for i, w in enumerate(concept)) for concept in self.vt]

</t>
<t tx="karstenw.20230303140319.137">@property
def vectors(self):
    """ Yields a dictionary of (Document.id, concepts),
        where each concept is a dictionary of (concept_index, weight)-items.
        for document in lsa.model:
            for concept in lsa.vectors(document.id):
                print(document, concept)
    """
    return self.u

</t>
<t tx="karstenw.20230303140319.138">def vector(self, id):
    if isinstance(id, Document):
        id = id.id
    return self.u[id]

</t>
<t tx="karstenw.20230303140319.139">def __getitem__(self, id):
    return self.u[id]

</t>
<t tx="karstenw.20230303140319.14">def pop(self, *args, **kwargs):
    if self._f:
        raise ReadOnlyError
    return super(readonlyodict, self).pop(*args, **kwargs)

</t>
<t tx="karstenw.20230303140319.140">def __contains__(self, id):
    return id in self.u

</t>
<t tx="karstenw.20230303140319.141">def __iter__(self):
    return iter(self.u)

</t>
<t tx="karstenw.20230303140319.142">def __len__(self):
    return len(self.u)

</t>
<t tx="karstenw.20230303140319.143">def transform(self, document):
    """ Given a document not in the model, returns a vector in LSA concept space.
        This happes automatically in Model.cosine_similarity(),
        but it must be done explicitly for Classifier.classify() input.
    """
    if document.id in self.u:
        return self.u[document.id]
    if document.id in _lsa_transform_cache:
        return _lsa_transform_cache[document.id]
    import numpy
    v = self.model.vector(document)
    v = [v[self._terms[i]] for i in range(len(v))]
    v = np.dot(np.dot(np.linalg.inv(np.diag(self.sigma)), self.vt), v)
    v = _lsa_transform_cache[document.id] = Vector(enumerate(v))
    return v

</t>
<t tx="karstenw.20230303140319.144">def mean(iterable, length=None):
    """ Returns the arithmetic mean of the values in the given iterable or iterator.
    """
    if length is None:
        if not hasattr(iterable, "__len__"):
            iterable = list(iterable)
        length = len(iterable)
    return sum(iterable) / float(length or 1)


</t>
<t tx="karstenw.20230303140319.145">def centroid(vectors=[], features=[]):
    """ Returns the center of the given list of vectors.
        For example: if each vector has two features, (x, y)-coordinates in 2D space,
        the centroid is the geometric center of the coordinates forming a polygon.
        Since vectors are sparse (i.e., features with weight 0 are omitted), 
        the list of all features (= Model.vector) must be given.
    """
    c = []
    for v in vectors:
        if isinstance(v, Cluster):
            c.extend(v.flatten())
        elif isinstance(v, Document):
            c.append(v.vector)
        else:
            c.append(v)
    if not features:
        features = _features(c)
    c = [(f, mean((v.get(f, 0) for v in c), len(c))) for f in features]
    c = Vector((f, w) for f, w in c if w != 0)
    return c


</t>
<t tx="karstenw.20230303140319.146">class DistanceMap(object):

    @others
</t>
<t tx="karstenw.20230303140319.147">def __init__(self, method=COSINE):
    """ A lazy map of cached distances between Vector objects.
    """
    self.method = method
    self._cache = {}

</t>
<t tx="karstenw.20230303140319.148">def __call__(self, v1, v2):
    return self.distance(v1, v2)

</t>
<t tx="karstenw.20230303140319.149">def distance(self, v1, v2):
    """ Returns the cached distance between two vectors.
    """
    try:
        # Two Vector objects for which the distance was already calculated.
        d = self._cache[(v1.id, v2.id)]
    except KeyError:
        # Two Vector objects for which the distance has not been calculated.
        d = self._cache[(v1.id, v2.id)] = distance(v1, v2, method=self.method)
    except AttributeError:
        # No "id" property, so not a Vector but a plain dict.
        d = distance(v1, v2, method=self.method)
    return d


</t>
<t tx="karstenw.20230303140319.15">def popitem(self, *args, **kwargs):
    if self._f:
        raise ReadOnlyError
    return super(readonlyodict, self).popitem(*args, **kwargs)

</t>
<t tx="karstenw.20230303140319.150">def cluster(method=KMEANS, vectors=[], **kwargs):
    """ Clusters the given list of vectors using the k-means or hierarchical algorithm.
    """
    if method == KMEANS:
        return k_means(vectors, **kwargs)
    if method == HIERARCHICAL:
        return hierarchical(vectors, **kwargs)

#--- K-MEANS ---------------------------------------------------------------------------------------
# k-means is fast but no optimal solution is guaranteed (random initialization).

# Initialization methods:
RANDOM, KMPP = "random", "kmeans++"


</t>
<t tx="karstenw.20230303140319.151">def k_means(vectors, k=None, iterations=10, distance=COSINE, seed=RANDOM, **kwargs):
    """ Returns a list of k clusters, where each cluster is a list of vectors (Lloyd's algorithm).
        Vectors are assigned to k random centers using a distance metric (EUCLIDEAN, COSINE, ...).
        Since the initial centers are chosen randomly (by default, seed=RANDOM),
        there is no guarantee of convergence or of finding an optimal solution.
        A more efficient way is to use seed=KMPP (k-means++ initialization algorithm).
    """
    features = kwargs.get("features") or _features(vectors)
    if k is None:
        k = sqrt(len(vectors) / 2)
    if k &lt; 2:
        return [[v for v in vectors]]
    if seed == KMPP:
        clusters = kmpp(vectors, k, distance)
    else:
        clusters = [[] for i in range(int(k))]
        for i, v in enumerate(sorted(vectors, key=lambda x: random())):
            # Randomly partition the vectors across k clusters.
            clusters[i % int(k)].append(v)
    # Cache the distance calculations between vectors (up to 4x faster).
    map = DistanceMap(method=distance)
    distance = map.distance
    converged = False
    while not converged and iterations &gt; 0 and k &gt; 0:
        # Calculate the center of each cluster.
        centroids = [centroid(cluster, features) for cluster in clusters]
        # Triangle inequality: one side is shorter than the sum of the two other sides.
        # We can exploit this to avoid costly distance() calls (up to 3x faster).
        p = 0.5 * kwargs.get("p", 0.8) # "Relaxed" triangle inequality (cosine distance is a semimetric) 0.25-0.5.
        D = {}
        for i in range(len(centroids)):
            for j in range(i, len(centroids)): # center1–center2 &lt; center1–vector + vector–center2 ?
                D[(i, j)] = D[(j, i)] = p * distance(centroids[i], centroids[j])
        # For every vector in every cluster,
        # check if it is nearer to the center of another cluster.
        # If so, assign it. When visualized, this produces a Voronoi diagram.
        converged = True
        for i in range(len(clusters)):
            for v in clusters[i]:
                nearest, d1 = i, distance(v, centroids[i])
                for j in range(len(clusters)):
                    if D[(i, j)] &lt; d1: # Triangle inequality (Elkan, 2003).
                        d2 = distance(v, centroids[j])
                        if d2 &lt; d1:
                            nearest = j
                if nearest != i: # Other cluster is nearer.
                    clusters[nearest].append(clusters[i].pop(clusters[i].index(v)))
                    converged = False
        iterations -= 1
        #print(iterations)
    return clusters

kmeans = k_means


</t>
<t tx="karstenw.20230303140319.152">def kmpp(vectors, k, distance=COSINE):
    """ The k-means++ initialization algorithm returns a set of initial clusers, 
        with the advantage that:
        - it generates better clusters than k-means(seed=RANDOM) on most data sets,
        - it runs faster than standard k-means,
        - it has a theoretical approximation guarantee.
    """
    # Cache the distance calculations between vectors (up to 4x faster).
    map = DistanceMap(method=distance)
    distance = map.distance
    # David Arthur, 2006, http://theory.stanford.edu/~sergei/slides/BATS-Means.pdf
    # Based on:
    # http://www.stanford.edu/~darthur/kmpp.zip
    # http://yongsun.me/2008/10/k-means-and-k-means-with-python
    # Choose one center at random.
    # Calculate the distance between each vector and the nearest center.
    centroids = [choice(vectors)]
    d = [distance(v, centroids[0]) for v in vectors]
    s = sum(d)
    for _ in range(int(k) - 1):
        # Choose a random number y between 0 and d1 + d2 + ... + dn.
        # Find vector i so that: d1 + d2 + ... + di &gt;= y &gt; d1 + d2 + ... + dj.
        # Perform a number of local tries so that y yields a small distance sum.
        i = 0
        for _ in range(int(2 + log(k))):
            y = random() * s
            for i1, v1 in enumerate(vectors):
                if y &lt;= d[i1]:
                    break
                y -= d[i1]
            s1 = sum(min(d[j], distance(v1, v2)) for j, v2 in enumerate(vectors))
            if s1 &lt; s:
                s, i = s1, i1
        # Add vector i as a new center.
        # Repeat until we have chosen k centers.
        centroids.append(vectors[i])
        d = [min(d[i], distance(v, centroids[-1])) for i, v in enumerate(vectors)]
        s = sum(d)
    # Assign points to the nearest center.
    clusters = [[] for i in range(int(k))]
    for v1 in vectors:
        d = [distance(v1, v2) for v2 in centroids]
        clusters[d.index(min(d))].append(v1)
    return clusters

#--- HIERARCHICAL ----------------------------------------------------------------------------------
# Hierarchical clustering is slow but the optimal solution guaranteed in O(len(vectors) ** 3).


</t>
<t tx="karstenw.20230303140319.153">class Cluster(list):

    @others
</t>
<t tx="karstenw.20230303140319.154">def __init__(self, *args, **kwargs):
    """ A nested list of Cluster and Vector objects, 
        returned from hierarchical() clustering.
    """
    list.__init__(self, *args, **kwargs)

</t>
<t tx="karstenw.20230303140319.155">@property
def depth(self):
    """ Yields the maximum depth of nested clusters.
        Cluster((1, Cluster((2, Cluster((3, 4)))))).depth =&gt; 2.
    """
    return max([0] + [1 + n.depth for n in self if isinstance(n, Cluster)])

</t>
<t tx="karstenw.20230303140319.156">def flatten(self, depth=1000):
    """ Flattens nested clusters to a list, down to the given depth.
        Cluster((1, Cluster((2, Cluster((3, 4)))))).flatten(1) =&gt; [1, 2, Cluster(3, 4)].
    """
    a = []
    for item in self:
        if isinstance(item, Cluster) and depth &gt; 0:
            a.extend(item.flatten(depth - 1))
        else:
            a.append(item)
    return a

</t>
<t tx="karstenw.20230303140319.157">def traverse(self, visit=lambda cluster: None):
    """ Calls the given visit() function on this cluster and each nested cluster, breadth-first.
    """
    visit(self)
    for item in self:
        if isinstance(item, Cluster):
            item.traverse(visit)

</t>
<t tx="karstenw.20230303140319.158">def __repr__(self):
    return "Cluster(%s)" % list.__repr__(self)


</t>
<t tx="karstenw.20230303140319.159">def sequence(i=0, f=lambda i: i + 1):
    """ Yields an infinite sequence, for example:
        sequence() =&gt; 0, 1, 2, 3, ...
        sequence(1.0, lambda i: i/2) =&gt; 1, 0.5, 0.25, 0.125, ...
    """
    # Used to generate unique vector id's in hierarchical().
    # We cannot use Vector.id, since the given vectors might be plain dicts.
    # We cannot use id(vector), since id() is only unique for the lifespan of the object.
    while True:
        yield i
        i = f(i)


</t>
<t tx="karstenw.20230303140319.16">def update(self, *args, **kwargs):
    if self._f:
        raise ReadOnlyError
    return super(readonlyodict, self).update(*args, **kwargs)

</t>
<t tx="karstenw.20230303140319.160">def hierarchical(vectors, k=1, iterations=1000, distance=COSINE, **kwargs):
    """ Returns a Cluster containing k items (vectors or clusters with nested items).
        With k=1, the top-level cluster contains a single cluster.
    """
    id = sequence()
    features = kwargs.get("features", _features(vectors))
    clusters = Cluster((v for v in shuffled(vectors)))
    centroids = [(next(id), v) for v in clusters]
    map = {}
    for _ in range(iterations):
        if len(clusters) &lt;= max(k, 1):
            break
        nearest, d0 = None, None
        for i, (id1, v1) in enumerate(centroids):
            for j, (id2, v2) in enumerate(centroids[i + 1:]):
                # Cache the distance calculations between vectors.
                # This is identical to DistanceMap.distance(),
                # but it is faster in the inner loop to use it directly.
                try:
                    d = map[(id1, id2)]
                except KeyError:
                    d = map[(id1, id2)] = _distance(v1, v2, method=distance)
                if d0 is None or d &lt; d0:
                    nearest, d0 = (i, j + i + 1), d
        # Pairs of nearest clusters are merged as we move up the hierarchy:
        i, j = nearest
        merged = Cluster((clusters[i], clusters[j]))
        clusters.pop(j)
        clusters.pop(i)
        clusters.append(merged)
        # Cache the center of the new cluster.
        v = centroid(merged.flatten(), features)
        centroids.pop(j)
        centroids.pop(i)
        centroids.append((next(id), v))
    return clusters

#from pattern.vector import Vector
#
#v1 = Vector(wings=0, beak=0, claws=1, paws=1, fur=1) # cat
#v2 = Vector(wings=0, beak=0, claws=0, paws=1, fur=1) # dog
#v3 = Vector(wings=1, beak=1, claws=1, paws=0, fur=0) # bird
#
#print(hierarchical([v1, v2, v3]))

#### CLASSIFIER ####################################################################################
# Classification can be used to predict the label of an unlabeled document.
# Classification is a supervised machine learning method that uses labeled documents
# (i.e., Document objects with a type) as training examples to statistically predict
# the label (type, class) of new documents, based on their similarity to the training examples
# using a distance metric (e.g., cosine similarity).

#--- CLASSIFIER BASE CLASS -------------------------------------------------------------------------

# The default baseline (i.e., the default predicted class) is the most frequent class:
MAJORITY, FREQUENCY = "majority", "frequency"


</t>
<t tx="karstenw.20230303140319.161">class Classifier(object):

    @others
</t>
<t tx="karstenw.20230303140319.162">def __init__(self, train=[], baseline=MAJORITY, **kwargs):
    """ A base class for Naive Bayes, k-NN and SVM.
        Trains a classifier on the given list of Documents or (document, type)-tuples,
        where document can be a Document, Vector, dict or string
        (dicts and strings are implicitly converted to vectors).
    """
    data = getattr(self, "_data", {})
    self.description = ""       # Description of the dataset: author e-mail, etc.
    self._data       = data     # Custom data to store when pickled.
    self._vectors    = []       # List of trained (type, vector)-tuples.
    self._classes    = {}       # Dict of (class, frequency)-items.
    self._baseline   = baseline # Default predicted class.
    # Train on the list of Document objects or (document, type)-tuples:
    for d in (isinstance(d, Document) and (d, d.type) or d for d in train):
        self.train(*d)
    # In Pattern 2.5-, Classifier.test() is a classmethod.
    # In Pattern 2.6+, it is replaced with Classifier._test() once instantiated:
    self.test = self._test

</t>
<t tx="karstenw.20230303140319.163">@property
def features(self):
    """ Yields a list of trained features.
    """
    return list(features(v for type, v in self._vectors))

</t>
<t tx="karstenw.20230303140319.164">@property
def classes(self):
    """ Yields a list of trained classes.
    """
    return list(self._classes.keys())

terms, types = features, classes

</t>
<t tx="karstenw.20230303140319.165">@property
def binary(self):
    """ Yields True if the classifier predicts either True (0) or False (1).
    """
    return sorted(self.classes) in ([False, True], [0, 1])

</t>
<t tx="karstenw.20230303140319.166">@property
def distribution(self):
    """ Yields a dictionary of trained (class, frequency)-items.
    """
    return self._classes.copy()

</t>
<t tx="karstenw.20230303140319.167">@property
def majority(self):
    """ Yields the majority class (= most frequent class).
    """
    d = sorted((v, k) for k, v in self._classes.items())
    return d and d[-1][1] or None

</t>
<t tx="karstenw.20230303140319.168">@property
def minority(self):
    """ Yields the minority class (= least frequent class).
    """
    d = sorted((v, k) for k, v in self._classes.items())
    return d and d[0][1] or None

</t>
<t tx="karstenw.20230303140319.169">@property
def baseline(self):
    """ Yields the most frequent class in the training data,
        or a user-defined class if Classifier(baseline != MAJORITY).
    """
    if self._baseline not in (MAJORITY, FREQUENCY):
        return self._baseline
    return ([(0, None)] + sorted([(v, k) for k, v in self._classes.items()]))[-1][1]

</t>
<t tx="karstenw.20230303140319.17">class readonlydict(dict):
    @others
# Read-only list, used for Model.documents.


</t>
<t tx="karstenw.20230303140319.170">@property
def weighted_random_baseline(self):
    """ Yields the weighted random baseline:
        accuracy with classes predicted randomly according to their distribution.
    """
    n = float(sum(self.distribution.values())) or 1
    return sum(map(lambda x: (x / n) ** 2, self.distribution.values()))

wrb = weighted_random_baseline

</t>
<t tx="karstenw.20230303140319.171">@property
def skewness(self):
    """ Yields 0.0 if the trained classes are evenly distributed.
        Yields &gt; +1.0 or &lt; -1.0 if the training data is highly skewed.
    """
    def moment(a, m, k=1):
        return sum([(x - m)**k for x in a]) / (len(a) or 1)
    # List each training instance by an int that represents its class:
    a = list(chain(*([i] * v for i, (k, v) in enumerate(self._classes.items()))))
    m = float(sum(a)) / len(a) # mean
    return moment(a, m, 3) / (moment(a, m, 2) ** 1.5 or 1)

</t>
<t tx="karstenw.20230303140319.172">def train(self, document, type=None):
    """ Trains the classifier with the given document of the given type (i.e., class).
        A document can be a Document, Vector, dict, list or string.
        If no type is given, Document.type will be used instead.
    """
    type, vector = self._vector(document, type)
    self._vectors.append((type, vector))
    self._classes[type] = self._classes.get(type, 0) + 1

</t>
<t tx="karstenw.20230303140319.173">def classify(self, document, discrete=True):
    """ Returns the type with the highest probability for the given document.
        Returns a dict of (class, probability)-items if discrete=False.
    """
    # This method must be implemented in subclass.
    if not discrete:
        return Probabilities(self, {})
    return self.baseline

</t>
<t tx="karstenw.20230303140319.174">def _vector(self, document, type=None, **kwargs):
    """ Returns a (type, Vector)-tuple for the given document.
        If the document is part of a LSA-reduced model, returns the LSA concept vector.
        If the given type is None, returns document.type (if a Document is given).
    """
    if isinstance(document, Document):
        if type is None:
            type = document.type
        if document.model and document.model.lsa:
            return type, document.model.lsa[document.id] # LSA concept vector.
        return type, document.vector
    if isinstance(document, Vector):
        return type, document
    if isinstance(document, dict):
        return type, Vector(document, **kwargs)
    if isinstance(document, (list, tuple)):
        return type, Document(document, filter=None, stopwords=True).vector
    if isinstance(document, str):
        return type, Document(document, filter=None, stopwords=True).vector

</t>
<t tx="karstenw.20230303140319.175">@classmethod
def k_fold_cross_validation(cls, corpus=[], k=10, **kwargs):
    # Backwards compatibility.
    return K_fold_cross_validation(cls, documents=corpus, folds=k, **kwargs)

crossvalidate = cross_validate = cv = k_fold_cross_validation

</t>
<t tx="karstenw.20230303140319.176">@classmethod
def test(cls, corpus=[], d=0.65, folds=1, **kwargs):
    # Backwards compatibility.
    # In Pattern 2.5-, Classifier.test() is a classmethod.
    # In Pattern 2.6+, it is replaced with Classifier._test() once instantiated.
    corpus = kwargs.pop("documents", kwargs.pop("train", corpus))
    if folds &gt; 1:
        return K_fold_cross_validation(cls, documents=corpus, folds=folds, **kwargs)
    i = int(round(max(0.0, min(1.0, d)) * len(corpus)))
    d = shuffled(corpus)
    return cls(train=d[:i], **kwargs).test(d[i:])

</t>
<t tx="karstenw.20230303140319.177">def _test(self, documents=[], target=None, **kwargs):
    """ Returns an (accuracy, precision, recall, F1-score)-tuple for the given documents,
        with values between 0.0 and 1.0 (0-100%).
        Accuracy is the percentage of correct predictions for the given test set,
        but this metric can be misleading (e.g., classifier *always* predicts True).
        Precision is the percentage of predictions that were correct.
        Recall is the percentage of documents that were correctly labeled.
        F1-score is the harmonic mean of precision and recall.
    """
    return self.confusion_matrix(documents).test(target)

</t>
<t tx="karstenw.20230303140319.178">def auc(self, documents=[], k=10):
    """ Returns the area under the ROC-curve.
        Returns the probability (0.0-1.0) that a classifier will rank 
        a random positive document (True) higher than a random negative one (False).
    """
    return self.confusion_matrix(documents).auc(k)

</t>
<t tx="karstenw.20230303140319.179">def confusion_matrix(self, documents=[]):
    """ Returns the confusion matrix for the given test data,
        which is a list of Documents or (document, type)-tuples.
    """
    documents = [isinstance(d, Document) and (d, d.type) or d for d in documents]
    return ConfusionMatrix(self.classify, documents)

</t>
<t tx="karstenw.20230303140319.18">def __init__(self, *args, **kwargs):
    dict.__init__(self, *args, **kwargs)

</t>
<t tx="karstenw.20230303140319.180">def save(self, path, final=False):
    """ Saves the classifier as a gzipped pickle file.
    """
    if final:
        self.finalize()
    self.test = None # Can't pickle instancemethods.
    pdb.set_trace()
    f = gzip.GzipFile(path, "wb")
    # f = open( path, "wb" )
    # 1 = binary
    s = pickle.dumps(self, 1)
    f.write( s )
    f.close()

</t>
<t tx="karstenw.20230303140319.181">@classmethod
def load(cls, path):
    """ Loads the classifier from a gzipped pickle file.
    """
    f = gzip.GzipFile(path, "r")
    self = pickle.loads(f.read())
    self._on_load(path) # Initialize subclass (e.g., SVM).
    self.test = self._test
    f.close()
    return self

</t>
<t tx="karstenw.20230303140319.182">def _on_load(self, path):
    pass

</t>
<t tx="karstenw.20230303140319.183">def finalize(self):
    """ Removes training data from memory, keeping only the trained model,
        reducing file size with Classifier.save().
    """
    pass

</t>
<t tx="karstenw.20230303140319.184">#--- CLASSIFIER PROBABILITIES ----------------------------------------------------------------------
# Returned from Classifier.classify(v, discrete=False)


class Probabilities(defaultdict):

    @others
</t>
<t tx="karstenw.20230303140319.185">def __init__(self, classifier, *args, **kwargs):
    defaultdict.__init__(self, float)
    self.classifier = classifier
    self.update(*args, **kwargs)

</t>
<t tx="karstenw.20230303140319.186">@property
def max(self):
    """ Returns the (type, probability) with the highest probability.
    """
    try:
        # Ties are broken in favor of the majority class
        # (random winner for majority ties).
        b = self.classifier.baseline
        m = list(self.values())
        m = max(m)
        if self[b] &lt; m:
            return choice([x for x in self if self[x] == m &gt; 0]), m
        return b, m
    except:
        return b, 0.0

</t>
<t tx="karstenw.20230303140319.187">#--- CLASSIFIER EVALUATION -------------------------------------------------------------------------


class ConfusionMatrix(defaultdict):

    @others
</t>
<t tx="karstenw.20230303140319.188">def __init__(self, classify=lambda document: True, documents=[]):
    """ Returns the matrix of classes x predicted classes as a dictionary.
    """
    defaultdict.__init__(self, lambda: defaultdict(int))
    for document, type1 in documents:
        type2 = classify(document)
        self[type1][type2] += 1

</t>
<t tx="karstenw.20230303140319.189">def split(self):
    """ Returns an iterator over one-vs-all (type, TP, TN, FP, FN)-tuples.
    """
    return iter((type,) + self(type) for type in self)

</t>
<t tx="karstenw.20230303140319.19">@classmethod
def fromkeys(cls, k, default=None):
    return readonlydict((k, default) for k in k)

</t>
<t tx="karstenw.20230303140319.190">def __call__(self, target):
    """ Returns a (TP, TN, FP, FN)-tuple for the given class (one-vs-all).
    """
    TP = 0 # True positives.
    TN = 0 # True negatives.
    FP = 0 # False positives (type I error).
    FN = 0 # False negatives (type II error).
    for t1 in self:
        for t2, n in self[t1].items():
            if target == t1 == t2:
                TP += n
            if target != t1 == t2:
                TN += n
            if target == t1 != t2:
                FN += n
            if target == t2 != t1:
                FP += n
    return (TP, TN, FP, FN)

</t>
<t tx="karstenw.20230303140319.191">def test(self, target=None):
    """ Returns an (accuracy, precision, recall, F1-score)-tuple.
    """
    A = [] # Accuracy.
    P = [] # Precision.
    R = [] # Recall.
    for type, TP, TN, FP, FN in self.split():
        if type == target or target is None:
            # Calculate precision &amp; recall per class.
            A.append(float(TP + TN) / ((TP + TN + FP + FN)))
            P.append(float(TP) / ((TP + FP) or 1))
            R.append(float(TP) / ((TP + FN) or 1))
    # Macro-averaged:
    A = sum(A) / (len(A) or 1.0)
    P = sum(P) / (len(P) or 1.0)
    R = sum(R) / (len(R) or 1.0)
    F = 2.0 * P * R / ((P + R) or 1.0)
    return A, P, R, F

</t>
<t tx="karstenw.20230303140319.192">def auc(self, k=10):
    """ Returns the area under the ROC-curve.
    """
    roc = [(0.0, 0.0), (1.0, 1.0)]
    for type, TP, TN, FP, FN in self.split():
        x = FPR = float(FP) / ((FP + TN) or 1) # false positive rate
        y = TPR = float(TP) / ((TP + FN) or 1) #  true positive rate
        roc.append((x, y))
        #print("%s\t%s %s %s %s\t %s %s" % (TP, TN, FP, FN, FPR, TPR))
    roc = sorted(roc)
    # Trapzoidal rule: area = (a + b) * h / 2, where a=y0, b=y1 and h=x1-x0.
    return sum(0.5 * (x1 - x0) * (y1 + y0) for (x0, y0), (x1, y1) in sorted(zip(roc, roc[1:])))

</t>
<t tx="karstenw.20230303140319.193">@property
def table(self, padding=1):
    """ Returns the matrix as a string with rows and columns.
    """
    k = sorted(self)
    n = max([len(decode_utf8(x)) for x in k])
    n = max(n, *(len(str(self[k1][k2])) for k1 in k for k2 in k)) + padding
    s = "".ljust(n)
    for t1 in k:
        s += decode_utf8(t1).ljust(n)
    for t1 in k:
        s += "\n"
        s += decode_utf8(t1).ljust(n)
        for t2 in k:
            s += str(self[t1][t2]).ljust(n)
    return s

</t>
<t tx="karstenw.20230303140319.194">def __repr__(self):
    return repr(dict((k, dict(v)) for k, v in self.items()))


</t>
<t tx="karstenw.20230303140319.195">def K_fold_cross_validation(Classifier, documents=[], folds=10, **kwargs):
    """ Returns an (accuracy, precisiom, recall, F1-score, standard deviation)-tuple.
        For 10-fold cross-validation, performs 10 separate tests of the classifier,
        each with a different 9/10 training and 1/10 testing documents.
        The given list of documents contains Documents or (document, type)-tuples.
        The given classifier is a class (NB, KNN, SLP, SVM)
        which is initialized with the given optional parameters.
    """
    K = kwargs.pop("K", folds)
    s = kwargs.pop("shuffled", True)
    # Macro-average accuracy, precision, recall &amp; F1-score.
    m = [0.0, 0.0, 0.0, 0.0]
    f = []
    # Create shuffled folds to avoid a list sorted by type
    # (we take successive folds and the source data could be sorted).
    if isinstance(K, (int, float)):
        folds = list(_folds(shuffled(documents) if s else documents, K))
    # K tests with different train (d1) and test (d2) sets.
    for d1, d2 in folds:
        d1 = [isinstance(d, Document) and (d, d.type) or d for d in d1]
        d2 = [isinstance(d, Document) and (d, d.type) or d for d in d2]
        classifier = Classifier(train=d1, **kwargs)
        A, P, R, F = classifier.test(d2, **kwargs)
        m[0] += A
        m[1] += P
        m[2] += R
        m[3] += F
        f.append(F)
    # F-score mean &amp; variance.
    K = len(folds)
    u = float(sum(f)) / (K or 1.0)
    o = float(sum((x - u) ** 2 for x in f)) / (K - 1 or 1.0)
    o = sqrt(o)
    return tuple([v / (K or 1.0) for v in m] + [o])

kfoldcv = K_fold_cv = k_fold_cv = k_fold_cross_validation = K_fold_cross_validation


</t>
<t tx="karstenw.20230303140319.196">def folds(documents=[], K=10, **kwargs):
    """ Returns an iterator of K folds, where each fold is a (train, test)-tuple.
        For example, for 10-fold cross-validation, it yields 10 tuples,
        each with a different 9/10 training and 1/10 testing documents.
    """
    def chunks(iterable, n=10):
        # Returns an iterator of n lists of roughly equal size.
        # http://www.garyrobinson.net/2008/04/splitting-a-pyt.html
        a = list(iterable)
        i = 0
        j = 0
        for m in range(n):
            j = i + len(a[m::n])
            yield a[i:j]
            i = j
    k = kwargs.get("k", K)
    d = list(chunks(documents, max(k, 2)))
    for holdout in range(k):
        yield list(chain(*(d[:holdout] + d[holdout + 1:]))), d[holdout]

_folds = folds


</t>
<t tx="karstenw.20230303140319.197">def gridsearch(Classifier, documents=[], folds=10, **kwargs):
    """ Returns the test results for every combination of optional parameters,
        using K-fold cross-validation for the given classifier (NB, KNN, SLP, SVM).
        For example:
        for (A, P, R, F, o), p in gridsearch(SVM, data, c=[0.1, 1, 10]):
            print((A, P, R, F, o), p)
        &gt; (0.919, 0.921, 0.919, 0.920), {"c": 10}
        &gt; (0.874, 0.884, 0.865, 0.874), {"c": 1}
        &gt; (0.535, 0.424, 0.551, 0.454), {"c": 0.1}
    """
    def product(*args):
        # Yields the cartesian product of given iterables:
        # list(product([1, 2], [3, 4])) =&gt; [(1, 3), (1, 4), (2, 3), (2, 4)]
        p = [[]]
        for iterable in args:
            p = [x + [y] for x in p for y in iterable]
        for p in p:
            yield tuple(p)
    s = [] # [((A, P, R, F, o), parameters), ...]
    p = [] # [[("c", 0.1), ("c", 10), ...],
           #  [("gamma", 0.1), ("gamma", 0.2), ...], ...]
    for k, v in kwargs.items():
        p.append([(k, v) for v in v])
    for p in product(*p):
        p = dict(p)
        s.append((K_fold_cross_validation(Classifier, documents, folds, **p), p))
    return sorted(s, reverse=True)


</t>
<t tx="karstenw.20230303140319.198">def feature_selection(documents=[], top=None, method=CHISQUARED, threshold=0.0):
    """ Returns an iterator of (feature, weight, (probability, class))-tuples,
        sorted by the given feature selection method (IG, GR, X2) and document frequency threshold.
    """
    a = []
    for d in documents:
        if not isinstance(d, Document):
            d = Document(d[0], type=d[1], stopwords=True)
        a.append(d)
    m = Model(a, weight=None)
    p = m.posterior_probability
    c = m.classes
    for w, f in m.feature_selection(top, method, threshold, weighted=True):
        # For each feature, retrieve the class with the maximum probabilty.
        yield f, w, max([(p(f, type), type) for type in c])

fsel = feature_selection

#--- NAIVE BAYES CLASSIFIER ------------------------------------------------------------------------

MULTINOMIAL = "multinomial" # Feature weighting.
BINOMIAL = "binomial"    # Feature occurs in class (1) or not (0).
BERNOUILLI = "bernouilli"  # Feature occurs in class (1) or not (0).


</t>
<t tx="karstenw.20230303140319.199">class NB(Classifier):

    @others
Bayes = NaiveBayes = NB

</t>
<t tx="karstenw.20230303140319.2">def shi(i, base="0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"):
    """ Returns a short string hash for a given int.
    """
    s = []
    while i &gt; 0:
        i, r = divmod(i, len(base))
        s.append(base[r])
    return "".join(reversed(s))

#--- LIST FUNCTIONS --------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140319.20">def copy(self):
    return readonlydict(self)

</t>
<t tx="karstenw.20230303140319.200">def __init__(self, train=[], baseline=MAJORITY, method=MULTINOMIAL, alpha=0.0001, **kwargs):
    """ Naive Bayes is a simple supervised learning method for text classification.
        Documents are classified based on the probability that a feature occurs in a class 
        (independent of other features).
    """
    self._classes    = {}     # {class: frequency}
    self._features   = {}     # {feature: frequency}
    self._likelihood = {}     # {class: {feature: frequency}}
    self._cache      = {}     # Cache log likelihood sums.
    self._method     = method # MULTINOMIAL or BERNOUILLI.
    self._alpha      = alpha  # Smoothing.
    Classifier.__init__(self, train, baseline)

</t>
<t tx="karstenw.20230303140319.201">@property
def method(self):
    return self._method

</t>
<t tx="karstenw.20230303140319.202">@property
def features(self):
    return list(self._features.keys())

</t>
<t tx="karstenw.20230303140319.203">def train(self, document, type=None):
    """ Trains the classifier with the given document of the given type (i.e., class).
        A document can be a Document, Vector, dict, list or string.
        If no type is given, Document.type will be used instead.
    """
    # Calculate the probability of a class.
    # Calculate the probability of a feature.
    # Calculate the probability of a feature occuring in a class (= conditional probability).
    type, vector = self._vector(document, type=type)
    self._classes[type] = self._classes.get(type, 0) + 1
    self._likelihood.setdefault(type, {})
    self._cache.pop(type, None)
    for f, w in vector.items():
        if self._method in (BINARY, BINOMIAL, BERNOUILLI):
            w = 1
        self._features[f] = self._features.get(f, 0) + 1
        self._likelihood[type][f] = self._likelihood[type].get(f, 0) + w

</t>
<t tx="karstenw.20230303140319.204">def classify(self, document, discrete=True):
    """ Returns the type with the highest probability for the given document.
        If the classifier has been trained on LSA concept vectors
        you need to supply LSA.transform(document).
    """
    # Given red &amp; round, what is the likelihood that it is an apple?
    # p = p(red|apple) * p(round|apple) * p(apple) / (p(red) * p(round))
    # The multiplication can cause underflow so we use log() instead.
    # For unknown features, we smoothen with an alpha value.
    v = self._vector(document)[1]
    m = self._method
    a = self._alpha
    n = self._classes.values()
    n = float(sum(n))
    p = defaultdict(float)
    for type in self._classes:
        if m == MULTINOMIAL:
            if type not in self._cache: # 10x faster
                self._cache[type] = float(sum(self._likelihood[type].values()))
            d = self._cache[type]
        if m == BINOMIAL \
        or m == BERNOUILLI:
            d = float(self._classes[type])
        L = self._likelihood[type]
        g = sum(log((L[f] if f in L else a) / d) for f in v)
        g = exp(g) * self._classes[type] / n # prior
        p[type] = g
    # Normalize probability estimates.
    s = sum(p.values()) or 1
    for type in p:
        p[type] /= s
    if not discrete:
        return Probabilities(self, p)
    try:
        # Ties are broken in favor of the majority class
        # (random winner for majority ties).
        m = max(p.values())
        p = sorted((self._classes[type], type) for type, g in p.items() if g == m &gt; 0)
        p = [type for frequency, type in p if frequency == p[0][0]]
        return choice(p)
    except:
        return self.baseline

</t>
<t tx="karstenw.20230303140319.205">#--- K-NEAREST NEIGHBOR CLASSIFIER -----------------------------------------------------------------


class KNN(Classifier):

    @others
NearestNeighbor = kNN = KNN

#from pattern.vector import Document, KNN
#
#d1 = Document("cats have stripes, purr and drink milk", type="cat")
#d2 = Document("cows are black and white, they moo and give milk", type="cow")
#d3 = Document("birds have wings and can fly", type="bird")
#
#knn = KNN()
#for d in (d1, d2, d3):
#    knn.train(d)
#
#print(knn.binary)
#print(knn.classes)
#print(knn.classify(Document("something that can fly")))
#print(KNN.test((d1, d2, d3), folds=2))

</t>
<t tx="karstenw.20230303140319.206">def __init__(self, train=[], baseline=MAJORITY, k=10, distance=COSINE, **kwargs):
    """ k-nearest neighbor (kNN) is a simple supervised learning method for text classification.
        Documents are classified by a majority vote of nearest neighbors (cosine distance)
        in the training data.
    """
    self.k = k               # Number of nearest neighbors to observe.
    self.distance = distance # COSINE, EUCLIDEAN, ...
    Classifier.__init__(self, train, baseline)

</t>
<t tx="karstenw.20230303140319.207">def train(self, document, type=None):
    """ Trains the classifier with the given document of the given type (i.e., class).
        A document can be a Document, Vector, dict, list or string.
        If no type is given, Document.type will be used instead.
    """
    Classifier.train(self, document, type)

</t>
<t tx="karstenw.20230303140319.208">def classify(self, document, discrete=True):
    """ Returns the type with the highest probability for the given document.
        If the classifier has been trained on LSA concept vectors
        you need to supply LSA.transform(document).
    """
    # Distance is calculated between the document vector and all training instances.
    # This will make KNN slow in higher dimensions.
    classes = {}
    v1 = self._vector(document)[1]
    D = ((distance(v1, v2, method=self.distance), type) for type, v2 in self._vectors)
    D = ((d, type) for d, type in D if d &lt; 1) # Nothing in common if distance=1.0.
    D = heapq.nsmallest(self.k, D)            # k-least distant.
    # Normalize probability estimates.
    s = sum(1 - d for d, type in D) or 1
    p = defaultdict(float)
    for d, type in D:
        p[type] += (1 - d) / s
    if not discrete:
        return Probabilities(self, p)
    try:
        # Ties are broken in favor of the majority class
        # (random winner for majority ties).
        m = max(p.values())
        p = sorted((self._classes[type], type) for type, w in p.items() if w == m &gt; 0)
        p = [type for frequency, type in p if frequency == p[0][0]]
        return choice(p)
    except:
        return self.baseline

</t>
<t tx="karstenw.20230303140319.21">def __setitem__(self, k, v):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.22">def __delitem__(self, k):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.23">def pop(self, k, default=None):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.24">def popitem(self, kv):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.25">def clear(self):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.26">def update(self, kv):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.27">def setdefault(self, k, default=None):
    if k in self:
        return self[k]
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.28">class readonlylist(list):
    @others
#### DOCUMENT ######################################################################################

#--- STOP WORDS ------------------------------------------------------------------------------------
# A dictionary of (language, words)-items of function words, for example: {"en": {"the": True}}.
# - de: 950+, Marco Götze &amp; Steffen Geyer
# - en: 550+, Martin Porter (http://snowball.tartarus.org)
# - es: 300+, Martin Porter
# - fr: 550+, Martin Porter, Audrey Baneyx
# - nl: 100+, Martin Porter, Damien van Holten

stopwords = _stopwords = {}
for f in glob.glob(os.path.join(MODULE, "stopwords-*.txt")):
    language = os.path.basename(f)[-6:-4] # stopwords-[en].txt
    w = codecs.open(f, encoding="utf-8")
    w = (w.strip() for w in w.read().split(","))
    stopwords[language] = dict.fromkeys(w, True)

# The following English words could also be meaningful nouns:

#from pattern.vector import stopwords
#for w in ["mine", "us", "will", "can", "may", "might"]:
#    stopwords["en"].pop(w)

#--- WORD COUNT ------------------------------------------------------------------------------------
# Simple bag-of-word models are often made up of word frequencies or character trigram frequencies.

PUNCTUATION = ".,;:!?()[]{}`'\"@#$^&amp;*+-|=~_"


</t>
<t tx="karstenw.20230303140319.29">def __init__(self, *args, **kwargs):
    list.__init__(self, *args, **kwargs)

</t>
<t tx="karstenw.20230303140319.3">def shuffled(iterable, **kwargs):
    """ Returns a copy of the given list with the items in random order.
    """
    seed(kwargs.get("seed"))
    return sorted(list(iterable), key=lambda x: random())


</t>
<t tx="karstenw.20230303140319.30">def __setitem__(self, i, v):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.31">def __delitem__(self, i):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.32">def append(self, v):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.33">def extend(self, v):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.34">def insert(self, i, v):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.35">def remove(self, v):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.36">def pop(self, i):
    raise ReadOnlyError

</t>
<t tx="karstenw.20230303140319.37">def words(string, filter=lambda w: w.strip("'").isalnum(), punctuation=PUNCTUATION, **kwargs):
    """ Returns a list of words (alphanumeric character sequences) from the given string.
        Common punctuation marks are stripped from words.
    """
    string = decode_utf8(string)
    string = re.sub(r"([a-z|A-Z])'(m|s|ve|re|ll|d)", "\\1 &lt;QUOTE/&gt;\\2", string)
    string = re.sub(r"(c|d|gl|j|l|m|n|s|t|un)'([a-z|A-Z])", "\\1&lt;QUOTE/&gt; \\2", string)
    words = (w.strip(punctuation).replace("&lt;QUOTE/&gt;", "'", 1) for w in string.split())
    words = (w for w in words if filter is None or filter(w) is not False)
    words = [w for w in words if w]
    return words

PORTER, LEMMA = "porter", "lemma"


</t>
<t tx="karstenw.20230303140319.38">def stem(word, stemmer=PORTER, **kwargs):
    """ Returns the base form of the word when counting words in count().
        With stemmer=PORTER, the Porter2 stemming algorithm is used.
        With stemmer=LEMMA, either uses Word.lemma or inflect.singularize().
        (with optional parameter language="en", pattern.en.inflect is used).
    """
    if hasattr(word, "string") and stemmer in (PORTER, None):
        word = word.string
    if isinstance(word, str):
        word = decode_utf8(word.lower())
    if stemmer is None:
        return word.lower()
    if stemmer == PORTER:
        return _stemmer.stem(word, **kwargs)
    if stemmer == LEMMA:
        if hasattr(word, "lemma"): # pattern.en.Word
            w = word.string.lower()
            if word.lemma is not None:
                return word.lemma
            if word.pos == "NNS":
                return singularize(w)
            if word.pos.startswith(("VB", "MD")):
                return conjugate(w, "infinitive") or w
            if word.pos.startswith(("JJ",)):
                return predicative(w)
            if word.pos.startswith(("DT", "PR", "WP")):
                return singularize(w, pos=word.pos)
            return w
        return singularize(word, pos=kwargs.get("pos", "NN"))
    if hasattr(stemmer, "__call__"):
        return decode_utf8(stemmer(word))
    return word.lower()


</t>
<t tx="karstenw.20230303140319.39">def count(words=[], top=None, threshold=0, stemmer=None, exclude=[], stopwords=False, language=None, **kwargs):
    """ Returns a dictionary of (word, count)-items, in lowercase.
        Words in the exclude list and stop words (by default, English) are not counted.
        Words whose count falls below (or equals) the given threshold are excluded.
        Words that are not in the given top most counted are excluded.
    """
    # An optional dict-parameter can be used to specify a subclass of dict,
    # e.g., count(words, dict=readonlydict) as used in Document.
    count = kwargs.get("dict", dict)()
    for w in words:
        w1 = w
        w2 = w
        if hasattr(w, "string"): # pattern.en.Word
            w1 = w.string.lower()
        if isinstance(w, str):
            w1 = w.lower()
            w2 = w.lower()
        if (stopwords or w1 not in _stopwords.get(language or "en", ())) and w1 not in exclude:
            if stemmer is not None:
                w2 = stem(w2, stemmer, **kwargs).lower()
            dict.__setitem__(count, w2, (w2 in count) and count[w2] + 1 or 1)
    for k in list(count.keys()):
        if count[k] &lt;= threshold:
            dict.__delitem__(count, k)
    if top is not None:
        count = count.__class__(heapq.nsmallest(top, list(count.items()), key=lambda kv: (-kv[1], kv[0])))
    return count


</t>
<t tx="karstenw.20230303140319.4">def chunk(iterable, n):
    """ Returns an iterator of n successive equal-sized chunks from the given list.
    """
    # list(chunk([1, 2, 3, 4], n=2)) =&gt; [[1, 2], [3, 4]]
    a = list(iterable)
    n = int(n)
    i = 0
    j = 0
    for m in range(n):
        j = i + len(a[m::n])
        yield a[i:j]
        i = j


</t>
<t tx="karstenw.20230303140319.40">def character_ngrams(string="", n=3, top=None, threshold=0, exclude=[], **kwargs):
    """ Returns a dictionary of (character n-gram, count)-items.
        N-grams in the exclude list are not counted.
        N-grams whose count falls below (or equals) the given threshold are excluded.
        N-grams that are not in the given top most counted are excluded.
    """
    # An optional dict-parameter can be used to specify a subclass of dict,
    # e.g., count(words, dict=readonlydict) as used in Document.
    count = defaultdict(int)
    if n &gt; 0:
        for i in range(len(string) - n + 1):
            w = string[i:i + n]
            if w not in exclude:
                count[w] += 1
    if threshold &gt; 0:
        count = dict((k, v) for k, v in count.items() if v &gt; threshold)
    if top is not None:
        count = dict(heapq.nsmallest(top, list(count.items()), key=lambda kv: (-kv[1], kv[0])))
    return kwargs.get("dict", dict)(count)

chngrams = character_ngrams

#--- DOCUMENT --------------------------------------------------------------------------------------
# A Document is a bag of words in which each word is a feature.
# A Document is represented as a vector of weighted (TF-IDF) features.
# A Document can be part of a training model used for learning (i.e., clustering or classification).

_UID = 0
_SESSION = shi(int(time() * 1000)) # Avoid collision with pickled documents.


</t>
<t tx="karstenw.20230303140319.41">def _uid():
    """ Returns a string id, for example: "NPIJYaS-1", "NPIJYaS-2", ...
        The string part is based on the current time, the number suffix is auto-incremental.
    """
    global _UID
    _UID += 1
    return _SESSION + "-" + str(_UID)

# Term relevance weight:
TF, TFIDF, TF_IDF, BINARY = \
    "tf", "tf-idf", "tf-idf", "binary"


</t>
<t tx="karstenw.20230303140319.42">class Document(object):
    # Document(string = "",
    #          filter = lambda w: w.lstrip("'").isalnum(),
    #     punctuation = PUNCTUATION,
    #             top = None,
    #       threshold = 0,
    #         stemmer = None,
    #         exclude = [],
    #       stopwords = False,
    #            name = None,
    #            type = None,
    #        language = None,
    #     description = None
    # )
    @others
Bag = BagOfWords = BOW = Document

#--- VECTOR ----------------------------------------------------------------------------------------
# A Vector represents document terms (called features) and their tf or tf * idf relevance weight.
# A Vector is a sparse represenation: i.e., a dictionary with only those features &gt; 0.
# This is fast, usually also faster than LSA which creates a full vector space with non-zero values.
# Document vectors can be used to calculate similarity between documents,
# for example in a clustering or classification algorithm.

# To find the average feature length in a model:
# sum(len(d.vector) for d in model.documents) / float(len(model))


</t>
<t tx="karstenw.20230303140319.43">def __init__(self, string="", **kwargs):
    """ An unordered bag-of-words representation of the given string, list, dict or Sentence.
        Lists can contain tuples (of), strings or numbers.
        Dicts can contain tuples (of), strings or numbers as keys, and floats as values.
        Document.words stores a dict of (word, count)-items.
        Document.vector stores a dict of (word, weight)-items, 
        where weight is the term frequency normalized (0.0-1.0) to remove document length bias.
        Punctuation marks are stripped from the words.
        Stop words in the exclude list are excluded from the document.
        Only top words whose count exceeds the threshold are included in the document.        
    """
    kwargs.setdefault("filter", lambda w: w.lstrip("'").isalnum())
    kwargs.setdefault("threshold", 0)
    kwargs.setdefault("dict", readonlydict)
    # A string of words: map to read-only dict of (word, count)-items.
    if string is None:
        w = kwargs["dict"]()
        v = None
    elif isinstance(string, str):
        w = words(string, **kwargs)
        w = count(w, **kwargs)
        v = None
    # A list of words: map to read-only dict of (word, count)-items.
    elif isinstance(string, (list, tuple)) and not string.__class__.__name__ == "Text":
        w = string
        w = count(w, **kwargs)
        v = None
    # A set of unique words: map to ready-only dict of (word, 1)-items.
    elif isinstance(string, set):
        w = string
        w = kwargs["dict"].fromkeys(w, 1)
        v = None
    # A Vector of (word, weight)-items: copy as document vector.
    elif isinstance(string, Vector):
        w = string
        w = kwargs["dict"](w)
        v = Vector(w)
    # A dict of (word, count)-items: make read-only.
    elif isinstance(string, dict):
        w = string
        w = kwargs["dict"](w)
        v = None
    # pattern.en.Sentence with Word objects: can use stemmer=LEMMA.
    elif string.__class__.__name__ == "Sentence":
        w = string.words
        w = [w for w in w if kwargs["filter"](w.string)]
        w = count(w, **kwargs)
        v = None
    # pattern.en.Text with Sentence objects, can use stemmer=LEMMA.
    elif string.__class__.__name__ == "Text":
        w = []
        [w.extend(sentence.words) for sentence in string]
        w = [w for w in w if kwargs["filter"](w.string)]
        w = count(w, **kwargs)
        v = None
    # Another Document: copy words, wordcount, name and type.
    elif isinstance(string, Document):
        for k in ("name", "type", "label", "language", "description"):
            if hasattr(string, k):
                kwargs.setdefault(k, getattr(string, k))
        w = string.terms
        w = kwargs["dict"](w)
        v = None
    else:
        raise TypeError("document string is not str, unicode, list, dict, Vector, Sentence or Text.")
    self._id          = _uid()             # Document ID, used when comparing objects.
    self._name        = kwargs.get("name") # Name that describes the document content.
    self._type        = kwargs.get("type", # Type that describes the category or class of the document.
                        kwargs.get("label"))
    self._language    = kwargs.get("language")
    self._description = kwargs.get("description", "")
    self._terms       = w                  # Dictionary of (word, count)-items.
    self._vector      = v                  # Cached tf-idf vector.
    self._count       = None               # Total number of words (minus stop words).
    self._model       = None               # Parent Model.

</t>
<t tx="karstenw.20230303140319.44">@classmethod
def load(cls, path):
    """ Returns a new Document from the given text file path.
        The given text file must be generated with Document.save().
    """
    # Open unicode file.
    s = open(path, "rb").read()
    s = s.lstrip(codecs.BOM_UTF8)
    s = decode_utf8(s)
    a = {}
    v = {}
    # Parse document name and type.
    # Parse document terms and frequency.
    for s in s.splitlines():
        if s.startswith("#"): # comment
            a["description"] = a.get("description", "") + s.lstrip("#").strip() + "\n"
        elif s.startswith("@name:"):
            a["name"] = s[len("@name:") + 1:].replace("\\n", "\n")
        elif s.startswith("@type:"):
            a["type"] = s[len("@type:") + 1:].replace("\\n", "\n")
        elif s.startswith("@language:"):
            a["lang"] = s[len("@lang:") + 1:].replace("\\n", "\n")
        else:
            s = s.split(" ")
            w, f = " ".join(s[:-1]), s[-1]
            if f.isdigit():
                v[w] = int(f)
            else:
                v[w] = float(f)
    return cls(v, name = a.get("name"),
                  type = a.get("type"),
              language = a.get("lang"),
           description = a.get("description").rstrip("\n"))

</t>
<t tx="karstenw.20230303140319.45">def save(self, path):
    """ Saves the document as a text file at the given path.
        The file content has the following format:
        # Cat document.
        @name: cat
        @type: animal
        a 3
        cat 2
        catch 1
        claw 1
        ...
    """
    s = []
    # Parse document description.
    for x in self.description.split("\n"):
        s.append("# %s" % x)
    # Parse document name, type and language.
    for k, v in (("@name:", self.name), ("@type:", self.type), ("@lang:", self.language)):
        if v is not None:
            s.append("%s %s" % (k, v.replace("\n", "\\n")))
    # Parse document terms and frequency.
    for w, f in sorted(self.terms.items()):
        if isinstance(f, int):
            s.append("%s %i" % (w, f))
        if isinstance(f, float):
            s.append("%s %.3f" % (w, f))
    s = "\n".join(s)
    s = encode_utf8(s)
    # Save unicode file.
    f = open(path, "wb")
    f.write(codecs.BOM_UTF8)
    f.write(s)
    f.close()

</t>
<t tx="karstenw.20230303140319.46">def _get_model(self):
    return self._model

</t>
<t tx="karstenw.20230303140319.47">def _set_model(self, model):
    self._vector = None
    self._model and self._model._update()
    self._model = model
    self._model and self._model._update()

model = corpus = property(_get_model, _set_model)

</t>
<t tx="karstenw.20230303140319.48">@property
def id(self):
    return self._id

</t>
<t tx="karstenw.20230303140319.49">@property
def name(self):
    return self._name

</t>
<t tx="karstenw.20230303140319.5">def mix(iterables=[], n=10):
    """ Returns an iterator that alternates the given lists, in n chunks.
    """
    # list(mix([[1, 2, 3, 4], ["a", "b"]], n=2)) =&gt; [1, 2, "a", 3, 4, "b"]
    a = [list(chunk(x, n)) for x in iterables]
    for i in range(int(n)):
        for x in a:
            for item in x[i]:
                yield item


</t>
<t tx="karstenw.20230303140319.50">@property
def type(self):
    return self._type

</t>
<t tx="karstenw.20230303140319.51">@property
def label(self):
    return self._type

</t>
<t tx="karstenw.20230303140319.52">@property
def language(self):
    return self._language

</t>
<t tx="karstenw.20230303140319.53">@property
def description(self):
    return self._description

</t>
<t tx="karstenw.20230303140319.54">@property
def terms(self):
    return self._terms

</t>
<t tx="karstenw.20230303140319.55">@property
def words(self):
    return self._terms

</t>
<t tx="karstenw.20230303140319.56">@property
def features(self):
    return list(self._terms.keys())

</t>
<t tx="karstenw.20230303140319.57">@property
def count(self):
    # Yields the number of words in the document representation.
    # Cache the word count so we can reuse it when calculating tf.
    if not self._count:
        self._count = sum(self.terms.values())
    return self._count

</t>
<t tx="karstenw.20230303140319.58">@property
def wordcount(self):
    return self._count

</t>
<t tx="karstenw.20230303140319.59">def __len__(self):
    return len(self.terms)

</t>
<t tx="karstenw.20230303140319.6">def bin(iterable, key=lambda x: x, value=lambda x: x):
    """ Returns a dictionary with items in the given list grouped by the given key.
    """
    # bin([["a", 1], ["a", 2], ["b", 3]], key=lambda x: x[0]) =&gt;
    # {"a": [["a", 1], ["a", 2]],
    #  "b": [["b", 3]]
    # }
    m = defaultdict(list)
    for x in iterable:
        m[key(x)].append(value(x))
    return m


</t>
<t tx="karstenw.20230303140319.60">def __iter__(self):
    return iter(self.terms)

</t>
<t tx="karstenw.20230303140319.61">def __contains__(self, word):
    return word in self.terms

</t>
<t tx="karstenw.20230303140319.62">def __getitem__(self, word):
    return self.terms.__getitem__(word)

</t>
<t tx="karstenw.20230303140319.63">def get(self, word, default=None):
    return self.terms.get(word, default)

</t>
<t tx="karstenw.20230303140319.64">def term_frequency(self, word):
    """ Returns the term frequency of a given word in the document (0.0-1.0).
        tf = number of occurences of the word / number of words in document.
        The more occurences of the word, the higher its relative tf weight.
    """
    return float(self.terms.get(word, 0)) / (self.count or 1)

tf = term_frequency

</t>
<t tx="karstenw.20230303140319.65">def term_frequency_inverse_document_frequency(self, word, weight=TFIDF):
    """ Returns the word relevance as tf * idf (0.0-1.0).
        The relevance is a measure of how frequent the word occurs in the document,
        compared to its frequency in other documents in the model.
        If the document is not incorporated in a model, simply returns tf weight.
    """
    if self.model is not None and weight == TFIDF:
        # Use tf if no model, or idf==None (happens when the word is not in the model).
        idf = self.model.idf(word)
        idf = idf is None and 1 or idf
        return self.tf(word) * idf
    return self.tf(word)

tf_idf = tfidf = term_frequency_inverse_document_frequency

</t>
<t tx="karstenw.20230303140319.66">def information_gain(self, word):
    """ Returns the information gain for the given word (0.0-1.0).
    """
    if self.model is not None:
        return self.model.ig(word)
    return 0.0

ig = infogain = information_gain

</t>
<t tx="karstenw.20230303140319.67">def gain_ratio(self, word):
    """ Returns the information gain ratio for the given word (0.0-1.0).
    """
    if self.model is not None:
        return self.model.gr(word)
    return 0.0

gr = gainratio = gain_ratio

</t>
<t tx="karstenw.20230303140319.68">@property
def vector(self):
    """ Yields the document vector, a dictionary of (word, relevance)-items from the document.
        The relevance is tf, tf * idf, infogain or binary if the document is part of a Model, 
        based on the value of Model.weight (TF, TFIDF, IG, GR, BINARY, None).
        The document vector is used to calculate similarity between two documents,
        for example in a clustering or classification algorithm.
    """
    if not self._vector:
        # See the Vector class below = a dict with extra functionality (copy, norm).
        # When a document is added/deleted from a model, the cached vector is deleted.
        w = getattr(self.model, "weight", TF)
        if w not in (TF, TFIDF, IG, INFOGAIN, GR, GAINRATIO, BINARY):
            f = lambda w: float(self._terms[w]); w=None
        if w == BINARY:
            f = lambda w: int(self._terms[w] &gt; 0)
        if w == TF:
            f = self.tf
        if w == TFIDF:
            f = self.tf_idf
        if w in (IG, INFOGAIN):
            f = self.model.ig
        if w in (GR, GAINRATIO):
            f = self.model.gr
        self._vector = Vector(((w, f(w)) for w in self.terms), weight=w)
    return self._vector

</t>
<t tx="karstenw.20230303140319.69">@property
def concepts(self):
    """ Yields the document concept vector if the document is part of an LSA model.
    """
    return self.model and self.model.lsa and self.model.lsa.concepts.get(self.id) or None

</t>
<t tx="karstenw.20230303140319.7">def pimap(iterable, function, *args, **kwargs):
    """ Returns an iterator of function(x, *args, **kwargs) for the iterable (x1, x2, x3, ...).
        The function is applied in parallel over available CPU cores.
    """
    from multiprocessing import Pool
    global worker

    def worker(x):
        return function(x, *args, **kwargs)
    return Pool(processes=None).imap(worker, iterable)

#--- READ-ONLY DICTIONARY --------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303140319.70">def keywords(self, top=10, normalized=True):
    """ Returns a sorted list of (relevance, word)-tuples that are top keywords in the document.
        With normalized=True, weights are normalized between 0.0 and 1.0 (their sum will be 1.0).
    """
    n = normalized and sum(self.vector.values()) or 1.0
    v = ((f / n, w) for w, f in self.vector.items())
    v = heapq.nsmallest(top, v, key=lambda v: (-v[0], v[1]))
    return v

</t>
<t tx="karstenw.20230303140319.71">def cosine_similarity(self, document):
    """ Returns the similarity between the two documents as a number between 0.0-1.0.
        If both documents are part of the same model the calculations are cached for reuse.
    """
    if self.model is not None:
        return self.model.cosine_similarity(self, document)
    if document.model is not None:
        return document.model.cosine_similarity(self, document)
    return cosine_similarity(self.vector, document.vector)

similarity = cosine_similarity

</t>
<t tx="karstenw.20230303140319.72">def copy(self):
    d = Document(None, name=self.name, type=self.type, description=self.description)
    dict.update(d.terms, self.terms)
    return d

</t>
<t tx="karstenw.20230303140319.73">def __eq__(self, document):
    return isinstance(document, Document) and self.id == document.id

</t>
<t tx="karstenw.20230303140319.74">def __ne__(self, document):
    return not self.__eq__(document)

</t>
<t tx="karstenw.20230303140319.75">def __repr__(self):
    return "Document(id=%s%s%s)" % (
        repr(self._id),
             self.name and ", name=%s" % repr(self.name) or "",
             self.type and ", type=%s" % repr(self.type) or "")

# This is required because we overwrite the parent's __eq__() method.
# Otherwise objects will be unhashable in Python 3.
# More information: http://docs.python.org/3.6/reference/datamodel.html#object.__hash__
__hash__ = object.__hash__

</t>
<t tx="karstenw.20230303140319.76">class Vector(readonlydict):

    id = 0

    @others
#--- VECTOR DISTANCE -------------------------------------------------------------------------------
# The "distance" between two vectors can be calculated using different metrics.
# For vectors that represent text, cosine similarity is a good metric.
# For more information, see Domain Similarity Measures (Vincent Van Asch, 2012).

# The following functions can be used if you work with Vectors or plain dictionaries,
# instead of Documents and Models (which use caching for cosine similarity).


</t>
<t tx="karstenw.20230303140319.77">def __init__(self, *args, **kwargs):
    """ A dictionary of (feature, weight)-items of the features (terms, words) in a Document.
        A vector can be used to compare the document to another document with a distance metric.
        For example, vectors with 2 features (x, y) can be compared using 2D Euclidean distance.
        Vectors that represent text documents can be compared using cosine similarity.
    """
    s = kwargs.pop("sparse", True)
    f = ()
    w = None
    if len(args) &gt; 0:
        # From a Vector (copy weighting scheme).
        if isinstance(args[0], Vector):
            w = args[0].weight
        # From a dict.
        if isinstance(args[0], dict):
            f = list(args[0].items())
        # From an iterator.
        elif hasattr(args[0], "__iter__"):
            f = iter(args[0])
    Vector.id  += 1
    self.id     = Vector.id               # Unique ID.
    self.weight = kwargs.pop("weight", w) # TF, TFIDF, IG, BINARY or None.
    self._norm  = None                    # Cached L2-norm.
    # Exclude zero weights (sparse=True).
    f = chain(f, list(kwargs.items()))
    f = ((k, v) for k, v in f if not s or v != 0)
    readonlydict.__init__(self, f)

</t>
<t tx="karstenw.20230303140319.78">@classmethod
def fromkeys(cls, k, default=None, **kwargs):
    return Vector(((k, default) for k in k), **kwargs)

</t>
<t tx="karstenw.20230303140319.79">@property
def features(self):
    return list(self.keys())

</t>
<t tx="karstenw.20230303140319.8">class ReadOnlyError(Exception):
    pass


</t>
<t tx="karstenw.20230303140319.80">@property
def l2_norm(self):
    """ Yields the Frobenius matrix norm (cached).
        n = the square root of the sum of the absolute squares of the values.
        The matrix norm is used to normalize (0.0-1.0) cosine similarity between documents.
    """
    if self._norm is None:
        self._norm = sum(w * w for w in self.values()) ** 0.5
    return self._norm

norm = l2 = L2 = L2norm = l2norm = L2_norm = l2_norm

</t>
<t tx="karstenw.20230303140319.81">def copy(self):
    return Vector(self, weight=self.weight, sparse=False)

</t>
<t tx="karstenw.20230303140319.82">def __call__(self, vector={}):
    """ Vector(vector) returns a new vector updated with values from the given vector.
        No new features are added. For example: Vector({1:1, 2:2})({1:0, 3:3}) =&gt; {1:0, 2:2}.
    """
    if isinstance(vector, (Document, Model)):
        vector = vector.vector
    v = self.copy()
    s = dict.__setitem__
    for f, w in vector.items():
        if f in v:
            s(v, f, w)
    return v

</t>
<t tx="karstenw.20230303140319.83">def features(vectors=[]):
    """ Returns the set of unique features for all given vectors.
    """
    return set(chain(*vectors))

_features = features


</t>
<t tx="karstenw.20230303140319.84">def sparse(v):
    """ Returns the vector with features that have weight 0 removed.
    """
    for f, w in list(v.items()):
        if w == 0:
            del v[f]
    return v


</t>
<t tx="karstenw.20230303140319.85">def relative(v):
    """ Returns the vector with feature weights normalized so that their sum is 1.0 (in-place).
    """
    n = float(sum(v.values())) or 1.0
    s = dict.__setitem__
    for f in v: # Modified in-place.
        s(v, f, v[f] / n)
    return v

normalize = rel = relative


</t>
<t tx="karstenw.20230303140319.86">def l2_norm(v):
    """ Returns the L2-norm of the given vector.
    """
    if isinstance(v, Vector):
        return v.l2_norm
    return sum(w * w for w in v.values()) ** 0.5

norm = l2 = L2 = L2norm = l2norm = L2_norm = l2_norm


</t>
<t tx="karstenw.20230303140319.87">def cosine_similarity(v1, v2):
    """ Returns the cosine similarity of the given vectors.
    """
    s = sum(v1.get(f, 0) * w for f, w in v2.items())
    s = float(s) / (l2_norm(v1) * l2_norm(v2) or 1)
    return s

cos = cosine_similarity


</t>
<t tx="karstenw.20230303140319.88">def tf_idf(vectors=[], base=2.71828): # Euler's number
    """ Calculates tf * idf on the vector feature weights (in-place).
    """
    df = {}
    for v in vectors:
        for f in v:
            if v[f] != 0:
                df[f] = df[f] + 1 if f in df else 1.0
    n = len(vectors)
    s = dict.__setitem__
    for v in vectors:
        for f in v: # Modified in-place.
            s(v, f, v[f] * (log(n / df[f], base)))
    return vectors

tfidf = tf_idf

COSINE, EUCLIDEAN, MANHATTAN, CHEBYSHEV, HAMMING = \
    "cosine", "euclidean", "manhattan", "chebyshev", "hamming"


</t>
<t tx="karstenw.20230303140319.89">def distance(v1, v2, method=COSINE):
    """ Returns the distance between two vectors.
    """
    if method == COSINE:
        return 1 - cosine_similarity(v1, v2)
    if method == EUCLIDEAN: # Squared Euclidean distance is used (1.5x faster).
        return sum((v1.get(w, 0) - v2.get(w, 0)) ** 2 for w in set(chain(v1, v2)))
    if method == MANHATTAN:
        return sum(abs(v1.get(w, 0) - v2.get(w, 0)) for w in set(chain(v1, v2)))
    if method == CHEBYSHEV:
        return max(abs(v1.get(w, 0) - v2.get(w, 0)) for w in set(chain(v1, v2)))
    if method == HAMMING:
        d = sum(not (w in v1 and w in v2 and v1[w] == v2[w]) for w in set(chain(v1, v2)))
        d = d / float(max(len(v1), len(v2)) or 1)
        return d
    if isinstance(method, type(distance)):
        # Given method is a function of the form: distance(v1, v2) =&gt; float.
        return method(v1, v2)

_distance = distance


</t>
<t tx="karstenw.20230303140319.9">class readonlyodict(OrderedDict):
    @others
# Read-only dictionary, used for Document.terms and Document.vector
# (updating these directly invalidates the Document and Model cache).


</t>
<t tx="karstenw.20230303140319.90">def entropy(p=[], base=None):
    """ Returns the Shannon entropy for the given list of probabilities
        as a value between 0.0-1.0, where higher values indicate uncertainty.
    """
    # entropy([1.0]) =&gt; 0.0, one possible outcome with a 100% chance
    # entropy([0.5, 0.5]) =&gt; 1.0, two outcomes with a 50% chance each (random).
    p = list(p)
    s = float(sum(p)) or 1.0
    s = s if len(p) &gt; 1 else max(s, 1.0)
    b = base or max(len(p), 2)
    return -sum(x / s * log(x / s, b) for x in p if x != 0) or 0.0

#### MODEL #########################################################################################

#--- MODEL -----------------------------------------------------------------------------------------
# A Model is a representation of a collection of documents as bag-of-words.
# A Model is a matrix (or vector space) with features as columns and documents as rows,
# where each document is a vector of features (e.g., words) and feature weights (e.g., frequency).
# The matrix is used to calculate adjusted weights (e.g., tf * idf), document similarity and LSA.

# Export formats:
ORANGE, WEKA = "orange", "weka"

# LSA reduction methods:
NORM, L1, L2, TOP300 = "norm", "L1", "L2", "top300"

# Feature selection methods:
INFOGAIN, GAINRATIO, CHISQUARE, CHISQUARED = "infogain", "gainratio", "chisquare", "chisquared"
IG, GR, X2, DF = "ig", "gr", "x2", "df"

# Clustering methods:
KMEANS, HIERARCHICAL = "k-means", "hierarchical"

# Resampling methods:
MINORITY, MAJORITY = "minority", "majority"


</t>
<t tx="karstenw.20230303140319.91">class Model(object):

    @others
#   def cluster(self, method=KMEANS, k=10, iterations=10)
#   def cluster(self, method=HIERARCHICAL, k=1, iterations=1000)
    def cluster(self, method=KMEANS, **kwargs):
        """ Clustering is an unsupervised machine learning method for grouping similar documents.
            - k-means clustering returns a list of k clusters (each is a list of documents).
            - hierarchical clustering returns a list of documents and Cluster objects,
              where a Cluster is a list of documents and other clusters (see Cluster.flatten()).
        """
        # The optional documents parameter can be a selective list
        # of documents in the model to cluster.
        documents = kwargs.get("documents", self.documents)
        if not getattr(self, "lsa", None):
            # Using document vectors:
            vectors, features = [d.vector for d in documents], list(self.vector.keys())
        else:
            # Using LSA concept space:
            vectors, features = [self.lsa[d.id] for d in documents], list(range(len(self.lsa)))
        # Create a dictionary of vector.id =&gt; Document.
        # We need it to map the clustered vectors back to the actual documents.
        map = dict((v.id, documents[i]) for i, v in enumerate(vectors))
        if method in (KMEANS, "kmeans"):
            clusters = k_means(vectors,
                             k = kwargs.pop("k", 10),
                    iterations = kwargs.pop("iterations", 10),
                      features = features, **kwargs)
        if method == HIERARCHICAL:
            clusters = hierarchical(vectors,
                             k = kwargs.pop("k", 1),
                    iterations = kwargs.pop("iterations", 1000),
                      features = features, **kwargs)
        if method in (KMEANS, "kmeans"):
            clusters = [[map[v.id] for v in cluster] for cluster in clusters]
        if method == HIERARCHICAL:
            clusters.traverse(visit=lambda cluster: \
                [cluster.__setitem__(i, map[v.id])
                    for i, v in enumerate(cluster) if not isinstance(v, Cluster)])
        return clusters

    def latent_semantic_analysis(self, dimensions=NORM):
        """ Creates LSA concept vectors by reducing the vector space's dimensionality.
            Each concept vector has the given number of features (concepts).
            The concept vectors are consequently used in Model.cosine_similarity(), Model.cluster()
            and classification. This can be faster for high-dimensional vectors (i.e., many features).
            The reduction can be undone by setting Model.lsa=False.
        """
        self._lsa = LSA(self, k=dimensions)
        self._cos = {}
        return self._lsa

    reduce = latent_semantic_analysis

    def resample(self, n=MINORITY):
        """ Modifies the model so that it contains n documents of each type.
            With MINORITY, n = the number of documents in the minority class.
            With MAJORITY, n = the number of documents in the majority class
            (generates artificial documents for minority classes).
        """
        # Based on:
        # Liu, Ghosh &amp; Martin (2007). Generative oversampling for mining imbalanced datasets.
        # http://wwwmath.uni-muenster.de/u/lammers/EDU/ws07/Softcomputing/Literatur/4-DMI5467.pdf
        #
        # p1: {class: count}
        # p2: {class: max(len(vector))}
        # p3: {class: {feature: [weights]}}
        p1 = defaultdict(int)
        p2 = defaultdict(int)
        p3 = defaultdict(lambda: defaultdict(list))
        for d in self.documents:
            p1[d.type] += 1
        for d in self.documents:
            p2[d.type] = max(p2[d.type], len(d.terms))
        for d in self.documents:
            for f, w in d.terms.items():
                p3[d.type][f].append(w)
        # List features with weight 0.0.
        for t in p3:
            for f in p3[t]:
                p3[t][f].extend([0] * (p1[t] - len(p3[t][f])))
        if n == MINORITY:
            n = min(p1.values() or [0])
        if n == MAJORITY:
            n = max(p1.values() or [0])
        a = []
        x = defaultdict(int)
        # 1) Undersampling.
        for d in self.documents:
            if x[d.type] &lt; n:
                x[d.type] += 1
                a.append(d)
        # 2) Oversampling.
        # Generate new documents in minority classes,
        # taking into account the number of features
        # and feature weights in existing documents.
        for t, i in p1.items():
            for j in range(i, n):
                v = {}
                for f in shuffled(p3[t].keys()[:p2[t]]):
                    w = choice(p3[t][f])
                    w = w if v else p3[t][f][0]
                    if w:
                        v[f] = w
                a.append(Document(v))
        self.clear()
        self.extend(a)
        return self

    def condensed_nearest_neighbor(self, k=1, distance=COSINE):
        """ Returns a filtered list of documents, without impairing classification accuracy.
            Iteratively constructs a set of "prototype" documents.
            Documents that are correctly classified by the set are discarded.
            Documents that are incorrectly classified by the set are added to the set.
        """
        d = DistanceMap(method=distance)
        u = []
        v = list(self.documents)
        b = False
        while not b:
            b = True
            for i, x in enumerate(v):
                nn = heapq.nsmallest(k, ((d(x.vector, y.vector), y) for y in u))
                if not u or x.type in (y.type for d, y in nn):
                    b = False
                    u.append(x)
                    v.pop(i)
                    break
        return v

    cnn = condensed_nearest_neighbor

    def posterior_probability(self, word, type):
        """ Returns the probability that a document with the given word is of the given type.
        """
        if not self._pp:
            # p1: {class: count}
            # p2: {feature: {class: count}}
            # p3: {feature: count}
            # p4: {(feature, class): probability}
            p1 = defaultdict(float)
            p2 = defaultdict(lambda: defaultdict(float))
            p3 = defaultdict(float)
            p4 = defaultdict(float)
            for d in self.documents:
                p1[d.type] += 1
            for d in self.documents:
                for f in d.terms:
                    p2[f][d.type] += 1 / p1[d.type]
                    p3[f] += 1
            for t in p1:
                for f in p3:
                    p4[(f, t)] = p1[t] * p2[f][t] / p3[f]
            self._pp = p4
        return self._pp[(word, type)]

    pp = probability = posterior_probability

    def chi_squared(self, word):
        """ Returns the chi-squared p-value (0.0-1.0) for the given feature.
            When p &lt; 0.05, the feature is biased to a class (document type),
            i.e., it is a significant predictor for that class.
        """
        if not self._x2:
            from pattern.metrics import chi2
            # p1: {class: count}
            # p2: {class: {feature: count}}
            # p3: {feature: count}
            # p4: {feature: p-value}
            p1 = defaultdict(float)
            p2 = defaultdict(lambda: defaultdict(float))
            p3 = defaultdict(float)
            p4 = defaultdict(float)
            for d in self.documents:
                p1[d.type] += 1
            for d in self.documents:
                for f in d.terms:
                    p2[d.type][f] += 1
                    p3[f] += 1
            for f in p3:
                p4[f] = chi2(observed=[[p2[t][f] for t in p2], [p1[t] - p2[t][f] for t in p2]])[1]
            self._x2 = p4
        return self._x2[word]

    X2 = x2 = chi2 = chi_square = chi_squared

    def information_gain(self, word):
        """ Returns the information gain (IG, 0.0-1.0) for the given feature,
            by measuring how much the feature contributes to each document type (class).
            High information gain means low entropy. Low entropy means predictability,
            i.e., a feature that is biased towards some class(es),
            i.e., a feature that occurs more in one document type and less in others.
        """
        if not self._ig:
            # Based on Vincent Van Asch, http://www.clips.ua.ac.be/~vincent/scripts/textgain.py
            # IG(f) = H(C) - sum(p(v) * H(C|v) for v in V)
            # where C is the set of class labels,
            # where V is the set of values for feature f,
            # where p(v) is the probability that feature f has value v,
            # where C|v is the distribution of value v for feature f per class.
            # H is the entropy for a list of probabilities.
            # Lower entropy indicates predictability, i.e., some values are more probable.
            # H([0.50, 0.50]) = 1.00
            # H([0.75, 0.25]) = 0.81
            H = entropy
            # C =&gt; {class: count}
            C = dict.fromkeys(self.classes, 0)
            for d in self.documents:
                C[d.type] += 1
            HC = H(list(C.values()))
            # V =&gt; {feature: {value: {class: count}}}
            F = set(self.features)
            V = dict((f, defaultdict(lambda: defaultdict(lambda: 0))) for f in F)
            for d in self.documents:
                if self.weight in (IG, GR, INFOGAIN, GAINRATIO):
                    d_vector = dict.fromkeys(d.terms, True)
                else:
                    d_vector = d.vector
                # Count features by value per class.
                # Equal-width binning.
                # Features with float values are taken to range between 0.0-1.0,
                # for which 10 discrete intervals are used (0.1, 0.2, 0.3, ...).
                for f, v in d_vector.items():
                    if isinstance(v, float):
                        v = round(v, 1)
                    V[f][v][d.type] += 1
                #for f in F - set(d_vector):
                #    V[f][0][type] += 1
                # We also need to count features with value 0.0.
                # This is done with the two lines above, however
                # the code below is over a 1000x faster (less dict.__getitem__).
            for f in F:
                for type, n in C.items():
                    V[f][0][type] += n - sum(V[f][v][type] for v in V[f])
            # IG
            for f in F:
                Vf = V[f]
                n = sum(sum(Vf[v].values()) for v in Vf) # total value count
                n = float(n) or 1
                ig = HC
                si = 0 # split info
                for Cv in Vf.values():
                    Cv = list(Cv.values())
                    pv = sum(Cv) / n
                    ig = ig - pv * H(Cv)
                    si = si + H([pv])
                self._ig[f] = ig
                self._gr[f] = ig / (si or 1)
        return self._ig.get(word, 0.0)

    IG = ig = infogain = gain = information_gain

    def gain_ratio(self, word):
        """ Returns the information gain ratio (GR, 0.0-1.0) for the given feature.
        """
        if not self._gr:
            self.ig(word)
        return self._gr[word]

    GR = gr = gainratio = gain_ratio

    def feature_selection(self, top=100, method=CHISQUARED, threshold=0.0, weighted=False):
        """ Returns a list with the most informative features (terms), using information gain.
            This is a subset of Model.features that can be used to build a Classifier
            that is faster (less features = less matrix columns) but still efficient.
            The given document frequency threshold excludes features that occur in 
            less than the given percentage of documents (i.e., outliers).
        """
        if method is None:
            f = lambda w: 1.0
        if method in (X2, CHISQUARE, CHISQUARED, "X2"):
            f = lambda w: 1.0 - self.x2(w)
        if method in (IG, INFOGAIN):
            f = self.ig
        if method in (GR, GAINRATIO):
            f = self.gr
        if method == DF:
            f = self.df
        if hasattr(method, "__call__"):
            f = method
        subset = ((f(w), w) for w in self.terms if self.df(w) &gt;= threshold)
        subset = sorted(subset, key=itemgetter(1))
        subset = sorted(subset, key=itemgetter(0), reverse=True)
        subset = subset[:top if top is not None else len(subset)]
        subset = subset if weighted else [w for x, w in subset]
        return subset

    def filter(self, features=[], documents=[]):
        """ Returns a new Model with documents only containing the given list of features,
            for example a subset returned from Model.feature_selection().
        """
        documents = documents or self.documents
        features = set(features)
        model = Model(weight=self.weight)
        model.extend([
            Document(dict((w, f) for w, f in d.terms.items() if w in features),
                       name = d.name,
                       type = d.type,
                   language = d.language,
                description = d.description) for d in documents])
        return model

    def train(self, *args, **kwargs):
        """ Trains Model.classifier with the document vectors.
            Each document is expected to have a Document.type.
            Model.predict() can then be used to predict the type of other (unknown) documents.
        """
        if len(args) == 0:
            # Model.train(classifier=KNN)
            Classifier = kwargs.pop("Classifier", NB)
        if len(args) &gt;= 1:
            # Model.train(KNN, k=1)
            Classifier = args[0]; args = args[1:]
        kwargs["train"] = self
        self._classifier = Classifier(*args, **kwargs)
        self._classifier.finalize()

    def predict(self, *args, **kwargs):
        """ Returns the type for a given document,
            based on the similarity of documents in the trained Model.classifier.
        """
        return self._classifier.classify(*args, **kwargs)

# Backwards compatibility.
Corpus = Model

#### FREQUENT CONCEPT SETS #########################################################################
# Agrawal R. &amp; Srikant R. (1994), Fast algorithms for mining association rules in large databases.
# Based on: https://gist.github.com/1423287


</t>
<t tx="karstenw.20230303140319.92">def __init__(self, documents=[], weight=TFIDF):
    """ A model is a bag-of-word representation of a corpus of documents, 
        where each document vector is a bag of (word, relevance)-items.
        Vectors can then be compared for similarity using a distance metric.
        The weighting scheme can be: relative TF, TFIDF (default), IG, BINARY, None,
        where None means that the original weights are used.
    """
    self.description = ""             # Description of the dataset: author e-mail, etc.
    self._documents  = readonlylist() # List of documents (read-only).
    self._index      = {}             # Document.name =&gt; Document.
    self._df         = {}             # Cache of document frequency per word.
    self._cos        = {}             # Cache of ((d1.id, d2.id), relevance)-items (cosine similarity).
    self._pp         = {}             # Cache of ((word, type), probability)-items.
    self._x2         = {}             # Cache of (word, chi-squared p-value)-items.
    self._ig         = {}             # Cache of (word, information gain)-items.
    self._gr         = {}             # Cache of (word, information gain ratio)-items.
    self._inverted   = {}             # Cache of word =&gt; Document.
    self._vector     = None           # Cache of model vector with all the features in the model.
    self._classifier = None           # Classifier trained on the documents in the model (NB, KNN, SVM).
    self._lsa        = None           # LSA matrix with reduced dimensionality.
    self._weight     = weight         # Weight used in Document.vector (TF, TFIDF, IG, BINARY or None).
    self._update()
    self.extend(documents)

</t>
<t tx="karstenw.20230303140319.93">@property
def documents(self):
    return self._documents

docs = documents

</t>
<t tx="karstenw.20230303140319.94">@property
def terms(self):
    return list(self.vector.keys())

features = words = terms

</t>
<t tx="karstenw.20230303140319.95">@property
def classes(self):
    return list(set(d.type for d in self.documents))

labels = classes

</t>
<t tx="karstenw.20230303140319.96">@property
def classifier(self):
    return self._classifier

</t>
<t tx="karstenw.20230303140319.97">@property
def distribution(self):
    p = defaultdict(int)
    for d in self.documents:
        p[d.type] += 1
    return p

</t>
<t tx="karstenw.20230303140319.98">def _get_lsa(self):
    return self._lsa

</t>
<t tx="karstenw.20230303140319.99">def _set_lsa(self, v=None):
    self._update() # Clear the cache.
    self._lsa = v

lsa = property(_get_lsa, _set_lsa)

</t>
<t tx="karstenw.20230303140320.1">#--- INFORMATION GAIN TREE --------------------------------------------------------------------------


class IGTreeNode(list):

    @others
</t>
<t tx="karstenw.20230303140320.10">def train(self, document, type=None):
    Classifier.train(self, document, type)

</t>
<t tx="karstenw.20230303140320.11">def _train(self):
    """ Calculates information gain ratio for the features in the training data.
        Constructs the search tree.
    """
    m = Model((Document(set(v), type=type) for type, v in self._vectors), weight=BINARY)

    f = sorted(m.features, key=getattr(m, self._method), reverse=True)

    # print(f)
    # print(self._vectors)

    sys.setrecursionlimit(max(len(f) * 2, 1000))
    self._root = self._tree([(v, type) for type, v in self._vectors], features=f)

</t>
<t tx="karstenw.20230303140320.12">def classify(self, document, discrete=True):
    """ Returns the type with the highest probability for the given document.
        If the classifier has been trained on LSA concept vectors
        you need to supply LSA.transform(document).
    """
    if self._root is None:
        self._train()

    res = self._search(self._root, self._vector(document)[1])
    return res

</t>
<t tx="karstenw.20230303140320.13">def finalize(self):
    """ Removes training data from memory, keeping only the IG tree,
        reducing file size with Classifier.save().
    """
    if self._root is None:
        self._train()
    self._vectors = []

</t>
<t tx="karstenw.20230303140320.14">#--- SINGLE-LAYER PERCEPTRON ------------------------------------------------------------------------


def softmax(p):
    """ Returns a dict with float values that sum to 1.0
        (using generalized logistic regression).
    """
    if p:
        v = list(p.values())
        m = max(v)
        e = list([exp(x - m) for x in v]) # prevent overflow
        s = sum(e)
        v = list([x / s for x in e])
        p = defaultdict(float, list(zip(list(p.keys()), v)))
    return p

#print(softmax({"cat": +1, "dog": -1})) # {"cat": 0.88, "dog": 0.12}
#print(softmax({"cat": +2, "dog": -2})) # {"cat": 0.98, "dog": 0.02}


</t>
<t tx="karstenw.20230303140320.15">class SLP(Classifier):

    @others
AP = AveragedPerceptron = Perceptron = SLP

# Perceptron learns one training example at a time,
# adjusting weights if the example is predicted wrong.
# Higher accuracy can be achieved by doing multiple iterations:

#from pattern.vector import Perceptron, shuffled
#
#p = Perceptron()
#for i in range(5):
#    for v in shuffled(data):
#        p.train(v)

#p = Perceptron()
#p.train({"woof":1, "howl":1}, "dog")
#p.train({"woof":1, "bark":1}, "dog")
#p.train({"woof":1, "bark":1}, "dog")
#p.train({"meow":1, "purr":1}, "cat")
#print p.features
#print p.classify({"woof":1, "bark":1}, discrete=False).max

</t>
<t tx="karstenw.20230303140320.16">def __init__(self, train=[], baseline=MAJORITY, iterations=1, **kwargs):
    """ Perceptron (SLP, single-layer averaged perceptron) is a simple artificial neural network,
        a supervised learning method sometimes used for i.a. part-of-speech tagging.
        Documents are classified based on the neuron that outputs the highest weight
        for the given inputs (i.e., document vector features).
        A feature is taken to occur in a vector (1) or not (0), i.e. BINARY weight.
    """
    self._weight = defaultdict(dict) # {class: {feature: (weight, weight sum, timestamp)}}
    self._iterations = iterations
    self._iteration = 0
    train = list(train)
    train = chain(*(shuffled(train) for i in range(iterations)))
    Classifier.__init__(self, train, baseline)

</t>
<t tx="karstenw.20230303140320.17">@property
def iterations(self):
    return self._iterations

</t>
<t tx="karstenw.20230303140320.18">@property
def features(self):
    return list(set(chain(*(list(f.keys()) for f in self._weight.values()))))

</t>
<t tx="karstenw.20230303140320.19">def train(self, document, type=None):
    """ Trains the classifier with the given document of the given type (i.e., class).
        A document can be a Document, Vector, dict, list or string.
        If no type is given, Document.type will be used instead.
    """
    def _update(type, feature, weight, i):
        # Collins M. (2002). Discriminative Training Methods for Hidden Markov Models. EMNLP 2002.
        # Based on: http://honnibal.wordpress.com/2013/09/11/
        # Accumulate average weights (prevents overfitting).
        # Instead of keeping all intermediate results and averaging them at the end,
        # we keep a running sum and the iteration in which the sum was last modified.
        w = self._weight[type]
        w0, w1, j = w[feature] if feature in w else (random() * 2 - 1, 0, 0)
        w0 += weight
        w[feature] = (w0, (i - j) * w0 + w1, i)
    type, vector = self._vector(document, type=type)
    self._classes[type] = self._classes.get(type, 0) + 1
    t1 = type
    t2 = SLP.classify(self, document)
    if t1 != t2: # Error correction.
        self._iteration += 1
        for f in vector:
            _update(t1, f, +1, self._iteration)
            _update(t2, f, -1, self._iteration)

</t>
<t tx="karstenw.20230303140320.2">def __init__(self, feature=None, value=None, type=None):
    self.feature = feature
    self.value = value
    self.type = type

</t>
<t tx="karstenw.20230303140320.20">def classify(self, document, discrete=True):
    """ Returns the type with the highest probability for the given document.
        If the classifier has been trained on LSA concept vectors
        you need to supply LSA.transform(document).
    """
    v = self._vector(document)[1]
    i = self._iteration or 1
    i = float(i)
    p = defaultdict(float)
    for type, w in self._weight.items():
        #p[type] = sum(w[f][0] for f in v if f in w) # Without averaging.
        s = 0.
        for f in v:
            if f in w:
                w0, w1, j = w[f]
                s += ((i - j) * w0 + w1) / i
        p[type] = s
    # Normalize probability estimates.
    p = softmax(p)
    #m = min(chain(p.values(), (0,)))
    #s = sum(x-m for x in p.values()) or 1
    #for type in p:
    #    p[type] -= m
    #    p[type] /= s
    if not discrete:
        return Probabilities(self, p)
    try:
        # Ties are broken in favor of the majority class
        # (random winner for majority ties).
        m = max(p.values())
        p = sorted((self._classes[type], type) for type, w in p.items() if w == m &gt; 0)
        p = [type for frequency, type in p if frequency == p[0][0]]
        return choice(p)
    except:
        return self.baseline

</t>
<t tx="karstenw.20230303140320.21">def finalize(self):
    """ Removes training data from memory, keeping only the node weights,
        reducing file size with Classifier.save().
    """
    self._vectors = []

</t>
<t tx="karstenw.20230303140320.22">#--- BACKPROPAGATION NEURAL NETWORK -----------------------------------------------------------------
# "Deep learning" refers to deep neural networks and deep belief systems.
# Deep neural networks are networks that have hidden layers between the input and output layers.
# By contrast, Perceptron directly feeds the input to the output layer.

# Weight initialization:
RANDOM = "random"


def matrix(m, n, a=0.0, b=0.0):
    """ Returns an n x m matrix with values 0.0.
        If a and b are given, values are uniformly random between a and b.
    """
    if a == b == 0:
        return [[0.0] * n for i in range(m)]
    return [[uniform(a, b) for j in range(n)] for i in range(m)]


</t>
<t tx="karstenw.20230303140320.23">def sigmoid(x):
    """ Forward propagation activation function.
    """
    #return 1.0 / (1.0 + math.exp(-x))
    return tanh(x)


</t>
<t tx="karstenw.20230303140320.24">def sigmoid_derivative(y):
    """ Backward propagation activation function derivative.
    """
    #return y * (1.0 - y)
    return 1.0 - y * y


</t>
<t tx="karstenw.20230303140320.25">class BPNN(Classifier):

    @others
MLP = ANN = NN = NeuralNetwork = BPNN

#nn = BPNN()
#nn._weight_initialization(2, 1, hidden=2)
#nn._train([
#    ([0,0], [0]),
#    ([0,1], [1]),
#    ([1,0], [1]),
#    ([1,1], [0])
#])
#print(nn._classify([0,0]))
#print(nn._classify([0,1]))
#print

</t>
<t tx="karstenw.20230303140320.26">def __init__(self, train=[], baseline=MAJORITY, layers=2, iterations=1000, **kwargs):
    """ Backpropagation neural network (BPNN) is a supervised learning method 
        bases on a network of interconnected neurons
        inspired by an animal's nervous system (i.e., the brain).
    """
    # Based on:
    # http://www.cs.pomona.edu/classes/cs30/notes/cs030neural.py
    # http://arctrix.com/nas/python/bpnn.py
    self._layers = layers
    self._iterations = iterations
    self._rate = kwargs.get("rate", 0.5)
    self._momentum = kwargs.get("momentum", 0.1)
    self._trained = False
    Classifier.__init__(self, train, baseline)

</t>
<t tx="karstenw.20230303140320.27">@property
def layers(self):
    return self._layers

</t>
<t tx="karstenw.20230303140320.28">@property
def iterations(self):
    return self._iterations

</t>
<t tx="karstenw.20230303140320.29">@property
def rate(self):
    return self._rate

</t>
<t tx="karstenw.20230303140320.3">@property
def children(self):
    return self

</t>
<t tx="karstenw.20230303140320.30">@property
def momentum(self):
    return self._momentum

learningrate = learning_rate = rate

</t>
<t tx="karstenw.20230303140320.31">def _weight_initialization(self, i=1, o=1, hidden=1, method=RANDOM, a=0.0, b=1.0):
    """ Initializes the network with the given number of input, hidden, output nodes.
        Initializes the node weights uniformly random between a and b.
    """
    i += 1 # bias
    # Node activation.
    self._ai = [1.0] * i
    self._ao = [1.0] * o
    self._ah = [1.0] * hidden
    # Node weights (w) and recent change (c).
    self._wi = matrix(i, hidden, a, b)
    self._ci = matrix(i, hidden)
    self._wo = matrix(hidden, o, a, b)
    self._co = matrix(hidden, o)

</t>
<t tx="karstenw.20230303140320.32">def _propagate_forward(self, input=[]):
    """ Propagates the input through the network and returns the output activiation.
    """
    ai, ao, ah, wi, wo = self._ai, self._ao, self._ah, self._wi, self._wo
    assert len(input) == len(ai) - 1
    # Activate input nodes.
    for i, v in enumerate(input):
        ai[i] = v
    # Activate hidden nodes.
    for j, v in enumerate(ah):
        ah[j] = sigmoid(sum((v * wi[i][j] for i, v in enumerate(ai))))
    # Activate output nodes.
    for k, v in enumerate(ao):
        ao[k] = sigmoid(sum((v * wo[j][k] for j, v in enumerate(ah))))
    return list(ao)

</t>
<t tx="karstenw.20230303140320.33">def _propagate_backward(self, output=[], rate=0.5, momentum=0.1):
    """ Propagates the output through the network and
        generates delta for hidden and output nodes.
        The learning rate determines speed vs. accuracy of the algorithm.
    """
    ai, ao, ah, wi, wo, ci, co = self._ai, self._ao, self._ah, self._wi, self._wo, self._ci, self._co
    # Compute delta for output nodes.
    do = [0.0] * len(ao)
    for k, v in enumerate(ao):
        error = output[k] - v
        do[k] = error * sigmoid_derivative(v)
    # Compute delta for hidden nodes.
    dh = [0.0] * len(ah)
    for j, v in enumerate(ah):
        error = sum(do[k] * wo[j][k] for k in range(len(ao)))
        dh[j] = error * sigmoid_derivative(v)
    # Update output weights.
    for j, v1 in enumerate(ah):
        for k, v2 in enumerate(ao):
            change = do[k] * v1
            wo[j][k] += rate * change + momentum * co[j][k]
            co[j][k] = change
    # Update input weight.
    for i, v1 in enumerate(ai):
        for j, v2 in enumerate(ah):
            change = dh[j] * v1
            wi[i][j] += rate * change + momentum * ci[i][j]
            ci[i][j] = change
    # Compute and return error.
    return sum(0.5 * (output[k] - v) ** 2 for k, v in enumerate(ao))

_backprop = _propagate_backward

</t>
<t tx="karstenw.20230303140320.34">def _train(self, data=[], iterations=1000, rate=0.5, momentum=0.1):
    """ Trains the network with the given data using backpropagation.
        The given data is a list of (input, output)-tuples, 
        where each input and output a list of values.
        For example, to learn the XOR-function:
        nn = BPNN()
        nn._weight_initialization(2, 1, hidden=2)
        nn._train([
            ([0,0], [0]),
            ([0,1], [1]),
            ([1,0], [1]),
            ([1,1], [0])
        ])
        print(nn._classify([0,0]))
        print(nn._classify([0,1]))
    """
    # Error decreases with each iteration.
    for i in range(iterations):
        error = 0.0
        for input, output in data:
            self._propagate_forward(input)
            error += self._propagate_backward(output, rate, momentum)

</t>
<t tx="karstenw.20230303140320.35">def _classify(self, input):
    return self._propagate_forward(input)

</t>
<t tx="karstenw.20230303140320.36">def train(self, document, type=None):
    """ Trains the classifier with the given document of the given type (i.e., class).
        A document can be a Document, Vector, dict, list or string.
        If no type is given, Document.type will be used instead.
    """
    Classifier.train(self, document, type)
    self._trained = False

</t>
<t tx="karstenw.20230303140320.37">def classify(self, document, discrete=True):
    """ Returns the type with the highest probability for the given document.
        If the classifier has been trained on LSA concept vectors
        you need to supply LSA.transform(document).
    """
    if not self._trained:
        # Batch learning (we need to know the number of features in advance).
        n = float(len(self.classes)) - 1
        H1 = list(sorted(self.features))
        H2 = dict((x, i / n) for i, x in enumerate(self.classes))  # Class =&gt; float hash (0.0-1.0).
        H3 = dict((i / n, x) for i, x in enumerate(self.classes))  # Class reversed hash.
        v = [([v.get(f, 0.0) for f in H1], [H2[type]]) for type, v in self._vectors]
        self._h = (H1, H2, H3)
        self._weight_initialization(i=len(H1), o=1, hidden=self._layers, a=0.0, b=1.0)
        self._train(v, self._iterations, self._rate, self._momentum)
        self._trained = True
    H1, H2, H3 = self._h
    v = self._vector(document)[1]
    i = [v.get(f, 0.0) for f in H1]
    o = self._classify(i)[0]
    c = min(H3.keys(), key=lambda k: abs(k - o))
    c = H3[c]
    return c

</t>
<t tx="karstenw.20230303140320.38">def finalize(self):
    """ Removes training data from memory, keeping only the node weights,
        reducing file size with Classifier.save().
    """
    self._vectors = []

</t>
<t tx="karstenw.20230303140320.39">#--- SUPPORT VECTOR MACHINE ------------------------------------------------------------------------
# Pattern comes bundled with LIBSVM 3.17:
# http://www.csie.ntu.edu.tw/~cjlin/libsvm/
#
# Compiled binaries for 32-bit and 64-bit Windows, Mac OS X and Ubuntu are included.
# If no binary works, SVM() raises an ImportError,
# and you will need to download and compile LIBSVM from source.
# If Mac OS X complains during compilation, rename -soname" to "-install_name" in libsvm/Makefile.
# If the binary is named "libsvm.so.2", strip the ".2".
# Put the binary (i.e., "libsvm.dll" or "libsvm.so") in pattern/vector/svm/.
# Windows binaries can be downloaded from:
# http://www.lfd.uci.edu/~gohlke/pythonlibs/#libsvm

# SVM extensions:
LIBSVM, LIBLINEAR = \
    "libsvm", "liblinear"

# SVM type:
SVC = CLASSIFICATION = 0
SVR = REGRESSION = 3
SVO = DETECTION = 2 # One-class SVM: X belongs to the class or not?

# SVM kernels:
LINEAR = 0 # Straight line: u' * v
POLYNOMIAL = 1 # Curved line: (gamma * u' * v + coef0) ** degree
RADIAL = RBF = 2 # Curved path: exp(-gamma * |u-v| ** 2)

# SVM solvers:
LOGIT = L2LR = 0 # LIBLINEAR L2 logistic regression

# The simplest way to divide two clusters is a straight line.
# If the clusters are separated by a curved line,
# separation may be easier in higher dimensions (using a kernel).


class SVM(Classifier):

    @others
#---------------------------------------------------------------------------------------------------
# "Nothing beats SVM + character n-grams."
# Character n-grams seem to capture all information: morphology, context, frequency, ...
# SVM will discover the most informative features.
# Each row in the CSV is a score (positive = +1, negative = –1) and a Dutch book review.
# Can we learn from this dataset to predict sentiment? Yes we can!
# The following script demonstrates sentiment analysis for Dutch book reviews,
# with 90% accuracy, in 10 lines of Python code:

#from pattern.db import CSV
#from pattern.vector import SVM, chngrams, kfoldcv
#
#def v(s):
#    return chngrams(s, n=4)
#
#data = CSV.load(os.path.join("..", "..", "test", "corpora", "polarity-nl-bol.com.csv"))
#data = map(lambda p, review: (v(review), int(p) &gt; 0), data)
#
#print(kfoldcv(SVM, data, folds=3))

#---------------------------------------------------------------------------------------------------
# I hate to spoil your party..." by Lars Buitinck.
# As pointed out by Lars Buitinck, words + word-level bigrams with TF-IDF can beat the 90% boundary:

#from pattern.db import CSV
#from pattern.en import ngrams
#from pattern.vector import Model, SVM, gridsearch
#
#def v(s):
#    return count(words(s) + ngrams(s, n=2))
#
#data = CSV.load(os.path.join("..", "..", "test", "corpora", "polarity-nl-bol.com.csv"))
#data = map(lambda p, review: Document(v(review), type=int(p) &gt; 0), data)
#data = Model(data, weight="tf-idf")
#
#for p in gridsearch(SVM, data, c=[0.1, 1, 10], folds=3):
#    print(p)

# This reports 92% accuracy for the best run (c=10).
# Of course, it's optimizing for the same cross-validation
# that it's testing on, so this is easy to overfit.
# In scikit-learn it will run faster (4 seconds &lt;=&gt; 20 seconds), see: http://goo.gl/YqlRa

</t>
<t tx="karstenw.20230303140320.4">@property
def leaf(self):
    return len(self) == 0


</t>
<t tx="karstenw.20230303140320.40">def __init__(self, *args, **kwargs):
    """ Support Vector Machine (SVM) is a supervised learning method 
        where training documents are represented as points in n-dimensional space.
        The SVM constructs a number of hyperplanes that subdivide the space.
        Optional parameters:
        -      type = CLASSIFICATION, 
        -    kernel = LINEAR, 
        -    solver = 1, 
        -    degree = 3, 
        -     gamma = 1 / len(SVM.features), 
        -    coeff0 = 0, 
        -      cost = 1, 
        -   epsilon = 0.01, 
        -     cache = 100, 
        - shrinking = True, 
        - extension = (LIBSVM, LIBLINEAR), 
        -     train = []
    """
    from . import svm
    self._svm = svm
    # Cached LIBSVM or LIBLINEAR model:
    self._model = None
    # SVM.extensions is a tuple of extension modules that can be used.
    # By default, LIBLINEAR will be used for linear SVC (it is faster).
    # If you do not want to use LIBLINEAR, use SVM(extension=LIBSVM).
    self._extensions = \
        kwargs.get("extensions",
        kwargs.get("extension", (LIBSVM, LIBLINEAR)))
    # Optional parameters are read-only:
    # -  cost: higher cost = less margin for error (and risk of overfitting).
    # - gamma: influence ("radius") of each training example for RBF.
    if len(args) &gt; 0:
        kwargs.setdefault("train", args[0])
    if len(args) &gt; 1:
        kwargs.setdefault("type", args[1])
    if len(args) &gt; 2:
        kwargs.setdefault("kernel", args[2])
    for k1, k2, v in (
        (       "type", "s", CLASSIFICATION),
        (     "kernel", "t", LINEAR),
        (     "solver", "f", 1),   # For LIBLINEAR.
        (     "degree", "d", 3),   # For POLYNOMIAL.
        (      "gamma", "g", 0),   # For POLYNOMIAL + RADIAL.
        (     "coeff0", "r", 0),   # For POLYNOMIAL.
        (       "cost", "c", 1),   # Can be optimized with gridsearch().
        (    "epsilon", "p", 0.1),
        (         "nu", "n", 0.5),
        (      "cache", "m", 100), # MB
        (  "shrinking", "h", True)):
        v = kwargs.get(k2, kwargs.get(k1, v))
        setattr(self, "_" + k1, v)
    # Type aliases.
    if self._type == "svc":
        self._type = SVC
    if self._type == "svr":
        self._type = SVR
    if self._type == "svo":
        self._type = SVO
    # Kernel aliases.
    if self._kernel == "rbf":
        self._kernel = RBF
    # Solver aliases.
    if self._solver == "logit":
        self._solver = LOGIT
    Classifier.__init__(self, train=kwargs.get("train", []), baseline=MAJORITY)

</t>
<t tx="karstenw.20230303140320.41">@property
def extension(self):
    """ Yields the extension module used (LIBSVM or LIBLINEAR).
    """
    if LIBLINEAR in self._extensions and \
      self._svm.LIBLINEAR and \
      self._type == CLASSIFICATION and \
      self._kernel == LINEAR:
        return LIBLINEAR
    return LIBSVM

</t>
<t tx="karstenw.20230303140320.42">@property
def _extension(self):
    """ Yields the extension module object,
        e.g., pattern/vector/svm/3.17/libsvm-mac64.so.
    """
    if self.extension == LIBLINEAR:
        return self._svm.liblinear.liblinear
    return self._svm.libsvm.libsvm

</t>
<t tx="karstenw.20230303140320.43">@property
def type(self):
    return self._type

</t>
<t tx="karstenw.20230303140320.44">@property
def kernel(self):
    return self._kernel

</t>
<t tx="karstenw.20230303140320.45">@property
def solver(self):
    return self._solver

</t>
<t tx="karstenw.20230303140320.46">@property
def degree(self):
    return self._degree

</t>
<t tx="karstenw.20230303140320.47">@property
def gamma(self):
    return self._gamma

</t>
<t tx="karstenw.20230303140320.48">@property
def coeff0(self):
    return self._coeff0

</t>
<t tx="karstenw.20230303140320.49">@property
def cost(self):
    return self._cost

</t>
<t tx="karstenw.20230303140320.5">class IGTree(Classifier):

    @others
IGTREE = IGTree

doc1 = (''' "Dynamic Programming is a very powerful mathematical technique, often utilised in programming, for solving optimization problems. Normally, minimizing or maximizing.
Greedy algorithms focus on making the best local choice at each decision making stage. Without a proof of correctness, such an algorithm is likely to fail. With Dynamic Programming, we can design our own algorithm which searches for all possibilities (which ensures correctness) whilst storing the results to avoid having to recomputed (leading to computational efficiency).
Dynamic Programming solves problems by combining the solutions of subproblems. These subproblems are not, however, independent. Subproblems can share subsubproblems, but the solution to one subproblem doesnt necessarily affect the solutions to other subproblems stemming from the same problem.
Dynamic Programming reduces computation time by solving subproblems in a bottom-up way. It stores the solution to a subproblem the first time it is solved, meaning that it can look up the solution when that subproblem is encountered subsequently.
The key to Dynamic Programming is to find the structure of optimal solutions. The steps required are as follows:
1.	Generalise the structure of an optimal solution
2.	Recursively define the value of an optimal solution
3.	Compute the optimal solution values either top-down (with caching), or bottom-up using a table
4.	Generate the optimal solution of these computed values
","In mathematics and computer science, dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure (described below). The method takes much less time than naive methods.
The term was originally used in the 1940s by Richard Bellman to describe the process of solving problems where one needs to find the best decisions one after another. By 1953, he had refined this to the modern meaning. The field was founded as a systems analysis and engineering topic that is recognized by the IEEE. Bellman's contribution is remembered in the name of the Bellman equation, a central result of dynamic programming which restates an optimization problem in recursive form.
The word ""programming"" in ""dynamic programming"" has no particular connection to computer programming at all, and instead comes from the term ""mathematical programming"", a synonym for optimization. Thus, the ""program"" is the optimal plan for action that is produced. For instance, a finalized schedule of events at an exhibition is sometimes called a program. Programming, in this sense, means finding an acceptable plan of action, an algorithm.
Optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem. For example, the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices, and then using this to pick the best overall path, as shown in Figure 1. In general, we can solve a problem with optimal substructure using a three-step process:
   1. Break the problem into smaller subproblems.
   2. Solve these problems optimally using this three-step process recursively.
   3. Use these optimal solutions to construct an optimal solution for the original problem.
The subproblems are, themselves, solved by dividing them into sub-subproblems, and so on, until we reach some simple case that is solvable in constant time.
Figure 2. The subproblem graph for the Fibonacci sequence. That it is not a tree but a DAG indicates overlapping subproblems.
To say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems. For example, in the Fibonacci sequence, F3 = F1 + F2 and F4 = F2 + F3 — computing each number involves computing F2. Because both F3 and F4 are needed to compute F5, a naive approach to computing F5 may end up computing F2 twice or more. This applies whenever overlapping subproblems are present: a naive approach may waste time recomputing optimal solutions to subproblems it has already solved.
In order to avoid this, we instead save the solutions to problems we have already solved. Then, if we need to solve the same problem later, we can retrieve and reuse our already-computed solution. This approach is called memoization (not memorization, although this term also fits). If we are sure we won't need a particular solution anymore, we can throw it away to save space. In some cases, we can even compute the solutions to subproblems we know that we'll need in advance.
''', True)

doc2 = ('''" In probability theory, Bayes' theorem (often called Bayes' law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations. For example, a patient may be observed to have certain symptoms. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.
As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications.
Suppose there is a co-ed school having 60% boys and 40% girls as students. The girl students wear trousers or skirts in equal numbers; the boys all wear trousers. An observer sees a (random) student from a distance; all they can see is that this student is wearing trousers. What is the probability this student is a girl? The correct answer can be computed using Bayes' theorem.
The event A is that the student observed is a girl, and the event B is that the student observed is wearing trousers. To compute P(A|B), we first need to know:
P(B|A'), or the probability of the student wearing trousers given that the student is a boy. This is given as 1.
P(A), or the probability that the student is a girl regardless of any other information. Since the observers sees a random student, meaning that all students have the same probability of being observed, and the fraction of girls among the students is 40%, this probability equals 0.4.
P(A'), or the probability that the student is a boy regardless of any other information (A' is the complementary event to A). This is 60%, or 0.6.
P(B|A), or the probability of the student wearing trousers given that the student is a girl. As they are as likely to wear skirts as trousers, this is 0.5","In probability theory, Bayes' theorem (often called Bayes' law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations. For example, a patient may be observed to have certain symptoms. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. (See example 2)
As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.
Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:
    P(A|B) = \frac{P(B | A)\, P(A)}{P(B)}.
Each term in Bayes' theorem has a conventional name:
    * P(A) is the prior probability or marginal probability of A. It is ""prior"" in the sense that it does not take into account any information about B.
    * P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.
    * P(B|A) is the conditional probability of B given A.
    * P(B) is the prior or marginal probability of B, and acts as a normalizing constant.
Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.
","3"''', False)


doc3 = ('''"A Vector space model (or term vector model) is an algebraic way of representing text documents (and any objects, in general) as vectors of identifiers, such as index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first application was in the SMART Information Retrieval System.
A document can be represented as a vector. Every dimension relates to a different term. If a term appears in the document, the terms value in the vector is non-zero. Many different methods of calculating these values, sometimes known as (term) weights, have been developed. tf-idf weighting is one of the most well known schemes. (see below example).
The definition of a term depends on the application. Normally a term is a single word, keyword, or a longer phrase. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus).
The vector space model has some limitations:
1.	Longer documents are represented poorly because the documents have poor similarity values (namely a small scalar product and a large dimensionality)
2.	Search keywords have to precisely match document terms; word substrings could potentially result in a ""false positive match""
3.	Semantic sensitivity; documents with a similar context, but different term vocabulary won't be associated, resulting in a ""false negative match"".
4.	The order in which terms appear in the document is lost in a vector space representation.
","Vector space model (or term vector model) is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first use was in the SMART Information Retrieval System.
A document is represented as a vector. Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting (see the example below).
The definition of term depends on the application. Typically terms are single words, keywords, or longer phrases. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus).
The vector space model has the following limitations:
   1. Long documents are poorly represented because they have poor similarity values (a small scalar product and a large dimensionality)
   2. Search keywords must precisely match document terms; word substrings might result in a ""false positive match""
   3. Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a ""false negative match"".
   4. The order in which the terms appear in the document is lost in the vector space representation.''', True)

#The algorythm is very sensitive to words which contain in new text.

# docs = [doc1, doc2, doc3]
# v = IGTree(method=GAINRATIO)
# for document, t in docs:
#     v.train(document, type=t)
#
# for type, message in (
#         (True, doc1[0][:int(len(doc1[0])*0.5)]),
#         (False, doc2[0][:int(len(doc2[0])*0.5)]),
#         (True, doc3[0][:int(len(doc3[0])*0.5)])):
#     print(v.classify(message))



#from pattern.db import csv, pd
#data = csv(pd("..", "..", "test", "corpora", "polarity-nl-bol.com.csv"))
#data = ((review, score) for score, review in data)
#
#print(kfoldcv(IGTree, data, folds=3))

</t>
<t tx="karstenw.20230303140320.50">@property
def epsilon(self):
    return self._epsilon

</t>
<t tx="karstenw.20230303140320.51">@property
def nu(self):
    return self._nu

</t>
<t tx="karstenw.20230303140320.52">@property
def cache(self):
    return self._cache

</t>
<t tx="karstenw.20230303140320.53">@property
def shrinking(self):
    return self._shrinking

s, t, d, g, r, c, p, n, m, h = (
    type, kernel, degree, gamma, coeff0, cost, epsilon, nu, cache, shrinking
)

</t>
<t tx="karstenw.20230303140320.54">@property
def support_vectors(self):
    """ Yields the support vectors.
    """
    if self._model is None:
        self._train()
    if self.extension == LIBLINEAR:
        return []
    return self._model[0].get_SV()

sv = support_vectors

</t>
<t tx="karstenw.20230303140320.55">def _train(self):
    """ Calls libsvm.svm_train() to create a model.
        Vector classes and features are mapped to integers.
    """
    # Note: LIBLINEAR feature indices start from 1 (not 0).
    M  = [v for type, v in self._vectors]                        # List of vectors.
    H1 = dict((w, i + 1) for i, w in enumerate(self.features))     # Feature =&gt; integer hash.
    H2 = dict((w, i + 1) for i, w in enumerate(self.classes))      # Class =&gt; integer hash.
    H3 = dict((i + 1, w) for i, w in enumerate(self.classes))      # Class reversed hash.
    x  = list(map(lambda v: dict(map(lambda k: (H1[k], v[k]), v)), M)) # Hashed vectors.
    y  = list(map(lambda v: H2[v[0]], self._vectors))                  # Hashed classes.
    # For linear SVC, use LIBLINEAR which is faster.
    # For kernel SVC, use LIBSVM.
    if self.extension == LIBLINEAR:
        f = self._svm.liblinearutil.train
        o = "-s %s -c %s -p %s -q" % (
            self._solver,     # -f
            self._cost,       # -c
            self._epsilon     # -p
        )
    else:
        f = self._svm.libsvmutil.svm_train
        o = "-s %s -t %s -d %s -g %s -r %s -c %s -p %s -n %s -m %s -h %s -b %s -q" % (
            self._type,       # -s
            self._kernel,     # -t
            self._degree,     # -d
            self._gamma,      # -g
            self._coeff0,     # -r
            self._cost,       # -c
            self._epsilon,    # -p
            self._nu,         # -n
            self._cache,      # -m
        int(self._shrinking), # -h
        int(self._type != DETECTION), # -b
        )
    # Cache the model and the feature hash.
    # SVM.train() will remove the cached model (since it needs to be retrained).
    self._model = (f(y, x, o.split()), H1, H2, H3)

</t>
<t tx="karstenw.20230303140320.56">def _classify(self, document, probability=False):
    """ Calls libsvm.svm_predict() with the cached model.
        For CLASSIFICATION, returns the predicted class.
        For CLASSIFICATION with probability=True, returns a {class: weight} dict.
        For REGRESSION, returns a float.
    """
    if self._model is None:
        return None
    M = self._model[0]
    H1 = self._model[1]
    H2 = self._model[2]
    H3 = self._model[3]
    n = len(H1)
    v = self._vector(document)[1]
    v = dict(map(lambda k: (H1.get(k[1], k[0] + n + 1), v[k[1]]), enumerate(v)))
    s = getattr(self, "_solver", None)
    # For linear SVC, use LIBLINEAR which is 10x faster.
    # For kernel SVC, use LIBSVM.
    if self.extension == LIBLINEAR:
        f = self._svm.liblinearutil.predict
        o = "-b %s -q" % (int(probability) if s == 0 else 0)
    else:
        f = self._svm.libsvmutil.svm_predict
        o = "-b %s -q" % (int(probability))
    p = f([0], [v], M, o)
    # Note: LIBLINEAR only supports probabilities for logistic regression (solver=0).
    if self._type == CLASSIFICATION and probability and self.extension == LIBLINEAR and s != 0:
        return Probabilities(self, defaultdict(float, ((H3[p[0][0]], 1.0),)))
    if self._type == CLASSIFICATION and probability and self.extension == LIBLINEAR:
        return Probabilities(self, defaultdict(float, ((H3[i + 1], w) for i, w in enumerate(p[2][0]))))
    if self._type == CLASSIFICATION and probability:
        return Probabilities(self, defaultdict(float, ((H3[i + 0], w) for i, w in enumerate(p[2][0]))))
    if self._type == CLASSIFICATION:
        return H3.get(int(p[0][0]))
    if self._type == REGRESSION:
        return p[0][0]
    if self._type == DETECTION:
        return p[0][0] &gt; 0 # -1 = outlier =&gt; return False
    return p[0][0]

</t>
<t tx="karstenw.20230303140320.57">def train(self, document, type=None):
    """ Trains the classifier with the given document of the given type (i.e., class).
        A document can be a Document, Vector, dict, list or string.
        If no type is given, Document.type will be used instead.
    """
    Classifier.train(self, document, type)
    self._model = None

</t>
<t tx="karstenw.20230303140320.58">def classify(self, document, discrete=True):
    """ Returns the type with the highest probability for the given document.
        If the classifier has been trained on LSA concept vectors
        you need to supply LSA.transform(document).
    """
    if self._model is None:
        self._train()
    return self._classify(document, probability=not discrete)

</t>
<t tx="karstenw.20230303140320.59">def save(self, path, final=False):
    if self._model is None:
        self._train()
    if self.extension == LIBSVM:
        self._svm.libsvmutil.svm_save_model(path, self._model[0])
    if self.extension == LIBLINEAR:
        self._svm.liblinearutil.save_model(path, self._model[0])
    # Save LIBSVM/LIBLINEAR model as a string.
    # Unlink LIBSVM/LIBLINEAR binaries for cPickle.
    svm, model = self._svm, self._model
    self._svm = None
    self._model = (open(path, "rb").read(),) + model[1:]
    Classifier.save(self, path, final)
    self._svm = svm
    self._model = model

</t>
<t tx="karstenw.20230303140320.6">def __init__(self, train=[], baseline=MAJORITY, method=GAINRATIO, **kwargs):
    """ IGTREE is a supervised learning method
        where training data is represented as a tree ordered by information gain.
        A feature is taken to occur in a vector (1) or not (0), i.e. BINARY weight.
    """
    self._root = None
    self._method = method
    Classifier.__init__(self, train, baseline)

</t>
<t tx="karstenw.20230303140320.60">@classmethod
def load(cls, path):
    return Classifier.load(path)

</t>
<t tx="karstenw.20230303140320.61">def _on_load(self, path):
    # Called from Classifier.load().
    # The actual SVM model was stored as a string.
    # 1) Import pattern.vector.svm.
    # 2) Extract the model string and save it as a temporary file.
    # 3) Use pattern.vector.svm's LIBSVM or LIBLINEAR to load the file.
    # 4) Delete the temporary file.
    from . import svm                               # 1
    self._svm = svm
    if self._model is not None:
        f = tempfile.NamedTemporaryFile("r+b")
        f.write(self._model[0])              # 2
        f.seek(0)
        if self.extension == LIBLINEAR and not svm.LIBLINEAR:
            raise ImportError("can't import liblinear")
        if self.extension == LIBLINEAR:
            m = self._svm.liblinearutil.load_model(f.name)
        if self.extension == LIBSVM:
            m = self._svm.libsvmutil.svm_load_model(f.name)
        self._model = (m,) + self._model[1:] # 3
        f.close()                            # 4

</t>
<t tx="karstenw.20230303140320.62">def finalize(self):
    """ Removes training data from memory, keeping only the LIBSVM/LIBLINEAR trained model,
        reducing file size with Classifier.save() (e.g., 15MB =&gt; 3MB).
    """
    if self._model is None:
        self._train()
    self._vectors = []

</t>
<t tx="karstenw.20230303140320.63">#--- LOGISTIC REGRESSION ---------------------------------------------------------------------------
# Multinomial logistic regression (or Maximum Entropy) is competitive to SVM and gives probabilities.


class LR(Classifier):

    @others
LogisticRegression = SoftMaxRegression = MaximumEntropy = MaxEnt = ME = LR

</t>
<t tx="karstenw.20230303140320.64">def __init__(self, train=[], baseline=MAJORITY, iterations=100, **kwargs):
    """ Logistic Regression is a supervised machine learning method,
        Documents are assigned a probability for each possible class,
        based on the probability that a feature occurs in a class 
        (independent of other features).
    """
    import scipy
    import scipy.sparse
    import scipy.special
    import scipy.optimize
    self._iterations = iterations
    self._model = None
    Classifier.__init__(self, train, baseline)

</t>
<t tx="karstenw.20230303140320.65">@property
def iterations(self):
    return self._iterations

</t>
<t tx="karstenw.20230303140320.66">def _train(self):
    H1 = dict((w, i) for i, w in enumerate(self.classes))  #   class =&gt; index
    H2 = dict((i, w) for i, w in enumerate(self.classes))  #   index =&gt; class
    H3 = dict((w, i) for i, w in enumerate(self.features)) # feature =&gt; index
    H4 = [w for w in sorted(H3, key=H3.get)]               # feature by index
    # Sparse matrix.
    # A full matrix of 10,000 vectors, 100,000 features
    # would take 1,000,000,000 x 8 = 7.5GB.
    m = len(self._vectors)
    n = len(H4)
    try:
        x = scipy.sparse.lil_matrix((m, n))
        for i, (type, v) in enumerate(self._vectors):
            for w in v:
                x[i, H3[w]] = v[w]
        x = scipy.sparse.csr_matrix(x) # faster dot
        y = scipy.array([H1[type] for type, v in self._vectors])
        t = self._gradient_descent(x, y, l=0.1, iterations=self._iterations)
    except:
        t = None
    self._model = (t, H1, H2, H3, H4)

</t>
<t tx="karstenw.20230303140320.67">def _classify(self, document):
    if self._model is None:
        return None
    t, H1, H2, H3, H4 = self._model
    if t is not None:
        v = self._vector(document)[1]
        v = scipy.array([v.get(w, 0.0) for w in H4])
        v = scipy.hstack([[1], v])
        p = scipy.special.expit(scipy.dot(t, v)) # sigmoid
        p = dict((H2[i], p) for i, p in enumerate(p))
        return p
    return {}

</t>
<t tx="karstenw.20230303140320.68">def _gradient_descent(self, x, y, l=0.1, iterations=100):
    # The cost function computes the mean squared error J.
    # The cost function represents the average amount of incorrect predictions.
    # The gradient function is the derivative of the cost function.
    # The gradient function determines the direction in which to optimize.
    # Gradient descent then iteratively minimizes the cost (in max iterations).
    # L2 regularization prevents overfitting by excluding unlikely feature values.
    def log(z):
        return scipy.log(z.clip(min=1e-10))

    def cost(t, x, y, l=0.1):
        # Cost function.
        m = float(len(y))
        h = scipy.special.expit(x.dot(t)) # sigmoid
        J = 1 / m * (scipy.dot(-y, log(h)) - scipy.dot(1 - y, log(1 - h)))
        L2 = l / m / 2 * sum(t[1:] ** 2)
        return J + L2

    def gradient(t, x, y, l=0.1):
        # Cost function derivative.
        m = float(len(y))
        h = scipy.special.expit(x.dot(t)) # sigmoid
        g = 1 / m * x.T.dot((h - y).T).T # dot(h-y, x)
        L2 = l / m * scipy.insert(t[1:], 0, 0)
        return scipy.transpose(g + L2)
    # Gradient descent:
    if x.shape[0] &gt; 0:
        m = x.shape[0]
        n = x.shape[1]
        t = scipy.zeros((max(y) + 1, n + 1))
        x = scipy.sparse.hstack([scipy.ones((m, 1)), x])
        y = scipy.array(y)
        for i in range(max(y) + 1):
            t0 = scipy.zeros((n + 1, 1))
            t0 = scipy.transpose(t0)
            t[i, :] = scipy.optimize.fmin_cg(
                lambda t: cost(t, x, 0 + (y == i), l), t0,
                lambda t: gradient(t, x, 0 + (y == i), l),
                    maxiter = iterations,
                       disp = 0)
        return t # theta
    return None

</t>
<t tx="karstenw.20230303140320.69">def train(self, document, type=None):
    """ Trains the classifier with the given document of the given type (i.e., class).
        A document can be a Document, Vector, dict, list or string.
        If no type is given, Document.type will be used instead.
    """
    Classifier.train(self, document, type)
    self._model = None

</t>
<t tx="karstenw.20230303140320.7">@property
def method(self):
    return self._method

</t>
<t tx="karstenw.20230303140320.70">def classify(self, document, discrete=True):
    """ Returns the type with the highest probability for the given document.
        If the classifier has been trained on LSA concept vectors
        you need to supply LSA.transform(document).
    """
    if self._model is None:
        self._train()
    p = self._classify(document)
    if not discrete:
        return Probabilities(self, p)
    try:
        # Ties are broken in favor of the majority class
        # (random winner for majority ties).
        m = max(p.values())
        p = sorted((self._classes[type], type) for type, w in p.items() if w == m &gt; 0)
        p = [type for frequency, type in p if frequency == p[0][0]]
        return choice(p)
    except:
        return self.baseline

</t>
<t tx="karstenw.20230303140320.71">def save(self, path, final=False):
    if self._model is None:
        self._train()
    # Save array as string.
    f = StringIO()
    scipy.save(f, self._model[0])
    f.seek(0)
    self._model = (f.read(),) + self._model[1:]
    Classifier.save(self, path, final)

</t>
<t tx="karstenw.20230303140320.72">@classmethod
def load(cls, path):
    return Classifier.load(path)

</t>
<t tx="karstenw.20230303140320.73">def _on_load(self, path):
    # Called from Classifier.load().

    import scipy.sparse
    import scipy.special
    import scipy.optimize
    if self._model is not None:
        f = StringIO()
        f.write(self._model[0])
        f.seek(0)
        self._model = (scipy.load(f),) + self._model[1:]

</t>
<t tx="karstenw.20230303140320.74">def finalize(self):
    """ Removes training data from memory, keeping only the trained model (theta),
        reducing file size with Classifier.save() (e.g., 15MB =&gt; 3MB).
    """
    if self._model is None:
        self._train()
    self._vectors = []

</t>
<t tx="karstenw.20230303140320.75">#### GENETIC ALGORITHM #############################################################################


class GeneticAlgorithm(object):

    @others
GA = GeneticAlgorithm
</t>
<t tx="karstenw.20230303140320.76">def __init__(self, candidates=[], **kwargs):
    """ A genetic algorithm is a stochastic search method  based on natural selection.
        Each generation, the fittest candidates are selected and recombined into a new generation. 
        With each new generation the system converges towards an optimal fitness.
    """
    self.population = candidates
    self.generation = 0
    # Set GA.fitness(), crossover(), mutate() from function.
    for f in ("fitness", "combine", "mutate"):
        if f in kwargs:
            setattr(self, f, types.MethodType(kwargs[f], self))

</t>
<t tx="karstenw.20230303140320.77">def fitness(self, candidate):
    """ Must be implemented in a subclass, returns 0.0-1.0.
    """
    return 1.0

</t>
<t tx="karstenw.20230303140320.78">def combine(self, candidate1, candidate2):
    """ Must be implemented in a subclass, returns a new candidate.
    """
    return None

</t>
<t tx="karstenw.20230303140320.79">def mutate(self, candidate):
    """ Must be implemented in a subclass, returns a new candidate.
    """
    return None or candidate

</t>
<t tx="karstenw.20230303140320.8">def _tree(self, vectors=[], features=[]):
    """ Returns a tree of nested IGTREE.Node objects,
        where the given list of vectors contains (Vector, class)-tuples, and
        where the given list of features is sorted by information gain ratio.
    """
    # Daelemans, W., van den Bosch, A., Weijters, T. (1997).
    # IGTree: Using trees for compression and classification in lazy learning algorithms.
    # Artificial Intelligence Review 11, 407-423.
    vectors = list(vectors)
    features = list(features)

    if len(vectors) == 0 or len(features) == 0:
        return IGTreeNode()
    # {class: count}
    classes = defaultdict(int)
    for v, type in vectors:
        classes[type] += 1

    # Find the most frequent class for the set of vectors.
    c = max(classes, key=classes.__getitem__)

    # Find the most informative feature f.
    f = features[0]
    n = IGTreeNode(feature=f, type=c)
    # The current node has a hyperplane on feature f,
    # and the majority class in the set of vectors.
    if len(classes) == 1:
        return n
    if len(features) == 1:
        return n
    # Partition the set of vectors into subsets
    # (vectors with the same value for feature f are in the same subset).
    p = defaultdict(list)
    for v, type in vectors:
        #x = round(v.get(f, 0.0), 1)
        x = f in v
        p[x].append((v, type))
    # If not all vectors in a subset have the same class,
    # build IGTREE._tree(subset, features[1:]) and connect it to the current node.
    for x in p:
        if any((type != c) for v, type in p[x]):
            n.append(self._tree(p[x], features[1:]))
            n[-1].value = x
    # print(n.type)
    # print(n[-1].type)
    return n

</t>
<t tx="karstenw.20230303140320.80">def update(self, top=0.5, mutation=0.5):
    """ Updates the population by selecting the top fittest candidates,
        and recombining them into a new generation.
    """
    # 1) Selection.
    # Choose the top fittest candidates.
    # Including weaker candidates can be beneficial (diversity).
    p = sorted(self.population, key=self.fitness, reverse=True)
    p = p[:max(2, int(round(len(p) * top)))]
    # 2) Reproduction.
    # Choose random parents for crossover.
    # Mutation avoids local optima by maintaining genetic diversity.
    g = []
    n = len(p)
    for candidate in self.population:
        i = randint(0, n - 1)
        j = choice([x for x in range(n) if x != i]) if n &gt; 1 else 0
        g.append(self.combine(p[i], p[j]))
        if random() &lt;= mutation:
            g[-1] = self.mutate(g[-1])
    self.population = g
    self.generation += 1

</t>
<t tx="karstenw.20230303140320.81">@property
def avg(self):
    # Average fitness is supposed to increase each generation.
    return float(sum(map(self.fitness, self.population))) / len(self.population)

average_fitness = avg

</t>
<t tx="karstenw.20230303140320.9">def _search(self, node, vector):
    """ Returns the predicted class for the given Vector.
    """
    while True:
        #x = round(vector.get(node.feature, 0.0), 1)
        x = node.feature in vector
        b = False
        for n in node.children:
            if n.value == x:
                b = True
                break
        if b is False:
            return node.type
        node = n

</t>
<t tx="karstenw.20230303140323.1">##### PATTERN | VECTOR | PORTER STEMMER ############################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################
# The Porter2 stemming algorithm (or "Porter stemmer") is a process for removing the commoner
# morphological and inflexional endings from words in English.
# Its main use is as part of a term normalisation process that is usually done
# when setting up Information Retrieval systems.

# Reference:
# C.J. van Rijsbergen, S.E. Robertson and M.F. Porter, 1980.
# "New models in probabilistic information retrieval."
# London: British Library. (British Library Research and Development Report, no. 5587).
#
# http://tartarus.org/~martin/PorterStemmer/

# Comments throughout the source code were taken from:
# http://snowball.tartarus.org/algorithms/english/stemmer.html

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140338.1"></t>
<t tx="karstenw.20230303140342.1">@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140351.1">from __future__ import absolute_import
from __future__ import division

LIBSVM = LIBLINEAR = True

try:
    from . import libsvm
    from . import libsvmutil
except ImportError as e:
    LIBSVM = False
    raise e

try:
    from . import liblinear
    from . import liblinearutil
except:
    LIBLINEAR = False
</t>
<t tx="karstenw.20230303140359.1">@language python
@tabwidth -4
@others

fillprototype(liblinear.train, POINTER(model), [POINTER(problem), POINTER(parameter)])
fillprototype(liblinear.find_parameter_C, None, [POINTER(problem), POINTER(parameter), c_int, c_double, c_double, POINTER(c_double), POINTER(c_double)])
fillprototype(liblinear.cross_validation, None, [POINTER(problem), POINTER(parameter), c_int, POINTER(c_double)])

fillprototype(liblinear.predict_values, c_double, [POINTER(model), POINTER(feature_node), POINTER(c_double)])
fillprototype(liblinear.predict, c_double, [POINTER(model), POINTER(feature_node)])
fillprototype(liblinear.predict_probability, c_double, [POINTER(model), POINTER(feature_node), POINTER(c_double)])

fillprototype(liblinear.save_model, c_int, [c_char_p, POINTER(model)])
fillprototype(liblinear.load_model, POINTER(model), [c_char_p])

fillprototype(liblinear.get_nr_feature, c_int, [POINTER(model)])
fillprototype(liblinear.get_nr_class, c_int, [POINTER(model)])
fillprototype(liblinear.get_labels, None, [POINTER(model), POINTER(c_int)])
fillprototype(liblinear.get_decfun_coef, c_double, [POINTER(model), c_int, c_int])
fillprototype(liblinear.get_decfun_bias, c_double, [POINTER(model), c_int])

fillprototype(liblinear.free_model_content, None, [POINTER(model)])
fillprototype(liblinear.free_and_destroy_model, None, [POINTER(POINTER(model))])
fillprototype(liblinear.destroy_param, None, [POINTER(parameter)])
fillprototype(liblinear.check_parameter, c_char_p, [POINTER(problem), POINTER(parameter)])
fillprototype(liblinear.check_probability_model, c_int, [POINTER(model)])
fillprototype(liblinear.check_regression_model, c_int, [POINTER(model)])
fillprototype(liblinear.set_print_string_function, None, [CFUNCTYPE(None, c_char_p)])
</t>
<t tx="karstenw.20230303140415.1">#!/usr/bin/env python

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140421.1">@language python
@tabwidth -4
@others

fillprototype(libsvm.svm_train, POINTER(svm_model), [POINTER(svm_problem), POINTER(svm_parameter)])
fillprototype(libsvm.svm_cross_validation, None, [POINTER(svm_problem), POINTER(svm_parameter), c_int, POINTER(c_double)])

fillprototype(libsvm.svm_save_model, c_int, [c_char_p, POINTER(svm_model)])
fillprototype(libsvm.svm_load_model, POINTER(svm_model), [c_char_p])

fillprototype(libsvm.svm_get_svm_type, c_int, [POINTER(svm_model)])
fillprototype(libsvm.svm_get_nr_class, c_int, [POINTER(svm_model)])
fillprototype(libsvm.svm_get_labels, None, [POINTER(svm_model), POINTER(c_int)])
fillprototype(libsvm.svm_get_sv_indices, None, [POINTER(svm_model), POINTER(c_int)])
fillprototype(libsvm.svm_get_nr_sv, c_int, [POINTER(svm_model)])
fillprototype(libsvm.svm_get_svr_probability, c_double, [POINTER(svm_model)])

fillprototype(libsvm.svm_predict_values, c_double, [POINTER(svm_model), POINTER(svm_node), POINTER(c_double)])
fillprototype(libsvm.svm_predict, c_double, [POINTER(svm_model), POINTER(svm_node)])
fillprototype(libsvm.svm_predict_probability, c_double, [POINTER(svm_model), POINTER(svm_node), POINTER(c_double)])

fillprototype(libsvm.svm_free_model_content, None, [POINTER(svm_model)])
fillprototype(libsvm.svm_free_and_destroy_model, None, [POINTER(POINTER(svm_model))])
fillprototype(libsvm.svm_destroy_param, None, [POINTER(svm_parameter)])

fillprototype(libsvm.svm_check_parameter, c_char_p, [POINTER(svm_problem), POINTER(svm_parameter)])
fillprototype(libsvm.svm_check_probability_model, c_int, [POINTER(svm_model)])
fillprototype(libsvm.svm_set_print_string_function, None, [PRINT_STRING_FUN])

# ignore
</t>
<t tx="karstenw.20230303140429.1">#!/usr/bin/env python

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303140440.1"></t>
<t tx="karstenw.20230303140443.1"></t>
<t tx="karstenw.20230303140457.1"></t>
<t tx="karstenw.20230303141215.1">#### PATTERN | WEB #############################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

################################################################################
# Python API interface for various web services (Google, Twitter, Wikipedia, …)

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303141229.1">from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int, chr
from builtins import map, filter, zip
from builtins import object, range, next

from .utils import get_url_query, get_form_action, stringify_values, json_iter_parse

import os
import sys
import threading
import time
import socket
import requests
import datetime
import ssl

from io import open

try:
    # Python 3
    from urllib.parse import urlparse, urljoin, urlsplit, urlencode, quote_plus, unquote_plus
    from urllib.request import urlopen, Request, HTTPHandler, HTTPRedirectHandler, ProxyHandler, HTTPCookieProcessor, install_opener, build_opener
    from urllib.error import HTTPError as UrllibHTTPError
    from urllib.error import URLError as UrllibURLError
except ImportError:
    # Python 2
    from urlparse import urlparse, urljoin, urlsplit
    from urllib import urlencode, quote_plus, unquote_plus
    from urllib2 import urlopen, Request, HTTPHandler, HTTPRedirectHandler, ProxyHandler, HTTPCookieProcessor, install_opener, build_opener
    from urllib2 import HTTPError as UrllibHTTPError
    from urllib2 import URLError as UrllibURLError

import base64

from html.entities import name2codepoint

try:
    # Python 2
    import httplib
except ImportError:
    # Python 3
    import http.client as httplib

from html.parser import HTMLParser as _HTMLParser

try:
    # Python 3
    import cookielib
except ImportError:
    # Python 2
    import http.cookiejar as cookielib
import re
import xml.dom.minidom
import unicodedata
import string
try:
    # Python 2
    from cStringIO import StringIO
except ImportError:
    # Python 3
    from io import StringIO
import bisect
import itertools
try:
    # Python 2
    import new
except ImportError:
    # Python 3: We don't actually need it (in this case)
    new = None
import feedparser
import json

from . import api
from . import oauth
from . import locale

import bs4 as BeautifulSoup

# import pdb

try:
    # Import persistent Cache.
    # If this module is used separately,
    # a dict is used (i.e. this Python session only).
    from .cache import Cache, cache, TMP
except:
    cache = {}

try:
    from .imap import Mail, MailFolder, Message, GMAIL
    from .imap import MailError, MailServiceError, MailLoginError, MailNotLoggedIn
    from .imap import FROM, SUBJECT, DATE, BODY, ATTACHMENTS
except:
    pass

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

#### UNICODE #######################################################################################
# Latin-1 (ISO-8859-1) encoding is identical to Windows-1252 except for the code points 128-159:
# Latin-1 assigns control codes in this range, Windows-1252 has characters, punctuation, symbols
# assigned to these code points.

from pattern.helpers import encode_string, decode_string

u = decode_utf8 = decode_string
s = encode_utf8 = encode_string

GREMLINS = set([
    0x0152, 0x0153, 0x0160, 0x0161, 0x0178, 0x017E, 0x017D, 0x0192, 0x02C6,
    0x02DC, 0x2013, 0x2014, 0x201A, 0x201C, 0x201D, 0x201E, 0x2018, 0x2019,
    0x2020, 0x2021, 0x2022, 0x2026, 0x2030, 0x2039, 0x203A, 0x20AC, 0x2122
])


</t>
<t tx="karstenw.20230303141229.10">@property
def value(self):
    return self._response

</t>
<t tx="karstenw.20230303141229.11">@property
def error(self):
    return self._error

</t>
<t tx="karstenw.20230303141229.12">def __repr__(self):
    return "AsynchronousRequest(function='%s')" % self._function.__name__


</t>
<t tx="karstenw.20230303141229.13">def asynchronous(function, *args, **kwargs):
    """ Returns an AsynchronousRequest object for the given function.
    """
    return AsynchronousRequest(function, *args, **kwargs)

send = asynchronous

#### URL ###########################################################################################

# User agent and referrer.
# Used to identify the application accessing the web.
USER_AGENT = "Pattern/2.6 +http://www.clips.ua.ac.be/pattern"
REFERRER = "http://www.clips.ua.ac.be/pattern"

# Mozilla user agent.
# Websites can include code to block out any application except browsers.
MOZILLA = "Mozilla/5.0"

# HTTP request method.
GET = "get"  # Data is encoded in the URL.
POST = "post" # Data is encoded in the message body.

# URL parts.
# protocol://username:password@domain:port/path/page?query_string#anchor
PROTOCOL, USERNAME, PASSWORD, DOMAIN, PORT, PATH, PAGE, QUERY, ANCHOR = \
    "protocol", "username", "password", "domain", "port", "path", "page", "query", "anchor"

# MIME type.
MIMETYPE_WEBPAGE    = ["text/html"]
MIMETYPE_STYLESHEET = ["text/css"]
MIMETYPE_PLAINTEXT  = ["text/plain"]
MIMETYPE_PDF        = ["application/pdf"]
MIMETYPE_NEWSFEED   = ["application/rss+xml", "application/atom+xml"]
MIMETYPE_IMAGE      = ["image/gif", "image/jpeg", "image/png", "image/tiff"]
MIMETYPE_AUDIO      = ["audio/mpeg", "audio/mp4", "audio/x-aiff", "audio/x-wav"]
MIMETYPE_VIDEO      = ["video/mpeg", "video/mp4", "video/avi", "video/quicktime", "video/x-flv"]
MIMETYPE_ARCHIVE    = ["application/x-stuffit", "application/x-tar", "application/zip"]
MIMETYPE_SCRIPT     = ["application/javascript", "application/ecmascript"]


</t>
<t tx="karstenw.20230303141229.14">def extension(filename):
    """ Returns the extension in the given filename: "cat.jpg" =&gt; ".jpg".
    """
    return os.path.splitext(filename)[1]


</t>
<t tx="karstenw.20230303141229.15">def urldecode(query):
    """ Inverse operation of urlencode.
        Returns a dictionary of (name, value)-items from a URL query string.
    """
    def _format(s):
        if s == "" or s == "None":
            return None
        if s.lstrip("-").isdigit():
            return int(s)
        try:
            return float(s)
        except:
            return s
    if query:
        query = query.lstrip("?").split("&amp;")
        query = ((kv.split("=") + [None])[:2] for kv in query)
        if sys.version &gt; "3":
            query = ((u(unquote_plus(k)), _format(u(unquote_plus(v)))) for k, v in query if k != "")
        else:
            query = ((u(unquote_plus(bytestring(k))), _format(u(unquote_plus(bytestring(v))))) for k, v in query if k != "")
        return dict(query)
    return {}

url_decode = urldecode


</t>
<t tx="karstenw.20230303141229.16">def proxy(host, protocol="https"):
    """ Returns the value for the URL.open() proxy parameter.
        - host: host address of the proxy server.
    """
    return (host, protocol)


</t>
<t tx="karstenw.20230303141229.17">class Error(Exception):
    """ Base class for pattern.web errors.
    """

    @others
</t>
<t tx="karstenw.20230303141229.18">def __init__(self, *args, **kwargs):
    Exception.__init__(self, *args)
    self.src = kwargs.pop("src", None)
    self.url = kwargs.pop("url", None)

</t>
<t tx="karstenw.20230303141229.19">@property
def headers(self):
    return dict(list(self.src.headers.items()))


</t>
<t tx="karstenw.20230303141229.2">def fix(s, ignore=""):
    """ Returns a Unicode string that fixes common encoding problems (Latin-1, Windows-1252).
        For example: fix("clichÃ©") =&gt; "cliché".
    """
    # http://blog.luminoso.com/2012/08/20/fix-unicode-mistakes-with-python/
    if not isinstance(s, str):
        s = s.decode("utf-8")
        # If this doesn't work,
        # copy &amp; paste string in a Unicode .txt,
        # and then pass open(f).read() to fix().
    u = []
    i = 0
    for j, ch in enumerate(s):
        if ch in ignore:
            continue
        if ord(ch) &lt; 128: # ASCII
            continue
        if ord(ch) in GREMLINS:
            ch = ch.encode("windows-1252")
        else:
            try:
                ch = ch.encode("latin-1")
            except:
                ch = ch.encode("utf-8")
        u.append(s[i:j].encode("utf-8"))
        u.append(ch)
        i = j + 1
    u.append(s[i:].encode("utf-8"))
    u = b"".join(u)
    u = u.decode("utf-8", "replace")
    u = u.replace("\n", "\n ")
    u = u.split(" ")
    # Revert words that have the replacement character,
    # i.e., fix("cliché") should not return "clich�".
    for i, (w1, w2) in enumerate(zip(s.split(" "), u)):
        if "\ufffd" in w2: # �
            u[i] = w1
    u = " ".join(u)
    u = u.replace("\n ", "\n")
    return u


</t>
<t tx="karstenw.20230303141229.20">class URLError(Error):
    pass # URL contains errors (e.g. a missing t in htp://).


</t>
<t tx="karstenw.20230303141229.21">class URLTimeout(URLError):
    pass # URL takes to long to load.


</t>
<t tx="karstenw.20230303141229.22">class HTTPError(URLError):
    pass # URL causes an error on the contacted server.


</t>
<t tx="karstenw.20230303141229.23">class HTTP301Redirect(HTTPError):
    pass # Too many redirects.
         # The site may be trying to set a cookie and waiting for you to return it,
         # or taking other measures to discern a browser from a script.
         # For specific purposes you should build your own urllib2.HTTPRedirectHandler
         # and pass it to urllib2.build_opener() in URL.open()


</t>
<t tx="karstenw.20230303141229.3">def latin(s):
    """ Returns True if the string contains only Latin-1 characters
        (no Chinese, Japanese, Arabic, Cyrillic, Hebrew, Greek, ...).
    """
    if not isinstance(s, str):
        s = s.decode("utf-8")
    return all(unicodedata.name(ch).startswith("LATIN") for ch in s if ch.isalpha())

# For clearer source code:
bytestring = b = s

#### ASYNCHRONOUS REQUEST ##########################################################################


</t>
<t tx="karstenw.20230303141229.4">class AsynchronousRequest(object):

    @others
</t>
<t tx="karstenw.20230303141229.5">def __init__(self, function, *args, **kwargs):
    """ Executes the function in the background.
        AsynchronousRequest.done is False as long as it is busy, but the program will not halt in the meantime.
        AsynchronousRequest.value contains the function's return value once done.
        AsynchronousRequest.error contains the Exception raised by an erronous function.
        For example, this is useful for running live web requests while keeping an animation running.
        For good reasons, there is no way to interrupt a background process (i.e. Python thread).
        You are responsible for ensuring that the given function doesn't hang.
    """
    self._response = None # The return value of the given function.
    self._error = None # The exception (if any) raised by the function.
    self._time = time.time()
    self._function = function
    self._thread = threading.Thread(target=self._fetch, args=(function,) + args, kwargs=kwargs)
    self._thread.start()

</t>
<t tx="karstenw.20230303141229.6">def _fetch(self, function, *args, **kwargs):
    """ Executes the function and sets AsynchronousRequest.response.
    """
    try:
        self._response = function(*args, **kwargs)
    except Exception as e:
        self._error = e

</t>
<t tx="karstenw.20230303141229.7">def now(self):
    """ Waits for the function to finish and yields its return value.
    """
    self._thread.join()
    return self._response

</t>
<t tx="karstenw.20230303141229.8">@property
def elapsed(self):
    return time.time() - self._time

</t>
<t tx="karstenw.20230303141229.9">@property
def done(self):
    # pdb.set_trace()
    return not self._thread.is_alive()

</t>
<t tx="karstenw.20230303141230.1">class HTTP400BadRequest(HTTPError):
    pass # URL contains an invalid request.


</t>
<t tx="karstenw.20230303141230.10">class URL(object):

    @others
</t>
<t tx="karstenw.20230303141230.100">def search(self, query, type=SEARCH, start=None, count=None, sort=RELEVANCY, size=None, cached=True, **kwargs):
    """" Returns a list of results from DuckDuckGo for the given query.
    """
    if type != SEARCH:
        raise SearchEngineTypeError
    # 1) Construct request URL.
    url = URL(DUCKDUCKGO, method=GET, query={
        "q": query,
        "o": "json"
    })
    # 2) Restrict language.
    if type == SEARCH and self.language is not None:
        url.query["kl"] = self.language
    # 3) Parse JSON response.
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    data = url.download(cached=cached, **kwargs)
    data = json.loads(data)
    results = Results(DUCKDUCKGO, query, type)
    results.total = None
    for x in data.get("Results", []):
        if x.get("FirstURL"):
            r = Result(url=None)
            # Parse official website link.
            r.url = self.format(x.get("FirstURL"))
            r.title = self.format(data.get("Heading"))
            r.text = self.format(data.get("Abstract"))
            r.author = self.format(data.get("AbstractSource"))
            r.type = self.format(REFERENCE)
            results.append(r)
    for topic in data.get("RelatedTopics", []):
        for x in topic.get("Topics", [topic]):
            r = Result(url=None)
            r.url = x.get("FirstURL")
            # Parse title and type from URL (e.g., http://duckduckgo.com/d/Cats?kl=en).
            m = re.match(r"^http://duckduckgo.com/([a-z]/)?(.*?)(\?|$)", r.url)
            # Parse title: "Cats".
            s1 = m and m.group(2) or "" # Title: "Cats"
            s1 = u(decode_url(s1.encode("utf-8")))
            s1 = s1.strip().replace("_", " ")
            s1 = s1[:1].upper() + s1[1:]
            # Parse description; the part before the first "-" or "," was the link.
            s2 = x.get("Text", "").strip()
            s2 = re.sub(r" +", " ", s2)
            s2 = s2[:1].upper() + s2[1:] or ""
            s2 = s2.startswith(s1) \
                and "&lt;a href=\"%s\"&gt;%s&lt;/a&gt;%s" % (r.url, s1, s2[len(s1):]) \
                 or re.sub(r"^(.*?)( - | or |, )(.*?)", "&lt;a href=\"%s\"&gt;\\1&lt;/a&gt;\\2\\3" % r.url, s2)
            # Parse type: "d/" =&gt; "definition".
            s3 = m and m.group(1) or ""
            s3 = {"c": CATEGORY, "d": DEFINITION}.get(s3.rstrip("/"), "")
            s3 = topic.get("Name", "").lower() or s3
            s3 = re.sub("^in ", "", s3)
            # Format result.
            r.url = self.format(r.url)
            r.title = self.format(s1)
            r.text = self.format(s2)
            r.type = self.format(s3)
            results.append(r)
    return results

</t>
<t tx="karstenw.20230303141230.101">def answer(self, string, **kwargs):
    """ Returns a DuckDuckGo answer for the given string (e.g., math, spelling, ...)
    """
    url = URL(DUCKDUCKGO, method=GET, query={
        "q": string.encode("utf-8"),
        "o": "json"
    })
    kwargs.setdefault("cached", False)
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    data = url.download(**kwargs)
    data = json.loads(data)
    data = data.get(kwargs.get("field", "Answer"))
    return u(data)

</t>
<t tx="karstenw.20230303141230.102">def spelling(self, string):
    """ Returns a list of spelling suggestions for the given string.
    """
    s = self.answer("spell " + string, cached=True)
    s = re.findall(r"&lt;a.*?&gt;(.*?)&lt;/a&gt;", s)
    return s

</t>
<t tx="karstenw.20230303141230.103">def definition(self, string):
    """ Returns a dictionary definition for the given string.
    """
    s = self.answer(string, field="Definition", cached=True)
    s = re.sub(r"^.*? definition: ", "", s)
    s = re.sub(r"(^'''.*?''' |^)(.)(.*?)$",
        lambda m: m.group(1) + m.group(2).upper() + m.group(3), s)
    return s

</t>
<t tx="karstenw.20230303141230.104">class Faroo(SearchEngine):

    @others
#--- VKontakte -------------------------------------------------------------------------------------
# VKontakte is a Russian online social media and social networking service. It is available in several
# languages but it is especially popular among Russian-speaking users. VK had at least 477 million accounts
# and ss of May 2018 the site is available in 86 languages




# API Error Codes
AUTHORIZATION_FAILED = 5    # Invalid access token
PERMISSION_IS_DENIED = 7
ACCESS_DENIED = 15          # No access to call this method

LOGIN_URL = 'https://m.vk.com'
AUTHORIZE_URL = 'https://oauth.vk.com/authorize'
API_URL = 'https://api.vk.com/method/'

VKAPI_VERSION = '5.80'

</t>
<t tx="karstenw.20230303141230.105">def __init__(self, license = None, throttle = 0.5, language = None):
    SearchEngine.__init__(self, license or FAROO_LICENSE, throttle, language)

</t>
<t tx="karstenw.20230303141230.106">def search(self, query, type = SEARCH, start = 1, count = 10, sort = RELEVANCY, size = None, cached = True, **kwargs):
    if type != SEARCH:
        raise SearchEngineTypeError

    # 1) Create request URL.
    url = URL(FAROO, method=GET, query={
             "q" : query.replace(" ", "+"),
         "start" : 1 + (start - 1) * count,
        "length" : count,
           "key" : self.license or FAROO_LICENSE,
             "f" : "json"
    })
    # 2) Restrict language
    if self.language in ('en', 'de', 'zh'):
        url.query["language"] = self.language
    else:
        raise SearchEngineError("Language not supported")
    # 3) Parse JSON response.
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    try:
        data = url.download(cached = cached, user_agent = "*", **kwargs)
    except HTTP401Authentication:
        raise HTTP401Authentication("Faroo %s API requires an API key" % type)
    except HTTP403Forbidden:
        raise SearchEngineLimitError
    data = json.loads(data)
    results = Results(FAROO, query, type)
    results.total = int(data.get("count") or 0)
    for x in data.get("results", []):
        r = Result(url=None)
        r.url = self.format(x.get("url"))
        r.title = self.format(x.get("title"))
        r.text = self.format(x.get("kwic"))
        r.date = self.format(x.get("date"))
        r.author = self.format(x.get("author"))
        r.language = self.format(self.language or "")
        results.append(r)
    return results





</t>
<t tx="karstenw.20230303141230.107">class VKAPI(object):
    @others
</t>
<t tx="karstenw.20230303141230.108">def __init__(self, *args, **kwargs):
    session_class = kwargs.pop('session_class', Session)
    self.session = session_class(*args, **kwargs)

</t>
<t tx="karstenw.20230303141230.109">def __getattr__(self, method_name):
    return VKRequest(self, method_name)

</t>
<t tx="karstenw.20230303141230.11">def __init__(self, string="", method=GET, query={}, **kwargs):
    """ URL object with the individual parts available as attributes:
        For protocol://username:password@domain:port/path/page?query_string#anchor:
        - URL.protocol: http, https, ftp, ...
        - URL.username: username for restricted domains.
        - URL.password: password for restricted domains.
        - URL.domain  : the domain name, e.g. nodebox.net.
        - URL.port    : the server port to connect to.
        - URL.path    : the server path of folders, as a list, e.g. ['news', '2010']
        - URL.page    : the page name, e.g. page.html.
        - URL.query   : the query string as a dictionary of (name, value)-items.
        - URL.anchor  : the page anchor.
        If method is POST, the query string is sent with HTTP POST.
    """
    self.__dict__["method"] = method # Use __dict__ directly since __setattr__ is overridden.
    self.__dict__["_string"] = u(string)
    self.__dict__["_parts"] = None
    self.__dict__["_headers"] = None
    self.__dict__["_redirect"] = None
    if isinstance(string, URL):
        self.__dict__["method"] = string.method
        self.query.update(string.query)
    if len(query) &gt; 0:
        # Requires that we parse the string first (see URL.__setattr__).
        self.query.update(query)
    if len(kwargs) &gt; 0:
        # Requires that we parse the string first (see URL.__setattr__).
        self.parts.update(kwargs)

</t>
<t tx="karstenw.20230303141230.110">def __call__(self, method_name, **method_kwargs):
    return getattr(self, method_name)(**method_kwargs)


</t>
<t tx="karstenw.20230303141230.111">class VKRequest(object):
    __slots__ = ('_api', '_method_name', '_method_args')

    @others
</t>
<t tx="karstenw.20230303141230.112">def __init__(self, api, method_name):
    self._api = api
    self._method_name = method_name
    self._method_args = {}

</t>
<t tx="karstenw.20230303141230.113">def __getattr__(self, method_name):
    return VKRequest(self._api, self._method_name + '.' + method_name)

</t>
<t tx="karstenw.20230303141230.114">def __call__(self, **method_args):
    self._method_args = method_args
    return self._api.session.make_request(self)


</t>
<t tx="karstenw.20230303141230.115">class VK(SearchEngine):

    '''
    The instruction to get access_token:

    We will consider the authorization method in the social network VKontakte by a direct link through the VKontakte API (based on the OAuth protocol), called Implicit flow. Authorization by this method is performed through the VKontakte application specified in the form of an ID. This is the most secure method of authorization.

    The method of obtaining a token is to go through a special link containing the ID of created VKontakte application:
    https://oauth.vk.com/authorize?
    client_id=APP_ID&amp;scope=notify,photos,friends,audio,video,notes,pages,docs,status,questions,offers,wall,groups,messages,notifications,stats,ads,offline&amp;redirect_uri=http://api.vk.com/blank.html&amp;display=page&amp;response_type=token

    So, where to get the APP_ID?

    Receiving a token through its own application:

    1)register in VK - https://vk.com
    2)go to the application manage: https://vk.com/apps?act=manage
    3)create new application (chose standalone application)
    4)next, you need to approve the application by getting a confirmation code on your phone and typing it in a special field. Also, during the application approval process, you can link your mobile device to the VKontakte account. To do this, click the «Link Device» button. Otherwise, just click on the «Confirm via SMS» link without attaching the device to the page.
    5)After confirmation, you will see a page with information about the created application. In the left menu, click on the "Settings" item. There you can find the client_id, that is, the ID of your VKontakte application.
    6)This ID needs to be copied and pasted into our link instead of APP_ID. It should look something like this:

    https://oauth.vk.com/authorize?
    client_id=123456&amp;scope=notify,photos,friends,audio,video,notes,pages,docs,status,questions,offers,wall,groups,messages,notifications,stats,ads,offline&amp;redirect_uri=http://api.vk.com/blank.html&amp;display=page&amp;response_type=token

    7)Next, if you need to get the access key, you just need to go to this link.
    The link with access token will look like:

    http://api.vk.com/blank.html#access_token=*****&amp;expires_in=0&amp;user_id=*****


    Limits:
    The VKontakte API methods with the user access key can be accessed no more often than 3 times per second

    Number of requests limits:

    newsfeed.search — 1000 per day;
    wall.search — 1000 per day;
    wall.get — 5000 per day.
    '''

    NUM_REQUESTS_PER_SECOND = 3
    MAX_REQUEST_POSTS = 100
    MAX_NEWSFEED_POSTS = 200
    NEWSFEED_TIME_DELTA = 1000

    @others
</t>
<t tx="karstenw.20230303141230.116">def __init__(self, lisense, throttle=1.0, language=None):
    SearchEngine.__init__(self, lisense, throttle, language)

    self.access_token = lisense
    self.session = Session(access_token=lisense)
    self.api = VKAPI(self.session)

</t>
<t tx="karstenw.20230303141230.117">def _get_city_name_by_id(self, city_id):
    return self.api.database.getCitiesById(city_ids=[city_id], version=VKAPI_VERSION,
                                           access_token=self.access_token, lang=self.language)[0]['name']

</t>
<t tx="karstenw.20230303141230.118">def _get_country_name_by_id(self, country_id):
    return self.api.database.getCountriesById(country_ids=[country_id], version=VKAPI_VERSION,
                                              access_token=self.access_token, lang=self.language)[0]['name']

</t>
<t tx="karstenw.20230303141230.119">def _get_sex_by_id(self, sex_id):
    return 'f' if sex_id == 1 else 'm'

</t>
<t tx="karstenw.20230303141230.12">def _parse(self):
    """ Parses all the parts of the URL string to a dictionary.
        URL format: protocal://username:password@domain:port/path/page?querystring#anchor
        For example: http://user:pass@example.com:992/animal/bird?species=seagull&amp;q#wings
        This is a cached method that is only invoked when necessary, and only once.
    """
    p = urlsplit(self._string)
    P = {PROTOCOL: p[0],            # http
         USERNAME: "",             # user
         PASSWORD: "",             # pass
           DOMAIN: p[1],            # example.com
             PORT: "",             # 992
             PATH: p[2],            # [animal]
             PAGE: "",             # bird
            QUERY: urldecode(p[3]), # {"species": "seagull", "q": None}
           ANCHOR: p[4]             # wings
    }

    # Split the username and password from the domain.
    if "@" in P[DOMAIN]:
        P[USERNAME], \
        P[PASSWORD] = (p[1].split("@")[0].split(":") + [""])[:2]
        P[DOMAIN] = p[1].split("@")[1]
    # Split the port number from the domain.
    if ":" in P[DOMAIN]:
        P[DOMAIN], \
        P[PORT] = P[DOMAIN].split(":")
        P[PORT] = P[PORT].isdigit() and int(P[PORT]) or P[PORT]
    # Split the base page from the path.
    if "/" in P[PATH]:
        P[PAGE] = p[2].split("/")[-1]
        P[PATH] = p[2][:len(p[2]) - len(P[PAGE])].strip("/").split("/")
        P[PATH] = list(filter(lambda v: v != "", P[PATH]))
    else:
        P[PAGE] = p[2].strip("/")
        P[PATH] = []
    self.__dict__["_parts"] = P

# URL.string yields unicode(URL) by joining the different parts,
# if the URL parts have been modified.
</t>
<t tx="karstenw.20230303141230.120">def get_users_info(self, user_ids):
    """
    :param user_ids:  the list of user ids or their short names
    :return: the list of dicts, each dict contains the information about user:

    - uid
    - first_name
    - last_name
    -
    Additional fields:
    - sex
    - bdate
    - city
    - country
    - nickname
    - status
    - followers_count
    - photo_max_orig
    - screen_name
    - counters (number of different items):
           'albums',
           'videos',
           'audios',
           'notes',
           'photos',
           'friends',
           'online_friends',
           'mutual_friends',
           'followers',
           'subscriptions',
           'pages'

    Limits: the number of user_ids elements should be no more than 1000

    """

    fields = "sex, bdate, city, country, nickname, status, followers_count, counters, photo_max_orig, screen_name"

    users_info = self.api.users.get(user_ids=user_ids, fields=fields, v=VKAPI_VERSION, access_token=self.access_token)

    for i, user in enumerate(users_info):
        users_info[i]['city'] = self._get_city_name_by_id(int(user['city']['id']))
        users_info[i]['country'] = self._get_country_name_by_id(int(user['country']['id']))
        users_info[i]['sex'] = self._get_sex_by_id(user['sex'])

    return users_info

</t>
<t tx="karstenw.20230303141230.121">def get_user_posts(self, user_id, count=100, posts_type="all"):

    """
    :param user_id: user_id or user short name
    :return: posts from users' wall. You can set the post_type parameter to filter user posts:

    - suggests
    - postponed
    - owner — wall owner posts
    - others — other users posts
    - all — all posts on the wall (owner + others).

    """

    if not isinstance(user_id, int):
        user_info = self.api.users.get(user_ids=[user_id], v=VKAPI_VERSION, access_token=self.access_token)
        user_id = user_info[0]['id']

    offset = 0
    user_posts = []
    num_requests = 0

    while count &gt; 0:
        amount = min(self.MAX_REQUEST_POSTS, count)
        posts = self.api.wall.get(owner_id=user_id, version=VKAPI_VERSION, offset=offset, filter=posts_type,
                                  count=amount, access_token=self.access_token)
        # if there are no posts on the wall
        if posts[0] == 0:
            break

        count -= len(posts) - 1
        offset += len(posts) - 1
        user_posts.extend(posts[1:])
        num_requests += 1

        # if there are no other posts on the wall
        if offset == posts[0]:
            break

        if num_requests % self.NUM_REQUESTS_PER_SECOND == 0:
            time.sleep(self.throttle)

    return user_posts

</t>
<t tx="karstenw.20230303141230.122">def get_newsfeed_posts(self, query, count=100, end_time=None):
    """
    :param query: search request
    :param count: number of posts to be downloaded from newsfeed
    :param end_time: time in "yyyy.mm.dd" format, to which you want to receive news.
                    If the parameter is not specified, then it is considered equal to the current time.
    :return: posts from the newsfeed that match the given query
    """

    loaded = 0
    newsfeed_posts = []
    num_requests = 0

    if end_time is None:
        end_time = int(time.time())
    else:
        year, month, day = list(map(int, end_time.split(".")))
        end_time = time.mktime(datetime.date(year, month, day).timetuple())

    # we consider the last month as the max time period
    min_time = end_time - 2678401

    while count &gt; 0 and end_time &gt; min_time:
        amount = min(self.MAX_NEWSFEED_POSTS, count)
        posts = self.api.newsfeed.search(extended=1, q=query, v=VKAPI_VERSION, count=amount, end_time=end_time,
                                             start_time=end_time - self.NEWSFEED_TIME_DELTA, access_token=self.access_token)

        batch_len = len(posts['items'])
        loaded += batch_len
        count -= batch_len
        newsfeed_posts.extend(posts['items'])

        if len(posts['items']) &gt; 0:
            end_time = posts['items'][-1]['date']
        else:
            end_time -= self.NEWSFEED_TIME_DELTA

        num_requests += 1
        if num_requests % self.NUM_REQUESTS_PER_SECOND == 0:
            time.sleep(self.throttle)

    return newsfeed_posts


</t>
<t tx="karstenw.20230303141230.123">class VkException(Exception):
    pass


</t>
<t tx="karstenw.20230303141230.124">class VkAuthError(VkException):
    pass


</t>
<t tx="karstenw.20230303141230.125">class VkAPIError(VkException):
    __slots__ = ['error', 'code', 'message', 'request_params', 'redirect_uri']

    CAPTCHA_NEEDED = 14
    ACCESS_DENIED = 15

    @others
</t>
<t tx="karstenw.20230303141230.126">def __init__(self, error_data):
    super(VkAPIError, self).__init__()
    self.error_data = error_data
    self.code = error_data.get('error_code')
    self.message = error_data.get('error_msg')
    self.request_params = self.get_pretty_request_params(error_data)
    self.redirect_uri = error_data.get('redirect_uri')

</t>
<t tx="karstenw.20230303141230.127">@staticmethod
def get_pretty_request_params(error_data):
    request_params = error_data.get('request_params', ())
    request_params = {param['key']: param['value'] for param in request_params}
    return request_params

</t>
<t tx="karstenw.20230303141230.128">def __str__(self):
    error_message = '{self.code}. {self.message}. request_params = {self.request_params}'.format(self=self)
    if self.redirect_uri:
        error_message += ',\nredirect_uri = "{self.redirect_uri}"'.format(self=self)
    return error_message


</t>
<t tx="karstenw.20230303141230.129">class Session(object):

    @others
#--- TWITTER ---------------------------------------------------------------------------------------
# Twitter is an online social networking service and microblogging service,
# that enables users to post and read text-based messages of up to 280 characters ("tweets").
# https://dev.twitter.com/docs/api/1.1

TWITTER         = "https://api.twitter.com/1.1/"
TWITTER_STREAM  = "https://stream.twitter.com/1.1/statuses/filter.json"
TWITTER_STATUS  = "https://twitter.com/%s/status/%s"
TWITTER_LICENSE = api.license["Twitter"]
TWITTER_HASHTAG = re.compile(r"(\s|^)(#[a-z0-9_\-]+)", re.I)    # Word starts with "#".
TWITTER_MENTION = re.compile(r"(\s|^)(@[a-z0-9_\-]+)", re.I)    # Word starts with "@".
TWITTER_RETWEET = re.compile(r"(\s|^RT )(@[a-z0-9_\-]+)", re.I) # Word starts with "RT @".


</t>
<t tx="karstenw.20230303141230.13">def _get_string(self):
    return str(self)

</t>
<t tx="karstenw.20230303141230.130">def __init__(self, scope='offline', access_token='', timeout=10, **method_default_args):

    self.scope = scope
    self.access_token = access_token
    self.timeout = timeout
    self.method_default_args = method_default_args

    self.auth_session = requests.Session()
    self.requests_session = requests.Session()
    self.requests_session.headers['Accept'] = 'application/json'
    self.requests_session.headers['Content-Type'] = 'application/x-www-form-urlencoded'

</t>
<t tx="karstenw.20230303141230.131">def make_request(self, request):

    response = self.send_api_request(request)
    response.raise_for_status()

    # there are may be 2 dicts in one JSON
    # for example: "{'error': ...}{'response': ...}"
    for response_or_error in json_iter_parse(response.text):
        if 'response' in response_or_error:
            return response_or_error['response']

        elif 'error' in response_or_error:
            print("Request error occured")

</t>
<t tx="karstenw.20230303141230.132">def send_api_request(self, request):

    url = API_URL + request._method_name
    method_args = self.method_default_args.copy()
    method_args.update(stringify_values(request._method_args))
    if self.access_token:
        method_args['access_token'] = self.access_token

    response = self.requests_session.post(url, method_args, timeout=self.timeout)
    return response



</t>
<t tx="karstenw.20230303141230.133">class Twitter(SearchEngine):

    @others
</t>
<t tx="karstenw.20230303141230.134">def __init__(self, license=None, throttle=0.5, language=None):
    SearchEngine.__init__(self, license or TWITTER_LICENSE, throttle, language)
    self._pagination = {}

</t>
<t tx="karstenw.20230303141230.135">def _authenticate(self, url):
    url.query.update({
        "oauth_version": "1.0",
        "oauth_nonce": oauth.nonce(),
        "oauth_timestamp": oauth.timestamp(),
        "oauth_consumer_key": self.license[0],
        "oauth_token": self.license[2][0],
        "oauth_signature_method": "HMAC-SHA1"
    })
    url.query["oauth_signature"] = oauth.sign(url.string.split("?")[0], url.query,
        method = url.method,
        secret = self.license[1],
         token = self.license[2][1]
    )
    return url

</t>
<t tx="karstenw.20230303141230.136">def search(self, query, type=SEARCH, start=1, count=10, sort=RELEVANCY, size=None, cached=False, **kwargs):
    """ Returns a list of results from Twitter for the given query.
        - type : SEARCH,
        - start: Result.id or int,
        - count: maximum 100.
        There is a limit of 150+ queries per 15 minutes.
    """

    def f(v):
        v = v.get('extended_tweet', {}).get('full_text', v.get('full_text', v.get('text', '')))
        return v

    if type != SEARCH:
        raise SearchEngineTypeError
    if not query or count &lt; 1 or (isinstance(start, (int, float)) and start &lt; 1):
        return Results(TWITTER, query, type)
    if not isinstance(start, (int, float)):
        id = int(start) - 1 if start and start.isdigit() else ""
    else:
        if start == 1:
            self._pagination = {}
        if start &lt;= 10000:
            id = (query, kwargs.get("geo"), kwargs.get("date"), int(start) - 1, count)
            id = self._pagination.get(id, "")
        else:
            id = int(start) - 1
    # 1) Construct request URL.
    url = URL(TWITTER + "search/tweets.json?", method=GET)
    url.query = {
           "q": query,
      "max_id": id,
      'tweet_mode': 'extended',
       "count": min(count, 100)
    }
    # 2) Restrict location with geo=(latitude, longitude, radius).
    #    It can also be a (latitude, longitude)-tuple with default radius "10km".
    if "geo" in kwargs:
        url.query["geocode"] = ",".join((map(str, kwargs.pop("geo")) + ["10km"])[:3])
    # 3) Restrict most recent with date="YYYY-MM-DD".
    #    Only older tweets are returned.
    if "date" in kwargs:
        url.query["until"] = kwargs.pop("date")
    # 4) Restrict language.
    url.query["lang"] = self.language or ""
    # 5) Authenticate.
    url = self._authenticate(url)
    # 6) Parse JSON response.
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    try:
        data = url.download(cached=cached, **kwargs)
    except HTTP420Error:
        raise SearchEngineLimitError
    except HTTP429TooMayRequests:
        raise SearchEngineLimitError
    data = json.loads(data)
    results = Results(TWITTER, query, type)
    results.total = None
    for x in data.get("statuses", []):
        r = Result(url=None)
        r.id = self.format(x.get("id_str"))
        r.url = self.format(TWITTER_STATUS % (x.get("user", {}).get("screen_name"), x.get("id_str")))
        r.text = self.format(f(x))
        r.date = self.format(x.get("created_at"))
        r.author = self.format(x.get("user", {}).get("screen_name"))
        r.language = self.format(x.get("metadata", {}).get("iso_language_code"))
        r.shares = self.format(x.get("retweet_count", 0))
        r.profile = self.format(x.get("user", {}).get("profile_image_url")) # Profile picture URL.
        # Fetch original status if retweet is truncated (i.e., ends with "...").
        rt = x.get("retweeted_status", None)
        if rt:
            comment = re.search(r"^(.*? )RT", r.text)
            comment = comment.group(1) if comment else ""
            r.text = self.format("RT @%s: %s" % (rt["user"]["screen_name"], f(rt)))
        results.append(r)
    # Twitter.search(start=id, count=10) takes a tweet.id,
    # and returns 10 results that are older than this id.
    # In the past, start took an int used for classic pagination.
    # However, new tweets may arrive quickly,
    # so that by the time Twitter.search(start=2) is called,
    # it will yield results from page 1 (or even newer results).
    # For backward compatibility, we keep page cache,
    # that remembers the last id for a "page" for a given query,
    # when called in a loop.
    #
    # Store the last id retrieved.
    # If search() is called again with start+1, start from this id.
    if isinstance(start, (int, float)):
        k = (query, kwargs.get("geo"), kwargs.get("date"), int(start), count)
        if results:
            self._pagination[k] = str(int(results[-1].id) - 1)
        else:
            self._pagination[k] = id
    return results

</t>
<t tx="karstenw.20230303141230.137">def profile(self, query, start=1, count=10, **kwargs):
    """ Returns a list of results for the given author id, alias or search query.
    """
    # 1) Construct request URL.
    url = URL(TWITTER + "users/search.json?", method=GET, query={
           "q": query,
        "page": start,
       "count": count
    })
    url = self._authenticate(url)
    # 2) Parse JSON response.
    kwargs.setdefault("cached", True)
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    try:
        data = URL(url).download(**kwargs)
        data = json.loads(data)
    except HTTP400BadRequest:
        return []
    return [
        Result(url = "https://www.twitter.com/" + x.get("screen_name", ""),
                id = x.get("id_str", ""),              # 14898655
            handle = x.get("screen_name", ""),         # tom_de_smedt
              name = x.get("name", ""),                # Tom De Smedt
              text = x.get("description", ""),         # Artist, scientist, software engineer
          language = x.get("lang", ""),                # en
              date = x.get("created_at"),              # Sun May 10 10:00:00
            locale = x.get("location", ""),            # Belgium
           picture = x.get("profile_image_url", ""),   # http://pbs.twimg.com/...
           friends = int(x.get("followers_count", 0)), # 100
             posts = int(x.get("statuses_count", 0))   # 100
        ) for x in data
    ]

</t>
<t tx="karstenw.20230303141230.138">def trends(self, **kwargs):
    """ Returns a list with 10 trending topics on Twitter.
    """
    # 1) Construct request URL.
    url = URL("https://api.twitter.com/1.1/trends/place.json?id=1")
    url = self._authenticate(url)
    # 2) Parse JSON response.
    kwargs.setdefault("cached", False)
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    try:
        data = url.download(**kwargs)
        data = json.loads(data)
    except HTTP400BadRequest:
        return []
    return [u(x.get("name")) for x in data[0].get("trends", [])]

</t>
<t tx="karstenw.20230303141230.139">def stream(self, query, **kwargs):
    """ Returns a live stream of Result objects for the given query.
    """
    url = URL(TWITTER_STREAM)
    url.query["track"] = query
    url = self._authenticate(url)
    return TwitterStream(url, delimiter="\n", format=self.format, **kwargs)


</t>
<t tx="karstenw.20230303141230.14">def _set_string(self, v):
    self.__dict__["_string"] = u(v)
    self.__dict__["_parts"] = None

string = property(_get_string, _set_string)

</t>
<t tx="karstenw.20230303141230.140">class TwitterStream(Stream):

    @others
</t>
<t tx="karstenw.20230303141230.141">def __init__(self, socket, delimiter="\n", format=lambda s: s, **kwargs):
    kwargs.setdefault("timeout", 30)
    Stream.__init__(self, socket, delimiter, **kwargs)
    self.format = format

</t>
<t tx="karstenw.20230303141230.142">def parse(self, data):

    """ TwitterStream.queue will populate with Result objects as
        TwitterStream.update() is called iteratively.
    """
    def f(v):
        v = v.get('extended_tweet', {}).get('full_text', v.get('full_text', v.get('text', '')))
        return v

    if data.strip():
        x = json.loads(data)
        r = Result(url=None)
        r.id = self.format(x.get("id_str"))
        r.url = self.format(TWITTER_STATUS % (x.get("user", {}).get("screen_name"), x.get("id_str")))
        r.text = self.format(f(x))
        r.date = self.format(x.get("created_at"))
        r.author = self.format(x.get("user", {}).get("screen_name"))
        r.language = self.format(x.get("metadata", {}).get("iso_language_code"))
        r.shares = self.format(x.get("retweet_count", 0))
        r.profile = self.format(x.get("user", {}).get("profile_image_url")) # Profile picture URL.
        # Fetch original status if retweet is truncated (i.e., ends with "...").
        rt = x.get("retweeted_status", None)
        if rt:
            comment = re.search(r"^(.*? )RT", r.text)
            comment = comment.group(1) if comment else ""
            r.text = self.format("RT @%s: %s" % (rt["user"]["screen_name"], f(rt)))
        return r


</t>
<t tx="karstenw.20230303141230.143">def author(name):
    """ Returns a Twitter query-by-author-name that can be passed to Twitter.search().
        For example: Twitter().search(author("tom_de_smedt"))
    """
    return "from:%s" % name


</t>
<t tx="karstenw.20230303141230.144">def hashtags(string):
    """ Returns a list of hashtags (words starting with a #hash) from a tweet.
    """
    return [b for a, b in TWITTER_HASHTAG.findall(string)]


</t>
<t tx="karstenw.20230303141230.145">def mentions(string):
    """ Returns a list of mentions (words starting with a @author) from a tweet.
    """
    return [b for a, b in TWITTER_MENTION.findall(string)]


</t>
<t tx="karstenw.20230303141230.146">def retweets(string):
    """ Returns a list of retweets (words starting with a RT @author) from a tweet.
    """
    return [b for a, b in TWITTER_RETWEET.findall(string)]

#engine = Twitter()
#for i in range(2):
#    for tweet in engine.search("cat nap", cached=False, start=i+1, count=10):
#        print()
#        print(tweet.id)
#        print(tweet.url)
#        print(tweet.text)
#        print(tweet.author)
#        print(tweet.profile)
#        print(tweet.language)
#        print(tweet.date)
#        print(hashtags(tweet.text))
#        print(retweets(tweet.text))

#stream = Twitter().stream("cat")
#for i in range(10):
#    print(i)
#    stream.update()
#    for tweet in reversed(stream):
#        print(tweet.id)
#        print(tweet.text)
#        print(tweet.url)
#        print(tweet.language)
#    print()
#stream.clear()

#--- MEDIAWIKI -------------------------------------------------------------------------------------
# MediaWiki is a free wiki software application.
# MediaWiki powers popular websites such as Wikipedia, Wiktionary and Wikia.
# http://www.mediawiki.org/wiki/API:Main_page
# http://en.wikipedia.org/w/api.php

WIKIA = "http://wikia.com"
WIKIA = "https://www.fandom.com/explore"


WIKIPEDIA = "https://www.wikipedia.org/"
WIKIPEDIA_LICENSE = api.license["Wikipedia"]
MEDIAWIKI_LICENSE = None
MEDIAWIKI = "http://{SUBDOMAIN}.{DOMAIN}{API}"

# Pattern for meta links (e.g. Special:RecentChanges).
# http://en.wikipedia.org/wiki/Main_namespace
MEDIAWIKI_NAMESPACE = ["Main", "User", "Wikipedia", "File", "MediaWiki", "Template", "Help", "Category", "Portal", "Book"]
MEDIAWIKI_NAMESPACE += [s + " talk" for s in MEDIAWIKI_NAMESPACE] + ["Talk", "Special", "Media"]
MEDIAWIKI_NAMESPACE += ["WP", "WT", "MOS", "C", "CAT", "Cat", "P", "T", "H", "MP", "MoS", "Mos"]
_mediawiki_namespace = re.compile(r"^(" + "|".join(MEDIAWIKI_NAMESPACE) + "):", re.I)

# Pattern to identify disambiguation pages.
MEDIAWIKI_DISAMBIGUATION = "&lt;a href=\"/wiki/Help:Disambiguation\" title=\"Help:Disambiguation\"&gt;disambiguation&lt;/a&gt; page"

# Pattern to identify references, e.g. [12]
MEDIAWIKI_REFERENCE = r"\s*\[[0-9]{1,3}\]"

# Mediawiki.search(type=ALL).
ALL = "all"


</t>
<t tx="karstenw.20230303141230.147">class MediaWiki(SearchEngine):

    @others
</t>
<t tx="karstenw.20230303141230.148">def __init__(self, license=None, throttle=5.0, language="en"):
    SearchEngine.__init__(self, license or MEDIAWIKI_LICENSE, throttle, language)

</t>
<t tx="karstenw.20230303141230.149">@property
def _url(self):
    # Must be overridden in a subclass; see Wikia and Wikipedia.
    return None

</t>
<t tx="karstenw.20230303141230.15">@property
def parts(self):
    """ Yields a dictionary with the URL parts.
    """
    if not self._parts:
        self._parse()
    return self._parts

</t>
<t tx="karstenw.20230303141230.150">@property
def MediaWikiArticle(self):
    return MediaWikiArticle

</t>
<t tx="karstenw.20230303141230.151">@property
def MediaWikiSection(self):
    return MediaWikiSection

</t>
<t tx="karstenw.20230303141230.152">@property
def MediaWikiTable(self):
    return MediaWikiTable

</t>
<t tx="karstenw.20230303141230.153">def __iter__(self):
    return self.articles()

</t>
<t tx="karstenw.20230303141230.154">def articles(self, **kwargs):
    """ Returns an iterator over all MediaWikiArticle objects.
        Optional parameters can include those passed to
        MediaWiki.index(), MediaWiki.search() and URL.download().
    """
    for title in self.index(**kwargs):
        yield self.search(title, **kwargs)

# Backwards compatibility.
all = articles

</t>
<t tx="karstenw.20230303141230.155">def index(self, namespace=0, start=None, count=100, cached=True, **kwargs):
    """ Returns an iterator over all article titles (for a given namespace id).
    """
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    # Fetch article titles (default) or a custom id.
    id = kwargs.pop("_id", "title")
    # Loop endlessly (= until the last request no longer yields an "apcontinue").
    # See: http://www.mediawiki.org/wiki/API:Allpages
    while start != -1:
        url = URL(self._url, method=GET, query={
                 "action": "query",
                   "list": "allpages",
            "apnamespace": namespace,
                 "apfrom": start or "",
                "aplimit": min(count, 500),
          "apfilterredir": "nonredirects",
                 "format": "json"
        })
        data = url.download(cached=cached, **kwargs)
        data = json.loads(data)
        for x in data.get("query", {}).get("allpages", {}):
            if x.get(id):
                yield x[id]
        start = data.get("query-continue", {}).get("allpages", {})
        start = start.get("apcontinue", start.get("apfrom", -1))
    #raise StopIteration

# Backwards compatibility.
list = index

</t>
<t tx="karstenw.20230303141230.156">def search(self, query, type=SEARCH, start=1, count=10, sort=RELEVANCY, size=None, cached=True, **kwargs):
    """ With type=SEARCH, returns a MediaWikiArticle for the given query (case-sensitive).
        With type=ALL, returns a list of results. 
        Each result.title is the title of an article that contains the given query.
    """
    if type not in (SEARCH, ALL, "*"):
        raise SearchEngineTypeError
    if type == SEARCH: # Backwards compatibility.
        return self.article(query, cached=cached, **kwargs)
    if not query or start &lt; 1 or count &lt; 1:
        return Results(self._url, query, type)
    # 1) Construct request URL (e.g., Wikipedia for a given language).
    url = URL(self._url, method=GET, query={
        "action": "query",
          "list": "search",
      "srsearch": query,
      "sroffset": (start - 1) * count,
       "srlimit": min(count, 100),
        "srprop": "snippet",
        "format": "json"
    })
    # 2) Parse JSON response.
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    data = url.download(cached=cached, **kwargs)
    data = json.loads(data)
    data = data.get("query", {})
    results = Results(self._url, query, type)
    results.total = int(data.get("searchinfo", {}).get("totalhits", 0))
    for x in data.get("search", []):
        u = "http://%s/wiki/%s" % (URL(self._url).domain, x.get("title").replace(" ", "_"))
        r = Result(url=u)
        r.id = self.format(x.get("title"))
        r.title = self.format(x.get("title"))
        r.text = self.format(plaintext(x.get("snippet")))
        results.append(r)
    return results

</t>
<t tx="karstenw.20230303141230.157">def article(self, query, cached=True, **kwargs):
    """ Returns a MediaWikiArticle for the given query.
        The query is case-sensitive, for example on Wikipedia:
        - "tiger" = Panthera tigris,
        - "TIGER" = Topologically Integrated Geographic Encoding and Referencing.
    """
    url = URL(self._url, method=GET, query={
        "action": "parse",
          "page": query.replace(" ", "_"),
     "redirects": 1,
        "format": "json"
    })
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("timeout", 30) # Parsing the article takes some time.
    kwargs.setdefault("throttle", self.throttle)
    data = url.download(cached=cached, **kwargs)
    data = json.loads(data)
    data = data.get("parse", {})
    a = self._parse_article(data, query=query)
    a = self._parse_article_sections(a, data)
    a = self._parse_article_section_structure(a)
    if not a.html or "id=\"noarticletext\"" in a.html:
        return None
    return a

</t>
<t tx="karstenw.20230303141230.158">def _parse_article(self, data, **kwargs):
    return self.MediaWikiArticle(
              title = plaintext(data.get("displaytitle", data.get("title", ""))),
             source = data.get("text", {}).get("*", ""),
     disambiguation = data.get("text", {}).get("*", "").find(MEDIAWIKI_DISAMBIGUATION) &gt;= 0,
              links = [x["*"] for x in data.get("links", []) if not _mediawiki_namespace.match(x["*"])],
         categories = [x["*"] for x in data.get("categories", [])],
           external = [x for x in data.get("externallinks", [])],
              media = [x for x in data.get("images", [])],
          redirects = [x for x in data.get("redirects", [])],
          languages = dict([(x["lang"], x["*"]) for x in data.get("langlinks", [])]),
          language = self.language,
             parser = self, **kwargs)

</t>
<t tx="karstenw.20230303141230.159">def _parse_article_sections(self, article, data):
    # If "References" is a section in the article,
    # the HTML will contain a marker &lt;h*&gt;&lt;span class="mw-headline" id="References"&gt;.
    # http://en.wikipedia.org/wiki/Section_editing
    t = article.title
    d = 0
    i = 0
    for x in data.get("sections", {}):
        a = x.get("anchor")
        if a:
            p = r"&lt;h.&gt;\s*.*?\s*&lt;span class=\"mw-headline\" id=\"%s\"&gt;" % a
            p = re.compile(p)
            m = p.search(article.source, i)
            if m:
                j = m.start()
                t = plaintext(t)
                article.sections.append(self.MediaWikiSection(article,
                    title = t,
                    start = i,
                     stop = j,
                    level = d))
                t = plaintext(x.get("line", ""))
                d = int(x.get("level", 2)) - 1
                i = j
    return article

</t>
<t tx="karstenw.20230303141230.16">@property
def querystring(self):
    """ Yields the URL querystring: "www.example.com?page=1" =&gt; "page=1"
    """
    s = self.parts[QUERY].items()
    s = dict((bytestring(k), v if v is not None else "") for k, v in s)
    if sys.version &gt; "3":
        # Python 3
        s = urlencode(s)
    else:
        # Python 2: urlencode() expects byte strings
        t = {key: value.encode("utf-8") if isinstance(value, str) else value for key, value in s.items()}
        s = urlencode(t).decode("utf-8")
    return s

</t>
<t tx="karstenw.20230303141230.160">def _parse_article_section_structure(self, article):
    # Sections with higher level are children of previous sections with lower level.
    for i, s2 in enumerate(article.sections):
        for s1 in reversed(article.sections[:i]):
            if s1.level &lt; s2.level:
                s2.parent = s1
                s1.children.append(s2)
                break
    return article


</t>
<t tx="karstenw.20230303141230.161">class MediaWikiArticle(object):

    @others
</t>
<t tx="karstenw.20230303141230.162">def __init__(self, title="", source="", links=[], categories=[], languages={}, disambiguation=False, **kwargs):
    """ A MediaWiki article returned from MediaWiki.search().
        MediaWikiArticle.string contains the HTML content.
    """
    self.title          = title          # Article title.
    self.source         = source         # Article HTML content.
    self.sections       = []             # Article sections.
    self.links          = links          # List of titles of linked articles.
    self.categories     = categories     # List of categories. As links, prepend "Category:".
    self.external       = []             # List of external links.
    self.media          = []             # List of linked media (images, sounds, ...)
    self.disambiguation = disambiguation # True when the article is a disambiguation page.
    self.languages      = languages      # Dictionary of (language, article)-items, e.g. Cat =&gt; ("nl", "Kat")
    self.language       = kwargs.get("language", "en")
    self.redirects      = kwargs.get("redirects", [])
    self.parser         = kwargs.get("parser", MediaWiki())
    for k, v in kwargs.items():
        setattr(self, k, v)

</t>
<t tx="karstenw.20230303141230.163">def _plaintext(self, string, **kwargs):
    """ Strips HTML tags, whitespace and wiki markup from the HTML source, including:
        metadata, info box, table of contents, annotations, thumbnails, disambiguation link.
        This is called internally from MediaWikiArticle.string.
    """
    s = string
    # Strip meta &lt;table&gt; elements.
    s = strip_element(s, "table", "id=\"toc")             # Table of contents.
    s = strip_element(s, "table", "class=\"infobox")      # Infobox.
    s = strip_element(s, "table", "class=\"navbox")       # Navbox.
    s = strip_element(s, "table", "class=\"mbox")         # Message.
    s = strip_element(s, "table", "class=\"metadata")     # Metadata.
    s = strip_element(s, "table", "class=\".*?wikitable") # Table.
    s = strip_element(s, "table", "class=\"toc")          # Table (usually footer).
    # Strip meta &lt;div&gt; elements.
    s = strip_element(s, "div", "id=\"toc")               # Table of contents.
    s = strip_element(s, "div", "class=\"infobox")        # Infobox.
    s = strip_element(s, "div", "class=\"navbox")         # Navbox.
    s = strip_element(s, "div", "class=\"mbox")           # Message.
    s = strip_element(s, "div", "class=\"metadata")       # Metadata.
    s = strip_element(s, "div", "id=\"annotation")        # Annotations.
    s = strip_element(s, "div", "class=\"dablink")        # Disambiguation message.
    s = strip_element(s, "div", "class=\"magnify")        # Thumbnails.
    s = strip_element(s, "div", "class=\"thumb ")         # Thumbnail captions.
    s = strip_element(s, "div", "class=\"barbox")         # Bar charts.
    s = strip_element(s, "div", "class=\"noprint")        # Hidden from print.
    s = strip_element(s, "sup", "class=\"noprint")
    # Strip absolute elements (don't know their position).
    s = strip_element(s, "div", "style=\"position:absolute")
    # Strip meta &lt;span&gt; elements.
    s = strip_element(s, "span", "class=\"error")
    # Strip math formulas, add [math] placeholder.
    s = re.sub(r"&lt;img class=\"tex\".*?/&gt;", "[math]", s)   # LaTex math images.
    s = plaintext(s, **kwargs)
    # Strip [edit] link (language dependent.)
    s = re.sub(r"\[edit\]\s*", "", s)
    s = re.sub(r"\[%s\]\s*" % {
        "en": "edit",
        "es": "editar código",
        "de": "Bearbeiten",
        "fr": "modifier le code",
        "it": "modifica sorgente",
        "nl": "bewerken",
    }.get(self.language, "edit"), "", s)
    # Insert space before inline references.
    s = s.replace("[", " [").replace("  [", " [")
    # Strip inline references.
    #s = re.sub(r" \[[0-9]+\]", "", s)
    return s

</t>
<t tx="karstenw.20230303141230.164">def plaintext(self, **kwargs):
    return self._plaintext(self.source, **kwargs)

</t>
<t tx="karstenw.20230303141230.165">@property
def html(self):
    return self.source

</t>
<t tx="karstenw.20230303141230.166">@property
def src(self):
    return self.source

</t>
<t tx="karstenw.20230303141230.167">@property
def string(self):
    return self.plaintext()

</t>
<t tx="karstenw.20230303141230.168">def __repr__(self):
    return "MediaWikiArticle(title=%s)" % repr(self.title)


</t>
<t tx="karstenw.20230303141230.169">class MediaWikiSection(object):

    @others
</t>
<t tx="karstenw.20230303141230.17">def __getattr__(self, k):
    if k in self.__dict__:
        return self.__dict__[k]
    if k in self.parts:
        return self.__dict__["_parts"][k]
    raise AttributeError("'URL' object has no attribute '%s'" % k)

</t>
<t tx="karstenw.20230303141230.170">def __init__(self, article, title="", start=0, stop=0, level=1):
    """ A (nested) section in the content of a MediaWikiArticle.
    """
    self.article  = article # MediaWikiArticle the section is part of.
    self.parent   = None    # MediaWikiSection the section is part of.
    self.children = []      # MediaWikiSections belonging to this section.
    self.title    = title   # Section title.
    self._start   = start   # Section start index in MediaWikiArticle.string.
    self._stop    = stop    # Section stop index in MediaWikiArticle.string.
    self._level   = level   # Section depth (main title + intro = level 0).
    self._links   = None
    self._tables  = None

</t>
<t tx="karstenw.20230303141230.171">def plaintext(self, **kwargs):
    return self.article._plaintext(self.source, **kwargs)

</t>
<t tx="karstenw.20230303141230.172">@property
def source(self):
    return self.article.source[self._start:self._stop]

</t>
<t tx="karstenw.20230303141230.173">@property
def html(self):
    return self.source

</t>
<t tx="karstenw.20230303141230.174">@property
def src(self):
    return self.source

</t>
<t tx="karstenw.20230303141230.175">@property
def string(self):
    return self.plaintext()

</t>
<t tx="karstenw.20230303141230.176">@property
def content(self):
    # ArticleSection.string, minus the title.
    s = self.plaintext()
    t = plaintext(self.title)
    if s == t or (len(s) &gt; len(t)) and s.startswith(t) and s[len(t)] not in string.punctuation + " ":
        return s[len(t):].lstrip()
    return s

</t>
<t tx="karstenw.20230303141230.177">@property
def links(self, path="/wiki/"):
    """ Yields a list of Wikipedia links in this section. Similar
        in functionality to MediaWikiArticle.links.
    """
    if self._links is None:
        a = HTMLLinkParser().parse(self.source)
        a = (decode_url(a.url) for a in a)
        a = (a[len(path):].replace("_", " ") for a in a if a.startswith(path))
        a = (a for a in a if not _mediawiki_namespace.match(a))
        self._links = sorted(set(a))
    return self._links

</t>
<t tx="karstenw.20230303141230.178">@property
def tables(self):
    """ Yields a list of MediaWikiTable objects in the section.
    """
    if self._tables is None:
        self._tables = []
        for style in ("wikitable", "sortable wikitable"):
            b = "&lt;table class=\"%s\"" % style, "&lt;/table&gt;"
            p = self.article._plaintext
            f = find_between
            for s in f(b[0], b[1], self.source):
                t = self.article.parser.MediaWikiTable(self,
                     title = p((f(r"&lt;caption.*?&gt;", "&lt;/caption&gt;", s) + [""])[0]),
                    source = b[0] + s + b[1])
                # 1) Parse &lt;td&gt; and &lt;th&gt; content and format it as plain text.
                # 2) Parse &lt;td colspan=""&gt; attribute, duplicate spanning cells.
                # 3) For &lt;th&gt; in the first row, update MediaWikiTable.headers.
                for i, row in enumerate(f(r"&lt;tr", "&lt;/tr&gt;", s)):
                    r1 = f(r"&lt;t[d|h]", r"&lt;/t[d|h]&gt;", row)
                    r1 = (((f(r'colspan="', r'"', v) + [1])[0], v[v.find("&gt;") + 1:]) for v in r1)
                    r1 = ((int(n), v) for n, v in r1)
                    r2 = []
                    [[r2.append(p(v)) for j in range(n)] for n, v in r1]
                    if i == 0 and "&lt;/th&gt;" in row:
                        t.headers = r2
                    else:
                        t.rows.append(r2)
                self._tables.append(t)
    return self._tables

</t>
<t tx="karstenw.20230303141230.179">@property
def level(self):
    return self._level

depth = level

</t>
<t tx="karstenw.20230303141230.18">def __setattr__(self, k, v):
    if k in self.__dict__:
        self.__dict__[k] = u(v)
        return
    if k == "string":
        self._set_string(v)
        return
    if k == "query":
        self.parts[k] = v
        return
    if k in self.parts:
        self.__dict__["_parts"][k] = u(v)
        return
    raise AttributeError("'URL' object has no attribute '%s'" % k)

</t>
<t tx="karstenw.20230303141230.180">def __repr__(self):
    return "MediaWikiSection(title=%s)" % repr(self.title)


</t>
<t tx="karstenw.20230303141230.181">class MediaWikiTable(object):

    @others
#--- MEDIAWIKI: WIKIPEDIA --------------------------------------------------------------------------
# Wikipedia is a collaboratively edited, multilingual, free Internet encyclopedia.
# Wikipedia depends on MediaWiki.


</t>
<t tx="karstenw.20230303141230.182">def __init__(self, section, title="", headers=[], rows=[], source=""):
    """ A &lt;table class="wikitable&gt; in a MediaWikiSection.
    """
    self.section = section # MediaWikiSection the table is part of.
    self.source  = source  # Table HTML.
    self.title   = title   # Table title.
    self.headers = headers # List of table headers.
    self.rows    = rows    # List of table rows, each a list of cells.

</t>
<t tx="karstenw.20230303141230.183">def plaintext(self, **kwargs):
    return self.article._plaintext(self.source, **kwargs)

</t>
<t tx="karstenw.20230303141230.184">@property
def html(self):
    return self.source

</t>
<t tx="karstenw.20230303141230.185">@property
def src(self):
    return self.source

</t>
<t tx="karstenw.20230303141230.186">@property
def string(self):
    return self.plaintext()

</t>
<t tx="karstenw.20230303141230.187">def __repr__(self):
    return "MediaWikiTable(title=%s)" % repr(self.title)

</t>
<t tx="karstenw.20230303141230.188">class Wikipedia(MediaWiki):

    @others
</t>
<t tx="karstenw.20230303141230.189">def __init__(self, license=None, throttle=5.0, language="en"):
    """ Mediawiki search engine for http://[language].wikipedia.org.
    """
    SearchEngine.__init__(self, license or WIKIPEDIA_LICENSE, throttle, language)
    self._subdomain = language

</t>
<t tx="karstenw.20230303141230.19">def open(self, timeout=10, proxy=None, user_agent=USER_AGENT, referrer=REFERRER, authentication=None):
    """ Returns a connection to the url from which data can be retrieved with connection.read().
        When the timeout amount of seconds is exceeded, raises a URLTimeout.
        When an error occurs, raises a URLError (e.g. HTTP404NotFound).
    """
    url = self.string
    # Handle local files directly
    if os.path.exists(url):
        return urlopen(url)
    # Handle method=POST with query string as a separate parameter.
    post = self.method == POST and self.querystring or None
    socket.setdefaulttimeout(timeout)
    # Handle proxies and cookies.
    handlers = []
    if proxy:
        handlers.append(ProxyHandler({proxy[1]: proxy[0]}))
    handlers.append(HTTPCookieProcessor(cookielib.CookieJar()))
    handlers.append(HTTPHandler)
    install_opener(build_opener(*handlers))
    # Send request.
    try:
        request = Request(url, post, {
                    "User-Agent": user_agent,
                       "Referer": referrer
                     })
        # Basic authentication is established with authentication=(username, password).
        if authentication is not None:
            authentication = tuple(encode_utf8(x) for x in authentication)
            request.add_header("Authorization", "Basic %s" %
                decode_utf8(base64.b64encode(b'%s:%s' % authentication)))
        return urlopen(request)
    except UrllibHTTPError as e:
        if e.code == 301:
            raise HTTP301Redirect(src=e, url=url)
        if e.code == 400:
            raise HTTP400BadRequest(src=e, url=url)
        if e.code == 401:
            raise HTTP401Authentication(src=e, url=url)
        if e.code == 403:
            raise HTTP403Forbidden(src=e, url=url)
        if e.code == 404:
            raise HTTP404NotFound(src=e, url=url)
        if e.code == 414:
            raise HTTP414RequestURITooLong(src=e, url=url)
        if e.code == 420:
            raise HTTP420Error(src=e, url=url)
        if e.code == 429:
            raise HTTP429TooMayRequests(src=e, url=url)
        if e.code == 500:
            raise HTTP500InternalServerError(src=e, url=url)
        if e.code == 503:
            raise HTTP503ServiceUnavailable(src=e, url=url)
        raise HTTPError(str(e), src=e, url=url)
    except httplib.BadStatusLine as e:
        raise HTTPError(str(e), src=e, url=url)
    except socket.timeout as e:
        raise URLTimeout(src=e, url=url)
    except socket.error as e:
        if "timed out" in str((e.args + ("", ""))[0]) \
        or "timed out" in str((e.args + ("", ""))[1]):
            raise URLTimeout(src=e, url=url)
        raise URLError(str(e), src=e, url=url)
    except UrllibURLError as e:
        if "timed out" in str(e.reason):
            raise URLTimeout(src=e, url=url)
        raise URLError(str(e), src=e, url=url)
    except ValueError as e:
        raise URLError(str(e), src=e, url=url)

</t>
<t tx="karstenw.20230303141230.190">@property
def _url(self):
    s = MEDIAWIKI
    s = s.replace("{SUBDOMAIN}", self._subdomain)
    s = s.replace("{DOMAIN}", "wikipedia.org")
    s = s.replace("{API}", "/w/api.php")
    return s

</t>
<t tx="karstenw.20230303141230.191">@property
def MediaWikiArticle(self):
    return WikipediaArticle

</t>
<t tx="karstenw.20230303141230.192">@property
def MediaWikiSection(self):
    return WikipediaSection

</t>
<t tx="karstenw.20230303141230.193">@property
def MediaWikiTable(self):
    return WikipediaTable


</t>
<t tx="karstenw.20230303141230.194">class WikipediaArticle(MediaWikiArticle):

    @others
</t>
<t tx="karstenw.20230303141230.195">def download(self, media, **kwargs):
    """ Downloads an item from MediaWikiArticle.media and returns the content.
        Note: images on Wikipedia can be quite large, and this method uses screen-scraping,
              so Wikipedia might not like it that you download media in this way.
        To save the media in a file:
        data = article.download(media)
        open(filename+extension(media),"w").write(data)
    """
    url = "http://%s.wikipedia.org/wiki/File:%s" % (self.__dict__.get("language", "en"), media)
    if url not in cache:
        time.sleep(1)
    data = URL(url).download(**kwargs)
    data = re.search(r"upload.wikimedia.org/.*?/%s" % media, data)
    data = data and URL("http://" + data.group(0)).download(**kwargs) or None
    return data

</t>
<t tx="karstenw.20230303141230.196">def __repr__(self):
    return "WikipediaArticle(title=%s)" % repr(self.title)


</t>
<t tx="karstenw.20230303141230.197">class WikipediaSection(MediaWikiSection):
    @others
</t>
<t tx="karstenw.20230303141230.198">def __repr__(self):
    return "WikipediaSection(title=%s)" % repr(self.title)


</t>
<t tx="karstenw.20230303141230.199">class WikipediaTable(MediaWikiTable):
    @others
#article = Wikipedia().search("cat")
#for section in article.sections:
#    print("  "*(section.level-1) + section.title)
#if article.media:
#    data = article.download(article.media[2])
#    f = open(article.media[2], "w")
#    f.write(data)
#    f.close()
#
#article = Wikipedia(language="nl").search("borrelnootje")
#print(article.string)

#for result in Wikipedia().search("\"cat's\"", type="*"):
#    print(result.title)
#    print(result.text)
#    print()

#--- MEDIAWIKI: WIKTIONARY -------------------------------------------------------------------------
# Wiktionary is a collaborative project to produce a free-content multilingual dictionary.


</t>
<t tx="karstenw.20230303141230.2">class HTTP401Authentication(HTTPError):
    pass # URL requires a login and password.


</t>
<t tx="karstenw.20230303141230.20">def download(self, timeout=10, cached=True, throttle=0, proxy=None, user_agent=USER_AGENT, referrer=REFERRER, authentication=None, unicode=False, **kwargs):
    """ Downloads the content at the given URL (by default it will be cached locally).
        Unless unicode=False, the content is returned as a unicode string.
    """
    # Filter OAuth parameters from cache id (they will be unique for each request).
    if self._parts is None and self.method == GET and "oauth_" not in self._string:
        id = self._string
    else:
        id = repr(self.parts)
        id = re.sub("u{0,1}'oauth_.*?': u{0,1}'.*?', ", "", id)
    # Keep a separate cache of unicode and raw download for same URL.
    if unicode is True:
        id = "u" + id
    if cached and id in cache:
        if isinstance(cache, dict): # Not a Cache object.
            return cache[id]
        if unicode is True:
            return cache[id]
        if unicode is False:
            return cache.get(id, unicode=False)
    t = time.time()
    # Open a connection with the given settings, read it and (by default) cache the data.
    try:
        data = self.open(timeout, proxy, user_agent, referrer, authentication).read()
    except socket.timeout as e:
        raise URLTimeout(src=e, url=self.string)
    if unicode is True:
        data = u(data)
    if cached:
        cache[id] = data
    if throttle:
        time.sleep(max(throttle - (time.time() - t), 0))
    return data

</t>
<t tx="karstenw.20230303141230.200">def __repr__(self):
    return "WikipediaTable(title=%s)" % repr(self.title)

</t>
<t tx="karstenw.20230303141230.201">class Wiktionary(MediaWiki):

    @others
</t>
<t tx="karstenw.20230303141230.202">def __init__(self, license=None, throttle=5.0, language="en"):
    """ Mediawiki search engine for http://[language].wiktionary.com.
    """
    SearchEngine.__init__(self, license or MEDIAWIKI_LICENSE, throttle, language)
    self._subdomain = language

</t>
<t tx="karstenw.20230303141230.203">@property
def _url(self):
    s = MEDIAWIKI
    s = s.replace("{SUBDOMAIN}", self._subdomain)
    s = s.replace("{DOMAIN}", "wiktionary.org")
    s = s.replace("{API}", "/w/api.php")
    return s

</t>
<t tx="karstenw.20230303141230.204">@property
def MediaWikiArticle(self):
    return WiktionaryArticle

</t>
<t tx="karstenw.20230303141230.205">@property
def MediaWikiSection(self):
    return WiktionarySection

</t>
<t tx="karstenw.20230303141230.206">@property
def MediaWikiTable(self):
    return WiktionaryTable


</t>
<t tx="karstenw.20230303141230.207">class WiktionaryArticle(MediaWikiArticle):
    @others
</t>
<t tx="karstenw.20230303141230.208">def __repr__(self):
    return "WiktionaryArticle(title=%s)" % repr(self.title)


</t>
<t tx="karstenw.20230303141230.209">class WiktionarySection(MediaWikiSection):
    @others
</t>
<t tx="karstenw.20230303141230.21">def read(self, *args, **kwargs):
    return self.open(**kwargs).read(*args)

</t>
<t tx="karstenw.20230303141230.210">def __repr__(self):
    return "WiktionarySection(title=%s)" % repr(self.title)


</t>
<t tx="karstenw.20230303141230.211">class WiktionaryTable(MediaWikiTable):
    @others
#--- MEDIAWIKI: WIKIA ------------------------------------------------------------------------------
# Wikia (formerly Wikicities) is a free web hosting service and a wiki farm for wikis.
# Wikia hosts several hundred thousand wikis using MediaWiki.

# Author: Robert Elwell (2012)

</t>
<t tx="karstenw.20230303141230.212">def __repr__(self):
    return "WiktionaryTable(title=%s)" % repr(self.title)


</t>
<t tx="karstenw.20230303141230.213">class Wikia(MediaWiki):

    @others
</t>
<t tx="karstenw.20230303141230.214">def __init__(self, domain="www", license=None, throttle=5.0, language="en"):
    """ Mediawiki search engine for http://[domain].wikia.com.
    """
    SearchEngine.__init__(self, license or MEDIAWIKI_LICENSE, throttle, language)
    self._subdomain = domain

</t>
<t tx="karstenw.20230303141230.215">@property
def _url(self):
    s = MEDIAWIKI
    s = s.replace("{SUBDOMAIN}", self._subdomain)
    s = s.replace("{DOMAIN}", "wikia.com")
    s = s.replace("{API}", '/api.php')
    return s

</t>
<t tx="karstenw.20230303141230.216">@property
def MediaWikiArticle(self):
    return WikiaArticle

</t>
<t tx="karstenw.20230303141230.217">@property
def MediaWikiSection(self):
    return WikiaSection

</t>
<t tx="karstenw.20230303141230.218">@property
def MediaWikiTable(self):
    return WikiaTable

</t>
<t tx="karstenw.20230303141230.219">def articles(self, **kwargs):
    if kwargs.pop("batch", True):
        # We can take advantage of Wikia's search API to reduce bandwith.
        # Instead of executing a query to retrieve each article,
        # we query for a batch of (10) articles.
        iterator = self.index(_id="pageid", **kwargs)
        while True:
            batch, done = [], False
            try:
                for i in range(10):
                    batch.append( next(iterator) )
            except StopIteration:
                done = True # No more articles, finish batch and raise StopIteration.
            url = URL(self._url.replace("api.php", "wikia.php"), method=GET, query={
              "controller": "WikiaSearch",
                  "method": "getPages",
                     "ids": '|'.join(str(id) for id in batch),
                  "format": "json"
            })
            kwargs.setdefault("unicode", True)
            kwargs.setdefault("cached", True)
            kwargs["timeout"] = 10 * (1 + len(batch))
            data = url.download(**kwargs)
            data = json.loads(data)
            for x in (data or {}).get("pages", {}).values():
                yield WikiaArticle(title=x.get("title", ""), source=x.get("html", ""))
            if done:
                # raise StopIteration
                return
    for title in self.index(**kwargs):
        yield self.search(title, **kwargs)


</t>
<t tx="karstenw.20230303141230.22">@property
def exists(self, timeout=10):
    """ Yields False if the URL generates a HTTP404NotFound error.
    """
    try:
        self.open(timeout)
    except HTTP404NotFound:
        return False
    except HTTPError:
        return True
    except URLTimeout:
        return True
    except URLError:
        return False
    except:
        return True
    return True

</t>
<t tx="karstenw.20230303141230.220">class WikiaArticle(MediaWikiArticle):
    @others
</t>
<t tx="karstenw.20230303141230.221">def __repr__(self):
    return "WikiaArticle(title=%s)" % repr(self.title)


</t>
<t tx="karstenw.20230303141230.222">class WikiaSection(MediaWikiSection):
    @others
</t>
<t tx="karstenw.20230303141230.223">def __repr__(self):
    return "WikiaSection(title=%s)" % repr(self.title)


</t>
<t tx="karstenw.20230303141230.224">class WikiaTable(MediaWikiTable):
    @others
#--- DBPEDIA --------------------------------------------------------------------------------------------------
# DBPedia is a database of structured information mined from Wikipedia.
# DBPedia data is stored as RDF triples: (subject, predicate, object),
# e.g., X is-a Actor, Y is-a Country, Z has-birthplace Country, ...

# DBPedia can be queried using SPARQL:
# http://www.w3.org/TR/rdf-sparql-query/
# A SPARQL query yields rows that match all triples in the WHERE clause.
# A SPARQL query uses ?wildcards in triple subject/object to select fields.

# For example:
# &gt; PREFIX dbo: &lt;http://dbpedia.org/ontology/&gt;
# &gt; SELECT ?actor ?place
# &gt; WHERE {
# &gt;     ?actor a dbo:Actor; dbo:birthPlace ?place.
# &gt;     ?place a dbo:Country.
# &gt; }
#
# - Each row in the results has an "actor" and a "place" field.
# - The actor is of the class "Actor".
# - The place is of the class "Country".
# - Only actors for which a place of birth is known are retrieved.
#
# The fields are RDF resources, e.g.:
# http://dbpedia.org/resource/Australia

# Author: Kenneth Koch (2013) &lt;kkoch986@gmail.com&gt;

DBPEDIA = "http://dbpedia.org/sparql?"

SPARQL = "sparql"


</t>
<t tx="karstenw.20230303141230.225">def __repr__(self):
    return "WikiaTable(title=%s)" % repr(self.title)

</t>
<t tx="karstenw.20230303141230.226">class DBPediaQueryError(HTTP400BadRequest):
    pass


</t>
<t tx="karstenw.20230303141230.227">class DBPediaResource(str):
    @others
</t>
<t tx="karstenw.20230303141230.228">@property
def name(self):
    # http://dbpedia.org/resource/Australia =&gt; Australia
    s = re.sub("^http://dbpedia.org/resource/", "", self)
    s = s.replace("_", " ")
    if sys.version_info[0] &lt; 3:
        s = encode_utf8(s)
        s = decode_url(s)
        s = decode_utf8(s)
    return s


</t>
<t tx="karstenw.20230303141230.229">class DBPedia(SearchEngine):

    @others
#--- FLICKR ----------------------------------------------------------------------------------------
# Flickr is a popular image hosting and video hosting website.
# http://www.flickr.com/services/api/

FLICKR = "https://api.flickr.com/services/rest/"
FLICKR_LICENSE = api.license["Flickr"]

INTERESTING = "interesting"


</t>
<t tx="karstenw.20230303141230.23">@property
def mimetype(self, timeout=10):
    """ Yields the MIME-type of the document at the URL, or None.
        MIME is more reliable than simply checking the document extension.
        You can then do: URL.mimetype in MIMETYPE_IMAGE.
    """
    try:
        return self.headers["content-type"].split(";")[0]
    except KeyError:
        return None

</t>
<t tx="karstenw.20230303141230.230">def __init__(self, license=None, throttle=1.0, language=None):
    SearchEngine.__init__(self, license, throttle, language)

</t>
<t tx="karstenw.20230303141230.231">def search(self, query, type=SPARQL, start=1, count=10, sort=RELEVANCY, size=None, cached=False, **kwargs):
    """ Returns a list of results from DBPedia for the given SPARQL query.
        - type : SPARQL,
        - start: no maximum,
        - count: maximum 1000,
        There is a limit of 10 requests/second.
        Maximum query execution time is 120 seconds.
    """
    if type not in (SPARQL,):
        raise SearchEngineTypeError
    if not query or count &lt; 1 or start &lt; 1:
        return Results(DBPEDIA, query, type)
    # 1) Construct request URL.
    url = URL(DBPEDIA, method=GET)
    url.query = {
        "format": "json",
         "query": "%s OFFSET %s LIMIT %s" % (query,
                    (start - 1) * min(count, 1000),
                    (start - 0) * min(count, 1000)
        )
    }
    # 2) Parse JSON response.
    try:
        data = URL(url).download(cached=cached, timeout=30, **kwargs)
        data = json.loads(data)
    except HTTP400BadRequest as e:
        raise DBPediaQueryError(e.src.read().splitlines()[0])
    except HTTP403Forbidden:
        raise SearchEngineLimitError
    results = Results(DBPEDIA, url.query, type)
    results.total = None
    for x in data["results"]["bindings"]:
        r = Result(url=None)
        for k in data["head"]["vars"]:
            t1 = x[k].get("type", "literal") # uri | literal | typed-literal
            t2 = x[k].get("datatype", "?")   # http://www.w3.org/2001/XMLSchema#float | int | date
            v = x[k].get("value")
            v = self.format(v)
            if t1 == "uri":
                v = DBPediaResource(v)
            if t2.endswith("float"):
                v = float(v)
            if t2.endswith("int"):
                v = int(v)
            dict.__setitem__(r, k, v)
        results.append(r)
    return results

</t>
<t tx="karstenw.20230303141230.232">class Flickr(SearchEngine):

    @others
</t>
<t tx="karstenw.20230303141230.233">def __init__(self, license=None, throttle=5.0, language=None):
    SearchEngine.__init__(self, license or FLICKR_LICENSE, throttle, language)

</t>
<t tx="karstenw.20230303141230.234">def search(self, query, type=IMAGE, start=1, count=10, sort=RELEVANCY, size=None, cached=True, **kwargs):
    """ Returns a list of results from Flickr for the given query.
        Retrieving the URL of a result (i.e. image) requires an additional query.
        - type : SEARCH, IMAGE,
        - start: maximum undefined,
        - count: maximum 500,
        - sort : RELEVANCY, LATEST or INTERESTING.
        There is no daily limit.
    """
    if type not in (SEARCH, IMAGE):
        raise SearchEngineTypeError
    if not query or count &lt; 1 or start &lt; 1 or start &gt; 500 / count:
        return Results(FLICKR, query, IMAGE)
    # 1) Construct request URL.
    url = FLICKR + "?"
    url = URL(url, method=GET, query={
       "api_key": self.license or "",
        "method": "flickr.photos.search",
          "text": query.replace(" ", "_"),
          "page": start,
      "per_page": min(count, 500),
          "sort": {RELEVANCY: "relevance",
                       LATEST: "date-posted-desc",
                  INTERESTING: "interestingness-desc"}.get(sort)
    })
    if kwargs.get("copyright", True) is False:
        # With copyright=False, only returns Public Domain and Creative Commons images.
        # http://www.flickr.com/services/api/flickr.photos.licenses.getInfo.html
        # 5: "Attribution-ShareAlike License"
        # 7: "No known copyright restriction"
        url.query["license"] = "5,7"
    # 2) Parse XML response.
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    data = url.download(cached=cached, **kwargs)
    data = xml.dom.minidom.parseString(bytestring(data))
    results = Results(FLICKR, query, IMAGE)
    results.total = int(data.getElementsByTagName("photos")[0].getAttribute("total"))
    for x in data.getElementsByTagName("photo"):
        r = FlickrResult(url=None)
        r.__dict__["_id"] = x.getAttribute("id")
        r.__dict__["_size"] = size
        r.__dict__["_license"] = self.license
        r.__dict__["_throttle"] = self.throttle
        r.text = self.format(x.getAttribute("title"))
        r.author = self.format(x.getAttribute("owner"))
        results.append(r)
    return results


</t>
<t tx="karstenw.20230303141230.235">class FlickrResult(Result):

    @others
#images = Flickr().search("kitten", count=10, size=SMALL)
#for img in images:
#    print(bytestring(img.description))
#    print(img.url)
#
#data = img.download()
#f = open("kitten"+extension(img.url), "wb")
#f.write(data)
#f.close()

#--- FACEBOOK --------------------------------------------------------------------------------------
# Facebook is a popular online social networking service.
# https://developers.facebook.com/docs/reference/api/

FACEBOOK = "https://graph.facebook.com/"
FACEBOOK_LICENSE = api.license["Facebook"]

FEED     = "feed"      # Facebook timeline.
COMMENTS = "comments"  # Facebook comments (for a given news feed post).
LIKES    = "likes"     # Facebook likes (for a given post or comment).
FRIENDS  = "friends"   # Facebook friends (for a given profile id).


</t>
<t tx="karstenw.20230303141230.236">@property
def url(self):
    # Retrieving the url of a FlickrResult (i.e. image location) requires another query.
    # Note: the "Original" size no longer appears in the response,
    # so Flickr might not like it if we download it.
    url = FLICKR + "?method=flickr.photos.getSizes&amp;photo_id=%s&amp;api_key=%s" % (self._id, self._license)
    data = URL(url).download(throttle=self._throttle, unicode=True)
    data = xml.dom.minidom.parseString(bytestring(data))
    size = {TINY: "Thumbnail",
            SMALL: "Small",
           MEDIUM: "Medium",
            LARGE: "Original"}.get(self._size, "Medium")
    for x in data.getElementsByTagName("size"):
        if size == x.getAttribute("label"):
            return x.getAttribute("source")
        if size == "Original":
            url = x.getAttribute("source")
            url = url[:-len(extension(url)) - 2] + "_o" + extension(url)
            return u(url)

</t>
<t tx="karstenw.20230303141230.237">class FacebookResult(Result):
    @others
</t>
<t tx="karstenw.20230303141230.238">def __repr__(self):
    return "Result(id=%s)" % repr(self.id)


</t>
<t tx="karstenw.20230303141230.239">class Facebook(SearchEngine):

    @others
#--- PRODUCT REVIEWS -------------------------------------------------------------------------------
# ProductWiki is an open web-based product information resource.
# http://connect.productwiki.com/connect-api/

PRODUCTWIKI = "http://api.productwiki.com/connect/api.aspx"
PRODUCTWIKI_LICENSE = api.license["ProductWiki"]


</t>
<t tx="karstenw.20230303141230.24">@property
def headers(self, timeout=10):
    """ Yields a dictionary with the HTTP response headers.
    """
    if self.__dict__["_headers"] is None:
        try:
            h = dict(self.open(timeout).info())
        except URLError:
            h = {}
        self.__dict__["_headers"] = h

    # Backward compatibility (Python 2)
    if "Content-Type" in self.__dict__["_headers"]:
        self.__dict__["_headers"]["content-type"] = self.__dict__["_headers"]["Content-Type"]

    return self.__dict__["_headers"]

</t>
<t tx="karstenw.20230303141230.240">def __init__(self, license=None, throttle=1.0, language=None):
    SearchEngine.__init__(self, license or FACEBOOK_LICENSE, throttle, language)

</t>
<t tx="karstenw.20230303141230.241">@property
def _token(self):
    # Yields the "application access token" (stored in api.license["Facebook"]).
    # With this license, we can view public content.
    # To view more information, we need a "user access token" as license key.
    # This token can be retrieved manually from:
    #  http://www.clips.ua.ac.be/pattern-facebook
    # Or parsed from this URL:
    #  https://graph.facebook.com/oauth/authorize?type=user_agent
    #   &amp;client_id=332061826907464
    #   &amp;redirect_uri=http://www.clips.ua.ac.be/pattern-facebook
    #   &amp;scope=read_stream,user_birthday,user_likes,user_photos,friends_birthday,friends_likes
    # The token is valid for a limited duration.
    return URL(FACEBOOK + "oauth/access_token?", query={
           "grant_type": "client_credentials",
            "client_id": "332061826907464",
        "client_secret": "81ff4204e73ecafcd87635a3a3683fbe"
    }).download().split("=")[1]

</t>
<t tx="karstenw.20230303141230.242">def search(self, query, type=SEARCH, start=1, count=10, cached=False, **kwargs):
    """ Returns a list of results from Facebook public status updates for the given query.
        - query: string, or Result.id for NEWS and COMMENTS,
        - type : SEARCH,
        - start: 1,
        - count: maximum 100 for SEARCH and NEWS, 1000 for COMMENTS and LIKES.
        There is an hourly limit of +-600 queries (actual amount undisclosed).
    """
    # Facebook.search(type=SEARCH) returns public posts + author.
    # Facebook.search(type=NEWS) returns posts for the given author (id | alias | "me").
    # Facebook.search(type=COMMENTS) returns comments for the given post id.
    # Facebook.search(type=LIKES) returns authors for the given author, post or comments.
    # Facebook.search(type=FRIENDS) returns authors for the given author.
    # An author is a Facebook user or other entity (e.g., a product page).
    if type not in (SEARCH, NEWS, COMMENTS, LIKES, FRIENDS):
        raise SearchEngineTypeError
    if type in (SEARCH, NEWS):
        max = 100
    if type in (COMMENTS, LIKES):
        max = 1000
    if type in (FRIENDS,):
        max = 10000
    if not query or start &lt; 1 or count &lt; 1:
        return Results(FACEBOOK, query, SEARCH)
    if isinstance(query, FacebookResult):
        query = query.id
    # 1) Construct request URL.
    if type == SEARCH:
        url = FACEBOOK + type
        url = URL(url, method=GET, query={
                     "q": query,
                  "type": "post",
          "access_token": self.license,
                "offset": (start - 1) * min(count, max),
                 "limit": (start - 0) * min(count, max)
        })
    if type in (NEWS, FEED, COMMENTS, LIKES, FRIENDS):
        url = FACEBOOK + (u(query) or "me").replace(FACEBOOK, "") + "/" + type.replace("news", "feed")
        url = URL(url, method=GET, query={
          "access_token": self.license,
                "offset": (start - 1) * min(count, max),
                 "limit": (start - 0) * min(count, max),
        })
    if type in (SEARCH, NEWS, FEED):
        url.query["fields"] = ",".join((
            "id", "from", "name", "story", "message", "link", "picture", "created_time", "shares",
            "comments.limit(1).summary(true)",
               "likes.limit(1).summary(true)"
        ))
    # 2) Parse JSON response.
    kwargs.setdefault("cached", cached)
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    try:
        data = URL(url).download(**kwargs)
    except HTTP400BadRequest:
        raise HTTP401Authentication
    data = json.loads(data)
    results = Results(FACEBOOK, query, SEARCH)
    results.total = None
    for x in data.get("data", []):
        r = FacebookResult(url=None)
        r.id       = self.format(x.get("id"))
        r.url      = self.format(x.get("link"))
        r.text     = self.format(x.get("story", x.get("message", x.get("name"))))
        r.date     = self.format(x.get("created_time"))
        r.votes    = self.format(x.get("like_count", x.get("likes", {}).get("summary", {}).get("total_count", 0)) + 0)
        r.shares   = self.format(x.get("shares", {}).get("count", 0))
        r.comments = self.format(x.get("comments", {}).get("summary", {}).get("total_count", 0) + 0)
        r.author   = self.format(x.get("from", {}).get("id", "")), \
                     self.format(x.get("from", {}).get("name", ""))
        # Set Result.text to author name for likes.
        if type in (LIKES, FRIENDS):
            r.author = \
               self.format(x.get("id", "")), \
               self.format(x.get("name", ""))
            r.text = self.format(x.get("name"))
        # Set Result.url to full-size image.
        if re.match(r"^http(s?)://www\.facebook\.com/photo", r.url) is not None:
            r.url = x.get("picture", "").replace("_s", "_b") or r.url
        # Set Result.title to object id.
        if re.match(r"^http(s?)://www\.facebook\.com/", r.url) is not None:
            r.title = r.url.split("/")[-1].split("?")[0]
        results.append(r)
    return results

</t>
<t tx="karstenw.20230303141230.243">def profile(self, id=None, **kwargs):
    """ Returns a Result for the given author id or alias.
    """
    # 1) Construct request URL.
    url = FACEBOOK + (u(id or "me")).replace(FACEBOOK, "")
    url = URL(url, method=GET, query={"access_token": self.license})
    kwargs.setdefault("cached", True)
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    # 2) Parse JSON response.
    try:
        data = URL(url).download(**kwargs)
        data = json.loads(data)
    except HTTP400BadRequest:
        raise HTTP401Authentication
    return Result(
            id = data.get("id", ""),                   # 123456...
           url = data.get("link", ""),                 # https://www.facebook.com/tomdesmedt
        handle = data.get("username", ""),             # tomdesmedt
          name = data.get("name"),                     # Tom De Smedt
          text = data.get("description", ""),          # Artist, scientist, software engineer
      language = data.get("locale", "").split("_")[0], # en_US
          date = data.get("birthday", ""),             # 10/10/1000
        gender = data.get("gender", "")[:1],           # m
        locale = data.get("hometown", {}).get("name", ""),
         votes = int(data.get("likes", 0)) # (for product pages)
    )

page = profile

</t>
<t tx="karstenw.20230303141230.244">class ProductWiki(SearchEngine):

    @others
# Backwards compatibility.
Products = ProductWiki

#for r in ProductWiki().search("tablet"):
#    print(r.title)
#    print(r.score)
#    print(r.reviews)
#    print()

#--- NEWS FEED -------------------------------------------------------------------------------------
# Based on the Universal Feed Parser by Mark Pilgrim:
# http://www.feedparser.org/


</t>
<t tx="karstenw.20230303141230.245">def __init__(self, license=None, throttle=5.0, language=None):
    SearchEngine.__init__(self, license or PRODUCTWIKI_LICENSE, throttle, language)

</t>
<t tx="karstenw.20230303141230.246">def search(self, query, type=SEARCH, start=1, count=10, sort=RELEVANCY, size=None, cached=True, **kwargs):
    """ Returns a list of results from Productwiki for the given query.
        Each Result.reviews is a list of (review, score)-items.
        - type : SEARCH,
        - start: maximum undefined,
        - count: 20,
        - sort : RELEVANCY.
        There is no daily limit.
    """
    if type != SEARCH:
        raise SearchEngineTypeError
    if not query or start &lt; 1 or count &lt; 1:
        return Results(PRODUCTWIKI, query, type)
    # 1) Construct request URL.
    url = PRODUCTWIKI + "?"
    url = URL(url, method=GET, query={
           "key": self.license or "",
             "q": query,
          "page": start,
            "op": "search",
        "fields": "proscons", # "description,proscons" is heavy.
        "format": "json"
    })
    # 2) Parse JSON response.
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    data = URL(url).download(cached=cached, **kwargs)
    data = json.loads(data)
    results = Results(PRODUCTWIKI, query, type)
    results.total = None
    for x in data.get("products", [])[:count]:
        r = Result(url=None)
        r.__dict__["title"] = u(x.get("title"))
        r.__dict__["text"] = u(x.get("text"))
        r.__dict__["reviews"] = []
        reviews = x.get("community_review") or {}
        for p in reviews.get("pros", []):
            r.reviews.append((p.get("text", ""), int(p.get("score")) or +1))
        for p in reviews.get("cons", []):
            r.reviews.append((p.get("text", ""), int(p.get("score")) or -1))
        r.__dict__["score"] = int(sum(score for review, score in r.reviews))
        results.append(r)
    # Highest score first.
    results.sort(key=lambda r: r.score, reverse=True)
    return results

</t>
<t tx="karstenw.20230303141230.247">class Newsfeed(SearchEngine):

    @others
feeds = {
          "Nature": "http://feeds.nature.com/nature/rss/current",
         "Science": "http://www.sciencemag.org/rss/podcast.xml",
  "Herald Tribune": "http://www.iht.com/rss/frontpage.xml",
            "TIME": "http://feeds.feedburner.com/time/topstories",
             "CNN": "http://rss.cnn.com/rss/edition.rss",
}

#for r in Newsfeed().search(feeds["Nature"]):
#    print(r.title)
#    print(r.author)
#    print(r.url)
#    print(plaintext(r.text))
#    print()

#--- QUERY -----------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303141230.248">def __init__(self, license=None, throttle=1.0, language=None):
    SearchEngine.__init__(self, license, throttle, language)

</t>
<t tx="karstenw.20230303141230.249">def search(self, query, type=NEWS, start=1, count=10, sort=LATEST, size=SMALL, cached=True, **kwargs):
    """ Returns a list of results from the given RSS or Atom newsfeed URL.
    """
    if type != NEWS:
        raise SearchEngineTypeError
    if not query or start &lt; 1 or count &lt; 1:
        return Results(query, query, NEWS)
    # 1) Construct request URL.
    # 2) Parse RSS/Atom response.
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    tags = kwargs.pop("tags", [])
    data = URL(query).download(cached=cached, **kwargs)
    data = feedparser.parse(data)
    results = Results(query, query, NEWS)
    results.total = None
    for x in data["entries"][:count]:
        s = "\n\n".join([v.get("value") for v in x.get("content", [])]) or x.get("summary")
        r = Result(url=None)
        r.id       = self.format(x.get("id"))
        r.url      = self.format(x.get("link"))
        r.title    = self.format(x.get("title"))
        r.text     = self.format(s)
        r.date     = self.format(x.get("updated"))
        r.author   = self.format(x.get("author"))
        r.language = self.format(x.get("content") and \
                            x.get("content")[0].get("language") or \
                                           data.get("language"))
        for tag in tags:
            # Parse custom tags.
            # Newsfeed.search(tags=["dc:identifier"]) =&gt; Result.dc_identifier.
            tag = tag.replace(":", "_")
            r[tag] = self.format(x.get(tag))
        results.append(r)
    return results

</t>
<t tx="karstenw.20230303141230.25">@property
def redirect(self, timeout=10):
    """ Yields the redirected URL, or None.
    """
    if self.__dict__["_redirect"] is None:
        try:
            r = u(self.open(timeout).geturl())
        except URLError:
            r = None
        self.__dict__["_redirect"] = r != self.string and r or ""
    return self.__dict__["_redirect"] or None

</t>
<t tx="karstenw.20230303141230.250">def query(string, service=GOOGLE, **kwargs):
    """ Returns the list of search query results from the given service.
        For service=WIKIPEDIA, this is a single WikipediaArticle or None.
    """
    service = service.lower()
    if service in (GOOGLE, "google", "g"):
        engine = Google
    if service in (YAHOO, "yahoo", "y!"):
        engine = Yahoo
    if service in (BING, "bing"):
        engine = Bing
    if service in (DUCKDUCKGO, "duckduckgo", "ddg"):
        engine = DuckDuckGo
    if service in (TWITTER, "twitter", "tw"):
        engine = Twitter
    if service in (FACEBOOK, "facebook", "fb"):
        engine = Facebook
    if service in (WIKIPEDIA, "wikipedia", "wp"):
        engine = Wikipedia
    if service in (WIKIA, "wikia"):
        engine = Wikia
    if service in (DBPEDIA, "dbpedia", "dbp"):
        engine = DBPedia
    if service in (FLICKR, "flickr"):
        engine = Flickr
    try:
        kw = {}
        for a in ("license", "throttle", "language"):
            if a in kwargs:
                kw[a] = kwargs.pop(a)
        return engine(kw).search(string, **kwargs)
    except UnboundLocalError:
        raise SearchEngineError("unknown search engine '%s'" % service)

#--- WEB SORT --------------------------------------------------------------------------------------

SERVICES = {
    GOOGLE    : Google,
    YAHOO     : Yahoo,
    FAROO     : Faroo,
    BING      : Bing,
    TWITTER   : Twitter,
    WIKIPEDIA : Wikipedia,
    WIKIA     : Wikia,
    FLICKR    : Flickr,
    FACEBOOK  : Facebook
}


</t>
<t tx="karstenw.20230303141230.251">def sort(terms=[], context="", service=GOOGLE, license=None, strict=True, prefix=False, **kwargs):
    """ Returns a list of (percentage, term)-tuples for the given list of terms.
        Sorts the terms in the list according to search result count.
        When a context is defined, sorts according to relevancy to the context, e.g.:
        sort(terms=["black", "green", "red"], context="Darth Vader") =&gt;
        yields "black" as the best candidate, because "black Darth Vader" is more common in search results.
        - terms   : list of search terms,
        - context : term used for sorting,
        - service : web service name (GOOGLE, YAHOO, BING),
        - license : web service license id,
        - strict  : when True the query constructed from term + context is wrapped in quotes.
    """
    service = SERVICES.get(service, SearchEngine)(license, language=kwargs.pop("language", None))
    R = []
    for word in terms:
        q = prefix and (context + " " + word) or (word + " " + context)
        q.strip()
        q = strict and "\"%s\"" % q or q
        t = service in (WIKIPEDIA, WIKIA) and "*" or SEARCH
        r = service.search(q, type=t, count=1, **kwargs)
        R.append(r)
    s = float(sum([r.total or 1 for r in R])) or 1.0
    R = [((r.total or 1) / s, r.query) for r in R]
    R = sorted(R, reverse=kwargs.pop("reverse", True))
    return R

#print(sort(["black", "happy"], "darth vader", GOOGLE))

#### DOCUMENT OBJECT MODEL #########################################################################
# The Document Object Model (DOM) is a cross-platform and language-independent convention
# for representing and interacting with objects in HTML, XHTML and XML documents.
# The pattern.web DOM can be used to traverse HTML source code as a tree of nested elements.
# The pattern.web DOM is based on Beautiful Soup.

# Beautiful Soup is wrapped in DOM, Element and Text classes, resembling the Javascript DOM.
# Beautiful Soup can also be used directly, since it is imported here.
# L. Richardson (2004), http://www.crummy.com/software/BeautifulSoup/

SOUP = (
    BeautifulSoup.BeautifulSoup,
    BeautifulSoup.Tag,
    BeautifulSoup.NavigableString,
    BeautifulSoup.Comment
)

NODE, TEXT, COMMENT, ELEMENT, DOCUMENT = \
    "node", "text", "comment", "element", "document"

#--- NODE ------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303141230.252">class Node(object):

    @others
#--- TEXT ------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303141230.253">def __init__(self, html, type=NODE, **kwargs):
    """ The base class for Text, Comment and Element.
        All DOM nodes can be navigated in the same way (e.g. Node.parent, Node.children, ...)
    """
    self.type = type
    self._p = not isinstance(html, SOUP) and BeautifulSoup.BeautifulSoup(u(html), "lxml", **kwargs) or html

</t>
<t tx="karstenw.20230303141230.254">@property
def _beautifulSoup(self):
    # If you must, access the BeautifulSoup object with Node._beautifulSoup.
    return self._p

</t>
<t tx="karstenw.20230303141230.255">def __eq__(self, other):
    # Two Node objects containing the same BeautifulSoup object, are the same.
    return isinstance(other, Node) and hash(self._p) == hash(other._p)

</t>
<t tx="karstenw.20230303141230.256">def _wrap(self, x):
    # Navigating to other nodes yields either Text, Element or None.
    if isinstance(x, BeautifulSoup.Comment):
        return Comment(x)
    if isinstance(x, BeautifulSoup.Declaration):
        return Text(x)
    if isinstance(x, BeautifulSoup.NavigableString):
        return Text(x)
    if isinstance(x, BeautifulSoup.Tag):
        return Element(x)

</t>
<t tx="karstenw.20230303141230.257">@property
def parent(self):
    return self._wrap(self._p.parent)

</t>
<t tx="karstenw.20230303141230.258">@property
def children(self):
    return hasattr(self._p, "contents") and [self._wrap(x) for x in self._p.contents] or []

</t>
<t tx="karstenw.20230303141230.259">@property
def html(self):
    return self.__str__()

</t>
<t tx="karstenw.20230303141230.26">def __str__(self):
    # The string representation includes the query attributes with HTTP GET.
    P = self.parts
    u = []
    if P[PROTOCOL]:
        u.append("%s://" % P[PROTOCOL])
    if P[USERNAME]:
        u.append("%s:%s@" % (P[USERNAME], P[PASSWORD]))
    if P[DOMAIN]:
        u.append(P[DOMAIN])
    if P[PORT]:
        u.append(":%s" % P[PORT])
    if P[PORT] or P[DOMAIN] and not P[PATH] and not P[PAGE]:
        u.append("/")
    if P[PATH]:
        u.append("/%s/" % "/".join(P[PATH]))
    if P[PAGE] and len(u) &gt; 0:
        u[-1] = u[-1].rstrip("/")
    if P[PAGE]:
        u.append("/%s" % P[PAGE])
    if P[QUERY] and self.method == GET:
        u.append("?%s" % self.querystring)
    if P[ANCHOR]:
        u.append("#%s" % P[ANCHOR])
    u = "".join(u)
    u = u.lstrip("/")
    return u

</t>
<t tx="karstenw.20230303141230.260">@property
def source(self):
    return self.__str__()

</t>
<t tx="karstenw.20230303141230.261">@property
def next_sibling(self):
    return self._wrap(self._p.next_sibling)

</t>
<t tx="karstenw.20230303141230.262">@property
def previous_sibling(self):
    return self._wrap(self._p.previous_sibling)

next, prev, previous = \
    next_sibling, previous_sibling, previous_sibling

</t>
<t tx="karstenw.20230303141230.263">def traverse(self, visit=lambda node: None):
    """ Executes the visit function on this node and each of its child nodes.
    """
    visit(self)
    [node.traverse(visit) for node in self.children]

</t>
<t tx="karstenw.20230303141230.264">def remove(self, child):
    """ Removes the given child node (and all nested nodes).
    """
    child._p.extract()

</t>
<t tx="karstenw.20230303141230.265">def __bool__(self):
    return True

</t>
<t tx="karstenw.20230303141230.266">def __len__(self):
    return len(self.children)

</t>
<t tx="karstenw.20230303141230.267">def __iter__(self):
    return iter(self.children)

</t>
<t tx="karstenw.20230303141230.268">def __getitem__(self, index):
    return self.children[index]

</t>
<t tx="karstenw.20230303141230.269">def __repr__(self):
    return "Node(type=%s)" % repr(self.type)

</t>
<t tx="karstenw.20230303141230.27">def __repr__(self):
    return "URL(%s, method=%s)" % (repr(self.string), repr(self.method))

</t>
<t tx="karstenw.20230303141230.270">def __str__(self):
    return u(self._p)

</t>
<t tx="karstenw.20230303141230.271">def __call__(self, *args, **kwargs):
    pass

</t>
<t tx="karstenw.20230303141230.272">class Text(Node):
    """ Text represents a chunk of text without formatting in a HTML document.
        For example: "the &lt;b&gt;cat&lt;/b&gt;" is parsed to [Text("the"), Element("cat")].
    """

    @others
</t>
<t tx="karstenw.20230303141230.273">def __init__(self, string):
    Node.__init__(self, string, type=TEXT)

</t>
<t tx="karstenw.20230303141230.274">def __repr__(self):
    return "Text(%s)" % repr(self._p)


</t>
<t tx="karstenw.20230303141230.275">class Comment(Text):
    """ Comment represents a comment in the HTML source code.
        For example: "&lt;!-- comment --&gt;".
    """

    @others
#--- ELEMENT ---------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303141230.276">def __init__(self, string):
    Node.__init__(self, string, type=COMMENT)

</t>
<t tx="karstenw.20230303141230.277">def __repr__(self):
    return "Comment(%s)" % repr(self._p)

</t>
<t tx="karstenw.20230303141230.278">class Element(Node):

    @others
#--- DOCUMENT --------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303141230.279">def __init__(self, html):
    """ Element represents an element or tag in the HTML source code.
        For example: "&lt;b&gt;hello&lt;/b&gt;" is a "b"-Element containing a child Text("hello").
    """
    Node.__init__(self, html, type=ELEMENT)

</t>
<t tx="karstenw.20230303141230.28">def copy(self):
    return URL(self.string, self.method, self.query)


</t>
<t tx="karstenw.20230303141230.280">@property
def tagname(self):
    return self._p.name

tag = tagName = tagname

</t>
<t tx="karstenw.20230303141230.281">@property
def attributes(self):
    return self._p.attrs

attr = attrs = attributes

</t>
<t tx="karstenw.20230303141230.282">@property
def id(self):
    return self.attributes.get("id")

</t>
<t tx="karstenw.20230303141230.283">@property
def content(self):
    """ Yields the element content as a unicode string.
    """
    return "".join([u(x) for x in self._p.contents])

string = content

</t>
<t tx="karstenw.20230303141230.284">@property
def source(self):
    """ Yields the HTML source as a unicode string (tag + content).
    """
    return u(self._p)

html = src = source

</t>
<t tx="karstenw.20230303141230.285">def get_elements_by_tagname(self, v):
    """ Returns a list of nested Elements with the given tag name.
        The tag name can include a class (e.g. div.header) or an id (e.g. div#content).
    """
    if isinstance(v, str) and "#" in v:
        v1, v2 = v.split("#")
        v1 = v1 in ("*", "") or v1.lower()
        return [Element(x) for x in self._p.find_all(v1, id=v2)]
    if isinstance(v, str) and "." in v:
        v1, v2 = v.split(".")
        v1 = v1 in ("*", "") or v1.lower()
        return [Element(x) for x in self._p.find_all(v1, v2)]
    return [Element(x) for x in self._p.find_all(v in ("*", "") or v.lower())]

by_tag = getElementsByTagname = get_elements_by_tagname

</t>
<t tx="karstenw.20230303141230.286">def get_element_by_id(self, v):
    """ Returns the first nested Element with the given id attribute value.
    """
    return ([Element(x) for x in self._p.find_all(id=v, limit=1) or []] + [None])[0]

by_id = getElementById = get_element_by_id

</t>
<t tx="karstenw.20230303141230.287">def get_elements_by_classname(self, v):
    """ Returns a list of nested Elements with the given class attribute value.
    """
    return [Element(x) for x in (self._p.find_all(True, v))]

by_class = getElementsByClassname = get_elements_by_classname

</t>
<t tx="karstenw.20230303141230.288">def get_elements_by_attribute(self, **kwargs):
    """ Returns a list of nested Elements with the given attribute value.
    """
    return [Element(x) for x in (self._p.find_all(True, attrs=kwargs))]

by_attribute = by_attr = getElementsByAttribute = get_elements_by_attribute

</t>
<t tx="karstenw.20230303141230.289">def __call__(self, selector):
    """ Returns a list of nested Elements that match the given CSS selector.
        For example: Element("div#main p.comment a:first-child") matches:
    """
    return SelectorChain(selector).search(self)

</t>
<t tx="karstenw.20230303141230.29">def download(url="", method=GET, query={}, timeout=10, cached=True, throttle=0, proxy=None, user_agent=USER_AGENT, referrer=REFERRER, authentication=None, unicode=False):
    """ Downloads the content at the given URL (by default it will be cached locally).
        Unless unicode=False, the content is returned as a unicode string.
    """
    return URL(url, method, query).download(timeout, cached, throttle, proxy, user_agent, referrer, authentication, unicode)

#url = URL("http://user:pass@example.com:992/animal/bird?species#wings")
#print(url.parts)
#print(url.query)
#print(url.string)

#--- STREAMING URL BUFFER --------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303141230.290">def __getattr__(self, k):
    if k in self.__dict__:
        return self.__dict__[k]
    if k in self.attributes:
        return self.attributes[k]
    raise AttributeError("'Element' object has no attribute '%s'" % k)

</t>
<t tx="karstenw.20230303141230.291">def __contains__(self, v):
    if isinstance(v, Element):
        v = v.content
    return v in self.content

</t>
<t tx="karstenw.20230303141230.292">def __repr__(self):
    return "Element(tag=%s)" % repr(self.tagname)

</t>
<t tx="karstenw.20230303141230.293">class Document(Element):

    @others
DOM = Document

#article = Wikipedia().search("Document Object Model")
#dom = DOM(article.html)
#print(dom.get_element_by_id("References").source)
#print([element.attributes["href"] for element in dom.get_elements_by_tagname("a")])
#print(dom.get_elements_by_tagname("p")[0].next.previous.children[0].parent.__class__)
#print()

#--- DOM CSS SELECTORS -----------------------------------------------------------------------------
# CSS selectors are pattern matching rules (or selectors) to select elements in the DOM.
# CSS selectors may range from simple element tag names to rich contextual patterns.
# http://www.w3.org/TR/CSS2/selector.html

# "*"                 =  &lt;div&gt;, &lt;p&gt;, ...                (all elements)
# "*#x"               =  &lt;div id="x"&gt;, &lt;p id="x"&gt;, ...  (all elements with id="x")
# "div#x"             =  &lt;div id="x"&gt;                   (&lt;div&gt; elements with id="x")
# "div.x"             =  &lt;div class="x"&gt;                (&lt;div&gt; elements with class="x")
# "div[class='x']"    =  &lt;div class="x"&gt;                (&lt;div&gt; elements with attribute "class"="x")
# "div:contains('x')" =  &lt;div&gt;xyz&lt;/div&gt;                 (&lt;div&gt; elements that contain "x")
# "div:first-child"   =  &lt;div&gt;&lt;a&gt;1st&lt;a&gt;&lt;a&gt;&lt;/a&gt;&lt;/div&gt;    (first child inside a &lt;div&gt;)
# "div a"             =  &lt;div&gt;&lt;p&gt;&lt;a&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;      (all &lt;a&gt;'s inside a &lt;div&gt;)
# "div, a"            =  &lt;div&gt;, &lt;a&gt;                     (all &lt;a&gt;'s and &lt;div&gt; elements)
# "div + a"           =  &lt;div&gt;&lt;/div&gt;&lt;a&gt;&lt;/a&gt;             (all &lt;a&gt;'s directly preceded by &lt;div&gt;)
# "div &gt; a"           =  &lt;div&gt;&lt;a&gt;&lt;/a&gt;&lt;/div&gt;             (all &lt;a&gt;'s directly inside a &lt;div&gt;)
# "div &lt; a"                                            (all &lt;div&gt;'s directly containing an &lt;a&gt;)

# Selectors are case-insensitive.


</t>
<t tx="karstenw.20230303141230.294">def __init__(self, html, **kwargs):
    """ Document is the top-level element in the Document Object Model.
        It contains nested Element, Text and Comment nodes.
    """
    # Aliases for BeautifulSoup optional parameters:
    # kwargs["selfClosingTags"] = kwargs.pop("self_closing", kwargs.get("selfClosingTags"))
    Node.__init__(self, u(html).strip(), type=DOCUMENT, **kwargs)

</t>
<t tx="karstenw.20230303141230.295">@property
def declaration(self):
    """ Yields the &lt;!doctype&gt; declaration, as a TEXT Node or None.
    """
    for child in self.children:
        if isinstance(child._p, (BeautifulSoup.Declaration, BeautifulSoup.Doctype)):
            return child

</t>
<t tx="karstenw.20230303141230.296">@property
def head(self):
    return self._wrap(self._p.head)

</t>
<t tx="karstenw.20230303141230.297">@property
def body(self):
    return self._wrap(self._p.body)

</t>
<t tx="karstenw.20230303141230.298">@property
def tagname(self):
    return None

tag = tagname

</t>
<t tx="karstenw.20230303141230.299">def __repr__(self):
    return "Document()"

</t>
<t tx="karstenw.20230303141230.3">class HTTP403Forbidden(HTTPError):
    pass # URL is not accessible (user-agent?)


</t>
<t tx="karstenw.20230303141230.30">def bind(object, method, function):
    """ Attaches the function as a method with the given name to the given object.
    """
    if new:
        # Python 2
        setattr(object, method, new.instancemethod(function, object))
    else:
        # Python 3: There is no good reason to use this function in Python 3.
        setattr(object, method, function)


</t>
<t tx="karstenw.20230303141230.300">def _encode_space(s):
    return s.replace(" ", "&lt;!space!&gt;")


</t>
<t tx="karstenw.20230303141230.301">def _decode_space(s):
    return s.replace("&lt;!space!&gt;", " ")


</t>
<t tx="karstenw.20230303141230.302">class Selector(object):

    @others
</t>
<t tx="karstenw.20230303141230.303">def __init__(self, s):
    """ A simple CSS selector is a type (e.g., "p") or universal ("*") selector
        followed by id selectors, attribute selectors, or pseudo-elements.
    """
    self.string = s
    s = s.strip()
    s = s.lower()
    s = s.startswith(("#", ".", ":")) and "*" + s or s
    s = s.replace("#", " #") + " #" # #id
    s = s.replace(".", " .")        # .class
    s = s.replace(":", " :")        # :pseudo-element
    s = s.replace("[", " [")        # [attribute="value"]
    s = re.sub(r"\[.*?\]",
        lambda m: re.sub(r" (\#|\.|\:)", "\\1", m.group(0)), s)
    s = re.sub(r"\[.*?\]",
        lambda m: _encode_space(m.group(0)), s)
    s = re.sub(r":contains\(.*?\)",
        lambda m: _encode_space(m.group(0)), s)
    s = s.split(" ")
    self.tag, self.id, self.classes, self.pseudo, self.attributes = (
         s[0],
          [x[1:] for x in s if x[0] == "#"][0],
      set([x[1:] for x in s if x[0] == "."]),
      set([x[1:] for x in s if x[0] == ":"]),
     dict(self._parse_attribute(x) for x in s if x[0] == "[")
    )

</t>
<t tx="karstenw.20230303141230.304">def _parse_attribute(self, s):
    """ Returns an (attribute, value)-tuple for the given attribute selector.
    """
    s = s.strip("[]")
    s = s.replace("'", "")
    s = s.replace('"', "")
    s = _decode_space(s)
    s = re.sub(r"(\~|\||\^|\$|\*)\=", "=\\1", s)
    s = s.split("=") + [True]
    s = s[:2]
    if s[1] is not True:
        r = r"^%s$"
        if s[1].startswith(("~", "|", "^", "$", "*")):
            p, s[1] = s[1][0], s[1][1:]
            if p == "~":
                r = r"(^|\s)%s(\s|$)"
            if p == "|":
                r = r"^%s(-|$)" # XXX doesn't work with spaces.
            if p == "^":
                r = r"^%s"
            if p == "$":
                r = r"%s$"
            if p == "*":
                r = r"%s"
        s[1] = re.compile(r % s[1], re.I)
    return s[:2]

</t>
<t tx="karstenw.20230303141230.305">def _first_child(self, e):
    """ Returns the first child Element of the given element.
    """
    if isinstance(e, Node):
        for e in e.children:
            if isinstance(e, Element):
                return e

</t>
<t tx="karstenw.20230303141230.306">def _next_sibling(self, e):
    """ Returns the first next sibling Element of the given element.
    """
    while isinstance(e, Node):
        e = e.next_element
        if isinstance(e, Element):
            return e

</t>
<t tx="karstenw.20230303141230.307">def _previous_sibling(self, e):
    """ Returns the last previous sibling Element of the given element.
    """
    while isinstance(e, Node):
        e = e.previous_element
        if isinstance(e, Element):
            return e

</t>
<t tx="karstenw.20230303141230.308">def _contains(self, e, s):
    """ Returns True if string s occurs in the given element (case-insensitive).
    """
    s = re.sub(r"^contains\((.*?)\)$", "\\1", s)
    s = re.sub(r"^[\"']|[\"']$", "", s)
    s = _decode_space(s)
    return re.search(s.lower(), e.content.lower()) is not None

</t>
<t tx="karstenw.20230303141230.309">def match(self, e):
    """ Returns True if the given element matches the simple CSS selector.
    """
    if not isinstance(e, Element):
        return False
    if self.tag not in (e.tag, "*"):
        return False
    if self.id not in ((e.id or "").lower(), "", None):
        return False
    if self.classes.issubset(set(map(lambda s: s.lower(), e.attr.get("class", "")))) is False:
        return False
    if "first-child" in self.pseudo and self._first_child(e.parent) != e:
        return False
    if any(x.startswith("contains") and not self._contains(e, x) for x in self.pseudo):
        return False # jQuery :contains("...") selector.
    for k, v in self.attributes.items():
        if k not in e.attrs or not (v is True or re.search(v, " ".join(e.attrs[k])) is not None):
            return False
    return True

</t>
<t tx="karstenw.20230303141230.31">class Stream(list):

    @others
</t>
<t tx="karstenw.20230303141230.310">def search(self, e):
    """ Returns the nested elements that match the simple CSS selector.
    """
    # Map tag to True if it is "*".
    tag = self.tag == "*" or self.tag
    # Map id into a case-insensitive **kwargs dict.
    i = lambda s: re.compile(r"\b%s(?=$|\s)" % s, re.I)
    a = {"id": i(self.id)} if self.id else {}
    a.update(list(map(lambda kv: (kv[0], kv[1]), list(self.attributes.items()))))
    # Match tag + id + all classes + relevant pseudo-elements.
    if not isinstance(e, Element):
        return []
    if len(self.classes) == 0 or len(self.classes) &gt;= 2:
        e = list(map(Element, e._p.find_all(tag, attrs=a)))
    if len(self.classes) == 1:
        e = list(map(Element, e._p.find_all(tag, attrs=dict(a, **{"class": i(list(self.classes)[0])}))))
    if len(self.classes) &gt;= 2:
        e = list(filter(lambda e: self.classes.issubset(set(e.attr.get("class", ""))), e))
    if "first-child" in self.pseudo:
        e = list(filter(lambda e: e == self._first_child(e.parent), e))
    if any(x.startswith("contains") for x in self.pseudo):
        e = list(filter(lambda e: all(not x.startswith("contains") or self._contains(e, x) for x in self.pseudo), e))
    return e

</t>
<t tx="karstenw.20230303141230.311">def __repr__(self):
    return "Selector(%s)" % repr(self.string)


</t>
<t tx="karstenw.20230303141230.312">class SelectorChain(list):

    @others
#dom = DOM("""
#&lt;html&gt;
#&lt;head&gt;&lt;/head&gt;
#&lt;body&gt;
#    &lt;div id="#main"&gt;
#        &lt;span class="11 22 33"&gt;x&lt;/span&gt;
#    &lt;/div&gt;
#&lt;/body&gt;
#&lt;/hmtl&gt;
#""")
#
#print(dom("*[class='11']"))
#print(dom("*[class^='11']"))
#print(dom("*[class~='22']"))
#print(dom("*[class$='33']"))
#print(dom("*[class*='3']"))

#### WEB CRAWLER ###################################################################################
# Tested with a crawl across 1,000 domains so far.


</t>
<t tx="karstenw.20230303141230.313">def __init__(self, s):
    """ A selector is a chain of one or more simple selectors,
        separated by combinators (e.g., "&gt;").
    """
    self.string = s
    for s in s.split(","):
        s = s.lower()
        s = s.strip()
        s = re.sub(r" +", " ", s)
        s = re.sub(r" *\&gt; *", " &gt;", s)
        s = re.sub(r" *\&lt; *", " &lt;", s)
        s = re.sub(r" *\+ *", " +", s)
        s = re.sub(r"\[.*?\]",
            lambda m: _encode_space(m.group(0)), s)
        s = re.sub(r":contains\(.*?\)",
            lambda m: _encode_space(m.group(0)), s)
        self.append([])
        for s in s.split(" "):
            if not s.startswith(("&gt;", "&lt;", "+")):
                self[-1].append((" ", Selector(s)))
            elif s.startswith("&gt;"):
                self[-1].append(("&gt;", Selector(s[1:])))
            elif s.startswith("&lt;"):
                self[-1].append(("&lt;", Selector(s[1:])))
            elif s.startswith("+"):
                self[-1].append(("+", Selector(s[1:])))

</t>
<t tx="karstenw.20230303141230.314">def search(self, e):
    """ Returns the nested elements that match the CSS selector chain.
    """
    m, root = [], e
    for chain in self:
        e = [root]
        for combinator, s in chain:
            # Search Y, where:
            if combinator == " ":
                # X Y =&gt; X is ancestor of Y
                e = list(map(s.search, e))
                e = list(itertools.chain(*e))
            if combinator == "&gt;":
                # X &gt; Y =&gt; X is parent of Y
                e = list(map(lambda e: list(filter(s.match, e.children)), e))
                e = list(itertools.chain(*e))
            if combinator == "&lt;":
                # X &lt; Y =&gt; X is child of Y
                e = list(map(lambda e: e.parent, e))
                e = list(filter(s.match, e))
            if combinator == "+":
                # X + Y =&gt; X directly precedes Y
                e = list(map(s._next_sibling, e))
                e = list(filter(s.match, e))
        m.extend(e)
    return m

</t>
<t tx="karstenw.20230303141230.315">class Link(object):

    @others
</t>
<t tx="karstenw.20230303141230.316">def __init__(self, url, text="", relation="", referrer=""):
    """ A hyperlink parsed from a HTML document, in the form:
        &lt;a href="url"", title="text", rel="relation"&gt;xxx&lt;/a&gt;.
    """
    self.url, self.text, self.relation, self.referrer = \
        u(url), u(text), u(relation), u(referrer),

</t>
<t tx="karstenw.20230303141230.317">@property
def description(self):
    return self.text

</t>
<t tx="karstenw.20230303141230.318">def __repr__(self):
    return "Link(url=%s)" % repr(self.url)

# Used for sorting in Crawler.links:
</t>
<t tx="karstenw.20230303141230.319">def __eq__(self, link):
    return self.url == link.url

</t>
<t tx="karstenw.20230303141230.32">def __init__(self, url, delimiter="\n", **kwargs):
    """ Buffered stream of data from a given URL.
    """
    self.socket = URL(url).open(**kwargs)
    self.buffer = ""
    self.delimiter = delimiter

</t>
<t tx="karstenw.20230303141230.320">def __ne__(self, link):
    return self.url != link.url

</t>
<t tx="karstenw.20230303141230.321">def __lt__(self, link):
    return self.url &lt; link.url

</t>
<t tx="karstenw.20230303141230.322">def __gt__(self, link):
    return self.url &gt; link.url


</t>
<t tx="karstenw.20230303141230.323">class HTMLLinkParser(HTMLParser):

    @others
</t>
<t tx="karstenw.20230303141230.324">def __init__(self):
    HTMLParser.__init__(self)

</t>
<t tx="karstenw.20230303141230.325">def parse(self, html, url=""):
    """ Returns a list of Links parsed from the given HTML string.
    """
    if html is None:
        return None
    self._url = url
    self._data = []
    self.feed(self.clean(html))
    self.close()
    self.reset()
    return self._data

</t>
<t tx="karstenw.20230303141230.326">def handle_starttag(self, tag, attributes):
    if tag == "a":
        attributes = dict(attributes)
        if "href" in attributes:
            link = Link(url = attributes.get("href"),
                       text = attributes.get("title"),
                   relation = attributes.get("rel", ""),
                   referrer = self._url)
            self._data.append(link)


</t>
<t tx="karstenw.20230303141230.327">def base(url):
    """ Returns the URL domain name:
        http://en.wikipedia.org/wiki/Web_crawler =&gt; en.wikipedia.org
    """
    return urlparse(url).netloc


</t>
<t tx="karstenw.20230303141230.328">def abs(url, base=None):
    """ Returns the absolute URL:
        ../media + http://en.wikipedia.org/wiki/ =&gt; http://en.wikipedia.org/media
    """
    if url.startswith("#") and base is not None and not base.endswith("/"):
        if not re.search("[^/]/[^/]", base):
            base += "/"
    return urljoin(base, url)

DEPTH = "depth"
BREADTH = "breadth"

FIFO = "fifo" # First In, First Out.
FILO = "filo" # First In, Last Out.
LIFO = "lifo" # Last In, First Out (= FILO).


</t>
<t tx="karstenw.20230303141230.329">class Crawler(object):

    @others
Spider = Crawler

#class Polly(Crawler):
#    def visit(self, link, source=None):
#        print("visited:", link.url, "from:", link.referrer)
#    def fail(self, link):
#        print("failed:", link.url)
#
#p = Polly(links=["http://nodebox.net/"], domains=["nodebox.net"], delay=5)
#while not p.done:
#    p.crawl(method=DEPTH, cached=True, throttle=5)

#--- CRAWL FUNCTION --------------------------------------------------------------------------------
# Functional approach to crawling.


</t>
<t tx="karstenw.20230303141230.33">def update(self, bytes=1024):
    """ Reads a number of bytes from the stream.
        If a delimiter is encountered, calls Stream.parse() on the packet.
    """
    packets = []
    self.buffer += self.socket.read(bytes).decode("utf-8")
    self.buffer = self.buffer.split(self.delimiter, 1)
    while len(self.buffer) &gt; 1:
        data = self.buffer[0]
        data = self.parse(data)
        if data is not None:
            packets.append(data)
        self.buffer = self.buffer[-1]
        self.buffer = self.buffer.split(self.delimiter, 1)
    self.buffer = self.buffer[-1]
    self.extend(packets)
    return packets

</t>
<t tx="karstenw.20230303141230.330">def __init__(self, links=[], domains=[], delay=20.0, parse=HTMLLinkParser().parse, sort=FIFO):
    """ A crawler can be used to browse the web in an automated manner.
        It visits the list of starting URLs, parses links from their content, visits those, etc.
        - Links can be prioritized by overriding Crawler.priority().
        - Links can be ignored by overriding Crawler.follow().
        - Each visited link is passed to Crawler.visit(), which can be overridden.
    """
    self.parse    = parse
    self.delay    = delay   # Delay between visits to the same (sub)domain.
    self.domains  = domains # Domains the crawler is allowed to visit.
    self.history  = {}      # Domain name =&gt; time last visited.
    self.visited  = {}      # URLs visited.
    self._queue   = []      # URLs scheduled for a visit: (priority, time, Link).
    self._queued  = {}      # URLs scheduled so far, lookup dictionary.
    self.QUEUE    = 10000   # Increase or decrease according to available memory.
    self.sort     = sort
    # Queue given links in given order:
    for link in (isinstance(links, str) and [links] or links):
        self.push(link, priority=1.0, sort=FIFO)

</t>
<t tx="karstenw.20230303141230.331">@property
def done(self):
    """ Yields True if no further links are scheduled to visit.
    """
    return len(self._queue) == 0

</t>
<t tx="karstenw.20230303141230.332">def push(self, link, priority=1.0, sort=FILO):
    """ Pushes the given link to the queue.
        Position in the queue is determined by priority.
        Equal ranks are sorted FIFO or FILO.
        With priority=1.0 and FILO, the link is inserted to the queue.
        With priority=0.0 and FIFO, the link is appended to the queue.
    """
    if not isinstance(link, Link):
        link = Link(url=link)
    dt = time.time()
    dt = sort == FIFO and dt or 1 / dt
    bisect.insort(self._queue, (1 - priority, dt, link))
    self._queued[link.url] = True

</t>
<t tx="karstenw.20230303141230.333">def pop(self, remove=True):
    """ Returns the next Link queued to visit and removes it from the queue.
        Links on a recently visited (sub)domain are skipped until Crawler.delay has elapsed.
    """
    now = time.time()
    for i, (priority, dt, link) in enumerate(self._queue):
        if self.delay &lt;= now - self.history.get(base(link.url), 0):
            if remove is True:
                self._queue.pop(i)
                self._queued.pop(link.url, None)
            return link

</t>
<t tx="karstenw.20230303141230.334">@property
def next(self):
    """ Returns the next Link queued to visit (without removing it).
    """
    return self.pop(remove=False)

</t>
<t tx="karstenw.20230303141230.335">def crawl(self, method=DEPTH, **kwargs):
    """ Visits the next link in Crawler._queue.
        If the link is on a domain recently visited (&lt; Crawler.delay) it is skipped.
        Parses the content at the link for new links and adds them to the queue,
        according to their Crawler.priority().
        Visited links (and content) are passed to Crawler.visit().
    """
    link = self.pop()
    if link is None:
        return False
    if link.url not in self.visited:
        t = time.time()
        url = URL(link.url)
        if url.mimetype == "text/html":
            try:
                kwargs.setdefault("unicode", True)
                html = url.download(**kwargs)
                for new in self.parse(html, url=link.url):
                    new.url = abs(new.url, base=url.redirect or link.url)
                    new.url = self.normalize(new.url)
                    # 1) Parse new links from HTML web pages.
                    # 2) Schedule unknown links for a visit.
                    # 3) Only links that are not already queued are queued.
                    # 4) Only links for which Crawler.follow() is True are queued.
                    # 5) Only links on Crawler.domains are queued.
                    if new.url == link.url:
                        continue
                    if new.url in self.visited:
                        continue
                    if new.url in self._queued:
                        continue
                    if self.follow(new) is False:
                        continue
                    if self.domains and not base(new.url).endswith(tuple(self.domains)):
                        continue
                    # 6) Limit the queue (remove tail), unless you are Google.
                    if self.QUEUE is not None and \
                       self.QUEUE * 1.25 &lt; len(self._queue):
                        self._queue = self._queue[:self.QUEUE]
                        self._queued.clear()
                        self._queued.update(dict((q[2].url, True) for q in self._queue))
                    # 7) Position in the queue is determined by Crawler.priority().
                    # 8) Equal ranks are sorted FIFO or FILO.
                    self.push(new, priority=self.priority(new, method=method), sort=self.sort)
                self.visit(link, source=html)
            except URLError:
                # URL can not be reached (HTTP404NotFound, URLTimeout).
                self.fail(link)
        else:
            # URL MIME-type is not HTML, don't know how to handle.
            self.fail(link)
        # Log the current time visited for the domain (see Crawler.pop()).
        # Log the URL as visited.
        self.history[base(link.url)] = t
        self.visited[link.url] = True
        return True
    # Nothing happened, we already visited this link.
    return False

</t>
<t tx="karstenw.20230303141230.336">def normalize(self, url):
    """ Called from Crawler.crawl() to normalize URLs.
        For example: return url.split("?")[0]
    """
    # All links pass through here (visited or not).
    # This can be a place to count backlinks.
    return url

</t>
<t tx="karstenw.20230303141230.337">def follow(self, link):
    """ Called from Crawler.crawl() to determine if it should follow this link.
        For example: return "nofollow" not in link.relation
    """
    return True

</t>
<t tx="karstenw.20230303141230.338">def priority(self, link, method=DEPTH):
    """ Called from Crawler.crawl() to determine the priority of this link,
        as a number between 0.0-1.0. Links with higher priority are visited first.
    """
    # Depth-first search dislikes external links to other (sub)domains.
    external = base(link.url) != base(link.referrer)
    if external is True:
        if method == DEPTH:
            return 0.75
        if method == BREADTH:
            return 0.85
    return 0.80

</t>
<t tx="karstenw.20230303141230.339">def visit(self, link, source=None):
    """ Called from Crawler.crawl() when the link is crawled.
        When source=None, the link is not a web page (and was not parsed),
        or possibly a URLTimeout occured (content size too big).
    """
    pass

</t>
<t tx="karstenw.20230303141230.34">def parse(self, data):
    """ Must be overridden in a subclass.
    """
    return data

</t>
<t tx="karstenw.20230303141230.340">def fail(self, link):
    """ Called from Crawler.crawl() for link whose MIME-type could not be determined,
        or which raised a URLError on download.
    """
    pass

</t>
<t tx="karstenw.20230303141230.341">def crawl(links=[], domains=[], delay=20.0, parse=HTMLLinkParser().parse, sort=FIFO, method=DEPTH, **kwargs):
    """ Returns a generator that yields (Link, source)-tuples of visited pages.
        When the crawler is idle, it yields (None, None).
    """
    # The scenarios below defines "idle":
    # - crawl(delay=10, throttle=0)
    #   The crawler will wait 10 seconds before visiting the same subdomain.
    #   The crawler will not throttle downloads, so the next link is visited instantly.
    #   So sometimes (None, None) is returned while it waits for an available subdomain.
    # - crawl(delay=0, throttle=10)
    #   The crawler will wait 10 seconds after each and any visit.
    #   The crawler will not delay before visiting the same subdomain.
    #   So usually a result is returned each crawl.next(), but each call takes 10 seconds.
    # - asynchronous(crawl().next)
    #   AsynchronousRequest.value is set to (Link, source) once AsynchronousRequest.done=True.
    #   The program will not halt in the meantime (i.e., the next crawl is threaded).
    crawler = Crawler(links, domains, delay, parse, sort)
    bind(crawler, "visit", \
        lambda crawler, link, source=None: \
            setattr(crawler, "crawled", (link, source))) # Define Crawler.visit() on-the-fly.
    while not crawler.done:
        crawler.crawled = (None, None)
        crawler.crawl(method, **kwargs)
        yield crawler.crawled

#for link, source in crawl("http://www.clips.ua.ac.be/", delay=0, throttle=1, cached=False):
#    print(link)

#g = crawl("http://www.clips.ua.ac.be/"")
#for i in range(10):
#    p = asynchronous(g.next)
#    while not p.done:
#        print("zzz...")
#        time.sleep(0.1)
#    link, source = p.value
#    print(link)


#### DOCUMENT PARSER ###############################################################################
# Not to be confused with Document, which is the top-level element in the HTML DOM.

</t>
<t tx="karstenw.20230303141230.342">class DocumentParserError(Exception):
    pass


</t>
<t tx="karstenw.20230303141230.343">class DocumentParser(object):

    @others
#--- PDF PARSER ------------------------------------------------------------------------------------
#  Yusuke Shinyama, PDFMiner, http://www.unixuser.org/~euske/python/pdfminer/


</t>
<t tx="karstenw.20230303141230.344">def __init__(self, path, *args, **kwargs):
    """ Parses a text document (e.g., .pdf or .docx),
        given as a file path or a string.
    """
    self.content = self._parse(path, *args, **kwargs)

</t>
<t tx="karstenw.20230303141230.345">def _open(self, path):
    """ Returns a file-like object with a read() method,
        from the given file path or string.
    """
    if isinstance(path, str) and os.path.exists(path):
        return open(path, "rb")
    if hasattr(path, "read"):
        return path
    return StringIO(path)

</t>
<t tx="karstenw.20230303141230.346">def _parse(self, path, *args, **kwargs):
    """ Returns a plaintext Unicode string parsed from the given document.
    """
    return plaintext(decode_utf8(self.open(path).read()))

</t>
<t tx="karstenw.20230303141230.347">@property
def string(self):
    return self.content

</t>
<t tx="karstenw.20230303141230.348">def __str__(self):
    return self.content

</t>
<t tx="karstenw.20230303141230.349">class PDFError(DocumentParserError):
    pass


</t>
<t tx="karstenw.20230303141230.35">def clear(self):
    list.__init__(self, [])


</t>
<t tx="karstenw.20230303141230.350">class PDF(DocumentParser):

    @others
#--- OOXML PARSER ----------------------------------------------------------------------------------
# python-docx, https://github.com/python-openxml/python-docx


</t>
<t tx="karstenw.20230303141230.351">def __init__(self, path, output="txt"):
    self.content = self._parse(path, format=output)

</t>
<t tx="karstenw.20230303141230.352">def _parse(self, path, *args, **kwargs):
    # The output is useful for mining but not for display.
    # Alternatively, PDF(format="html") preserves some layout.
    from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
    from pdfminer.pdfpage import PDFPage
    from pdfminer.converter import TextConverter, HTMLConverter
    from pdfminer.layout import LAParams
    try:
        m = PDFResourceManager()
        s = StringIO()
        p = kwargs.get("format", "txt").endswith("html") and HTMLConverter or TextConverter
        p = p(m, s, codec="utf-8", laparams=LAParams())
        interpreter = PDFPageInterpreter(m, p)
        f = self._open(path)
        for page in PDFPage.get_pages(f, maxpages=0, password=""):
            interpreter.process_page(page)
        f.close()
    except Exception as e:
        raise PDFError(str(e))
    s = s.getvalue()
    s = decode_utf8(s)
    s = s.strip()
    s = re.sub(r"([a-z])\-\n", "\\1", s) # Hyphenation.
    s = s.replace("\n\n", "&lt;!-- #p --&gt;") # Paragraphs.
    s = s.replace("\n", " ")
    s = s.replace("&lt;!-- #p --&gt;", "\n\n")
    s = collapse_spaces(s)
    return s

</t>
<t tx="karstenw.20230303141230.353">class DOCXError(DocumentParserError):
    pass


</t>
<t tx="karstenw.20230303141230.354">class DOCX(DocumentParser):

    @others
#---------------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303141230.355">def _parse(self, path, *args, **kwargs):
    from docx import Document
    document = Document(path)
    try:
        s = [paragraph.text for paragraph in document.paragraphs]
    except Exception as e:
        raise DOCXError(str(e))
    s = "\n\n".join(p for p in s)
    s = decode_utf8(s)
    s = collapse_spaces(s)
    return s

</t>
<t tx="karstenw.20230303141230.356">def parsepdf(path, *args, **kwargs):
    """ Returns the content as a Unicode string from the given .pdf file.
    """
    return PDF(path, *args, **kwargs).content


</t>
<t tx="karstenw.20230303141230.357">def parsedocx(path, *args, **kwargs):
    """ Returns the content as a Unicode string from the given .docx file.
    """
    return DOCX(path, *args, **kwargs).content


</t>
<t tx="karstenw.20230303141230.358">def parsehtml(path, *args, **kwargs):
    """ Returns the content as a Unicode string from the given .html file.
    """
    return plaintext(DOM(path, *args, **kwargs).body)


</t>
<t tx="karstenw.20230303141230.359">def parsedoc(path, format=None):
    """ Returns the content as a Unicode string from the given document (.html., .pdf, .docx).
    """
    if isinstance(path, str):
        if format == "pdf" or path.endswith(".pdf"):
            return parsepdf(path)
        if format == "docx" or path.endswith(".docx"):
            return parsedocx(path)
        if format == "html" or path.endswith((".htm", ".html", ".xhtml")):
            return parsehtml(path)
    # Brute-force approach if the format is unknown.
    for f in (parsepdf, parsedocx, parsehtml):
        try:
            return f(path)
        except:
            pass
</t>
<t tx="karstenw.20230303141230.36">def stream(url, delimiter="\n", parse=lambda data: data, **kwargs):
    """ Returns a new Stream with the given parse method.
    """
    stream = Stream(url, delimiter, **kwargs)
    bind(stream, "parse", lambda stream, data: parse(data))
    return stream

#--- FIND URLs -------------------------------------------------------------------------------------
# Functions for parsing URL's and e-mail adresses from strings.

RE_URL_PUNCTUATION = ("\"'{(&gt;", "\"'.,;)}")
RE_URL_HEAD = r"[%s|\[|\s]" % "|".join(RE_URL_PUNCTUATION[0])      # Preceded by space, parenthesis or HTML tag.
RE_URL_TAIL = r"[%s|\]]*[\s|\&lt;]" % "|".join(RE_URL_PUNCTUATION[1]) # Followed by space, punctuation or HTML tag.
RE_URL1 = r"(https?://.*?)" + RE_URL_TAIL                          # Starts with http:// or https://
RE_URL2 = RE_URL_HEAD + r"(www\..*?\..*?)" + RE_URL_TAIL           # Starts with www.
RE_URL3 = RE_URL_HEAD + r"([\w|-]*?\.(com|net|org|edu|de|uk))" + RE_URL_TAIL

RE_URL1, RE_URL2, RE_URL3 = (
    re.compile(RE_URL1, re.I),
    re.compile(RE_URL2, re.I),
    re.compile(RE_URL3, re.I))


</t>
<t tx="karstenw.20230303141230.37">def find_urls(string, unique=True):
    """ Returns a list of URLs parsed from the string.
        Works on http://, https://, www. links or domain names ending in .com, .org, .net.
        Links can be preceded by leading punctuation (open parens)
        and followed by trailing punctuation (period, comma, close parens).
    """
    string = u(string)
    string = string.replace("\u2024", ".")
    string = string.replace(" ", "  ")
    matches = []
    for p in (RE_URL1, RE_URL2, RE_URL3):
        for m in p.finditer(" %s " % string):
            s = m.group(1)
            s = s.split("\"&gt;")[0].split("'&gt;")[0] # google.com"&gt;Google =&gt; google.com
            if not unique or s not in matches:
                matches.append(s)
    return matches

links = find_urls

RE_EMAIL = re.compile(r"[\w\-\.\+]+@(\w[\w\-]+\.)+[\w\-]+") # tom.de+smedt@clips.ua.ac.be


</t>
<t tx="karstenw.20230303141230.38">def find_email(string, unique=True):
    """ Returns a list of e-mail addresses parsed from the string.
    """
    string = u(string).replace("\u2024", ".")
    matches = []
    for m in RE_EMAIL.finditer(string):
        s = m.group(0)
        if not unique or s not in matches:
            matches.append(s)
    return matches


</t>
<t tx="karstenw.20230303141230.39">def find_between(a, b, string):
    """ Returns a list of substrings between a and b in the given string.
    """
    p = "%s(.*?)%s" % (a, b)
    p = re.compile(p, re.DOTALL | re.I)
    return [m for m in p.findall(string)]

#### PLAIN TEXT ####################################################################################
# Functions for stripping HTML tags from strings.

BLOCK = [
    "title", "h1", "h2", "h3", "h4", "h5", "h6", "p",
    "center", "blockquote", "div", "table", "ul", "ol", "dl", "pre", "code", "form"
]

SELF_CLOSING = ["br", "hr", "img"]

# Element tag replacements for a stripped version of HTML source with strip_tags().
# Block-level elements are followed by linebreaks,
# list items are preceded by an asterisk ("*").
LIST_ITEM = "*"
blocks = dict.fromkeys(BLOCK + ["br", "tr", "td"], ("", "\n\n"))
blocks.update({
    "li": ("%s " % LIST_ITEM, "\n"),
   "img": ("", ""),
    "br": ("", "\n"),
    "th": ("", "\n"),
    "tr": ("", "\n"),
    "td": ("", "\t"),
})


</t>
<t tx="karstenw.20230303141230.4">class HTTP404NotFound(HTTPError):
    pass # URL doesn't exist on the internet.


</t>
<t tx="karstenw.20230303141230.40">class HTMLParser(_HTMLParser):

    @others
</t>
<t tx="karstenw.20230303141230.41">def clean(self, html):
    html = decode_utf8(html)
    html = html.replace("/&gt;", " /&gt;")
    html = html.replace("  /&gt;", " /&gt;")
    html = html.replace("&lt;!", "&amp;lt;!")
    html = html.replace("&amp;lt;!DOCTYPE", "&lt;!DOCTYPE")
    html = html.replace("&amp;lt;!doctype", "&lt;!doctype")
    html = html.replace("&amp;lt;!--", "&lt;!--")
    return html


</t>
<t tx="karstenw.20230303141230.42">class HTMLTagstripper(HTMLParser):

    @others
# As a function:
strip_tags = HTMLTagstripper().strip


</t>
<t tx="karstenw.20230303141230.43">def __init__(self):
    HTMLParser.__init__(self)

</t>
<t tx="karstenw.20230303141230.44">def strip(self, html, exclude=[], replace=blocks):
    """ Returns the HTML string with all element tags (e.g. &lt;p&gt;) removed.
        - exclude    : a list of tags to keep. Element attributes are stripped.
                       To preserve attributes a dict of (tag name, [attribute])-items can be given.
        - replace    : a dictionary of (tag name, (replace_before, replace_after))-items.
                       By default, block-level elements are separated with linebreaks.
    """
    if html is None:
        return None
    self._exclude = isinstance(exclude, dict) and exclude or dict.fromkeys(exclude, [])
    self._replace = replace
    self._data = []
    self.feed(self.clean(html))
    self.close()
    self.reset()
    return "".join(self._data)

</t>
<t tx="karstenw.20230303141230.45">def handle_starttag(self, tag, attributes):
    if tag in BLOCK and self._data and self._data[-1][-1:] != "\n":
        # Block-level elements always break to a new line.
        self._data.append("\n")
    if tag in self._exclude:
        # Create the tag attribute string,
        # including attributes defined in the HTMLTagStripper._exclude dict.
        a = len(self._exclude[tag]) &gt; 0 and attributes or []
        a = ["%s=\"%s\"" % (k, v) for k, v in a if k in self._exclude[tag]]
        a = (" " + " ".join(a)).rstrip()
        self._data.append("&lt;%s%s&gt;" % (tag, a))
    if tag in self._replace:
        self._data.append(self._replace[tag][0])
    if tag in self._replace and tag in SELF_CLOSING:
        self._data.append(self._replace[tag][1])

</t>
<t tx="karstenw.20230303141230.46">def handle_endtag(self, tag):
    if tag in self._exclude and self._data and self._data[-1].startswith("&lt;" + tag):
        # Never keep empty elements (e.g. &lt;a&gt;&lt;/a&gt;).
        self._data.pop(-1)
        return
    if tag in self._exclude:
        self._data.append("&lt;/%s&gt;" % tag)
    if tag in self._replace:
        self._data.append(self._replace[tag][1])

</t>
<t tx="karstenw.20230303141230.47">def handle_data(self, data):
    self._data.append(data.strip("\n\t"))

</t>
<t tx="karstenw.20230303141230.48">def handle_comment(self, comment):
    if "comment" in self._exclude or \
           "!--" in self._exclude:
        self._data.append("&lt;!--%s--&gt;" % comment)

</t>
<t tx="karstenw.20230303141230.49">def strip_element(string, tag, attributes=""):
    """ Removes all elements with the given tagname and attributes from the string.
        Open and close tags are kept in balance.
        No HTML parser is used: strip_element(s, "a", 'class="x"') matches
        '&lt;a class="x"&gt;' or '&lt;a href="x" class="x"&gt;' but not "&lt;a class='x'&gt;".
    """
    s = string.lower() # Case-insensitive.
    t = tag.strip("&lt;/&gt;")
    a = (" " + attributes.lower().strip()).rstrip()
    i = 0
    j = 0
    while j &gt;= 0:
        #i = s.find("&lt;%s%s" % (t, a), i)
        m = re.search(r"&lt;%s[^\&gt;]*?%s" % (t, a), s[i:])
        i = i + m.start() if m else -1
        j = s.find("&lt;/%s&gt;" % t, i + 1)
        opened, closed = s[i:j].count("&lt;%s" % t), 1
        while opened &gt; closed and j &gt;= 0:
            k = s.find("&lt;/%s&gt;" % t, j + 1)
            opened += s[j:k].count("&lt;%s" % t)
            closed += 1
            j = k
        if i &lt; 0:
            return string
        if j &lt; 0:
            return string[:i]
        string = string[:i] + string[j + len(t) + 3:]; s = string.lower()
    return string


</t>
<t tx="karstenw.20230303141230.5">class HTTP414RequestURITooLong(HTTPError):
    pass # URL is too long.


</t>
<t tx="karstenw.20230303141230.50">def strip_between(a, b, string):
    """ Removes anything between (and including) string a and b inside the given string.
    """
    p = "%s.*?%s" % (a, b)
    p = re.compile(p, re.DOTALL | re.I)
    return re.sub(p, "", string)


</t>
<t tx="karstenw.20230303141230.51">def strip_javascript(html):
    return strip_between("&lt;script.*?&gt;", "&lt;/script&gt;", html)


</t>
<t tx="karstenw.20230303141230.52">def strip_inline_css(html):
    return strip_between("&lt;style.*?&gt;", "&lt;/style&gt;", html)


</t>
<t tx="karstenw.20230303141230.53">def strip_comments(html):
    return strip_between("&lt;!--", "--&gt;", html)


</t>
<t tx="karstenw.20230303141230.54">def strip_forms(html):
    return strip_between("&lt;form.*?&gt;", "&lt;/form&gt;", html)

RE_AMPERSAND = re.compile("\&amp;(?!\#)")           # &amp; not followed by #
RE_UNICODE = re.compile(r'&amp;(#?)(x|X?)(\w+);') # &amp;#201;


</t>
<t tx="karstenw.20230303141230.55">def encode_entities(string):
    """ Encodes HTML entities in the given string ("&lt;" =&gt; "&amp;lt;").
        For example, to display "&lt;em&gt;hello&lt;/em&gt;" in a browser,
        we need to pass "&amp;lt;em&amp;gt;hello&amp;lt;/em&amp;gt;" (otherwise "hello" in italic is displayed).
    """
    if isinstance(string, str):
        string = RE_AMPERSAND.sub("&amp;amp;", string)
        string = string.replace("&lt;", "&amp;lt;")
        string = string.replace("&gt;", "&amp;gt;")
        string = string.replace('"', "&amp;quot;")
        string = string.replace("'", "&amp;#39;")
    return string


</t>
<t tx="karstenw.20230303141230.56">def decode_entities(string):
    """ Decodes HTML entities in the given string ("&amp;lt;" =&gt; "&lt;").
    """
    # http://snippets.dzone.com/posts/show/4569
    def replace_entity(match):
        hash, hex, name = match.group(1), match.group(2), match.group(3)
        if hash == "#" or name.isdigit():
            if hex == "":
                return chr(int(name))                 # "&amp;#38;" =&gt; "&amp;"
            if hex.lower() == "x":
                return chr(int("0x" + name, 16))      # "&amp;#x0026;" = &gt; "&amp;"
        else:
            cp = name2codepoint.get(name) # "&amp;amp;" =&gt; "&amp;"
            return chr(cp) if cp else match.group()   # "&amp;foo;" =&gt; "&amp;foo;"
    if isinstance(string, str):
        return RE_UNICODE.subn(replace_entity, string)[0]
    return string


</t>
<t tx="karstenw.20230303141230.57">def encode_url(string):
    return quote_plus(bytestring(string)) # "black/white" =&gt; "black%2Fwhite".


</t>
<t tx="karstenw.20230303141230.58">def decode_url(string):
    return u(unquote_plus(bytestring(string)))

RE_SPACES = re.compile("( |\xa0)+", re.M) # Matches one or more spaces.
RE_TABS = re.compile(r"\t+", re.M)      # Matches one or more tabs.


</t>
<t tx="karstenw.20230303141230.59">def collapse_spaces(string, indentation=False, replace=" "):
    """ Returns a string with consecutive spaces collapsed to a single space.
        Whitespace on empty lines and at the end of each line is removed.
        With indentation=True, retains leading whitespace on each line.
    """
    p = []
    for x in string.splitlines():
        n = indentation and len(x) - len(x.lstrip()) or 0
        p.append(x[:n] + RE_SPACES.sub(replace, x[n:]).strip())
    return "\n".join(p)


</t>
<t tx="karstenw.20230303141230.6">class HTTP420Error(HTTPError):
    pass # Used by Twitter for rate limiting.


</t>
<t tx="karstenw.20230303141230.60">def collapse_tabs(string, indentation=False, replace=" "):
    """ Returns a string with (consecutive) tabs replaced by a single space.
        Whitespace on empty lines and at the end of each line is removed.
        With indentation=True, retains leading whitespace on each line.
    """
    p = []
    for x in string.splitlines():
        n = indentation and len(x) - len(x.lstrip()) or 0
        p.append(x[:n] + RE_TABS.sub(replace, x[n:]).strip())
    return "\n".join(p)


</t>
<t tx="karstenw.20230303141230.61">def collapse_linebreaks(string, threshold=1):
    """ Returns a string with consecutive linebreaks collapsed to at most the given threshold.
        Whitespace on empty lines and at the end of each line is removed.
    """
    n = "\n" * threshold
    p = [s.rstrip() for s in string.splitlines()]
    string = "\n".join(p)
    string = re.sub(n + r"+", n, string)
    return string


</t>
<t tx="karstenw.20230303141230.62">def plaintext(html, keep=[], replace=blocks, linebreaks=2, indentation=False):
    """ Returns a string with all HTML tags removed.
        Content inside HTML comments, the &lt;style&gt; tag and the &lt;script&gt; tags is removed.
        - keep        : a list of tags to keep. Element attributes are stripped.
                        To preserve attributes a dict of (tag name, [attribute])-items can be given.
        - replace     : a dictionary of (tag name, (replace_before, replace_after))-items.
                        By default, block-level elements are followed by linebreaks.
        - linebreaks  : the maximum amount of consecutive linebreaks,
        - indentation : keep left line indentation (tabs and spaces)?
    """
    if isinstance(html, Element):
        html = html.content
    if not keep.__contains__("script"):
        html = strip_javascript(html)
    if not keep.__contains__("style"):
        html = strip_inline_css(html)
    if not keep.__contains__("form"):
        html = strip_forms(html)
    if not keep.__contains__("comment") and \
       not keep.__contains__("!--"):
        html = strip_comments(html)
    html = html.replace("\r", "\n")
    html = decode_entities(html)
    html = strip_tags(html, exclude=keep, replace=replace)
    html = collapse_spaces(html, indentation)
    html = collapse_tabs(html, indentation)
    html = collapse_linebreaks(html, linebreaks)
    html = html.strip()
    return html

#### SEARCH ENGINE #################################################################################

SEARCH    = "search"    # Query for pages (i.e. links to websites).
IMAGE     = "image"     # Query for images.
NEWS      = "news"      # Query for news items.

TINY      = "tiny"      # Image size around 100x100.
SMALL     = "small"     # Image size around 200x200.
MEDIUM    = "medium"    # Image size around 500x500.
LARGE     = "large"     # Image size around 1000x1000.

RELEVANCY = "relevancy" # Sort results by most relevant.
LATEST    = "latest"    # Sort results by most recent.


</t>
<t tx="karstenw.20230303141230.63">class Result(dict):

    @others
</t>
<t tx="karstenw.20230303141230.64">def __init__(self, url, **kwargs):
    """ An item in a list of results returned by SearchEngine.search().
        All dictionary keys are available as Unicode string attributes.
        - id       : unique identifier,
        - url      : the URL of the referred web content,
        - title    : the title of the content at the URL,
        - text     : the content text,
        - language : the content language,
        - author   : for news items and posts, the author,
        - date     : for news items and posts, the publication date.
    """
    dict.__init__(self)
    self.url = url
    self.id = kwargs.pop("id", "")
    self.title = kwargs.pop("title", "")
    self.text = kwargs.pop("text", "")
    self.language = kwargs.pop("language", "")
    self.author = kwargs.pop("author", "")
    self.date = kwargs.pop("date", "")
    self.votes = kwargs.pop("votes", 0) # (e.g., Facebook likes)
    self.shares = kwargs.pop("shares", 0) # (e.g., Twitter retweets)
    self.comments = kwargs.pop("comments", 0)
    for k, v in kwargs.items():
        self[k] = v

</t>
<t tx="karstenw.20230303141230.65">@property
def txt(self):
    return self.text

</t>
<t tx="karstenw.20230303141230.66">@property
def description(self):
    return self.text # Backwards compatibility.

</t>
<t tx="karstenw.20230303141230.67">@property
def likes(self):
    return self.votes

</t>
<t tx="karstenw.20230303141230.68">@property
def retweets(self):
    return self.shares

</t>
<t tx="karstenw.20230303141230.69">def download(self, *args, **kwargs):
    """ Download the content at the given URL.
        By default it will be cached - see URL.download().
    """
    return URL(self.url).download(*args, **kwargs)

</t>
<t tx="karstenw.20230303141230.7">class HTTP429TooMayRequests(HTTPError):
    pass # Used by Twitter for rate limiting.


</t>
<t tx="karstenw.20230303141230.70">def _format(self, v):
    if isinstance(v, bytes): # Store strings as unicode.
        return u(v)
    if v is None:
        return ""
    return v

</t>
<t tx="karstenw.20230303141230.71">def __getattr__(self, k):
    return self.get(k, "")

</t>
<t tx="karstenw.20230303141230.72">def __getitem__(self, k):
    return self.get(k, "")

</t>
<t tx="karstenw.20230303141230.73">def __setattr__(self, k, v):
    self.__setitem__(k, v)

</t>
<t tx="karstenw.20230303141230.74">def __setitem__(self, k, v):
    dict.__setitem__(self, u(k), self._format(v))

</t>
<t tx="karstenw.20230303141230.75">def setdefault(self, k, v=None):
    return dict.setdefault(self, u(k), self._format(v))

</t>
<t tx="karstenw.20230303141230.76">def update(self, *args, **kwargs):
    dict.update(self, [(u(k), self._format(v)) for k, v in dict(*args, **kwargs).items()])

</t>
<t tx="karstenw.20230303141230.77">def __repr__(self):
    return "Result(%s)" % repr(dict((k, v) for k, v in self.items() if v))


</t>
<t tx="karstenw.20230303141230.78">class Results(list):

    @others
</t>
<t tx="karstenw.20230303141230.79">def __init__(self, source=None, query=None, type=SEARCH, total=0):
    """ A list of results returned from SearchEngine.search().
        - source: the service that yields the results (e.g. GOOGLE, TWITTER).
        - query : the query that yields the results.
        - type  : the query type (SEARCH, IMAGE, NEWS).
        - total : the total result count.
                  This is not the length of the list, but the total number of matches for the given query.
    """
    self.source = source
    self.query = query
    self.type = type
    self.total = total


</t>
<t tx="karstenw.20230303141230.8">class HTTP500InternalServerError(HTTPError):
    pass # Generic server error.


</t>
<t tx="karstenw.20230303141230.80">class SearchEngine(object):

    @others
</t>
<t tx="karstenw.20230303141230.81">def __init__(self, license=None, throttle=1.0, language=None):
    """ A base class for a web service.
        - license  : license key for the API,
        - throttle : delay between requests (avoid hammering the server).
        Inherited by: Google, Bing, Wikipedia, Twitter, Facebook, Flickr, ...
    """
    self.license = license
    self.throttle = throttle    # Amount of sleep time after executing a query.
    self.language = language    # Result.language restriction (e.g., "en").
    self.format = lambda x: x   # Formatter applied to each attribute of each Result.

</t>
<t tx="karstenw.20230303141230.82">def search(self, query, type=SEARCH, start=1, count=10, sort=RELEVANCY, size=None, cached=True, **kwargs):
    return Results(source=None, query=query, type=type)


</t>
<t tx="karstenw.20230303141230.83">class SearchEngineError(HTTPError):
    pass


</t>
<t tx="karstenw.20230303141230.84">class SearchEngineTypeError(SearchEngineError):
    pass # Raised when an unknown type is passed to SearchEngine.search().


</t>
<t tx="karstenw.20230303141230.85">class SearchEngineLimitError(SearchEngineError):
    pass # Raised when the query limit for a license is reached.

#--- GOOGLE ----------------------------------------------------------------------------------------
# Google Search is a web search engine owned by Google Inc.
# Google Custom Search is a paid service.
# https://code.google.com/apis/console/
# http://code.google.com/apis/customsearch/v1/overview.html

GOOGLE = "https://www.googleapis.com/customsearch/v1?"
GOOGLE_LICENSE = api.license["Google"]
GOOGLE_CUSTOM_SEARCH_ENGINE = "000579440470800426354:_4qo2s0ijsi"

# Search results can start with: "Jul 29, 2007 ...",
# which is the date of the page parsed by Google from the content.
RE_GOOGLE_DATE = re.compile("^([A-Z][a-z]{2} [0-9]{1,2}, [0-9]{4}) {0,1}...")


</t>
<t tx="karstenw.20230303141230.86">class Google(SearchEngine):

    @others
#--- YAHOO -----------------------------------------------------------------------------------------
# Yahoo! Search is a web search engine owned by Yahoo! Inc.
# Yahoo! BOSS ("Build Your Own Search Service") is a paid service.
# http://developer.yahoo.com/search/

YAHOO = "http://yboss.yahooapis.com/ysearch/"
YAHOO_LICENSE = api.license["Yahoo"]


</t>
<t tx="karstenw.20230303141230.87">def __init__(self, license=None, throttle=0.5, language=None):
    SearchEngine.__init__(self, license or GOOGLE_LICENSE, throttle, language)

</t>
<t tx="karstenw.20230303141230.88">def search(self, query, type=SEARCH, start=1, count=10, sort=RELEVANCY, size=None, cached=True, **kwargs):
    """ Returns a list of results from Google for the given query.
        - type : SEARCH,
        - start: maximum 100 results =&gt; start 1-10 with count=10,
        - count: maximum 10,
        There is a daily limit of 10,000 queries. Google Custom Search is a paid service.
    """
    if type != SEARCH:
        raise SearchEngineTypeError
    if not query or count &lt; 1 or start &lt; 1 or start &gt; (100 / count):
        return Results(GOOGLE, query, type)
    # 1) Create request URL.
    url = URL(GOOGLE, query={
          "key": self.license or GOOGLE_LICENSE,
           "cx": GOOGLE_CUSTOM_SEARCH_ENGINE,
            "q": query,
        "start": 1 + (start - 1) * count,
          "num": min(count, 10),
          "alt": "json"
    })

    # 2) Restrict language.
    if self.language is not None:
        url.query["lr"] = "lang_" + self.language

    # 3) Parse JSON response.
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    data = url.download(cached=cached, **kwargs)
    data = json.loads(data)
    if data.get("error", {}).get("code") == 403:
        raise SearchEngineLimitError
    results = Results(GOOGLE, query, type)
    results.total = int(data.get("queries", {}).get("request", [{}])[0].get("totalResults") or 0)
    for x in data.get("items", []):
        r = Result(url=None)
        r.url      = self.format(x.get("link"))
        r.title    = self.format(x.get("title"))
        r.text     = self.format(x.get("htmlSnippet").replace("&lt;br&gt;  ", "").replace("&lt;b&gt;...&lt;/b&gt;", "..."))
        r.language = self.language or ""
        r.date     = ""
        if not r.date:
            # Google Search results can start with a date (parsed from the content):
            m = RE_GOOGLE_DATE.match(r.text)
            if m:
                r.date = m.group(1)
                r.text = "..." + r.text[len(m.group(0)):]
        results.append(r)
    return results

</t>
<t tx="karstenw.20230303141230.89">def translate(self, string, input="en", output="fr", **kwargs):
    """ Returns the translation of the given string in the desired output language.
        Google Translate is a paid service, license without billing raises HTTP401Authentication.
    """
    url = URL("https://www.googleapis.com/language/translate/v2?", method=GET, query={
           "key": self.license or GOOGLE_LICENSE,
             "q": string.encode("utf-8"), # 1000 characters maximum
        "source": input,
        "target": output
    })
    kwargs.setdefault("cached", False)
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    if input == output:
        return string
    try:
        data = url.download(**kwargs)
    except HTTP403Forbidden:
        raise HTTP401Authentication("Google translate API is a paid service")
    data = json.loads(data)
    data = data.get("data", {}).get("translations", [{}])[0].get("translatedText", "")
    data = decode_entities(data)
    return u(data)

</t>
<t tx="karstenw.20230303141230.9">class HTTP503ServiceUnavailable(HTTPError):
    pass # Used by Bing for rate limiting.


</t>
<t tx="karstenw.20230303141230.90">def identify(self, string, **kwargs):
    """ Returns a (language, confidence)-tuple for the given string.
        Google Translate is a paid service, license without billing raises HTTP401Authentication.
    """
    url = URL("https://www.googleapis.com/language/translate/v2/detect?", method=GET, query={
           "key": self.license or GOOGLE_LICENSE,
             "q": string[:1000].encode("utf-8")
    })
    kwargs.setdefault("cached", False)
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    try:
        data = url.download(**kwargs)
    except HTTP403Forbidden:
        raise HTTP401Authentication("Google translate API is a paid service")
    data = json.loads(data)
    data = data.get("data", {}).get("detections", [[{}]])[0][0]
    data = u(data.get("language")), float(data.get("confidence"))
    return data

</t>
<t tx="karstenw.20230303141230.91">class Yahoo(SearchEngine):

    @others
#--- BING ------------------------------------------------------------------------------------------
# Bing is a web search engine owned by Microsoft.
# Bing Search API is a paid service.
# https://datamarket.azure.com/dataset/5BA839F1-12CE-4CCE-BF57-A49D98D29A44
# https://datamarket.azure.com/account/info

BING = "https://api.datamarket.azure.com/Bing/Search/"
BING_LICENSE = api.license["Bing"]


</t>
<t tx="karstenw.20230303141230.92">def __init__(self, license=None, throttle=0.5, language=None):
    SearchEngine.__init__(self, license or YAHOO_LICENSE, throttle, language)

</t>
<t tx="karstenw.20230303141230.93">def _authenticate(self, url):
    url.query.update({
        "oauth_version": "1.0",
        "oauth_nonce": oauth.nonce(),
        "oauth_timestamp": oauth.timestamp(),
        "oauth_consumer_key": self.license[0],
        "oauth_signature_method": "HMAC-SHA1"
    })
    url.query["oauth_signature"] = oauth.sign(url.string.split("?")[0], url.query,
        method = url.method,
        secret = self.license[1]
    )
    return url

</t>
<t tx="karstenw.20230303141230.94">def search(self, query, type=SEARCH, start=1, count=10, sort=RELEVANCY, size=None, cached=True, **kwargs):
    """ Returns a list of results from Yahoo for the given query.
        - type : SEARCH, IMAGE or NEWS,
        - start: maximum 1000 results =&gt; start 1-100 with count=10, 1000/count,
        - count: maximum 50, or 35 for images.
        There is no daily limit, however Yahoo BOSS is a paid service.
    """
    if type not in (SEARCH, IMAGE, NEWS):
        raise SearchEngineTypeError
    if type == SEARCH:
        url = YAHOO + "web"
    if type == IMAGE:
        url = YAHOO + "images"
    if type == NEWS:
        url = YAHOO + "news"
    if not query or count &lt; 1 or start &lt; 1 or start &gt; 1000 / count:
        return Results(YAHOO, query, type)
    # 1) Create request URL.
    url = URL(url, method=GET, query={
             "q": query.replace(" ", "+"),
         "start": 1 + (start - 1) * count,
         "count": min(count, type == IMAGE and 35 or 50),
        "format": "json"
    })
    # 2) Restrict language.
    if self.language is not None:
        market = locale.market(self.language)
        if market:
            url.query["market"] = market.lower()
    # 3) Authenticate.
    url = self._authenticate(url)
    # 4) Parse JSON response.
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    try:
        data = url.download(cached=cached, **kwargs)
    except HTTP401Authentication:
        raise HTTP401Authentication("Yahoo %s API is a paid service" % type)
    except HTTP403Forbidden:
        raise SearchEngineLimitError
    data = json.loads(data)
    data = data.get("bossresponse") or {}
    data = data.get({SEARCH: "web", IMAGE: "images", NEWS: "news"}[type], {})
    results = Results(YAHOO, query, type)
    results.total = int(data.get("totalresults") or 0)
    for x in data.get("results", []):
        r = Result(url=None)
        r.url = self.format(x.get("url", x.get("clickurl")))
        r.title = self.format(x.get("title"))
        r.text = self.format(x.get("abstract"))
        r.date = self.format(x.get("date"))
        r.author = self.format(x.get("source"))
        r.language = self.format(x.get("language") and \
                                 x.get("language").split(" ")[0] or self.language or "")
        results.append(r)
    return results

</t>
<t tx="karstenw.20230303141230.95">class Bing(SearchEngine):

    @others
#--- DUCKDUCKGO ------------------------------------------------------------------------------------
# DuckDuckGo is a privacy-respecting aggregate search engine,
# with information from Wikipedia, WikiHow, Wikia, GitHub, The Free Dictionary, etc.
# https://duckduckgo.com/api.html
# https://duckduckgo.com/params.html

DUCKDUCKGO = "http://api.duckduckgo.com/"
DUCKDUCKGO_LICENSE = api.license["DuckDuckGo"]

# Results from DuckDuckGo have a Result.type with semantic information,
# e.g., "apple" =&gt; "plant and plant parts". Known types:
REFERENCE, CATEGORY, DEFINITION = \
    "reference", "category", "definition"


</t>
<t tx="karstenw.20230303141230.96">def __init__(self, license=None, throttle=0.5, language=None):
    SearchEngine.__init__(self, license or BING_LICENSE, throttle, language)

</t>
<t tx="karstenw.20230303141230.97">def search(self, query, type=SEARCH, start=1, count=10, sort=RELEVANCY, size=None, cached=True, **kwargs):
    """" Returns a list of results from Bing for the given query.
         - type : SEARCH, IMAGE or NEWS,
         - start: maximum 1000 results =&gt; start 1-100 with count=10, 1000/count,
         - count: maximum 50, or 15 for news,
         - size : for images, either SMALL, MEDIUM or LARGE.
         There is no daily query limit.
    """
    if type not in (SEARCH, IMAGE, NEWS):
        raise SearchEngineTypeError
    if type == SEARCH:
        src = "Web"
    if type == IMAGE:
        src = "Image"
    if type == NEWS:
        src = "News"
    if not query or count &lt; 1 or start &lt; 1 or start &gt; 1000 / count:
        return Results(BING + src + "?", query, type)
    # 1) Construct request URL.
    url = URL(BING + "Composite", method=GET, query={
           "Sources": "'" + src.lower() + "'",
             "Query": "'" + query + "'",
             "$skip": 1 + (start - 1) * count,
              "$top": min(count, type == NEWS and 15 or 50),
           "$format": "json",
    })
    # 2) Restrict image size.
    if size in (TINY, SMALL, MEDIUM, LARGE):
        url.query["ImageFilters"] = {
                TINY: "'Size:Small'",
               SMALL: "'Size:Small'",
              MEDIUM: "'Size:Medium'",
               LARGE: "'Size:Large'"}[size]
    # 3) Restrict language.
    if type in (SEARCH, IMAGE) and self.language is not None:
        url.query["Query"] = url.query["Query"][:-1] + " language: %s'" % self.language
    if type in (NEWS,) and self.language is not None:
        market = locale.market(self.language)
        if market:
            url.query["Market"] = "'" + market + "'"
    # 4) Parse JSON response.
    kwargs["authentication"] = ("", self.license)
    kwargs.setdefault("unicode", True)
    kwargs.setdefault("throttle", self.throttle)
    try:
        data = url.download(cached=cached, **kwargs)
    except HTTP401Authentication:
        raise HTTP401Authentication("Bing %s API is a paid service" % type)
    except HTTP503ServiceUnavailable:
        raise SearchEngineLimitError
    data = json.loads(data)
    data = data.get("d", {})
    data = data.get("results", [{}])[0]
    results = Results(BING, query, type)
    results.total = int(data.get(src + "Total") or 0)
    for x in data.get(src, []):
        r = Result(url=None)
        r.url = self.format(x.get("MediaUrl", x.get("Url")))
        r.title = self.format(x.get("Title"))
        r.text = self.format(x.get("Description", x.get("Snippet")))
        r.language = self.language or ""
        r.date = self.format(x.get("DateTime", x.get("Date")))
        r.author = self.format(x.get("Source"))
        results.append(r)
    return results

</t>
<t tx="karstenw.20230303141230.98">class DuckDuckGo(SearchEngine):

    @others
DDG = DuckDuckGo

#for r in DDG().search("cats"):
#    print(r.url)
#    print(r.title) # Can be used as a new query.
#    print(plaintext(r.text))
#    print(r.type)  # REFERENCE, CATEGORY, DEFINITION, "people", "sports" ...
#    print()

#print(DDG().definition("cat"))
#print(DDG().spelling("catnpa"))

#--- FAROO
#-----------------------------------------------------------------------------------------
# Faroo is web search engine with 1 million free queries per month.
# http://www.faroo.com/hp/api/api.html

FAROO = "http://www.faroo.com/api"
FAROO_LICENSE = api.license["Faroo"]


</t>
<t tx="karstenw.20230303141230.99">def __init__(self, license=None, throttle=0.5, language=None):
    SearchEngine.__init__(self, license or DUCKDUCKGO_LICENSE, throttle, language)

</t>
<t tx="karstenw.20230303141240.1">#--- API LICENSE CONFIGURATION -----------------------------------------------------------------------
# Default license keys used by pattern.web.SearchEngine to contact different API's.
# Google and Yahoo are paid services for which you need a personal license + payment method.
# The default Google license is for testing purposes (= 100 daily queries).
# Wikipedia, Twitter and Facebook are free.
# Bing, Flickr and ProductsWiki use licenses shared among all Pattern users.

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303141246.1">@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303141253.1">from __future__ import unicode_literals
from builtins import dict

license = {}
license["Google"] = \
    "AIzaSyBxe9jC4WLr-Rry_5OUMOZ7PCsEyWpiU48"

license["Bing"] = \
    "VnJEK4HTlntE3SyF58QLkUCLp/78tkYjV1Fl3J7lHa0="

license["Yahoo"] = \
    ("", "") # OAuth (key, secret)

license["DuckDuckGo"] = \
    None

license["Faroo"] = \
    ""

license["Wikipedia"] = \
    None

license["Twitter"] = (
    "p7HUdPLlkKaqlPn6TzKkA", # OAuth (key, secret, token)
    "R7I1LRuLY27EKjzulutov74lKB0FjqcI2DYRUmsu7DQ", (
    "14898655-TE9dXQLrzrNd0Zwf4zhK7koR5Ahqt40Ftt35Y2qY",
    "q1lSRDOguxQrfgeWWSJgnMHsO67bqTd5dTElBsyTM"))

license["Facebook"] = \
    "332061826907464|jdHvL3lslFvN-s_sphK1ypCwNaY"

license["Flickr"] = \
    "787081027f43b0412ba41142d4540480"

license["ProductWiki"] = \
    "64819965ec784395a494a0d7ed0def32"
</t>
<t tx="karstenw.20230303141255.1">import re

try:
    # old - worked up to 3.8
    from collections import Iterable
except ImportError as err:
    # new - 3.11 needed this
    from collections.abc import Iterable


try:
    # Python 2
    str_type = unicode
except NameError:
    # Python 3
    str_type = str

STRING_LIKE_TYPES = (str_type, bytes, bytearray)

try:
    # Python 2
    from urlparse import urlparse, parse_qsl
except ImportError:
    # Python 3
    from urllib.parse import urlparse, parse_qsl

try:
    import simplejson as json
except ImportError:
    import json


</t>
<t tx="karstenw.20230303141255.2">def json_iter_parse(response_text):
    decoder = json.JSONDecoder(strict=False)
    idx = 0
    while idx &lt; len(response_text):
        obj, idx = decoder.raw_decode(response_text, idx)
        yield obj


</t>
<t tx="karstenw.20230303141255.3">def stringify_values(dictionary):
    stringified_values_dict = {}
    for key, value in dictionary.items():
        if isinstance(value, Iterable) and not isinstance(value, STRING_LIKE_TYPES):
            value = u','.join(map(str_type, value))
        stringified_values_dict[key] = value
    return stringified_values_dict


</t>
<t tx="karstenw.20230303141255.4">def get_url_query(url):
    parsed_url = urlparse(url)
    url_query = parse_qsl(parsed_url.fragment)
    # login_response_url_query can have multiple key
    url_query = dict(url_query)
    return url_query


</t>
<t tx="karstenw.20230303141255.5">def get_form_action(html):
    form_action = re.findall(r'&lt;form(?= ).* action="(.+)"', html)
    if form_action:
        return form_action[0]


</t>
<t tx="karstenw.20230303141255.6">def censor_access_token(access_token):
    if isinstance(access_token, str_type) and len(access_token) &gt;= 12:
        return '{}***{}'.format(access_token[:4], access_token[-4:])
    elif access_token:
        return '***'
    else:
        return access_token
</t>
<t tx="karstenw.20230303141303.1">from __future__ import unicode_literals
from __future__ import division

import re

from builtins import str, bytes, dict, int
from builtins import object, range

#---------------------------------------------------------------------------------------------------
# Note: this module is optimized for performance.
# There is little gain in using more regular expressions.

VOWELS = ["a", "e", "i", "o", "u", "y"]
DOUBLE = ["bb", "dd", "ff", "gg", "mm", "nn", "pp", "rr", "tt"]
VALID_LI = ["b", "c", "d", "e", "g", "h", "k", "m", "n", "r", "t"]


</t>
<t tx="karstenw.20230303141303.10">def has_vowel(w):
    """ Returns True if there is a vowel in the given string.
    """
    for ch in w:
        if ch in VOWELS:
            return True
    return False


</t>
<t tx="karstenw.20230303141303.11">def vowel_consonant_pairs(w, max=None):
    """ Returns the number of consecutive vowel-consonant pairs in the word.
    """
    m = 0
    for i, ch in enumerate(w):
        if is_vowel(ch) and i &lt; len(w) - 1 and is_consonant(w[i + 1]):
            m += 1
            # An optimisation to stop searching once we reach the amount of &lt;vc&gt; pairs we need.
            if m == max:
                break
    return m

</t>
<t tx="karstenw.20230303141303.12">#--- REPLACEMENT RULES -----------------------------------------------------------------------------


def step_1a(w):
    """ Step 1a handles -s suffixes.
    """
    if w.endswith("s"):
        if w.endswith("sses"):
            return w[:-2]
        if w.endswith("ies"):
            # Replace by -ie if preceded by just one letter,
            # otherwise by -i (so ties =&gt; tie, cries =&gt; cri).
            return len(w) == 4 and w[:-1] or w[:-2]
        if w.endswith(("us", "ss")):
            return w
        if find_vowel(w) &lt; len(w) - 2:
            # Delete -s if the preceding part contains a vowel not immediately before the -s
            # (so gas and this retain the -s, gaps and kiwis lose it).
            return w[:-1]
    return w


</t>
<t tx="karstenw.20230303141303.13">def step_1b(w):
    """ Step 1b handles -ed and -ing suffixes (or -edly and -ingly).
        Removes double consonants at the end of the stem and adds -e to some words.
    """
    if w.endswith("y") and w.endswith(("edly", "ingly")):
        w = w[:-2] # Strip -ly for next step.
    if w.endswith(("ed", "ing")):
        if w.endswith("ied"):
            # See -ies in step 1a.
            return len(w) == 4 and w[:-1] or w[:-2]
        if w.endswith("eed"):
            # Replace by -ee if preceded by at least one vowel-consonant pair.
            return R1(w).endswith("eed") and w[:-1] or w
        for suffix in ("ed", "ing"):
            # Delete if the preceding word part contains a vowel.
            # - If the word ends -at, -bl or -iz add -e (luxuriat =&gt; luxuriate).
            # - If the word ends with a double remove the last letter (hopp =&gt; hop).
            # - If the word is short, add e (hop =&gt; hope).
            if w.endswith(suffix) and has_vowel(w[:-len(suffix)]):
                w = w[:-len(suffix)]
                if w.endswith(("at", "bl", "iz")):
                    return w + "e"
                if is_double_consonant(w[-2:]):
                    return w[:-1]
                if is_short(w):
                    return w + "e"
    return w


</t>
<t tx="karstenw.20230303141303.14">def step_1c(w):
    """ Step 1c replaces suffix -y or -Y by -i if preceded by a non-vowel 
        which is not the first letter of the word (cry =&gt; cri, by =&gt; by, say =&gt; say).
    """
    if len(w) &gt; 2 and w.endswith(("y", "Y")) and is_consonant(w[-2]):
        return w[:-1] + "i"
    return w

suffixes2 = [
    ("al", (("ational", "ate"), ("tional", "tion"))),
    ("ci", (("enci", "ence"), ("anci", "ance"))),
    ("er", (("izer", "ize"),)),
    ("li", (("bli", "ble"), ("alli", "al"), ("entli", "ent"), ("eli", "e"), ("ousli", "ous"))),
    ("on", (("ization", "ize"), ("isation", "ize"), ("ation", "ate"))),
    ("or", (("ator", "ate"),)),
    ("ss", (("iveness", "ive"), ("fulness", "ful"), ("ousness", "ous"))),
    ("sm", (("alism", "al"),)),
    ("ti", (("aliti", "al"), ("iviti", "ive"), ("biliti", "ble"))),
    ("gi", (("logi", "log"),))
]


</t>
<t tx="karstenw.20230303141303.15">def step_2(w):
    """ Step 2 replaces double suffixes (singularization =&gt; singularize).
        This only happens if there is at least one vowel-consonant pair before the suffix.
    """
    for suffix, rules in suffixes2:
        if w.endswith(suffix):
            for A, B in rules:
                if w.endswith(A):
                    return R1(w).endswith(A) and w[:-len(A)] + B or w
    if w.endswith("li") and R1(w)[-3:-2] in VALID_LI:
        # Delete -li if preceded by a valid li-ending.
        return w[:-2]
    return w

suffixes3 = [
    ("e", (("icate", "ic"), ("ative", ""), ("alize", "al"))),
    ("i", (("iciti", "ic"),)),
    ("l", (("ical", "ic"), ("ful", ""))),
    ("s", (("ness", ""),))
]


</t>
<t tx="karstenw.20230303141303.16">def step_3(w):
    """ Step 3 replaces -ic, -ful, -ness etc. suffixes.
        This only happens if there is at least one vowel-consonant pair before the suffix.
    """
    for suffix, rules in suffixes3:
        if w.endswith(suffix):
            for A, B in rules:
                if w.endswith(A):
                    return R1(w).endswith(A) and w[:-len(A)] + B or w
    return w

suffixes4 = [
    ("al", ("al",)),
    ("ce", ("ance", "ence")),
    ("er", ("er",)),
    ("ic", ("ic",)),
    ("le", ("able", "ible")),
    ("nt", ("ant", "ement", "ment", "ent")),
    ("e",  ("ate", "ive", "ize")),
    (("m", "i", "s"), ("ism", "iti", "ous"))
]


</t>
<t tx="karstenw.20230303141303.17">def step_4(w):
    """ Step 4 strips -ant, -ent etc. suffixes.
        This only happens if there is more than one vowel-consonant pair before the suffix.
    """
    for suffix, rules in suffixes4:
        if w.endswith(suffix):
            for A in rules:
                if w.endswith(A):
                    return R2(w).endswith(A) and w[:-len(A)] or w
    if R2(w).endswith("ion") and w[:-3].endswith(("s", "t")):
        # Delete -ion if preceded by s or t.
        return w[:-3]
    return w


</t>
<t tx="karstenw.20230303141303.18">def step_5a(w):
    """ Step 5a strips suffix -e if preceded by multiple vowel-consonant pairs,
        or one vowel-consonant pair that is not a short syllable.
    """
    if w.endswith("e"):
        if R2(w).endswith("e") or R1(w).endswith("e") and not is_short_syllable(w, before=-1):
            return w[:-1]
    return w


</t>
<t tx="karstenw.20230303141303.19">def step_5b(w):
    """ Step 5b strips suffix -l if preceded by l and multiple vowel-consonant pairs,
        bell =&gt; bell, rebell =&gt; rebel.
    """
    if w.endswith("ll") and R2(w).endswith("l"):
        return w[:-1]
    return w

#--- EXCEPTIONS ------------------------------------------------------------------------------------

# Exceptions:
# - in, out and can stems could be seen as stop words later on.
# - Special -ly cases.
exceptions = {
    "skis": "ski",
    "skies": "sky",
    "dying": "die",
    "lying": "lie",
    "tying": "tie",
    "innings": "inning",
    "outings": "outing",
    "cannings": "canning",
    "idly": "idl",
    "gently": "gentl",
    "ugly": "ugli",
    "early": "earli",
    "only": "onli",
    "singly": "singl"
}

# Words that are never stemmed:
uninflected = dict.fromkeys([
    "sky",
    "news",
    "howe",
    "inning", "outing", "canning",
    "proceed", "exceed", "succeed",
    "atlas", "cosmos", "bias", "andes" # not plural forms
], True)

</t>
<t tx="karstenw.20230303141303.2">def is_vowel(s):
    return s in VOWELS


</t>
<t tx="karstenw.20230303141303.20">#--- STEMMER ---------------------------------------------------------------------------------------


def case_sensitive(stem, word):
    """ Applies the letter case of the word to the stem:
        Ponies =&gt; Poni
    """
    ch = []
    for i in range(len(stem)):
        if word[i] == word[i].upper():
            ch.append(stem[i].upper())
        else:
            ch.append(stem[i])
    return "".join(ch)


</t>
<t tx="karstenw.20230303141303.21">def upper_consonant_y(w):
    """ Sets the initial y, or y after a vowel, to Y.
        Of course, y is interpreted as a vowel and Y as a consonant.
    """
    a = []
    p = None
    for ch in w:
        if ch == "y" and (p is None or p in VOWELS):
            a.append("Y")
        else:
            a.append(ch)
        p = ch
    return "".join(a)

# If we stemmed a word once, we can cache the result and reuse it.
# By default, keep a history of a 10000 entries (&lt;500KB).
cache = {}


</t>
<t tx="karstenw.20230303141303.22">def stem(word, cached=True, history=10000, **kwargs):
    """ Returns the stem of the given word: ponies =&gt; poni.
        Note: it is often taken to be a crude error 
        that a stemming algorithm does not leave a real word after removing the stem. 
        But the purpose of stemming is to bring variant forms of a word together, 
        not to map a word onto its "paradigm" form. 
    """
    stem = word.lower()
    if cached and stem in cache:
        return case_sensitive(cache[stem], word)
    if cached and len(cache) &gt; history: # Empty cache every now and then.
        cache.clear()
    if len(stem) &lt;= 2:
        # If the word has two letters or less, leave it as it is.
        return case_sensitive(stem, word)
    if stem in exceptions:
        return case_sensitive(exceptions[stem], word)
    if stem in uninflected:
        return case_sensitive(stem, word)
    # Mark y treated as a consonant as Y.
    stem = upper_consonant_y(stem)
    for f in (step_1a, step_1b, step_1c, step_2, step_3, step_4, step_5a, step_5b):
        stem = f(stem)
    # Turn any remaining Y letters in the stem back into lower case.
    # Apply the case of the original word to the stem.
    stem = stem.lower()
    stem = case_sensitive(stem, word)
    if cached:
        cache[word.lower()] = stem.lower()
    return stem
</t>
<t tx="karstenw.20230303141303.3">def is_consonant(s):
    return s not in VOWELS


</t>
<t tx="karstenw.20230303141303.4">def is_double_consonant(s):
    return s in DOUBLE


</t>
<t tx="karstenw.20230303141303.5">def is_short_syllable(w, before=None):
    """ A short syllable in a word is either:
        - a vowel followed by a non-vowel other than w, x or Y and preceded by a non-vowel
        - a vowel at the beginning of the word followed by a non-vowel. 
        Checks the three characters before the given index in the word (or entire word if None).
    """
    if before is not None:
        i = before &lt; 0 and len(w) + before or before
        return is_short_syllable(w[max(0, i - 3):i])
    if len(w) == 3 and is_consonant(w[0]) and is_vowel(w[1]) and is_consonant(w[2]) and w[2] not in "wxY":
        return True
    if len(w) == 2 and is_vowel(w[0]) and is_consonant(w[1]):
        return True
    return False


</t>
<t tx="karstenw.20230303141303.6">def is_short(w):
    """ A word is called short if it consists of a short syllable preceded by zero or more consonants. 
    """
    return is_short_syllable(w[-3:]) and len([ch for ch in w[:-3] if ch in VOWELS]) == 0

# A point made at least twice in the literature is that words beginning with gener-
# are overstemmed by the Porter stemmer:
# generate =&gt; gener, generically =&gt; gener
# Moving the region one vowel-consonant pair to the right fixes this:
# generate =&gt; generat, generically =&gt; generic
overstemmed = ("gener", "commun", "arsen")

RE_R1 = re.compile(r"[aeiouy][^aeiouy]")


</t>
<t tx="karstenw.20230303141303.7">def R1(w):
    """ R1 is the region after the first non-vowel following a vowel, 
        or the end of the word if there is no such non-vowel. 
    """
    m = RE_R1.search(w)
    if m:
        return w[m.end():]
    return ""


</t>
<t tx="karstenw.20230303141303.8">def R2(w):
    """ R2 is the region after the first non-vowel following a vowel in R1, 
        or the end of the word if there is no such non-vowel.
    """
    if w.startswith(tuple(overstemmed)):
        return R1(R1(R1(w)))
    return R1(R1(w))


</t>
<t tx="karstenw.20230303141303.9">def find_vowel(w):
    """ Returns the index of the first vowel in the word.
        When no vowel is found, returns len(word).
    """
    for i, ch in enumerate(w):
        if ch in VOWELS:
            return i
    return len(w)


</t>
<t tx="karstenw.20230303141307.1">#!/usr/bin/env python

from __future__ import unicode_literals
from __future__ import absolute_import
from __future__ import division

from builtins import str, bytes, int
from builtins import object, range
from builtins import map, zip, filter

from ctypes import *
from ctypes.util import find_library
from os import path
import sys

try:
    import scipy
    from scipy import sparse
except:
    scipy = None
    sparse = None

__all__ = ['liblinear', 'feature_node', 'gen_feature_nodearray', 'problem',
           'parameter', 'model', 'toPyModel', 'L2R_LR', 'L2R_L2LOSS_SVC_DUAL',
           'L2R_L2LOSS_SVC', 'L2R_L1LOSS_SVC_DUAL', 'MCSVM_CS',
           'L1R_L2LOSS_SVC', 'L1R_LR', 'L2R_LR_DUAL', 'L2R_L2LOSS_SVR',
           'L2R_L2LOSS_SVR_DUAL', 'L2R_L1LOSS_SVR_DUAL', 'print_null']

try:
    dirname = path.dirname(path.abspath(__file__))
    if sys.platform == 'win32':
        liblinear = CDLL(path.join(dirname, 'windows\liblinear-2.20\liblinear.dll'))
    else:
        liblinear = CDLL(path.join(dirname, 'macos/liblinear-2.20/liblinear.so.3'))
except:
# For unix the prefix 'lib' is not considered.
    if find_library('linear'):
        liblinear = CDLL(find_library('linear'))
    elif find_library('liblinear'):
        liblinear = CDLL(find_library('liblinear'))
    else:
        libsvm = CDLL(path.join(path.dirname(__file__), 'ubuntu/liblinear-2.20/liblinear.so.3'))

L2R_LR = 0
L2R_L2LOSS_SVC_DUAL = 1
L2R_L2LOSS_SVC = 2
L2R_L1LOSS_SVC_DUAL = 3
MCSVM_CS = 4
L1R_L2LOSS_SVC = 5
L1R_LR = 6
L2R_LR_DUAL = 7
L2R_L2LOSS_SVR = 11
L2R_L2LOSS_SVR_DUAL = 12
L2R_L1LOSS_SVR_DUAL = 13

PRINT_STRING_FUN = CFUNCTYPE(None, c_char_p)


</t>
<t tx="karstenw.20230303141307.10">def csr_to_problem(x, prob):
    # Extra space for termination node and (possibly) bias term
    x_space = prob.x_space = scipy.empty((x.nnz + x.shape[0] * 2), dtype=feature_node)
    prob.rowptr = x.indptr.copy()
    prob.rowptr[1:] += 2 * scipy.arange(1,x.shape[0] + 1)
    prob_ind = x_space["index"]
    prob_val = x_space["value"]
    prob_ind[:] = -1
    if jit_enabled:
        csr_to_problem_jit(x.shape[0], x.data, x.indices, x.indptr, prob_val, prob_ind, prob.rowptr)
    else:
        csr_to_problem_nojit(x.shape[0], x.data, x.indices, x.indptr, prob_val, prob_ind, prob.rowptr)


</t>
<t tx="karstenw.20230303141307.11">class problem(Structure):
    _names = ["l", "n", "y", "x", "bias"]
    _types = [c_int, c_int, POINTER(c_double), POINTER(POINTER(feature_node)), c_double]
    _fields_ = genFields(_names, _types)

    @others
</t>
<t tx="karstenw.20230303141307.12">def __init__(self, y, x, bias = -1):
    if (not isinstance(y, (list, tuple))) and (not (scipy and isinstance(y, scipy.ndarray))):
        raise TypeError("type of y: {0} is not supported!".format(type(y)))

    if isinstance(x, (list, tuple)):
        if len(y) != len(x):
            raise ValueError("len(y) != len(x)")
    elif scipy != None and isinstance(x, (scipy.ndarray, sparse.spmatrix)):
        if len(y) != x.shape[0]:
            raise ValueError("len(y) != len(x)")
        if isinstance(x, scipy.ndarray):
            x = scipy.ascontiguousarray(x) # enforce row-major
        if isinstance(x, sparse.spmatrix):
            x = x.tocsr()
            pass
    else:
        raise TypeError("type of x: {0} is not supported!".format(type(x)))
    self.l = l = len(y)
    self.bias = -1

    max_idx = 0
    x_space = self.x_space = []
    if scipy != None and isinstance(x, sparse.csr_matrix):
        csr_to_problem(x, self)
        max_idx = x.shape[1]
    else:
        for i, xi in enumerate(x):
            tmp_xi, tmp_idx = gen_feature_nodearray(xi)
            x_space += [tmp_xi]
            max_idx = max(max_idx, tmp_idx)
    self.n = max_idx

    self.y = (c_double * l)()
    if scipy != None and isinstance(y, scipy.ndarray):
        scipy.ctypeslib.as_array(self.y, (self.l,))[:] = y
    else:
        for i, yi in enumerate(y):
            self.y[i] = yi

    self.x = (POINTER(feature_node) * l)()
    if scipy != None and isinstance(x, sparse.csr_matrix):
        base = addressof(self.x_space.ctypes.data_as(POINTER(feature_node))[0])
        x_ptr = cast(self.x, POINTER(c_uint64))
        x_ptr = scipy.ctypeslib.as_array(x_ptr,(self.l,))
        x_ptr[:] = self.rowptr[:-1] * sizeof(feature_node) + base
    else:
        for i, xi in enumerate(self.x_space):
            self.x[i] = xi

    self.set_bias(bias)

</t>
<t tx="karstenw.20230303141307.13">def set_bias(self, bias):
    if self.bias == bias:
        return
    if bias &gt;= 0 and self.bias &lt; 0:
        self.n += 1
        node = feature_node(self.n, bias)
    if bias &lt; 0 and self.bias &gt;= 0:
        self.n -= 1
        node = feature_node(-1, bias)

    if isinstance(self.x_space, list):
        for xi in self.x_space:
            xi[-2] = node
    else:
        self.x_space["index"][self.rowptr[1:] - 2] = node.index
        self.x_space["value"][self.rowptr[1:] - 2] = node.value

    self.bias = bias


</t>
<t tx="karstenw.20230303141307.14">class parameter(Structure):
    _names = ["solver_type", "eps", "C", "nr_weight", "weight_label", "weight", "p", "init_sol"]
    _types = [c_int, c_double, c_double, c_int, POINTER(c_int), POINTER(c_double), c_double, POINTER(c_double)]
    _fields_ = genFields(_names, _types)

    @others
</t>
<t tx="karstenw.20230303141307.15">def __init__(self, options = None):
    if options == None:
        options = ''
    self.parse_options(options)

</t>
<t tx="karstenw.20230303141307.16">def __str__(self):
    s = ''
    attrs = parameter._names + list(self.__dict__.keys())
    values = list(map(lambda attr: getattr(self, attr), attrs))
    for attr, val in zip(attrs, values):
        s += (' %s: %s\n' % (attr, val))
    s = s.strip()

    return s

</t>
<t tx="karstenw.20230303141307.17">def set_to_default_values(self):
    self.solver_type = L2R_L2LOSS_SVC_DUAL
    self.eps = float('inf')
    self.C = 1
    self.p = 0.1
    self.nr_weight = 0
    self.weight_label = None
    self.weight = None
    self.init_sol = None
    self.bias = -1
    self.flag_cross_validation = False
    self.flag_C_specified = False
    self.flag_solver_specified = False
    self.flag_find_C = False
    self.nr_fold = 0
    self.print_func = cast(None, PRINT_STRING_FUN)

</t>
<t tx="karstenw.20230303141307.18">def parse_options(self, options):
    if isinstance(options, list):
        argv = options
    elif isinstance(options, str):
        argv = options.split()
    else:
        raise TypeError("arg 1 should be a list or a str.")
    self.set_to_default_values()
    self.print_func = cast(None, PRINT_STRING_FUN)
    weight_label = []
    weight = []

    i = 0
    while i &lt; len(argv):
        if argv[i] == "-s":
            i = i + 1
            self.solver_type = int(argv[i])
            self.flag_solver_specified = True
        elif argv[i] == "-c":
            i = i + 1
            self.C = float(argv[i])
            self.flag_C_specified = True
        elif argv[i] == "-p":
            i = i + 1
            self.p = float(argv[i])
        elif argv[i] == "-e":
            i = i + 1
            self.eps = float(argv[i])
        elif argv[i] == "-B":
            i = i + 1
            self.bias = float(argv[i])
        elif argv[i] == "-v":
            i = i + 1
            self.flag_cross_validation = 1
            self.nr_fold = int(argv[i])
            if self.nr_fold &lt; 2:
                raise ValueError("n-fold cross validation: n must &gt;= 2")
        elif argv[i].startswith("-w"):
            i = i + 1
            self.nr_weight += 1
            weight_label += [int(argv[i - 1][2:])]
            weight += [float(argv[i])]
        elif argv[i] == "-q":
            self.print_func = PRINT_STRING_FUN(print_null)
        elif argv[i] == "-C":
            self.flag_find_C = True

        else:
            raise ValueError("Wrong options")
        i += 1

    liblinear.set_print_string_function(self.print_func)
    self.weight_label = (c_int * self.nr_weight)()
    self.weight = (c_double * self.nr_weight)()
    for i in range(self.nr_weight):
        self.weight[i] = weight[i]
        self.weight_label[i] = weight_label[i]

    # default solver for parameter selection is L2R_L2LOSS_SVC
    if self.flag_find_C:
        if not self.flag_cross_validation:
            self.nr_fold = 5
        if not self.flag_solver_specified:
            self.solver_type = L2R_L2LOSS_SVC
            self.flag_solver_specified = True
        elif self.solver_type not in [L2R_LR, L2R_L2LOSS_SVC]:
            raise ValueError("Warm-start parameter search only available for -s 0 and -s 2")

    if self.eps == float('inf'):
        if self.solver_type in [L2R_LR, L2R_L2LOSS_SVC]:
            self.eps = 0.01
        elif self.solver_type in [L2R_L2LOSS_SVR]:
            self.eps = 0.001
        elif self.solver_type in [L2R_L2LOSS_SVC_DUAL, L2R_L1LOSS_SVC_DUAL, MCSVM_CS, L2R_LR_DUAL]:
            self.eps = 0.1
        elif self.solver_type in [L1R_L2LOSS_SVC, L1R_LR]:
            self.eps = 0.01
        elif self.solver_type in [L2R_L2LOSS_SVR_DUAL, L2R_L1LOSS_SVR_DUAL]:
            self.eps = 0.1


</t>
<t tx="karstenw.20230303141307.19">class model(Structure):
    _names = ["param", "nr_class", "nr_feature", "w", "label", "bias"]
    _types = [parameter, c_int, c_int, POINTER(c_double), POINTER(c_int), c_double]
    _fields_ = genFields(_names, _types)

    @others
</t>
<t tx="karstenw.20230303141307.2">def print_null(s):
    return


</t>
<t tx="karstenw.20230303141307.20">def __init__(self):
    self.__createfrom__ = 'python'

</t>
<t tx="karstenw.20230303141307.21">def __del__(self):
    # free memory created by C to avoid memory leak
    if hasattr(self, '__createfrom__') and self.__createfrom__ == 'C':
        liblinear.free_and_destroy_model(pointer(self))

</t>
<t tx="karstenw.20230303141307.22">def get_nr_feature(self):
    return liblinear.get_nr_feature(self)

</t>
<t tx="karstenw.20230303141307.23">def get_nr_class(self):
    return liblinear.get_nr_class(self)

</t>
<t tx="karstenw.20230303141307.24">def get_labels(self):
    nr_class = self.get_nr_class()
    labels = (c_int * nr_class)()
    liblinear.get_labels(self, labels)
    return labels[:nr_class]

</t>
<t tx="karstenw.20230303141307.25">def get_decfun_coef(self, feat_idx, label_idx=0):
    return liblinear.get_decfun_coef(self, feat_idx, label_idx)

</t>
<t tx="karstenw.20230303141307.26">def get_decfun_bias(self, label_idx=0):
    return liblinear.get_decfun_bias(self, label_idx)

</t>
<t tx="karstenw.20230303141307.27">def get_decfun(self, label_idx=0):
    w = [liblinear.get_decfun_coef(self, feat_idx, label_idx) for feat_idx in range(1, self.nr_feature + 1)]
    b = liblinear.get_decfun_bias(self, label_idx)
    return (w, b)

</t>
<t tx="karstenw.20230303141307.28">def is_probability_model(self):
    return (liblinear.check_probability_model(self) == 1)

</t>
<t tx="karstenw.20230303141307.29">def is_regression_model(self):
    return (liblinear.check_regression_model(self) == 1)


</t>
<t tx="karstenw.20230303141307.3">def genFields(names, types):
    return list(zip(names, types))


</t>
<t tx="karstenw.20230303141307.30">def toPyModel(model_ptr):
    """
    toPyModel(model_ptr) -&gt; model

    Convert a ctypes POINTER(model) to a Python model
    """
    if bool(model_ptr) == False:
        raise ValueError("Null pointer")
    m = model_ptr.contents
    m.__createfrom__ = 'C'
    return m

</t>
<t tx="karstenw.20230303141307.4">def fillprototype(f, restype, argtypes):
    f.restype = restype
    f.argtypes = argtypes


</t>
<t tx="karstenw.20230303141307.5">class feature_node(Structure):
    _names = ["index", "value"]
    _types = [c_int, c_double]
    _fields_ = genFields(_names, _types)

    @others
</t>
<t tx="karstenw.20230303141307.6">def __str__(self):
    return '%d:%g' % (self.index, self.value)


</t>
<t tx="karstenw.20230303141307.7">def gen_feature_nodearray(xi, feature_max=None):
    if feature_max:
        assert(isinstance(feature_max, int))

    xi_shift = 0 # ensure correct indices of xi
    if scipy and isinstance(xi, tuple) and len(xi) == 2\
            and isinstance(xi[0], scipy.ndarray) and isinstance(xi[1], scipy.ndarray): # for a sparse vector
        index_range = xi[0] + 1 # index starts from 1
        if feature_max:
            index_range = index_range[scipy.where(index_range &lt;= feature_max)]
    elif scipy and isinstance(xi, scipy.ndarray):
        xi_shift = 1
        index_range = xi.nonzero()[0] + 1 # index starts from 1
        if feature_max:
            index_range = index_range[scipy.where(index_range &lt;= feature_max)]
    elif isinstance(xi, (dict, list, tuple)):
        if isinstance(xi, dict):
            index_range = xi.keys()
        elif isinstance(xi, (list, tuple)):
            xi_shift = 1
            index_range = range(1, len(xi) + 1)
        index_range = list(filter(lambda j: xi[j - xi_shift] != 0, index_range))

        if feature_max:
            index_range = list(filter(lambda j: j &lt;= feature_max, index_range))
        index_range = sorted(index_range)
    else:
        raise TypeError('xi should be a dictionary, list, tuple, 1-d numpy array, or tuple of (index, data)')

    ret = (feature_node * (len(index_range) + 2))()
    ret[-1].index = -1 # for bias term
    ret[-2].index = -1

    if scipy and isinstance(xi, tuple) and len(xi) == 2\
            and isinstance(xi[0], scipy.ndarray) and isinstance(xi[1], scipy.ndarray): # for a sparse vector
        for idx, j in enumerate(index_range):
            ret[idx].index = j
            ret[idx].value = (xi[1])[idx]
    else:
        for idx, j in enumerate(index_range):
            ret[idx].index = j
            ret[idx].value = xi[j - xi_shift]

    max_idx = 0
    if len(index_range) &gt; 0:
        max_idx = index_range[-1]
    return ret, max_idx

try:
    from numba import jit
    jit_enabled = True
except:
    jit = lambda x: x
    jit_enabled = False


</t>
<t tx="karstenw.20230303141307.8">@jit
def csr_to_problem_jit(l, x_val, x_ind, x_rowptr, prob_val, prob_ind, prob_rowptr):
    for i in range(l):
        b1,e1 = x_rowptr[i], x_rowptr[i + 1]
        b2,e2 = prob_rowptr[i], prob_rowptr[i + 1] - 2
        for j in range(b1,e1):
            prob_ind[j - b1 + b2] = x_ind[j] + 1
            prob_val[j - b1 + b2] = x_val[j]


</t>
<t tx="karstenw.20230303141307.9">def csr_to_problem_nojit(l, x_val, x_ind, x_rowptr, prob_val, prob_ind, prob_rowptr):
    for i in range(l):
        x_slice = slice(x_rowptr[i], x_rowptr[i + 1])
        prob_slice = slice(prob_rowptr[i], prob_rowptr[i + 1] - 2)
        prob_ind[prob_slice] = x_ind[x_slice] + 1
        prob_val[prob_slice] = x_val[x_slice]


</t>
<t tx="karstenw.20230303141310.1">from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import object, range
from builtins import map, zip, filter

import os
import sys
sys.path = [os.path.dirname(os.path.abspath(__file__))] + sys.path
from liblinear import *
from liblinear import __all__ as liblinear_all
from liblinear import scipy, sparse
from ctypes import c_double

__all__ = ['svm_read_problem', 'load_model', 'save_model', 'evaluations',
           'train', 'predict'] + liblinear_all


</t>
<t tx="karstenw.20230303141310.2">def svm_read_problem(data_file_name, return_scipy=False):
    """
    svm_read_problem(data_file_name, return_scipy=False) -&gt; [y, x], y: list, x: list of dictionary
    svm_read_problem(data_file_name, return_scipy=True)  -&gt; [y, x], y: ndarray, x: csr_matrix

    Read LIBSVM-format data from data_file_name and return labels y
    and data instances x.
    """
    prob_y = []
    prob_x = []
    row_ptr = [0]
    col_idx = []
    for i, line in enumerate(open(data_file_name)):
        line = line.split(None, 1)
        # In case an instance with all zero features
        if len(line) == 1:
            line += ['']
        label, features = line
        prob_y += [float(label)]
        if scipy is not None and return_scipy:
            nz = 0
            for e in features.split():
                ind, val = e.split(":")
                val = float(val)
                if val != 0:
                    col_idx += [int(ind) - 1]
                    prob_x += [val]
                    nz += 1
            row_ptr += [row_ptr[-1] + nz]
        else:
            xi = {}
            for e in features.split():
                ind, val = e.split(":")
                if val != 0:
                    xi[int(ind)] = float(val)
            prob_x += [xi]
    if scipy is not None and return_scipy:
        prob_y = scipy.array(prob_y)
        prob_x = scipy.array(prob_x)
        col_idx = scipy.array(col_idx)
        row_ptr = scipy.array(row_ptr)
        prob_x = sparse.csr_matrix((prob_x, col_idx, row_ptr))
    return (prob_y, prob_x)


</t>
<t tx="karstenw.20230303141310.3">def load_model(model_file_name):
    """
    load_model(model_file_name) -&gt; model

    Load a LIBLINEAR model from model_file_name and return.
    """
    model = liblinear.load_model(model_file_name.encode())
    if not model:
        print("can't open model file %s" % model_file_name)
        return None
    model = toPyModel(model)
    return model


</t>
<t tx="karstenw.20230303141310.4">def save_model(model_file_name, model):
    """
    save_model(model_file_name, model) -&gt; None

    Save a LIBLINEAR model to the file model_file_name.
    """
    liblinear.save_model(model_file_name.encode(), model)


</t>
<t tx="karstenw.20230303141310.5">def evaluations_scipy(ty, pv):
    """
    evaluations_scipy(ty, pv) -&gt; (ACC, MSE, SCC)
    ty, pv: ndarray

    Calculate accuracy, mean squared error and squared correlation coefficient
    using the true values (ty) and predicted values (pv).
    """
    if not (scipy is not None and isinstance(ty, scipy.ndarray) and isinstance(pv, scipy.ndarray)):
        raise TypeError("type of ty and pv must be ndarray")
    if len(ty) != len(pv):
        raise ValueError("len(ty) must be equal to len(pv)")
    ACC = 100.0 * (ty == pv).mean()
    MSE = ((ty - pv)**2).mean()
    l = len(ty)
    sumv = pv.sum()
    sumy = ty.sum()
    sumvy = (pv * ty).sum()
    sumvv = (pv * pv).sum()
    sumyy = (ty * ty).sum()
    with scipy.errstate(all = 'raise'):
        try:
            SCC = ((l * sumvy - sumv * sumy) * (l * sumvy - sumv * sumy)) / ((l * sumvv - sumv * sumv) * (l * sumyy - sumy * sumy))
        except:
            SCC = float('nan')
    return (float(ACC), float(MSE), float(SCC))


</t>
<t tx="karstenw.20230303141310.6">def evaluations(ty, pv, useScipy = True):
    """
    evaluations(ty, pv, useScipy) -&gt; (ACC, MSE, SCC)
    ty, pv: list, tuple or ndarray
    useScipy: convert ty, pv to ndarray, and use scipy functions for the evaluation

    Calculate accuracy, mean squared error and squared correlation coefficient
    using the true values (ty) and predicted values (pv).
    """
    if scipy is not None and useScipy:
        return evaluations_scipy(scipy.asarray(ty), scipy.asarray(pv))
    if len(ty) != len(pv):
        raise ValueError("len(ty) must be equal to len(pv)")
    total_correct = total_error = 0
    sumv = sumy = sumvv = sumyy = sumvy = 0
    for v, y in zip(pv, ty):
        if y == v:
            total_correct += 1
        total_error += (v - y) * (v - y)
        sumv += v
        sumy += y
        sumvv += v * v
        sumyy += y * y
        sumvy += v * y
    l = len(ty)
    ACC = 100.0 * total_correct / l
    MSE = total_error / l
    try:
        SCC = ((l * sumvy - sumv * sumy) * (l * sumvy - sumv * sumy)) / ((l * sumvv - sumv * sumv) * (l * sumyy - sumy * sumy))
    except:
        SCC = float('nan')
    return (float(ACC), float(MSE), float(SCC))


</t>
<t tx="karstenw.20230303141310.7">def train(arg1, arg2=None, arg3=None):
    """
    train(y, x [, options]) -&gt; model | ACC

    y: a list/tuple/ndarray of l true labels (type must be int/double).

    x: 1. a list/tuple of l training instances. Feature vector of
          each training instance is a list/tuple or dictionary.

       2. an l * n numpy ndarray or scipy spmatrix (n: number of features).

    train(prob [, options]) -&gt; model | ACC
    train(prob, param) -&gt; model | ACC

    Train a model from data (y, x) or a problem prob using
    'options' or a parameter param.

    If '-v' is specified in 'options' (i.e., cross validation)
    either accuracy (ACC) or mean-squared error (MSE) is returned.

    options:
            -s type : set type of solver (default 1)
              for multi-class classification
                     0 -- L2-regularized logistic regression (primal)
                     1 -- L2-regularized L2-loss support vector classification (dual)
                     2 -- L2-regularized L2-loss support vector classification (primal)
                     3 -- L2-regularized L1-loss support vector classification (dual)
                     4 -- support vector classification by Crammer and Singer
                     5 -- L1-regularized L2-loss support vector classification
                     6 -- L1-regularized logistic regression
                     7 -- L2-regularized logistic regression (dual)
              for regression
                    11 -- L2-regularized L2-loss support vector regression (primal)
                    12 -- L2-regularized L2-loss support vector regression (dual)
                    13 -- L2-regularized L1-loss support vector regression (dual)
            -c cost : set the parameter C (default 1)
            -p epsilon : set the epsilon in loss function of SVR (default 0.1)
            -e epsilon : set tolerance of termination criterion
                    -s 0 and 2
                            |f'(w)|_2 &lt;= eps*min(pos,neg)/l*|f'(w0)|_2,
                            where f is the primal function, (default 0.01)
                    -s 11
                            |f'(w)|_2 &lt;= eps*|f'(w0)|_2 (default 0.001)
                    -s 1, 3, 4, and 7
                            Dual maximal violation &lt;= eps; similar to liblinear (default 0.)
                    -s 5 and 6
                            |f'(w)|_inf &lt;= eps*min(pos,neg)/l*|f'(w0)|_inf,
                            where f is the primal function (default 0.01)
                    -s 12 and 13
                            |f'(alpha)|_1 &lt;= eps |f'(alpha0)|,
                            where f is the dual function (default 0.1)
            -B bias : if bias &gt;= 0, instance x becomes [x; bias]; if &lt; 0, no bias term added (default -1)
            -wi weight: weights adjust the parameter C of different classes (see README for details)
            -v n: n-fold cross validation mode
            -q : quiet mode (no outputs)
    """
    prob, param = None, None
    if isinstance(arg1, (list, tuple)) or (scipy and isinstance(arg1, scipy.ndarray)):
        assert isinstance(arg2, (list, tuple)) or (scipy and isinstance(arg2, (scipy.ndarray, sparse.spmatrix)))
        y, x, options = arg1, arg2, arg3
        prob = problem(y, x)
        param = parameter(options)
    elif isinstance(arg1, problem):
        prob = arg1
        if isinstance(arg2, parameter):
            param = arg2
        else:
            param = parameter(arg2)
    if prob is None or param is None:
        raise TypeError("Wrong types for the arguments")

    prob.set_bias(param.bias)
    liblinear.set_print_string_function(param.print_func)
    err_msg = liblinear.check_parameter(prob, param)
    if err_msg:
        raise ValueError('Error: %s' % err_msg)

    if param.flag_find_C:
        nr_fold = param.nr_fold
        best_C = c_double()
        best_rate = c_double()
        max_C = 1024
        if param.flag_C_specified:
            start_C = param.C
        else:
            start_C = -1.0
        liblinear.find_parameter_C(prob, param, nr_fold, start_C, max_C, best_C, best_rate)
        print("Best C = %lf  CV accuracy = %g%%\n" % (best_C.value, 100.0 * best_rate.value))
        return best_C.value, best_rate.value

    elif param.flag_cross_validation:
        l, nr_fold = prob.l, param.nr_fold
        target = (c_double * l)()
        liblinear.cross_validation(prob, param, nr_fold, target)
        ACC, MSE, SCC = evaluations(prob.y[:l], target[:l])
        if param.solver_type in [L2R_L2LOSS_SVR, L2R_L2LOSS_SVR_DUAL, L2R_L1LOSS_SVR_DUAL]:
            print("Cross Validation Mean squared error = %g" % MSE)
            print("Cross Validation Squared correlation coefficient = %g" % SCC)
            return MSE
        else:
            print("Cross Validation Accuracy = %g%%" % ACC)
            return ACC
    else:
        m = liblinear.train(prob, param)
        m = toPyModel(m)

        return m


</t>
<t tx="karstenw.20230303141310.8">def predict(y, x, m, options=""):
    """
    predict(y, x, m [, options]) -&gt; (p_labels, p_acc, p_vals)

    y: a list/tuple/ndarray of l true labels (type must be int/double).
       It is used for calculating the accuracy. Use [] if true labels are
       unavailable.

    x: 1. a list/tuple of l training instances. Feature vector of
          each training instance is a list/tuple or dictionary.

       2. an l * n numpy ndarray or scipy spmatrix (n: number of features).

    Predict data (y, x) with the SVM model m.
    options:
        -b probability_estimates: whether to output probability estimates, 0 or 1 (default 0); currently for logistic regression only
        -q quiet mode (no outputs)

    The return tuple contains
    p_labels: a list of predicted labels
    p_acc: a tuple including  accuracy (for classification), mean-squared
           error, and squared correlation coefficient (for regression).
    p_vals: a list of decision values or probability estimates (if '-b 1'
            is specified). If k is the number of classes, for decision values,
            each element includes results of predicting k binary-class
            SVMs. if k = 2 and solver is not MCSVM_CS, only one decision value
            is returned. For probabilities, each element contains k values
            indicating the probability that the testing instance is in each class.
            Note that the order of classes here is the same as 'model.label'
            field in the model structure.
    """

    def info(s):
        print(s)

    if scipy and isinstance(x, scipy.ndarray):
        x = scipy.ascontiguousarray(x) # enforce row-major
    elif sparse and isinstance(x, sparse.spmatrix):
        x = x.tocsr()
    elif not isinstance(x, (list, tuple)):
        raise TypeError("type of x: {0} is not supported!".format(type(x)))

    if (not isinstance(y, (list, tuple))) and (not (scipy and isinstance(y, scipy.ndarray))):
        raise TypeError("type of y: {0} is not supported!".format(type(y)))

    predict_probability = 0
    argv = options.split()
    i = 0
    while i &lt; len(argv):
        if argv[i] == '-b':
            i += 1
            predict_probability = int(argv[i])
        elif argv[i] == '-q':
            info = print_null
        else:
            raise ValueError("Wrong options")
        i += 1

    solver_type = m.param.solver_type
    nr_class = m.get_nr_class()
    nr_feature = m.get_nr_feature()
    is_prob_model = m.is_probability_model()
    bias = m.bias
    if bias &gt;= 0:
        biasterm = feature_node(nr_feature + 1, bias)
    else:
        biasterm = feature_node(-1, bias)
    pred_labels = []
    pred_values = []

    if scipy and isinstance(x, sparse.spmatrix):
        nr_instance = x.shape[0]
    else:
        nr_instance = len(x)

    if predict_probability:
        if not is_prob_model:
            raise TypeError('probability output is only supported for logistic regression')
        prob_estimates = (c_double * nr_class)()
        for i in range(nr_instance):
            if scipy and isinstance(x, sparse.spmatrix):
                indslice = slice(x.indptr[i], x.indptr[i + 1])
                xi, idx = gen_feature_nodearray((x.indices[indslice], x.data[indslice]), feature_max=nr_feature)
            else:
                xi, idx = gen_feature_nodearray(x[i], feature_max=nr_feature)
            xi[-2] = biasterm
            label = liblinear.predict_probability(m, xi, prob_estimates)
            values = prob_estimates[:nr_class]
            pred_labels += [label]
            pred_values += [values]
    else:
        if nr_class &lt;= 2:
            nr_classifier = 1
        else:
            nr_classifier = nr_class
        dec_values = (c_double * nr_classifier)()
        for i in range(nr_instance):
            if scipy and isinstance(x, sparse.spmatrix):
                indslice = slice(x.indptr[i], x.indptr[i + 1])
                xi, idx = gen_feature_nodearray((x.indices[indslice], x.data[indslice]), feature_max=nr_feature)
            else:
                xi, idx = gen_feature_nodearray(x[i], feature_max=nr_feature)
            xi[-2] = biasterm
            label = liblinear.predict_values(m, xi, dec_values)
            values = dec_values[:nr_classifier]
            pred_labels += [label]
            pred_values += [values]

    if len(y) == 0:
        y = [0] * nr_instance
    ACC, MSE, SCC = evaluations(y, pred_labels)

    if m.is_regression_model():
        info("Mean squared error = %g (regression)" % MSE)
        info("Squared correlation coefficient = %g (regression)" % SCC)
    else:
        info("Accuracy = %g%% (%d/%d) (classification)" % (ACC, int(round(nr_instance * ACC / 100)), nr_instance))

    return pred_labels, (ACC, MSE, SCC), pred_values
</t>
<t tx="karstenw.20230303141312.1">#!/usr/bin/env python

from __future__ import unicode_literals
from __future__ import absolute_import
from __future__ import division

from builtins import str, bytes, int
from builtins import object, range
from builtins import map, zip, filter

from ctypes import *
from ctypes.util import find_library
from os import path
import sys

__all__ = ['libsvm', 'svm_problem', 'svm_parameter',
           'toPyModel', 'gen_svm_nodearray', 'print_null', 'svm_node', 'C_SVC',
           'EPSILON_SVR', 'LINEAR', 'NU_SVC', 'NU_SVR', 'ONE_CLASS',
           'POLY', 'PRECOMPUTED', 'PRINT_STRING_FUN', 'RBF',
           'SIGMOID', 'c_double', 'svm_model']

try:
    dirname = path.dirname(path.abspath(__file__))
    if sys.platform == 'win32':
        libsvm = CDLL(path.join(dirname, 'windows\libsvm-3.22\libsvm.dll'))
    else:
        libsvm = CDLL(path.join(dirname, 'macos/libsvm-3.22/libsvm.so.2'))

except:
# For unix the prefix 'lib' is not considered.
    if find_library('svm'):
        libsvm = CDLL(find_library('svm'))
    elif find_library('libsvm'):
        libsvm = CDLL(find_library('libsvm'))
    else:
        libsvm = CDLL(path.join(path.dirname(__file__), 'ubuntu/libsvm-3.22/libsvm.so.2'))


C_SVC = 0
NU_SVC = 1
ONE_CLASS = 2
EPSILON_SVR = 3
NU_SVR = 4

LINEAR = 0
POLY = 1
RBF = 2
SIGMOID = 3
PRECOMPUTED = 4

PRINT_STRING_FUN = CFUNCTYPE(None, c_char_p)


</t>
<t tx="karstenw.20230303141312.10">class svm_parameter(Structure):
    _names = ["svm_type", "kernel_type", "degree", "gamma", "coef0",
            "cache_size", "eps", "C", "nr_weight", "weight_label", "weight",
            "nu", "p", "shrinking", "probability"]
    _types = [c_int, c_int, c_int, c_double, c_double,
            c_double, c_double, c_double, c_int, POINTER(c_int), POINTER(c_double),
            c_double, c_double, c_int, c_int]
    _fields_ = genFields(_names, _types)

    @others
</t>
<t tx="karstenw.20230303141312.11">def __init__(self, options = None):
    if options == None:
        options = ''
    self.parse_options(options)

</t>
<t tx="karstenw.20230303141312.12">def __str__(self):
    s = ''
    attrs = svm_parameter._names + list(self.__dict__.keys())
    values = list(map(lambda attr: getattr(self, attr), attrs))
    for attr, val in zip(attrs, values):
        s += (' %s: %s\n' % (attr, val))
    s = s.strip()

    return s

</t>
<t tx="karstenw.20230303141312.13">def set_to_default_values(self):
    self.svm_type = C_SVC
    self.kernel_type = RBF
    self.degree = 3
    self.gamma = 0
    self.coef0 = 0
    self.nu = 0.5
    self.cache_size = 100
    self.C = 1
    self.eps = 0.001
    self.p = 0.1
    self.shrinking = 1
    self.probability = 0
    self.nr_weight = 0
    self.weight_label = None
    self.weight = None
    self.cross_validation = False
    self.nr_fold = 0
    self.print_func = cast(None, PRINT_STRING_FUN)

</t>
<t tx="karstenw.20230303141312.14">def parse_options(self, options):
    if isinstance(options, list):
        argv = options
    elif isinstance(options, str):
        argv = options.split()
    else:
        raise TypeError("arg 1 should be a list or a str.")
    self.set_to_default_values()
    self.print_func = cast(None, PRINT_STRING_FUN)
    weight_label = []
    weight = []

    i = 0
    while i &lt; len(argv):
        if argv[i] == "-s":
            i = i + 1
            self.svm_type = int(argv[i])
        elif argv[i] == "-t":
            i = i + 1
            self.kernel_type = int(argv[i])
        elif argv[i] == "-d":
            i = i + 1
            self.degree = int(argv[i])
        elif argv[i] == "-g":
            i = i + 1
            self.gamma = float(argv[i])
        elif argv[i] == "-r":
            i = i + 1
            self.coef0 = float(argv[i])
        elif argv[i] == "-n":
            i = i + 1
            self.nu = float(argv[i])
        elif argv[i] == "-m":
            i = i + 1
            self.cache_size = float(argv[i])
        elif argv[i] == "-c":
            i = i + 1
            self.C = float(argv[i])
        elif argv[i] == "-e":
            i = i + 1
            self.eps = float(argv[i])
        elif argv[i] == "-p":
            i = i + 1
            self.p = float(argv[i])
        elif argv[i] == "-h":
            i = i + 1
            self.shrinking = int(argv[i])
        elif argv[i] == "-b":
            i = i + 1
            self.probability = int(argv[i])
        elif argv[i] == "-q":
            self.print_func = PRINT_STRING_FUN(print_null)
        elif argv[i] == "-v":
            i = i + 1
            self.cross_validation = 1
            self.nr_fold = int(argv[i])
            if self.nr_fold &lt; 2:
                raise ValueError("n-fold cross validation: n must &gt;= 2")
        elif argv[i].startswith("-w"):
            i = i + 1
            self.nr_weight += 1
            weight_label += [int(argv[i - 1][2:])]
            weight += [float(argv[i])]
        else:
            raise ValueError("Wrong options")
        i += 1

    libsvm.svm_set_print_string_function(self.print_func)
    self.weight_label = (c_int * self.nr_weight)()
    self.weight = (c_double * self.nr_weight)()
    for i in range(self.nr_weight):
        self.weight[i] = weight[i]
        self.weight_label[i] = weight_label[i]


</t>
<t tx="karstenw.20230303141312.15">class svm_model(Structure):
    _names = ['param', 'nr_class', 'l', 'SV', 'sv_coef', 'rho',
            'probA', 'probB', 'sv_indices', 'label', 'nSV', 'free_sv']
    _types = [svm_parameter, c_int, c_int, POINTER(POINTER(svm_node)),
            POINTER(POINTER(c_double)), POINTER(c_double),
            POINTER(c_double), POINTER(c_double), POINTER(c_int),
            POINTER(c_int), POINTER(c_int), c_int]
    _fields_ = genFields(_names, _types)

    @others
</t>
<t tx="karstenw.20230303141312.16">def __init__(self):
    self.__createfrom__ = 'python'

</t>
<t tx="karstenw.20230303141312.17">def __del__(self):
    # free memory created by C to avoid memory leak
    if hasattr(self, '__createfrom__') and self.__createfrom__ == 'C':
        libsvm.svm_free_and_destroy_model(pointer(self))

</t>
<t tx="karstenw.20230303141312.18">def get_svm_type(self):
    return libsvm.svm_get_svm_type(self)

</t>
<t tx="karstenw.20230303141312.19">def get_nr_class(self):
    return libsvm.svm_get_nr_class(self)

</t>
<t tx="karstenw.20230303141312.2">def print_null(s):
    return


</t>
<t tx="karstenw.20230303141312.20">def get_svr_probability(self):
    return libsvm.svm_get_svr_probability(self)

</t>
<t tx="karstenw.20230303141312.21">def get_labels(self):
    nr_class = self.get_nr_class()
    labels = (c_int * nr_class)()
    libsvm.svm_get_labels(self, labels)
    return labels[:nr_class]

</t>
<t tx="karstenw.20230303141312.22">def get_sv_indices(self):
    total_sv = self.get_nr_sv()
    sv_indices = (c_int * total_sv)()
    libsvm.svm_get_sv_indices(self, sv_indices)
    return sv_indices[:total_sv]

</t>
<t tx="karstenw.20230303141312.23">def get_nr_sv(self):
    return libsvm.svm_get_nr_sv(self)

</t>
<t tx="karstenw.20230303141312.24">def is_probability_model(self):
    return (libsvm.svm_check_probability_model(self) == 1)

</t>
<t tx="karstenw.20230303141312.25">def get_sv_coef(self):
    return [tuple(self.sv_coef[j][i] for j in xrange(self.nr_class - 1))
            for i in xrange(self.l)]

</t>
<t tx="karstenw.20230303141312.26">def get_SV(self):
    result = []
    for sparse_sv in self.SV[:self.l]:
        row = dict()

        i = 0
        while True:
            row[sparse_sv[i].index] = sparse_sv[i].value
            if sparse_sv[i].index == -1:
                break
            i += 1

        result.append(row)
    return result


</t>
<t tx="karstenw.20230303141312.27">def toPyModel(model_ptr):
    """
    toPyModel(model_ptr) -&gt; svm_model

    Convert a ctypes POINTER(svm_model) to a Python svm_model
    """
    if bool(model_ptr) == False:
        raise ValueError("Null pointer")
    m = model_ptr.contents
    m.__createfrom__ = 'C'
    return m

</t>
<t tx="karstenw.20230303141312.3">def genFields(names, types):
    return list(zip(names, types))


</t>
<t tx="karstenw.20230303141312.4">def fillprototype(f, restype, argtypes):
    f.restype = restype
    f.argtypes = argtypes


</t>
<t tx="karstenw.20230303141312.5">class svm_node(Structure):
    _names = ["index", "value"]
    _types = [c_int, c_double]
    _fields_ = genFields(_names, _types)

    @others
</t>
<t tx="karstenw.20230303141312.6">def __str__(self):
    return '%d:%g' % (self.index, self.value)


</t>
<t tx="karstenw.20230303141312.7">def gen_svm_nodearray(xi, feature_max=None, isKernel=None):
    if isinstance(xi, dict):
        index_range = xi.keys()
    elif isinstance(xi, (list, tuple)):
        if not isKernel:
            xi = [0] + xi  # idx should start from 1
        index_range = range(len(xi))
    else:
        raise TypeError('xi should be a dictionary, list or tuple')

    if feature_max:
        assert(isinstance(feature_max, int))
        index_range = list(filter(lambda j: j &lt;= feature_max, index_range))
    if not isKernel:
        index_range = list(filter(lambda j:xi[j] != 0, index_range))

    index_range = sorted(index_range)
    ret = (svm_node * (len(index_range) + 1))()
    ret[-1].index = -1
    for idx, j in enumerate(index_range):
        ret[idx].index = j
        ret[idx].value = xi[j]
    max_idx = 0
    if index_range:
        max_idx = index_range[-1]
    return ret, max_idx


</t>
<t tx="karstenw.20230303141312.8">class svm_problem(Structure):
    _names = ["l", "y", "x"]
    _types = [c_int, POINTER(c_double), POINTER(POINTER(svm_node))]
    _fields_ = genFields(_names, _types)

    @others
</t>
<t tx="karstenw.20230303141312.9">def __init__(self, y, x, isKernel=None):
    if len(y) != len(x):
        raise ValueError("len(y) != len(x)")
    self.l = l = len(y)

    max_idx = 0
    x_space = self.x_space = []
    for i, xi in enumerate(x):
        tmp_xi, tmp_idx = gen_svm_nodearray(xi,isKernel=isKernel)
        x_space += [tmp_xi]
        max_idx = max(max_idx, tmp_idx)
    self.n = max_idx

    self.y = (c_double * l)()
    for i, yi in enumerate(y):
        self.y[i] = yi

    self.x = (POINTER(svm_node) * l)()
    for i, xi in enumerate(self.x_space):
        self.x[i] = xi


</t>
<t tx="karstenw.20230303141314.1">from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import object, range
from builtins import map, zip, filter

import os
import sys
from .libsvm import *
from .libsvm import __all__ as svm_all

__all__ = ['evaluations', 'svm_load_model', 'svm_predict', 'svm_read_problem',
           'svm_save_model', 'svm_train'] + svm_all

sys.path = [os.path.dirname(os.path.abspath(__file__))] + sys.path


</t>
<t tx="karstenw.20230303141314.2">def svm_read_problem(data_file_name):
    """
    svm_read_problem(data_file_name) -&gt; [y, x]

    Read LIBSVM-format data from data_file_name and return labels y
    and data instances x.
    """
    prob_y = []
    prob_x = []
    for line in open(data_file_name):
        line = line.split(None, 1)
        # In case an instance with all zero features
        if len(line) == 1:
            line += ['']
        label, features = line
        xi = {}
        for e in features.split():
            ind, val = e.split(":")
            xi[int(ind)] = float(val)
        prob_y += [float(label)]
        prob_x += [xi]
    return (prob_y, prob_x)


</t>
<t tx="karstenw.20230303141314.3">def svm_load_model(model_file_name):
    """
    svm_load_model(model_file_name) -&gt; model

    Load a LIBSVM model from model_file_name and return.
    """
    model = libsvm.svm_load_model(model_file_name.encode())
    if not model:
        print("can't open model file %s" % model_file_name)
        return None
    model = toPyModel(model)
    return model


</t>
<t tx="karstenw.20230303141314.4">def svm_save_model(model_file_name, model):
    """
    svm_save_model(model_file_name, model) -&gt; None

    Save a LIBSVM model to the file model_file_name.
    """
    libsvm.svm_save_model(model_file_name.encode(), model)


</t>
<t tx="karstenw.20230303141314.5">def evaluations(ty, pv):
    """
    evaluations(ty, pv) -&gt; (ACC, MSE, SCC)

    Calculate accuracy, mean squared error and squared correlation coefficient
    using the true values (ty) and predicted values (pv).
    """
    if len(ty) != len(pv):
        raise ValueError("len(ty) must equal to len(pv)")
    total_correct = total_error = 0
    sumv = sumy = sumvv = sumyy = sumvy = 0
    for v, y in zip(pv, ty):
        if y == v:
            total_correct += 1
        total_error += (v - y) * (v - y)
        sumv += v
        sumy += y
        sumvv += v * v
        sumyy += y * y
        sumvy += v * y
    l = len(ty)
    ACC = 100.0 * total_correct / l
    MSE = total_error / l
    try:
        SCC = ((l * sumvy - sumv * sumy) * (l * sumvy - sumv * sumy)) / ((l * sumvv - sumv * sumv) * (l * sumyy - sumy * sumy))
    except:
        SCC = float('nan')
    return (ACC, MSE, SCC)


</t>
<t tx="karstenw.20230303141314.6">def svm_train(arg1, arg2=None, arg3=None):
    """
    svm_train(y, x [, options]) -&gt; model | ACC | MSE
    svm_train(prob [, options]) -&gt; model | ACC | MSE
    svm_train(prob, param) -&gt; model | ACC| MSE

    Train an SVM model from data (y, x) or an svm_problem prob using
    'options' or an svm_parameter param.
    If '-v' is specified in 'options' (i.e., cross validation)
    either accuracy (ACC) or mean-squared error (MSE) is returned.
    options:
        -s svm_type : set type of SVM (default 0)
            0 -- C-SVC		(multi-class classification)
            1 -- nu-SVC		(multi-class classification)
            2 -- one-class SVM
            3 -- epsilon-SVR	(regression)
            4 -- nu-SVR		(regression)
        -t kernel_type : set type of kernel function (default 2)
            0 -- linear: u'*v
            1 -- polynomial: (gamma*u'*v + coef0)^degree
            2 -- radial basis function: exp(-gamma*|u-v|^2)
            3 -- sigmoid: tanh(gamma*u'*v + coef0)
            4 -- precomputed kernel (kernel values in training_set_file)
        -d degree : set degree in kernel function (default 3)
        -g gamma : set gamma in kernel function (default 1/num_features)
        -r coef0 : set coef0 in kernel function (default 0)
        -c cost : set the parameter C of C-SVC, epsilon-SVR, and nu-SVR (default 1)
        -n nu : set the parameter nu of nu-SVC, one-class SVM, and nu-SVR (default 0.5)
        -p epsilon : set the epsilon in loss function of epsilon-SVR (default 0.1)
        -m cachesize : set cache memory size in MB (default 100)
        -e epsilon : set tolerance of termination criterion (default 0.001)
        -h shrinking : whether to use the shrinking heuristics, 0 or 1 (default 1)
        -b probability_estimates : whether to train a SVC or SVR model for probability estimates, 0 or 1 (default 0)
        -wi weight : set the parameter C of class i to weight*C, for C-SVC (default 1)
        -v n: n-fold cross validation mode
        -q : quiet mode (no outputs)
    """
    prob, param = None, None
    if isinstance(arg1, (list, tuple)):
        assert isinstance(arg2, (list, tuple))
        y, x, options = arg1, arg2, arg3
        param = svm_parameter(options)
        prob = svm_problem(y, x, isKernel=(param.kernel_type == PRECOMPUTED))
    elif isinstance(arg1, svm_problem):
        prob = arg1
        if isinstance(arg2, svm_parameter):
            param = arg2
        else:
            param = svm_parameter(arg2)
    if prob is None or param is None:
        raise TypeError("Wrong types for the arguments")

    if param.kernel_type == PRECOMPUTED:
        for xi in prob.x_space:
            idx, val = xi[0].index, xi[0].value
            if xi[0].index != 0:
                raise ValueError('Wrong input format: first column must be 0:sample_serial_number')
            if val &lt;= 0 or val &gt; prob.n:
                raise ValueError('Wrong input format: sample_serial_number out of range')

    if param.gamma == 0 and prob.n &gt; 0:
        param.gamma = 1.0 / prob.n
    libsvm.svm_set_print_string_function(param.print_func)
    err_msg = libsvm.svm_check_parameter(prob, param)
    if err_msg:
        raise ValueError('Error: %s' % err_msg)

    if param.cross_validation:
        l, nr_fold = prob.l, param.nr_fold
        target = (c_double * l)()
        libsvm.svm_cross_validation(prob, param, nr_fold, target)
        ACC, MSE, SCC = evaluations(prob.y[:l], target[:l])
        if param.svm_type in [EPSILON_SVR, NU_SVR]:
            print("Cross Validation Mean squared error = %g" % MSE)
            print("Cross Validation Squared correlation coefficient = %g" % SCC)
            return MSE
        else:
            print("Cross Validation Accuracy = %g%%" % ACC)
            return ACC
    else:
        m = libsvm.svm_train(prob, param)
        m = toPyModel(m)

        # If prob is destroyed, data including SVs pointed by m can remain.
        m.x_space = prob.x_space
        return m


</t>
<t tx="karstenw.20230303141314.7">def svm_predict(y, x, m, options=""):
    """
    svm_predict(y, x, m [, options]) -&gt; (p_labels, p_acc, p_vals)

    Predict data (y, x) with the SVM model m.
    options:
        -b probability_estimates: whether to predict probability estimates,
            0 or 1 (default 0); for one-class SVM only 0 is supported.
        -q : quiet mode (no outputs).

    The return tuple contains
    p_labels: a list of predicted labels
    p_acc: a tuple including  accuracy (for classification), mean-squared
           error, and squared correlation coefficient (for regression).
    p_vals: a list of decision values or probability estimates (if '-b 1'
            is specified). If k is the number of classes, for decision values,
            each element includes results of predicting k(k-1)/2 binary-class
            SVMs. For probabilities, each element contains k values indicating
            the probability that the testing instance is in each class.
            Note that the order of classes here is the same as 'model.label'
            field in the model structure.
    """

    def info(s):
        print(s)

    predict_probability = 0
    argv = options.split()
    i = 0
    while i &lt; len(argv):
        if argv[i] == '-b':
            i += 1
            predict_probability = int(argv[i])
        elif argv[i] == '-q':
            info = print_null
        else:
            raise ValueError("Wrong options")
        i += 1

    svm_type = m.get_svm_type()
    is_prob_model = m.is_probability_model()
    nr_class = m.get_nr_class()
    pred_labels = []
    pred_values = []

    if predict_probability:
        if not is_prob_model:
            raise ValueError("Model does not support probabiliy estimates")

        if svm_type in [NU_SVR, EPSILON_SVR]:
            info("Prob. model for test data: target value = predicted value + z,\n"
            "z: Laplace distribution e^(-|z|/sigma)/(2sigma),sigma=%g" % m.get_svr_probability());
            nr_class = 0

        prob_estimates = (c_double * nr_class)()
        for xi in x:
            xi, idx = gen_svm_nodearray(xi, isKernel=(m.param.kernel_type == PRECOMPUTED))
            label = libsvm.svm_predict_probability(m, xi, prob_estimates)
            values = prob_estimates[:nr_class]
            pred_labels += [label]
            pred_values += [values]
    else:
        if is_prob_model:
            info("Model supports probability estimates, but disabled in predicton.")
        if svm_type in (ONE_CLASS, EPSILON_SVR, NU_SVC):
            nr_classifier = 1
        else:
            nr_classifier = nr_class * (nr_class - 1) // 2
        dec_values = (c_double * nr_classifier)()
        for xi in x:
            xi, idx = gen_svm_nodearray(xi, isKernel=(m.param.kernel_type == PRECOMPUTED))
            label = libsvm.svm_predict_values(m, xi, dec_values)
            if(nr_class == 1):
                values = [1]
            else:
                values = dec_values[:nr_classifier]
            pred_labels += [label]
            pred_values += [values]

    ACC, MSE, SCC = evaluations(y, pred_labels)
    l = len(y)
    if svm_type in [EPSILON_SVR, NU_SVR]:
        info("Mean squared error = %g (regression)" % MSE)
        info("Squared correlation coefficient = %g (regression)" % SCC)
    else:
        info("Accuracy = %g%% (%d/%d) (classification)" % (ACC, int(l * ACC / 100), l))

    return pred_labels, (ACC, MSE, SCC), pred_values
</t>
<t tx="karstenw.20230303142559.1"></t>
<t tx="karstenw.20230303142603.1"></t>
<t tx="karstenw.20230303142608.1"></t>
<t tx="karstenw.20230303142620.1">#### PATTERN | WEB | IMAP ##########################################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303142634.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

import sys
import os
import re
import imaplib
import email
import time

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

# Import the Cache class from pattern.web so e-mails can be cached locally (faster):
try:
    from ..cache import cache
except:
    try:
        import os
        import sys
        sys.path.append(os.path.join(MODULE, ".."))
        from cache import cache
    except:
        try:
            from pattern.web.cache import cache
        except:
            cache = {}

from pattern.helpers import encode_string, decode_string

decode_utf8 = decode_string
encode_utf8 = encode_string


</t>
<t tx="karstenw.20230303142634.10">def __init__(self, username, password, service=GMAIL, port=993, secure=True):
    """ IMAP4 connection to a mailbox. With secure=True, SSL is used. 
        The standard port for SSL is 993.
        The standard port without SSL is 143.
    """
    self._username = username
    self._password = password
    self._host = service
    self._port = port
    self._secure = secure
    self._imap4 = None
    self._folders = None
    self.login(username, password)

</t>
<t tx="karstenw.20230303142634.11">@property
def _id(self):
    return "%s:%s@%s:%s" % (self._username, self._password, self._host, self._port)

</t>
<t tx="karstenw.20230303142634.12">@property
def imap4(self):
    if self._imap4 is None:
        raise MailNotLoggedIn
    return self._imap4

</t>
<t tx="karstenw.20230303142634.13">def login(self, username, password, **kwargs):
    """ Signs in to the mail account with the given username and password,
        raises a MailLoginError otherwise.
    """
    self.logout()
    self._secure = kwargs.get("secure", self._secure)
    self._imap4 = (self._secure and IMAP4_SSL or IMAP4)(self._host, self._port)
    try:
        status, response = self._imap4.login(username, password)
    except:
        raise MailLoginError
    if status != "OK":
        raise MailLoginError(response)

</t>
<t tx="karstenw.20230303142634.14">def logout(self):
    """ Signs out of the mail account.
    """
    if self._imap4 is not None:
        self._imap4.logout()
        self._imap4 = None

</t>
<t tx="karstenw.20230303142634.15">def __del__(self):
    if "_imap4" in self.__dict__:
        if self._imap4 is not None:
            self._imap4.logout()
            self._imap4 = None

</t>
<t tx="karstenw.20230303142634.16">@property
def folders(self):
    """ A dictionary of (name, MailFolder)-tuples.
        Default folders: inbox, trash, spam, receipts, ...
    """
    if self._folders is None:
        status, response = self.imap4.list()
        self._folders = [f.split(" \"")[-1].strip(" \"") for f in response]
        self._folders = [(_basename(f), MailFolder(self, f)) for f in self._folders]
        self._folders = [(f, o) for f, o in self._folders if f != ""]
        self._folders = dict(self._folders)
    return self._folders

</t>
<t tx="karstenw.20230303142634.17">def __getattr__(self, k):
    """ Each folder is accessible as Mail.[name].
    """
    if k in self.__dict__:
        return self.__dict__[k]
    if k in self.folders:
        return self.folders[k]
    raise AttributeError("'Mail' object has no attribute '%s'" % k)

</t>
<t tx="karstenw.20230303142634.18">def _decode(s, message):
    try:
        # Decode MIME header (e.g., "=?utf-8?q?").
        s = email.Header.decode_header(s)[0][0]
    except:
        pass
    try:
        # Decode message Content-Type charset to Unicode.
        # If all fails, try Latin-1 (common case).
        e = message.get("Content-Type")
        e = e.split("charset=")[-1].split(";")[0].strip("\"'").lower()
        s = s.decode(e)
    except:
        try:
            s = s.decode("utf-8")
        except:
            try:
                s = s.decode("latin-1")
            except:
                pass
    return s


</t>
<t tx="karstenw.20230303142634.19">class MailFolder(object):

    @others
#--- MAIL MESSAGE ----------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303142634.2">class IMAP4(imaplib.IMAP4):
    pass


</t>
<t tx="karstenw.20230303142634.20">def __init__(self, parent, name):
    """ A folder (inbox, spam, trash, ...) in a mailbox.
        E-mail messages can be searched and retrieved (including attachments) from a folder.
    """
    self._parent = parent
    self._name = name

</t>
<t tx="karstenw.20230303142634.21">@property
def parent(self):
    return self._parent

</t>
<t tx="karstenw.20230303142634.22">@property
def name(self):
    return _basename(self._name)

</t>
<t tx="karstenw.20230303142634.23">@property
def count(self):
    return len(self)

</t>
<t tx="karstenw.20230303142634.24">def search(self, q, field=FROM, cached=False):
    """ Returns a list of indices for the given query, latest-first.
        The search field can be FROM, DATE or SUBJECT.
    """
    id = "mail-%s-%s-%s-%s" % (self.parent._id, self.name, q, field)
    if cached and id in cache:
        status, response = "OK", [cache[id]]
    else:
        status, response = self.parent.imap4.select(self._name, readonly=1)
        status, response = self.parent.imap4.search(None, field.upper(), q)
        if cached:
            cache[id] = response[0]
    return sorted([int(i) - 1 for i in response[0].split()], reverse=True)

</t>
<t tx="karstenw.20230303142634.25">def read(self, i, attachments=False, cached=True):
    return self.__getitem__(i, attachments, cached)

</t>
<t tx="karstenw.20230303142634.26">def __getitem__(self, i, attachments=False, cached=True):
    """ Returns the mail message with the given index.
        Each message is a dictionary with date, from, subject, body, attachments entries.
        The attachments entry is a list of (MIME-type, str)-tuples.
    """
    i += 1
    id = "mail-%s-%s-%s-%s" % (self.parent._id, self.name, i, attachments)
    if cached and id in cache:
        m = cache[id]
    else:
        # Select the current mail folder.
        # Get the e-mail header.
        # Get the e-mail body, with or without file attachments.
        status, response = self.parent.imap4.select(self._name, readonly=1)
        status, response1 = self.parent.imap4.fetch(str(i), '(BODY.PEEK[HEADER])')
        status, response2 = self.parent.imap4.fetch(str(i), '(BODY.PEEK[%s])' % (not attachments and "TEXT" or ""))
        time.sleep(0.1)
        m = response1[0][1] + response2[0][1]
        # Cache the raw message for faster retrieval.
        if cached:
            cache[id] = m
    # Parse the raw message.
    m = email.message_from_string(m)
    d = Message([
             (DATE, _decode(m.get(DATE), m)),
             (FROM, _decode(m.get(FROM), m)),
          (SUBJECT, _decode(m.get(SUBJECT), m)),
             (BODY, ""),
      (ATTACHMENTS, [])])
    # Message body can be a list of parts, including file attachments.
    for p in (m.is_multipart() and m.get_payload() or [m]):
        if p.get_content_type() == "text/plain":
            d[BODY] += _decode(p.get_payload(decode=True), p)
        elif attachments:
            d[ATTACHMENTS].append((p.get_content_type(), p.get_payload()))
    for k in d:
        if isinstance(d[k], str):
            d[k] = d[k].strip()
            d[k] = d[k].replace("\r\n", "\n")
    return d

</t>
<t tx="karstenw.20230303142634.27">def __iter__(self):
    """ Returns an iterator over all the messages in the folder, latest-first.
    """
    for i in reversed(range(len(self))):
        yield self[i]

</t>
<t tx="karstenw.20230303142634.28">def __len__(self):
    status, response = self.parent.imap4.select(self.name, readonly=1)
    return int(response[0])

</t>
<t tx="karstenw.20230303142634.29">def __repr__(self):
    return "MailFolder(name=%s)" % repr(self.name)

</t>
<t tx="karstenw.20230303142634.3">class IMAP4_SSL(imaplib.IMAP4_SSL):
    pass

#### MAIL ##########################################################################################

GMAIL = "imap.gmail.com"

DATE, FROM, SUBJECT, BODY, ATTACHMENTS = \
    "date", "from", "subject", "body", "attachments"


</t>
<t tx="karstenw.20230303142634.30">class Message(dict):

    @others
</t>
<t tx="karstenw.20230303142634.31">@property
def author(self):
    return self.get(FROM, None)

</t>
<t tx="karstenw.20230303142634.32">@property
def date(self):
    return self.get(DATE, None)

</t>
<t tx="karstenw.20230303142634.33">@property
def subject(self):
    return self.get(SUBJECT, "")

</t>
<t tx="karstenw.20230303142634.34">@property
def body(self):
    return self.get(BODY, "")

</t>
<t tx="karstenw.20230303142634.35">@property
def attachments(self):
    return self.get(ATTACHMENTS, [])

</t>
<t tx="karstenw.20230303142634.36">@property
def email_address(self):
    m = re.search(r"&lt;(.*?)&gt;", self.author)
    return m and m.group(1) or ""

</t>
<t tx="karstenw.20230303142634.37">def __repr__(self):
    return "Message(from=%s, subject=%s)" % (
        repr(self.author),
        repr(self.subject))
</t>
<t tx="karstenw.20230303142634.4">def _basename(folder):
    # [Gmail]/INBOX =&gt; inbox
    f = folder.replace("[Gmail]/", "")
    f = f.replace("[Gmail]", "")
    f = f.replace("Mail", "")   # "Sent Mail" alias = "sent".
    f = f.replace("INBOX.", "") # "inbox.sent" alias = "sent".
    f = f.lower()
    f = f.strip()
    return f


</t>
<t tx="karstenw.20230303142634.5">class MailError(Exception):
    pass


</t>
<t tx="karstenw.20230303142634.6">class MailServiceError(MailError):
    pass


</t>
<t tx="karstenw.20230303142634.7">class MailLoginError(MailError):
    pass


</t>
<t tx="karstenw.20230303142634.8">class MailNotLoggedIn(MailError):
    pass


</t>
<t tx="karstenw.20230303142634.9">class Mail(object):

    @others
#--- MAIL FOLDER -----------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230303142647.1">#### PATTERN | WEB | LOCALE ########################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303142655.1">from __future__ import unicode_literals

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

#### LANGUAGE &amp; REGION #############################################################################
# IETF BCP 47 language-region code =&gt; (language, region, ISO-639 language code, ISO-3166 region code).
# Note: the list is incomplete (especially for African languages).
# Please help out by correcting errors and omissions.

LANGUAGE_REGION = {
    'aa-ET': ('Afar', 'Ethiopia', 'aa', 'ET'),
    'af-ZA': ('Afrikaans', 'South Africa', 'af', 'ZA'),
    'ar-AE': ('Arabic', 'United Arab Emirates', 'ar', 'AE'),
    'ar-BH': ('Arabic', 'Bahrain', 'ar', 'BH'),
    'ar-DZ': ('Arabic', 'Algeria', 'ar', 'DZ'),
    'ar-EG': ('Arabic', 'Egypt', 'ar', 'EG'),
    'ar-IQ': ('Arabic', 'Iraq', 'ar', 'IQ'),
    'ar-JO': ('Arabic', 'Jordan', 'ar', 'JO'),
    'ar-KW': ('Arabic', 'Kuwait', 'ar', 'KW'),
    'ar-LB': ('Arabic', 'Lebanon', 'ar', 'LB'),
    'ar-LY': ('Arabic', 'Libya', 'ar', 'LY'),
    'ar-MA': ('Arabic', 'Morocco', 'ar', 'MA'),
    'ar-OM': ('Arabic', 'Oman', 'ar', 'OM'),
    'ar-QA': ('Arabic', 'Qatar', 'ar', 'QA'),
    'ar-SA': ('Arabic', 'Saudi Arabia', 'ar', 'SA'),
    'ar-SD': ('Arabic', 'Sudan', 'ar', 'SD'),
    'ar-SY': ('Arabic', 'Syria', 'ar', 'SY'),
    'ar-TN': ('Arabic', 'Tunisia', 'ar', 'TN'),
    'ar-YE': ('Arabic', 'Yemen', 'ar', 'YE'),
    'be-BY': ('Belarusian', 'Belarus', 'be', 'BY'),
    'bg-BG': ('Bulgarian', 'Bulgaria', 'bg', 'BG'),
    'ca-AD': ('Catalan', 'Andorra', 'ca', 'AD'),
    'cs-CZ': ('Czech', 'Czech Republic', 'cs', 'CZ'),
    'da-DK': ('Danish', 'Denmark', 'da', 'DK'),
    'de-DE': ('German', 'Germany', 'de', 'DE'),
    'de-AT': ('German', 'Austria', 'de', 'AT'),
    'de-CH': ('German', 'Switzerland', 'de', 'CH'),
    'de-LI': ('German', 'Liechtenstein', 'de', 'LI'),
    'de-LU': ('German', 'Luxembourg', 'de', 'LU'),
    'el-GR': ('Greek', 'Greece', 'el', 'GR'),
    'en-AU': ('English', 'Australia', 'en', 'AU'),
    'en-BZ': ('English', 'Belize', 'en', 'BZ'),
    'en-CA': ('English', 'Canada', 'en', 'CA'),
    'en-GB': ('English', 'United Kingdom', 'en', 'GB'),
    'en-IE': ('English', 'Ireland', 'en', 'IE'),
    'en-JM': ('English', 'Jamaica', 'en', 'JM'),
    'en-KE': ('English', 'Kenya', 'en', 'KE'),
    'en-NZ': ('English', 'New Zealand', 'en', 'NZ'),
    'en-TT': ('English', 'Trinidad', 'en', 'TT'),
    'en-US': ('English', 'United States', 'en', 'US'),
    'en-ZA': ('English', 'South Africa', 'en', 'ZA'),
    'es-ES': ('Spanish', 'Spain', 'es', 'ES'),
    'es-AR': ('Spanish', 'Argentina', 'es', 'AQ'),
    'es-BO': ('Spanish', 'Bolivia', 'es', 'BO'),
    'es-CL': ('Spanish', 'Chile', 'es', 'CL'),
    'es-CO': ('Spanish', 'Colombia', 'es', 'CO'),
    'es-CR': ('Spanish', 'Costa Rica', 'es', 'CR'),
    'es-DO': ('Spanish', 'Dominican Republic', 'es', 'DO'),
    'es-EC': ('Spanish', 'Ecuador', 'es', 'EC'),
    'es-GT': ('Spanish', 'Guatemala', 'es', 'GT'),
    'es-HN': ('Spanish', 'Honduras', 'es', 'HN'),
    'es-MX': ('Spanish', 'Mexico', 'es', 'MX'),
    'es-NI': ('Spanish', 'Nicaragua', 'es', 'NI'),
    'es-PA': ('Spanish', 'Panama', 'es', 'PA'),
    'es-PE': ('Spanish', 'Peru', 'es', 'PE'),
    'es-PR': ('Spanish', 'Puerto Rico', 'es', 'PR'),
    'es-PY': ('Spanish', 'Paraguay', 'es', 'PY'),
    'es-SV': ('Spanish', 'El Salvador', 'es', 'SV'),
    'es-UY': ('Spanish', 'Uruguay', 'es', 'UY'),
    'es-VE': ('Spanish', 'Venezuela', 'es', 'VE'),
    'et-EE': ('Estonian', 'Estonia', 'et', 'EE'),
    'eu-PV': ('Basque', 'Basque Country', 'eu', 'PV'),
    'fa-IR': ('Farsi', 'Iran', 'fa', 'IR'),
    'fi-FI': ('Finnish', 'Finland', 'fi', 'FI'),
    'fo-FO': ('Faeroese', 'Faroe Islands', 'fo', 'FO'),
    'fr-CG': ('French', 'Congo', 'fr', 'CG'),
    'fr-FR': ('French', 'France', 'fr', 'FR'),
    'fr-BE': ('French', 'Belgium', 'fr', 'BE'),
    'fr-CA': ('French', 'Canada', 'fr', 'CA'),
    'fr-CH': ('French', 'Switzerland', 'fr', 'CH'),
    'fr-LU': ('French', 'Luxembourg', 'fr', 'LU'),
    'ga-IE': ('Irish', 'Ireland', 'ga', 'IE'),
    'gd-UK': ('Gaelic', 'Scotland', 'gd', 'UK'),
    'he-IL': ('Hebrew', 'Israel', 'he', 'IL'),
    'hi-IN': ('Hindi', 'India', 'hi', 'IN'),
    'hr-HR': ('Croatian', 'Croatia', 'hr', 'HR'),
    'hu-HU': ('Hungarian', 'Hungary', 'hu', 'HU'),
    'id-ID': ('Indonesian', 'Indonesia', 'id', 'ID'),
    'is-IS': ('Icelandic', 'Iceland', 'is', 'IS'),
    'it-IT': ('Italian', 'Italy', 'it', 'IT'),
    'it-CH': ('Italian', 'Switzerland', 'it', 'CH'),
    'ja-JA': ('Japanese', 'Japan', 'ja', 'JA'),
    'ka-GE': ('Georgian', 'Georgia', 'ka', 'GE'),
    'kg-CG': ('Kongo', 'Congo', 'kg', 'CG'),
    'kl-GL': ('Kalaallisut', 'Greenland', 'kl', 'GL'),
    'ko-KP': ('Korean', 'Johab', 'ko', 'KP'),
    'ln-CG': ('Lingala', 'Congo', 'ln', 'CG'),
    'lo-LA': ('Lao', 'Lao', 'lo', 'LA'),
    'lt-LT': ('Lithuanian', 'Lithuania', 'lt', 'LT'),
    'lv-LV': ('Latvian', 'Latvia', 'lv', 'LV'),
    'mk-ML': ('Macedonian', 'Macedonia', 'mk', 'MK'),
    'ms-MY': ('Malay', 'Malaysia', 'ms', 'MY'),
    'mt-MT': ('Maltese', 'Malta', 'mt', 'MT'),
    'nd-ZW': ('Ndebele', 'Zimbabwe', 'nd', 'ZW'),
    'nl-NL': ('Dutch', 'Netherlands', 'nl', 'NL'),
    'nl-BE': ('Dutch', 'Belgium', 'nl', 'BE'),
    'no-NO': ('Norwegian', 'Nynorsk', 'no', 'NO'),
    'om-ET': ('Oromo', 'Ethiopia', 'om', 'ET'),
    'om-KE': ('Oromo', 'Kenya', 'om', 'KE'),
    'pl-PL': ('Polish', 'Poland', 'pl', 'PL'),
    'pt-MZ': ('Portuguese', 'Mozambique', 'pt', 'PT'),
    'pt-PT': ('Portuguese', 'Portugal', 'pt', 'PT'),
    'pt-BR': ('Portuguese', 'Brazil', 'pt', 'BR'),
    'rm-IT': ('Rhaeto-Romanic', 'Italy', 'rm', 'IT'),
    'ro-RO': ('Romanian', 'Romania', 'ro', 'RO'),
    'ro-MO': ('Romanian', 'Republic of Moldova', 'ro', 'MO'),
    'ru-RU': ('Russian', 'Russia', 'ru', 'RU'),
    'rw-RW': ('Kinyarwanda', 'Rwanda', 'rw', 'RW'),
    'sk-SK': ('Slovak', 'Slovakia', 'sk', 'SK'),
    'sl-SI': ('Slovenian', 'Slovenia', 'sl', 'SI'),
    'sm-SM': ('Samoan', 'Samoa', 'sm', 'SM'),
    'so-KE': ('Somali', 'Kenya', 'so', 'KE'),
    'so-SO': ('Somali', 'Somalia', 'so', 'SO'),
    'sq-AL': ('Albanian', 'Albania', 'sq', 'AL'),
    'sr-RS': ('Serbian', 'Serbia', 'sr', 'RS'),
    'sv-SE': ('Swedish', 'Sweden', 'sv', 'SE'),
    'sw-SW': ('Swahili', 'Kenya', 'sw', 'KE'),
    'sw-TZ': ('Swahili', 'Tanzania', 'sw', 'TZ'),
    'sv-FI': ('Swedish', 'Finland', 'sv', 'FI'),
    'sx-ZA': ('Sotho', 'South Africa', 'sx', 'ZA'),
    'sz-FI': ('Sami', 'Sapmi', 'sz', 'FI'),
    'th-TH': ('Thai', 'Thailand', 'th', 'TH'),
    'tn-BW': ('Tswana', 'Botswana', 'tn', 'BW'),
    'to-TO': ('Tonga', 'Tonga', 'to', 'TO'),
    'tr-TR': ('Turkish', 'Turkey', 'tr', 'TR'),
    'ts-ZA': ('Tsonga', 'South Africa', 'ts', 'ZA'),
    'uk-UA': ('Ukrainian', 'Ukraine', 'uk', 'UA'),
    'ur-PK': ('Urdu', 'Pakistan', 'ur', 'PK'),
    'uz-UZ': ('Uzbek', 'Uzbekistan', 'uz', 'UZ'),
    've-ZA': ('Venda', 'South Africa', 've', 'ZA'),
    'vi-VN': ('Vietnamese', 'Vietnam', 'vi', 'VN'),
    'xh-ZA': ('Xhosa', 'South Africa', 'xh', 'ZA'),
    'zh-CN': ('Chinese', 'China', 'zh', 'CN'),
    'zh-HK': ('Chinese', 'Hong Kong', 'zh', 'HK'),
    'zh-SG': ('Chinese', 'Singapore', 'zh', 'SG'),
    'zh-TW': ('Chinese', 'Taiwan', 'zh', 'TW'),
    'zu-ZA': ('Zulu', 'South Africa', 'zu', 'ZA'),
    'zu-ZW': ('Zulu', 'Zimbabwe', 'zu', 'ZW')
}


</t>
<t tx="karstenw.20230303142655.10">def geocode(location):
    """ Returns a (latitude, longitude, language code, region)-tuple 
        for the given city (mostly capitals).
    """
    if location in GEOCODE:
        return GEOCODE[location]
    for k, v in GEOCODE.items():
        if location.lower() == k.lower():
            return v
</t>
<t tx="karstenw.20230303142655.2">def encode_language(name):
    """ Returns the language code for the given language name.
        For example: encode_language("dutch") =&gt; "nl".
    """
    for tag, (language, region, iso639, iso3166) in LANGUAGE_REGION.items():
        if language == name.capitalize():
            return iso639


</t>
<t tx="karstenw.20230303142655.3">def decode_language(code):
    """ Returns the language name for the given language code.
        For example: decode_language("nl") =&gt; "Dutch".
    """
    for tag, (language, region, iso639, iso3166) in LANGUAGE_REGION.items():
        if iso639 == code.lower():
            return language


</t>
<t tx="karstenw.20230303142655.4">def encode_region(name):
    """ Returns the region code for the given region name.
        For example: encode_region("belgium") =&gt; "BE".
    """
    for tag, (language, region, iso639, iso3166) in LANGUAGE_REGION.items():
        if region == name.capitalize():
            return iso3166


</t>
<t tx="karstenw.20230303142655.5">def decode_region(code):
    """ Returns the region name for the given region code.
        For example: decode_region("be") =&gt; "Belgium".
    """
    for tag, (language, region, iso639, iso3166) in LANGUAGE_REGION.items():
        if iso3166 == code.upper():
            return region


</t>
<t tx="karstenw.20230303142655.6">def languages(region):
    """ Returns a list of language codes for the given region code.
        For example: languages(encode_region("belgium")) =&gt; ["fr", "nl"]
    """
    v, a = region.upper(), []
    for tag, (language, region, iso639, iso3166) in LANGUAGE_REGION.items():
        if iso3166 == v:
            a.append(iso639)
    return sorted(a)


</t>
<t tx="karstenw.20230303142655.7">def regions(language):
    """ Returns a list of region codes for the given language code.
        For example: regions(encode_language("dutch")) =&gt; ["NL", "BE"]
    """
    x, a = language.lower(), []
    for tag, (language, region, iso639, iso3166) in LANGUAGE_REGION.items():
        if iso639 == x:
            a.append(iso3166)
    return sorted(a, key=lambda tag: tag.lower() != x and tag or "")


</t>
<t tx="karstenw.20230303142655.8">def regionalize(language):
    """ Returns a list of RFC-5646 language-region codes for the given language code.
        For example: regionalize("nl") =&gt; ["nl-nl", "nl-be"]
    """
    if not isinstance(language, str):
        return []
    if "-" in language:
        language, region = language.split("-")
        return [language.lower() + "-" + region.upper()]  # nl-nl =&gt; nl-NL
    main = lambda tag: tag in ("ar-AE", "en-US", "zh-CN") or tag[:2] == tag[3:].lower() # nl-NL
    a = [language + "-" + r for r in regions(language.lower())]
    a = sorted(a, key=main, reverse=True)
    return a


</t>
<t tx="karstenw.20230303142655.9">def market(language):
    """ Returns the first item from regionalize(language).
    """
    a = regionalize(language)
    a = len(a) &gt; 0 and a[0] or None
    return a

#print(encode_language("dutch")) # nl
#print(decode_language("nl"))    # Dutch
#print(encode_region("belgium")) # BE
#print(decode_region("be"))      # Belgium
#print(languages("be"))          # ["fr", "nl"]
#print(regions("nl"))            # ["NL", "BE"]
#print(regionalize("nl"))        # ["nl-NL", "nl-BE"]

### GEOCODE ########################################################################################
# capital =&gt; (latitude, longitude, ISO-639 language code, region)

GEOCODE = {
         'Abu Dhabi': ( 24.467,  54.367, "ar", "United Arab Emirates"),
             'Abuja': (  9.083,   7.533, "en", "Nigeria"),
             'Accra': (  5.550,  -0.217, "en", "Ghana"),
           'Algiers': ( 36.750,   3.050, "ar", "Algeria"),
             'Amman': ( 31.950,  35.933, "ar", "Jordan"),
         'Amsterdam': ( 52.383,   4.900, "nl", "Netherlands"),
            'Ankara': ( 39.933,  32.867, "tr", "Turkey"),
            'Astana': ( 51.167,  71.417, "ru", "Kazakhstan"),
          'Asuncion': (-25.267, -57.667, "es", "Paraguay"),
            'Athens': ( 37.983,  23.733, "el", "Greece"),
           'Baghdad': ( 33.333,  44.383, "ar", "Iraq"),
            'Bamako': ( 12.650,  -8.000, "fr", "Mali"),
           'Bangkok': ( 13.750, 100.517, "th", "Thailand"),
            'Bangui': (  4.367,  18.583, "fr", "Central African Republic"),
           'Beijing': ( 39.917, 116.383, "zh", "China"),
            'Beirut': ( 33.867,  35.500, "ar", "Lebanon"),
          'Belgrade': ( 44.833,  20.500, "sr", "Serbia"),
            'Berlin': ( 52.517,  13.400, "de", "Germany"),
              'Bern': ( 46.950,   7.433, "de", "Switzerland"),
            'Bissau': ( 11.850, -15.583, "pt", "Guinea"),
            'Bogota': (  4.600, -74.083, "es", "Colombia"),
          'Brasilia': (-15.783, -47.917, "pt", "Brazil"),
        'Bratislava': ( 48.150,  17.117, "sk", "Slovakia"),
       'Brazzaville': ( -4.250,  15.283, "fr", "Congo"),
          'Brussels': ( 50.833,   4.333, "nl", "Belgium"),
         'Bucharest': ( 44.433,  26.100, "ro", "Romania"),
          'Budapest': ( 47.500,  19.083, "hu", "Hungary"),
      'Buenos Aires': (-34.600, -58.667, "es", "Argentina"),
         'Bujumbura': ( -3.367,  29.350, "rn", "Burundi"),
             'Cairo': ( 30.050,  31.250, "ar", "Egypt"),
          'Canberra': (-35.283, 149.217, "en", "Australia"),
           'Caracas': ( 10.500, -66.933, "es", "Venezuela"),
          'Chisinau': ( 47.000,  28.850, "ro", "Moldova"),
           'Colombo': (  6.933,  79.850, "si", "Sri Lanka"),
           'Conakry': (  9.550, -13.700, "fr", "Guinea"),
        'Copenhagen': ( 55.667,  12.583, "da", "Denmark"),
             'Dakar': ( 24.633,  46.717, "fr", "Senegal"),
          'Damascus': ( 33.500,  36.300, "ar", "Syria"),
     'Dar es Salaam': ( -6.800,  39.283, "sw", "Tanzania"),
             'Dhaka': ( 23.717,  90.400, "bn", "Bangladesh"),
            'Dublin': ( 53.317,  -6.233, "en", "Ireland"),
          'Freetown': (  8.500, -13.250, "en", "Sierra Leone"),
       'George Town': ( 19.300, -81.383, "en", "Malaysia"),
        'Georgetown': (  6.800, -58.167, "en", "Guyana"),
    'Guatemala City': ( 14.617, -90.517, "es", "Guatemala"),
             'Hanoi': ( 21.033, 105.850, "vi", "Vietnam"),
            'Harare': (-17.833,  31.050, "en", "Zimbabwe"),
            'Havana': ( 23.117, -82.350, "es", "Cuba"),
          'Helsinki': ( 60.167,  24.933, "fi", "Finland"),
         'Islamabad': ( 33.700,  73.167, "ur", "Pakistan"),
           'Jakarta': ( -6.167, 106.817, "ms", "Indonesia"),
         'Jerusalem': ( 31.767,  35.233, "he", "Israel"),
              'Juba': (  4.850,  31.617, "en", "Sudan"),
             'Kabul': ( 34.517,  69.183, "fa", "Afghanistan"),
           'Kampala': (  0.317,  32.417, "en", "Uganda"),
         'Kathmandu': ( 27.717,  85.317, "ne", "Nepal"),
          'Khartoum': ( 15.600,  32.533, "ar", "Sudan"),
              'Kiev': ( 50.433,  30.517, "rw", "Ukraine"),
            'Kigali': ( -1.950,  30.067, "en", "Rwanda"),
          'Kingston': ( 18.000, -76.800, "fr", "Jamaica"),
          'Kinshasa': ( -4.317,  15.300, "ms", "Congo"),
      'Kuala Lumpur': (  3.167, 101.700, "ar", "Malaysia"),
       'Kuwait City': ( 29.367,  47.967, "uk", "Kuwait"),
            'La Paz': (-16.500, -68.150, "es", "Bolivia"),
              'Lima': (-12.050, -77.050, "es", "Peru"),
            'Lisbon': ( 38.717,  -9.133, "pt", "Portugal"),
         'Ljubljana': ( 46.050,  14.517, "sl", "Slovenia"),
              'Lome': (  6.133,   1.217, "fr", "Togo"),
            'London': ( 51.500,  -0.167, "en", "United Kingdom"),
            'Luanda': ( -8.833,  13.233, "pt", "Angola"),
            'Lusaka': (-15.417,  28.283, "en", "Zambia"),
        'Luxembourg': ( 49.600,   6.117, "cd", "Luxembourg"),
            'Madrid': ( 40.400,  -3.683, "es", "Spain"),
           'Managua': ( 12.150, -86.283, "es", "Nicaragua"),
            'Manila': ( 14.583, 121.000, "tl", "Philippines"),
            'Maputo': (-25.950,  32.583, "pt", "Mozambique"),
       'Mexico City': ( 19.433, -99.133, "es", "Mexico"),
             'Minsk': ( 53.900,  27.567, "be", "Belarus"),
         'Mogadishu': (  2.067,  45.367, "so", "Somalia"),
            'Monaco': ( 43.733,   7.417, "fr", "Monaco"),
          'Monrovia': (  6.300, -10.800, "en", "Liberia"),
        'Montevideo': (-34.883, -56.183, "es", "Uruguay"),
            'Moscow': ( 55.750,  37.583, "ru", "Russia"),
            'Muscat': ( 23.617,  58.583, "ar", "Oman"),
           'Nairobi': ( -1.283,  36.817, "en", "Kenya"),
            'Nassau': ( 25.083, -77.350, "en", "Bahamas"),
         'New Delhi': ( 28.600,  77.200, "hi", "India"),
          'New York': ( 40.756, -73.987, "en", "United States"),
            'Niamey': ( 13.517,   2.117, "fr", "Niger"),
              'Oslo': ( 59.917,  10.750, "no", "Norway"),
            'Ottawa': ( 45.417, -75.700, "en", "Canada"),
       'Panama City': (  8.967, -79.533, "es", "Panama"),
             'Paris': ( 48.867,   2.333, "fr", "France"),
       'Philipsburg': ( 18.017, -63.033, "en", "Sint Maarten"),
        'Phnom Penh': ( 11.550, 104.917, "km", "Cambodia"),
        'Port Louis': (-20.150,  57.483, "en", "Mauritius"),
    'Port-au-Prince': ( 18.533, -72.333, "fr", "Haiti"),
        'Porto-Novo': (  6.483,   2.617, "fr", "Benin"),
            'Prague': ( 50.083,  14.467, "cs", "Czech Republic"),
          'Pretoria': (-25.700,  28.217, "xh", "South Africa"),
         'Pyongyang': ( 39.017, 125.750, "ko", "North Korea"),
             'Quito': ( -0.217, -78.500, "es", "Ecuador"),
             'Rabat': ( 34.017,  -6.817, "ar", "Morocco"),
           'Rangoon': ( 16.800,  96.150, "my", "Myanmar"),
         'Reykjavik': ( 64.150, -21.950, "is", "Iceland"),
              'Riga': ( 56.950,  24.100, "lv", "Latvia"),
            'Riyadh': ( 24.633,  46.717, "ar", "Saudi Arabia"),
              'Rome': ( 41.900,  12.483, "it", "Italy"),
            'Saipan': ( 15.200, 145.750, "en", "Saipan"),
          'San Jose': (  9.933, -84.083, "es", "Costa Rica"),
          'San Juan': ( 18.467, -66.117, "es", "Puerto Rico"),
        'San Marino': ( 43.933,  12.417, "it", "San Marino"),
      'San Salvador': ( 13.700, -89.200, "es", "El Salvador"),
             'Sanaa': ( 15.350,  44.200, "ar", "Yemen"),
          'Santiago': (-33.450, -70.667, "es", "Chile"),
     'Santo Domingo': ( 18.467, -69.900, "es", "Domenican Republic"),
          'Sarajevo': ( 43.867,  18.417, "bo", "Bosnia and Herzegovina"),
             'Seoul': ( 37.550, 126.983, "ko", "South Korea"),
         'Singapore': (  1.283, 103.850, "en", "Singapore"),
            'Skopje': ( 42.000,  21.433, "mk", "Macedonia"),
             'Sofia': ( 42.683,  23.317, "bg", "Bulgaria"),
         'Stockholm': ( 59.333,  18.050, "sv", "Sweden"),
            'Taipei': ( 25.050, 121.500, "zh", "China"),
           'Tallinn': ( 59.433,  24.717, "et", "Estonia"),
          'Tashkent': ( 41.333,  69.300, "uz", "Uzbekistan"),
       'Tegucigalpa': ( 14.100, -87.217, "es", "Honduras"),
            'Tehran': ( 35.667,  51.417, "fa", "Iran"),
            'Tirana': ( 41.317,  19.817, "sq", "Albania"),
             'Tokyo': ( 35.683, 139.750, "ja", "Japan"),
          'Torshavn': ( 62.017,  -6.767, "fo", "Faroe Islands"),
           'Tripoli': ( 32.883,  13.167, "ar", "Libya"),
             'Tunis': ( 36.800,  10.183, "ar", "Tunis"),
             'Vaduz': ( 47.133,   9.517, "de", "Liechtenstein"),
      'Vatican City': ( 41.900,  12.450, "it", "Vatican City"),
            'Vienna': ( 48.200,  16.367, "de", "Austria"),
         'Vientiane': ( 17.967, 102.600, "lo", "Laos"),
           'Vilnius': ( 54.683,  25.317, "lt", "Lithuania"),
            'Warsaw': ( 52.250,  21.000, "pl", "Poland"),
       'Washington.': ( 38.883, -77.033, "en", "United States"),
        'Wellington': (-41.467, 174.850, "en", "New Zealand"),
      'Yamoussoukro': (  6.817,  -5.283, "fr", "Côte d'Ivoire"),
           'Yaounde': (  3.867,  11.517, "en", "Cameroon"),
            'Zagreb': ( 45.800,  16.000, "hr", "Croatia")
}


</t>
<t tx="karstenw.20230303142657.1">#### PATTERN | WEB | OAUTH #########################################################################
# -*- coding: utf-8 -*-
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

####################################################################################################
# Naive OAuth implementation for pattern.web.Yahoo and Yahoo! BOSS v2.

@others
@language python
@tabwidth -4
</t>
<t tx="karstenw.20230303142704.1">from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

try:
    # Python 3
    from urllib.parse import urlencode
    from urllib.parse import quote as urlquote
except ImportError:
    from urllib import urlencode
    from urllib import quote as urlquote

import hmac
import time
import random
import base64

try:
    from hashlib import sha1
    from hashlib import md5
except:
    import sha as sha1
    import md5
    md5 = md5.new

_diacritics = {
    "a": ("á", "ä", "â", "à", "å"),
    "e": ("é", "ë", "ê", "è"),
    "i": ("í", "ï", "î", "ì"),
    "o": ("ó", "ö", "ô", "ò", "ō", "ø"),
    "u": ("ú", "ü", "û", "ù", "ů"),
    "y": ("ý", "ÿ", "ý"),
    "s": ("š",),
    "c": ("ç", "č"),
    "n": ("ñ",),
    "z": ("ž",)
}

####################################################################################################


</t>
<t tx="karstenw.20230303142704.2">def HMAC_SHA1(key, text):
    return hmac.new(key.encode("utf-8"), text.encode("utf-8"), sha1).digest()


</t>
<t tx="karstenw.20230303142704.3">def nonce():
    return md5(str(time.time()).encode("utf-8") + str(random.random()).encode("utf-8")).hexdigest()


</t>
<t tx="karstenw.20230303142704.4">def timestamp():
    return int(time.time())


</t>
<t tx="karstenw.20230303142704.5">def escape(string):
    return urlquote(string, safe=b"~")


</t>
<t tx="karstenw.20230303142704.6">def utf8(string):
    return isinstance(string, str) and string.encode("utf-8") or str(string)


</t>
<t tx="karstenw.20230303142704.7">def normalize(string):
    # Normalize accents (é =&gt; e) for services that have problems with utf-8
    # (used to be the case with Yahoo BOSS but this appears to be fixed now).
    string = utf8(string)
    for k, v in _diacritics.items():
        for v in v:
            string = string.replace(v, k)
    return string


</t>
<t tx="karstenw.20230303142704.8">def base(url, data={}, method="GET"):
    # Signature base string: http://tools.ietf.org/html/rfc5849#section-3.4.1
    base = escape(utf8(method.upper())) + "&amp;"
    base += escape(utf8(url.rstrip("?"))) + "&amp;"
    base += escape(utf8("&amp;".join(["%s=%s" % (
            escape(utf8(k)),
            escape(utf8(v))) for k, v in sorted(data.items())])))
    return base


</t>
<t tx="karstenw.20230303142704.9">def sign(url, data={}, method="GET", secret="", token="", hash=HMAC_SHA1):
    # HMAC-SHA1 signature algorithm: http://tools.ietf.org/html/rfc5849#section-3.4.2
    signature = escape(utf8(secret)) + "&amp;" + escape(utf8(token))
    signature = hash(signature, base(url, data, method))
    signature = base64.b64encode(signature)
    return signature.decode("utf-8")

#CONSUMER_KEY = ""
#CONSUMER_SECRET = ""
#
#q = "cats"
#url = "http://yboss.yahooapis.com/ysearch/web"
#data = {
#    "q": normalize(q),
#    "start": 0,
#    "count": 50,
#    "format": "json",
#    "oauth_version": "1.0",
#    "oauth_nonce": nonce(),
#    "oauth_timestamp": timestamp(),
#    "oauth_consumer_key": CONSUMER_KEY,
#    "oauth_signature_method": "HMAC-SHA1"
#}
#data["oauth_signature"] = sign(url, data, secret=CONSUMER_SECRET)
#data = dict((k, utf8(v)) for k, v in data.items())
#
#print(url + "?" + urlencode(data))
</t>
<t tx="karstenw.20230304130553.1">#### PARSER ########################################################################################
# Pattern's text parsers are based on Brill's algorithm, or optionally on a trained language model.
# Brill's algorithm automatically acquires a lexicon of known words (aka tag dictionary),
# and a set of rules for tagging unknown words from a training corpus.
# Morphological rules are used to tag unknown words based on word suffixes (e.g., -ly = adverb).
# Contextual rules are used to tag unknown words based on a word's role in the sentence.
# Named entity rules are used to annotate proper nouns (NNP's: Google = NNP-ORG).
# When available, the parser will use a faster and more accurate language model (SLP, SVM, NB, ...).

#--- LEXICON ---------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230304130628.1">#--- LANGUAGE MODEL --------------------------------------------------------------------------------
# A language model determines the statistically most probable tag for an unknown word.
# A pattern.vector Classifier such as SLP can be used to produce a language model,
# by generalizing patterns from a treebank (i.e., a corpus of hand-tagged texts).
# For example:
# "generalizing/VBG from/IN patterns/NNS" and
# "dancing/VBG with/IN squirrels/NNS"
# both have a pattern -ing/VBG + [?] + NNS =&gt; IN.
# Unknown words preceded by -ing and followed by a plural noun will be tagged IN (preposition),
# unless (put simply) a majority of other patterns learned by the classifier disagrees.


</t>
<t tx="karstenw.20230304130812.1">#--- PHRASE CHUNKER --------------------------------------------------------------------------------

SEPARATOR = "/"

NN = r"NN|NNS|NNP|NNPS|NNPS?\-[A-Z]{3,4}|PR|PRP|PRP\$"
VB = r"VB|VBD|VBG|VBN|VBP|VBZ"
JJ = r"JJ|JJR|JJS"
RB = r"(?&lt;!W)RB|RBR|RBS"
CC = r"CC|CJ"

# Chunking rules.
# CHUNKS[0] = Germanic: RB + JJ precedes NN ("the round table").
# CHUNKS[1] = Romance : RB + JJ precedes or follows NN ("la table ronde", "une jolie fille").
CHUNKS = [[
    # Germanic languages: da, de, en, is, nl, no, sv (also applies to cs, pl, ru, ...)
    (  "NP", r"((NN)/)* ((DT|CD|CC)/)* ((RB|JJ)/)* (((JJ)/(CC|,)/)*(JJ)/)* ((NN)/)+"),
    (  "VP", r"(((MD|TO|RB)/)* ((VB)/)+ ((RP)/)*)+"),
    (  "VP", r"((MD)/)"),
    (  "PP", r"((IN|PP)/)+"),
    ("ADJP", r"((RB|JJ)/)* ((JJ)/,/)* ((JJ)/(CC)/)* ((JJ)/)+"),
    ("ADVP", r"((RB)/)+"),
], [
    # Romance languages: ca, es, fr, it, pt, ro
    (  "NP", r"((NN)/)* ((DT|CD|CC)/)* ((RB|JJ|,)/)* (((JJ)/(CC|,)/)*(JJ)/)* ((NN)/)+ ((RB|JJ)/)*"),
    (  "VP", r"(((MD|TO|RB)/)* ((VB)/)+ ((RP)/)* ((RB)/)*)+"),
    (  "VP", r"((MD)/)"),
    (  "PP", r"((IN|PP)/)+"),
    ("ADJP", r"((RB|JJ)/)* ((JJ)/,/)* ((JJ)/(CC)/)* ((JJ)/)+"),
    ("ADVP", r"((RB)/)+"),
]]

for i in (0, 1):
    for j, (tag, s) in enumerate(CHUNKS[i]):
        s = s.replace("NN", NN)
        s = s.replace("VB", VB)
        s = s.replace("JJ", JJ)
        s = s.replace("RB", RB)
        s = s.replace(" ", "")
        s = re.compile(s)
        CHUNKS[i][j] = (tag, s)

# Handle ADJP before VP,
# so that RB prefers next ADJP over previous VP.
CHUNKS[0].insert(1, CHUNKS[0].pop(3))
CHUNKS[1].insert(1, CHUNKS[1].pop(3))


</t>
<t tx="karstenw.20230304130935.1">### SENTIMENT POLARITY LEXICON #####################################################################
# A sentiment lexicon can be used to discern objective facts from subjective opinions in text.
# Each word in the lexicon has scores for:
# 1)     polarity: negative vs. positive    (-1.0 =&gt; +1.0)
# 2) subjectivity: objective vs. subjective (+0.0 =&gt; +1.0)
# 3)    intensity: modifies next word?      (x0.5 =&gt; x2.0)

# For English, adverbs are used as modifiers (e.g., "very good").
# For Dutch, adverbial adjectives are used as modifiers
# ("hopeloos voorspelbaar", "ontzettend spannend", "verschrikkelijk goed").
# Negation words (e.g., "not") reverse the polarity of the following word.

# Sentiment()(txt) returns an averaged (polarity, subjectivity)-tuple.
# Sentiment().assessments(txt) returns a list of (chunk, polarity, subjectivity, label)-tuples.

# Semantic labels are useful for fine-grained analysis, e.g.,
# negative words + positive emoticons could indicate cynicism.

# Semantic labels:
MOOD = "mood"  # emoticons, emojis
IRONY = "irony" # sarcasm mark (!)

NOUN, VERB, ADJECTIVE, ADVERB = \
    "NN", "VB", "JJ", "RB"

RE_SYNSET = re.compile(r"^[acdnrv][-_][0-9]+$")


</t>
<t tx="karstenw.20230304131008.1">#### MULTILINGUAL ##################################################################################
# The default functions in each language submodule, with an optional language parameter:
# from pattern.text import parse
# print(parse("The cat sat on the mat.", language="en"))
# print(parse("De kat zat op de mat.", language="nl"))

LANGUAGES = ["en", "es", "de", "fr", "it", "nl"]

_modules = {}


</t>
<t tx="karstenw.20230310124020.1">parser = Parser(
     lexicon = os.path.join(MODULE, "en-lexicon.txt"),    # A dict of known words =&gt; most frequent tag.
   frequency = os.path.join(MODULE, "en-frequency.txt"),  # A dict of word frequency.
       model = os.path.join(MODULE, "en-model.slp"),      # A SLP classifier trained on WSJ (01-07).
  morphology = os.path.join(MODULE, "en-morphology.txt"), # A set of suffix rules (e.g., -ly = adverb).
     context = os.path.join(MODULE, "en-context.txt"),    # A set of contextual rules.
    entities = os.path.join(MODULE, "en-entities.txt"),   # A dict of named entities: John = NNP-PERS.
     default = ("NN", "NNP", "CD"),
    language = "en"
)

lexicon = parser.lexicon # Expose lexicon.

sentiment = Sentiment(
        path = os.path.join(MODULE, "en-sentiment.xml"),
      synset = "wordnet_id",
   negations = ("no", "not", "n't", "never"),
   modifiers = ("RB",),
   modifier = lambda w: w.endswith("ly"),
   tokenizer = parser.find_tokens,
    language = "en"
)

spelling = Spelling(
        path=os.path.join(MODULE, "en-spelling.txt")
)


</t>
<t tx="karstenw.20230310152332.1">### SYNSET #########################################################################################

NOUNS = lambda: wn.all_lemma_names(wn.NOUN)
VERBS = lambda: wn.all_lemma_names(wn.VERB)
ADJECTIVES = lambda: wn.all_lemma_names(wn.ADJ)
ADVERBS = lambda: wn.all_lemma_names(wn.ADV)

NOUN, VERB, ADJECTIVE, ADVERB = \
    NN, VB, JJ, RB = \
        "NN", "VB", "JJ", "RB"

_pattern2wordnet = {NN : wn.NOUN, VB : wn.VERB, JJ : wn.ADJ, RB: wn.ADV}
_wordnet2pattern = {v : k for k, v in _pattern2wordnet.items()}
_wordnet2pattern[wn.ADJ_SAT] = JJ


</t>
<t tx="karstenw.20230310152428.1">### INFORMATION CONTENT ############################################################################
# Information Content (IC) is used to calculate semantic similarity in Synset.similarity().
# Information Content values for each synset are derived from word frequency in a given corpus.
# The idea is that less frequent words convey more information.
# Semantic similarity depends on the amount of information two concepts (synsets) have in common,
# given by the Most Speciﬁc Common Abstraction (MSCA), i.e. the shared ancestor in the taxonomy.
# http://www.d.umn.edu/~tpederse/Pubs/AAAI04PedersenT.pdf
# http://afflatus.ucd.ie/papers/ecai2004b.pdf

#IC = {} # Switch data file according to WordNet version:
#IC_CORPUS = os.path.join(MODULE, "resnik-ic" + VERSION[0] + ".txt")
#IC_MAX = 0

# def information_content(synset):
#     """ Returns the IC value for the given Synset (trained on the Brown corpus).
#     """
#     global IC_MAX
#     if not IC:
#         IC[NOUN] = {}
#         IC[VERB] = {}
#         for s in open(IC_CORPUS).readlines()[1:]: # Skip the header.
#             s = s.split()
#             id, w, pos = (
#                 int(s[0][:-1]),
#                 float(s[1]),
#                 s[0][-1] == "n" and NOUN or VERB)
#             if len(s) == 3 and s[2] == "ROOT":
#                 IC[pos][0] = IC[pos].get(0,0) + w
#             if w != 0:
#                 IC[pos][id] = w
#             if w &gt; IC_MAX:
#                 IC_MAX = w
#     return IC.get(synset.pos, {}).get(synset.id, 0.0) / IC_MAX

</t>
<t tx="karstenw.20230310152508.1">### WORDNET3 TO WORDNET2 ###########################################################################
# Map WordNet3 synset id's to WordNet2 synset id's.

_map32_pos1 = {NN: "n", VB: "v", JJ: "a", RB: "r"}
_map32_pos2 = {"n": NN, "v": VB, "a": JJ, "s" : JJ, "r": RB}
_map32_cache = None


</t>
<t tx="karstenw.20230310152553.1">#### SENTIWORDNET ##################################################################################
# http://nmis.isti.cnr.it/sebastiani/Publications/LREC06.pdf
# http://nmis.isti.cnr.it/sebastiani/Publications/LREC10.pdf

sys.path.insert(0, os.path.join(MODULE, "..", ".."))

try:
    from pattern.text import Sentiment
except:
    class Sentiment(object):
        PLACEHOLDER = True

sys.path.pop(0)


</t>
<t tx="karstenw.20230324114612.1">#--- DRAWING FUNCTIONS -----------------------------------------------------------------------------
# This module is standalone (i.e., it is not a graph rendering package).
# If you want to call Graph.draw() then line(), ellipse() and Text.draw() must be implemented.


</t>
<t tx="karstenw.20230324114641.1">#--- DEEPCOPY --------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230324114654.1">#### NODE ##########################################################################################

#--- NODE ------------------------------------------------------------------------------------------


</t>
<t tx="karstenw.20230324114735.1"></t>
<t tx="karstenw.20230324114833.1"></t>
<t tx="karstenw.20230324114855.1"></t>
<t tx="karstenw.20230324114943.1"></t>
<t tx="karstenw.20230324115018.1"></t>
<t tx="karstenw.20230324115334.1">#### COMMONSENSE SEMANTIC NETWORK ##################################################################

</t>
<t tx="karstenw.20230324115357.1">#### COMMONSENSE DATA ##############################################################################

#--- NODEBOX.NET/PERCEPTION ------------------------------------------------------------------------


</t>
<t tx="karstenw.20230324122147.1">#--- HEURISTICS ------------------------------------------------------------------------------------
# Similarity between concepts is measured using a featural approach:
# a comparison of the features/properties that are salient in each concept's halo.
# Commonsense.similarity() takes an optional "heuristic" parameter to tweak this behavior.
# It is a tuple of two functions:
# 1) function(concept) returns a list of salient properties (or other),
# 2) function(concept1, concept2) returns the cost to traverse this edge (0.0-1.0).

COMMONALITY = (
    # Similarity heuristic that only traverses relations between properties.
    lambda concept: concept.properties,
    lambda edge: 1 - int(edge.context == "properties" and \
                         edge.type != "is-opposite-of"))

</t>
<t tx="karstenw.20230508125637.1"></t>
<t tx="karstenw.20230508125640.1">@language python
@tabwidth -4
@others
</t>
<t tx="karstenw.20230508125701.1">#### PATTERN | CACHE ###############################################################################
# Copyright (c) 2010 University of Antwerp, Belgium
# Author: Tom De Smedt &lt;tom@organisms.be&gt;
# License: BSD (see LICENSE.txt for details).
# http://www.clips.ua.ac.be/pages/pattern

from __future__ import unicode_literals
from __future__ import division

from builtins import str, bytes, dict, int
from builtins import map, zip, filter
from builtins import object, range

from io import open

try:
    import hashlib
    md5 = hashlib.md5
except:
    import md5
    md5 = md5.new

from pattern.helpers import encode_string, decode_string

decode_utf8 = decode_string
encode_utf8 = encode_string

#### CACHE #########################################################################################
# Caching is implemented in URL.download(), which is used by all other downloaders.

import os
import glob
import tempfile
import datetime

from io import open

from codecs import BOM_UTF8
BOM_UTF8 = BOM_UTF8.decode('utf-8')

try:
    MODULE = os.path.dirname(os.path.realpath(__file__))
except:
    MODULE = ""

TMP = os.path.join(tempfile.gettempdir(), "pattern_web")


</t>
<t tx="karstenw.20230508125701.10">def __contains__(self, k):
    return os.path.exists(self._hash(k))

</t>
<t tx="karstenw.20230508125701.11">def __getitem__(self, k):
    return self.get(k)

</t>
<t tx="karstenw.20230508125701.12">def __setitem__(self, k, v):
    f = open(self._hash(k), "w", encoding = "utf-8")
    f.write(BOM_UTF8)
    v = decode_utf8(v)
    f.write(v)
    f.close()

</t>
<t tx="karstenw.20230508125701.13">def __delitem__(self, k):
    try:
        os.unlink(self._hash(k))
    except OSError:
        pass

</t>
<t tx="karstenw.20230508125701.14">def get(self, k, unicode=True):
    """ Returns the data stored with the given id.
        With unicode=True, returns a Unicode string.
    """
    if k in self:
        f = open(self._hash(k), "rb")
        v = f.read().lstrip(BOM_UTF8.encode("utf-8"))
        f.close()
        if unicode is True:
            return decode_utf8(v)
        else:
            return v
    raise KeyError(k)

</t>
<t tx="karstenw.20230508125701.15">def age(self, k):
    """ Returns the age of the cached item, in days.
    """
    p = self._hash(k)
    return os.path.exists(p) and (date_now() - date_modified(p)).days or 0

</t>
<t tx="karstenw.20230508125701.16">def clear(self, age=None):
    """ Clears all items from the cache (whose age is the given amount of days or older).
    """
    n = date_now()
    for p in glob.glob(os.path.join(self.path, "*")):
        if age is None or (n - date_modified(p)).days &gt;= age:
            os.unlink(p)

</t>
<t tx="karstenw.20230508125701.2">def date_now():
    return datetime.datetime.today()


</t>
<t tx="karstenw.20230508125701.3">def date_modified(path):
    return datetime.datetime.fromtimestamp(os.stat(path)[8])


</t>
<t tx="karstenw.20230508125701.4">class Cache(object):
    # cachefolder = os.path.join(MODULE,  "tmp")
    cachefolder = os.path.join(MODULE,  '..', '..', '..', '..',
                        "linguistics-data", "pattern-data", "webcache-tmp")
    cachefolder = os.path.abspath( cachefolder )
    @others
cache = Cache()
</t>
<t tx="karstenw.20230508125701.5">def __init__(self, path=cachefolder):
    """ Cache with data stored as files with hashed filenames.
        Content retrieved from URLs and search engines are stored in cache for performance.
        The path where the cache is stored can be given. This way you can manage persistent
        sets of downloaded data. If path=TMP, cached items are stored in a temporary folder.
    """
    self.path = path
    # print("pattern.web.cache.Cache.path=", path)

</t>
<t tx="karstenw.20230508125701.6">def _get_path(self):
    return self._path

</t>
<t tx="karstenw.20230508125701.7">def _set_path(self, path):
    if not os.path.isdir(path):
        os.makedirs(path)
    self._path = path
path = property(_get_path, _set_path)

</t>
<t tx="karstenw.20230508125701.8">def _hash(self, k):
    k = encode_utf8(k) # MD5 works on Python byte strings.
    return os.path.join(self.path, md5(k).hexdigest())

</t>
<t tx="karstenw.20230508125701.9">def __len__(self):
    return len(glob.glob(os.path.join(self.path, "*")))

</t>
</tnodes>
</leo_file>
